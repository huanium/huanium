\documentclass{book}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{xcolor}
%\definecolor{colby}{rgb}{0.0, 0.0, 0.5}
\definecolor{MIT}{RGB}{163, 31, 52}
\usepackage[pdftex]{hyperref}
%\hypersetup{colorlinks,urlcolor=colby}
\hypersetup{colorlinks,linkcolor={MIT},citecolor={MIT},urlcolor={MIT}}  



\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\ham}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}

\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}






\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}

%\hypersetup{colorlinks,urlcolor=colby}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
\begin{titlepage}\centering
 \clearpage
 \title{{\textsc{\textbf{EXPERIMENTAL ATOMIC PHYSICS}}}\\ \smallskip - A Quick Guide - \\}
 \author{\bigskip Huan Q. Bui}
  \affil{B.A., COLBY COLLEGE (2021) $\,$\\  
  	MASSACHUSETTS INSTITUTE OF TECHNOLOGY}
 \date{June 15, 2021 - \today}
 \maketitle
 \thispagestyle{empty}
\end{titlepage}

\noindent \textbf{Preface}


$\,$\\


\noindent Greetings, \\


While I have spent most of my undergraduate years in Professor Charles Conover's lab at Colby College working on cold atom experiments, I never had \textit{formal} training in atomic, molecular, and optical physics. The closest to formal training I have for AMO physics is a standard quantum mechanics course I took in the fall of my junior year. Most of the intuition I have for atomic physics, I learned from my discussions with Professor Conover or read from books and articles here and there. This article is my attempt at \textit{formally} teaching myself this subject. It will mostly contain basic/essential concepts in AMO physics. I'll also sprinkle in a few ideas or definitions that are relevant to what I do in Prof. Zwierlein's group. \\

This article is my version of an ``atomic physics dictionary,'' which should keep growing as I go along in my education and research at MIT. As a result, there is no good way for me to organize the topics here but by alphabetical order (hence ``dictionary''). I don't know how well I'll be able to curate this article, but we'll see. As the title suggests, this article is about \textit{experimental} atomic physics. So I will try to include not just atomic physics theory but also experimental techniques/designs and some control and electronics theory. \\


In any case, enjoy and please feel free to report any errors!


\newpage




\chapter*{A}



\section*{Atomic coherence}

In physics, two systems are said to be coherent if their frequency and waveform are identical. Two waves are said to be coherent if they have a constant relative phase. Similarly, \textbf{atomic coherence} is the induced coherence between levels of a multi-level atomic system sometimes observed when it interacts with a coherent electromagnetic field.\\


Atomic coherence is often discussed in the context of Rabi flopping. If a coherent, narrow bandwidth laser is applied to a two-level system, Rabi floppings between the ground and excited state populations occur. Now consider a system comprised of a large collection of identical two-level systems. All such systems will begin Rabi oscillation at the same time under the application of the mentioned laser source, and all of them will be in phase with each other. The system is coherent. However, after some finite time $t$, different atoms return to their ground states (via spontaneous decay) at different times, and Rabi oscillations begin to go out of phase. As time passes by, fewer atoms will be in phase. This is decoherence. \\

The coherence time is the time over which the system maybe considered coherent. 

\section*{Avoided crossing} 


\begin{figure}[!htb]
	\centering
	\includegraphics*[width=0.75\textwidth]{images/avoided_crossing}
	\caption{From Wikipedia}
\end{figure}


This concept is actually mathematically very easy to describe. Basically, avoided crossing is a phenomenon where two eigenvalues of a Hermitian matrix (say, of a Hamiltonian) which depend on some set of $N$ continuous real parameters cannot become equal (or ``cross'') in value except on a manifold of $N-2$ dimensions. For a mathematically rigorous definition, the reader may refer to the \textcolor{blue}{von Neumann-Wigner theorem}. However, since $N=1$ most of the time, we don't even worry about the second part (``except...'') of the definition.\\


An easy example is two-state systems. Consider a Hamiltonian $\widehat{H} = \text{diag}(E_1, E_2)$ with $E_1$ with off-diagonal perturbation $P$:
\begin{equation*}
\widehat{H}' = \widehat{H} + \widehat{P} = \begin{bmatrix}
E_1 & W \\ W^* & E_2
\end{bmatrix}.
\end{equation*}
Diagonalizing this new Hamiltonian gives the following eigenvalues:
\begin{equation*}
E_{\pm} = \f{1}{2}(E_1 + E_2) \pm \f{1}{2}\sqrt{(E_1 - E_2)^2 + 4\abs{W}^2}
\end{equation*}
Plotting $E_+$ and $E_-$ against $\Delta E = E_1  - E_2$, we get two branches of a parabola. When $W \neq 0$, we see that $E_+ \neq E_-$ when $\Delta  E = 0$. But when $W = 0$, $E_+ = E_-$ at $\Delta E = 0$. We say that the level crossings are avoided due to the perturbation. 


\chapter*{B}


\section*{Bloch's Theorem and Bloch States}


Consider a periodic potential $V(\mathbf{r})$ associated with a lattice whose \textcolor{blue}{primitive lattice translation vectors} are given by 
\begin{equation*}
\mathbf{T} = n_1 \mathbf{a}_1 +  n_2 \mathbf{a}_2  +  n_3 \mathbf{a}_3,
\end{equation*}
where $n_i$ are integers and $\mathbf{a_i}$ are the three noncoplanar vectors ($\mathbf{T}$ is basically vectors which translates from one vertex in the lattice to another arbitrary one). Since $V$ is periodic, we have
\begin{equation*}
V(\mathbf{T} + \mathbf{r}) = V(\mathbf{r}). 
\end{equation*}
In Fourier components, 
\begin{equation*}
V(\mathbf{r}) = \sum_\mathbf{G} V_\mathbf{G} e^{i\mathbf{G}\cdot \mathbf{r}}
\end{equation*}
where $\mathbf{G}$ are a set of vectors and $V_\mathbf{G}$ are Fourier coefficients. By the periodicity of $V$, we have
\begin{equation*}
e^{i\mathbf{G}\cdot \mathbf{T}} = 1 \implies \mathbf{G}\cdot \mathbf{T} = 2\rho \pi, \quad \rho \in \mathbb{Z}.
\end{equation*}
The only way to define $\mathbf{G}$ such that the above equation makes sense is:
\begin{equation*}
\mathbf{G} = m_1 \mathbf{A}_1 + m_2 \mathbf{A}_2 + m_3 \mathbf{A}_3
\end{equation*}
where $m_j$ are integers and $\mathbf{A}_j$ are three noncoplanar vectors defined by 
\begin{equation*}
\mathbf{a}_j \cdot \mathbf{A}_l = 2\pi \delta_{jl}
\end{equation*}
This shows the existence of an $r$-lattice implies that of a $k$-lattice, and we call $\mathbf{G}$ the \textcolor{blue}{reciprocal lattice}. \\


What set of functions describes the motion of electrons in such a potential? Since we want to reflect the translation symmetry of the lattice, we may impose the \textit{Born-von Karman periodic boundary condition} on the plane wave 
\begin{equation*}
\phi(\mathbf{r}) = e^{i(\mathbf{k}\cdot \mathbf{r} - \omega t)}
\end{equation*}
to get
\begin{equation*}
\phi(\mathbf{r} + N_j \mathbf{a_j}) = \phi(\mathbf{r})
\end{equation*}
where $j=1,2,3$ and $N = N_1N_2N_3$ is the number of primitive unit cells in the crystal; $N_j$ is the number of unit cells in the $j$th direction. From here, we have that 
\begin{equation*}
e^{iN_j \mathbf{k}\cdot \mathbf{a}_j} = 1.
\end{equation*}
Following a similar argument as before, we find that the only allowed $\mathbf{k}$ vectors are of the form
\begin{equation*}
\mathbf{k} = \sum^3_{j=1}\f{m_j}{N_j}\mathbf{A}_j
\end{equation*}








Now, consider a Schr\"{o}dinger equation with potential $V(\mathbf{r})$:
\begin{equation*}
\widehat{H}\psi = \lb -\f{\hbar^2 \nabla^2}{2m} + V(\mathbf{r}) \rb \psi = E\psi.
\end{equation*}
In Fourier components, we again have
\begin{equation*}
V(\mathbf{r}) = \sum_\mathbf{G} V_\mathbf{G} e^{i\mathbf{G}\cdot \mathbf{r}}.
\end{equation*}
Let us set the background potential to zero, i.e., $V_0 \equiv 0$. Next, let us write the solution $\phi(\mathbf{r})$ as a combination of plane waves obeying the Born-von Karman PBC:
\begin{equation*}
\psi(\mathbf{r}) = \sum_\mathbf{k}C_\mathbf{k} e^{i\mathbf{k}\cdot \mathbf{r}},
\end{equation*}
so that $\phi(\mathbf{r})$ also satisfies the Born-von Karman PBC. Plugging this into the SE, we find 
\begin{equation*}
\sum_\mathbf{k} \f{\hbar^2k^2}{2m} C_\mathbf{k} e^{i\mathbf{k}\cdot \mathbf{r}} + \underbrace{\lb \sum_\mathbf{G} V_\mathbf{G} e^{i\mathbf{G}\cdot \mathbf{r}} \rb \lb \sum_\mathbf{k} C_\mathbf{k} e^{i\mathbf{k}\cdot \mathbf{r}} \rb}_{V(\mathbf{r}\psi)} = E\sum_\mathbf{k} C_\mathbf{k} e^{i\mathbf{k}\cdot \mathbf{r}}
\end{equation*}
where we can re-write:
\begin{equation*}
V(\mathbf{r})\psi = \sum_{\mathbf{G},\mathbf{k}} V_\mathbf{G}C_\mathbf{k} e^{i(\mathbf{G}+ \mathbf{k})\cdot \mathbf{r}}  = \sum_{\mathbf{G},\mathbf{k}} V_\mathbf{G}C_{\mathbf{k}-\mathbf{G}} e^{i\mathbf{k}\cdot \mathbf{r}}.
\end{equation*}
With this, we can factor out $e^{i\mathbf{k}\cdot \mathbf{r}}$ in each term of the SE and use the fact that the plane waves form an orthogonal basis, we find 
\begin{equation*}
\lp \f{\hbar^2k^2}{2m} - E \rp C_\mathbf{k} + \sum_\mathbf{G} V_\mathbf{G} C_{\mathbf{k}-\mathbf{G}} = 0.
\end{equation*}

Let us write $\mathbf{k} = \mathbf{q} - \mathbf{G}'$ and let $\mathbf{G}'' = \mathbf{G}' + \mathbf{G}$, where $\mathbf{q}$ lies in the \textcolor{blue}{first Brillouin zone}. With this change of variables, we have the result
\begin{equation*}
\lp \f{\hbar^2 (\mathbf{q} - \mathbf{G}')^2}{2m} - E \rp C_{\mathbf{q} - \mathbf{G}'} + \sum_{\mathbf{G}''}V_{\mathbf{G}'' - \mathbf{G}'}C_{\mathbf{q} - \mathbf{G}''} = 0.
\end{equation*}


Now, we're ready for the statement of the \textbf{Bloch's Theorem}. The result above involves coefficients $C_{\mathbf{k}}$ in which $\mathbf{k} = \mathbf{q} - \mathbf{G}$, where $\mathbf{G}$ are general reciprocal lattice vectors. This means that if we fix $\mathbf{q}$, then the only $C_\mathbf{k}$ that feature are of the form $C_{\mathbf{q} - \mathbf{G}}$. In other words, for each $\mathbf{q}$, there is a wavefunction $\psi_\mathbf{q}(r)$ that takes the form
\begin{equation*}
\psi_\mathbf{q}(\mathbf{r}) = \sum_\mathbf{G} C_{\mathbf{q} - \mathbf{G}} e^{i(\mathbf{q} - \mathbf{G})\cdot \mathbf{r}},
\end{equation*}
where we have substituted $\mathbf{k} = \mathbf{q} - \mathbf{G}$. Factoring out $e^{i\mathbf{q}\cdot \mathbf{r}}$, we find 
\begin{equation*}
\boxed{\psi_\mathbf{q}(\mathbf{r})  = e^{i\mathbf{q}\cdot \mathbf{r}} \sum_\mathbf{G} C_{\mathbf{q} - \mathbf{G}} e^{-i \mathbf{G}\cdot \mathbf{r}} \equiv e^{i\mathbf{q}\cdot\mathbf{r}}u_{j,\mathbf{q}}}
\end{equation*}
So, the solution is a plane wave with wave vector within the first Brillouin zone TIMES a function with the periodicity of the lattice. Functions of this form are known as \textbf{Bloch functions} or \textbf{Bloch states}. They serve as a suitable basis for the wave functions or states of electrons in crystalline solids. \\



\textbf{Bloch's Theorem} is as follows: \textit{The eigenstates $\psi$ of a one-electron Hamiltonian defined above for all Bravais lattice translation vectors $\mathbf{T}$ can be chosen to be a plane wave times a function with the periodicity of the Bravais lattice.} We note two things:
\begin{itemize}
	\item This is true for any particle propagating in a lattice
	\item The theorem makes no assumption about the \textit{strength/depth} of the potential. 
\end{itemize}








\noindent {Note:} The terminologies in \textcolor{blue}{blue} can be found in \cite{kittel1996introduction} or Wikipedia. The concepts are simple enough, so I won't include their definitions here. 














\section*{Bose-Einstein Condensate (BEC)}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{C}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{D}



\section*{Dark SPOT MOT}


%\section*{Dicke regime}



\section*{Doppler temperature}


Doppler temperature is the minimum temperature achievable with Doppler cooling. When an atom absorbs a counter-propagating photon, its velocity decreases due to momentum conservation. The atom then re-emits the photon (via spontaneous emission) in a random direction, giving the atom a momentum kick in a random direction. The random kicks average to zero, so the atom averages a zero mean velocity, i.e., $< v > = 0$. However, $<v^2> \neq 0$, so there is heat supplies to the atom. At equilibrium, the heating and cooling rates are equal; this is the limit on the amount by which the atom can be cooled using this technique. \\

If the transitions used for Doppler cooling have natural linewidths $\gamma$, then the Doppler temperature is given by 
\begin{equation*}
T_\text{Doppler} = \f{\hbar \gamma}{2k_B}
\end{equation*} 
where $k_B$ is the Boltzmann constant. This is usually much higher than the \textcolor{blue}{recoil temperature}, which is the temperature associated with the momentum gained from spontaneous emission. 






\section*{Dynamical System \& Feedback}



This section is mostly based on Sections II and III of \cite{bechhoefer2005feedback}. \\


Dynamical systems are often represented by some differential equation. A simple example is a \textbf{low-pass filter}, to which many sensors are equivalent. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{images/low-pass-filter}
	\caption{From \cite{bechhoefer2005feedback}}
\end{figure}

Let the potentials be with respect to ground. By Kirchoff's law we have
\begin{equation*}
V_\text{out}(t) = \f{Q(t)}{C} = V_\text{in} - Ri(t) = V_\text{in} - R\dot{Q}(t) = V_\text{in} - RC\dot{V}_\text{out}.
\end{equation*}
So we have the following differential equation:
\begin{equation*}
\dot{V}_\text{out}(t) = -\f{1}{RC} V_\text{out}(t) + \f{1}{RC}V_\text{in}(t).
\end{equation*}


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{images/DDHO}
	\caption{From \cite{bechhoefer2005feedback}}
\end{figure}



Another simple (and canonical) system is the damped driven harmonic oscillator:
\begin{equation*}
m\ddot{q} + 2\gamma \dot{q} + kq = k q_0(t).
\end{equation*}
Here, $\omega_0^2 = k/m$ is the natural resonance frequency, and we define $\zeta = \gamma/ \sqrt{mk}$ to be the damping parameter, with $0 <\zeta <1 $ for underdamping and $\zeta > 1$ for overdamping. With these, we have
\begin{equation*}
\ddot{q} + 2\zeta \dot{q} + q = q_0(t). 
\end{equation*}
From standard ODE theory, this second-order equation can be written as a system of two coupled first order ODEs. 


\subsection*{Laplace transform}


While thinking in time domain can be most natural, sometimes it is much more useful and intuitive to solve problems in control theory/dynamics systems in frequency domain. There two main tools for this purpose: Fourier transform and Laplace transform. In control theory, the Laplace transform has certain advantages over the Fourier transform since physical problems often have initial values that are more compatible with Laplace-transform methods. While the inverse Laplace transform is not as symmetric and ``nice'' as the Fourier transform, it is capable of transforming functions which do not decay to zero at infinity such as the Heaviside step function. Similarly, the Laplace transform can also handle unstable systems. The Laplace transform $y(s)$ of some signal $y(t)$ is given by 
\begin{equation*}
y(s) \equiv \lag[y(t)] = \int^\infty_0 y(t)e^{-ts}\,dt.
\end{equation*}
Two important identities of the Laplace transform are:
\begin{equation*}
\lag[d^n y(t)/dt^n] = s^n y(s) \quad \text{and} \quad \lag\lb \int y(t)\,dt \rb = \f{1}{s}y(s).
\end{equation*}
These identities are what make the Laplace transform useful: it transforms a differential equation into an algebraic equation. For example, Laplace-transforming the first-order equation
\begin{equation*}
\dot{y}(t) = -\omega_0 y(t) + \omega_0 u(t)
\end{equation*}
gives
\begin{equation*}
sy(s) = \omega_0 [ u(s) - y(s)],
\end{equation*}
which gives
\begin{equation*}
G(s) \equiv \f{y(s)}{u(s)} = \f{1}{1+ s/\omega_0}.
\end{equation*}

Here, $G(s)$ is called the \textbf{transfer function}, $u(s)$ is the driving function, i.e., the \textbf{input}, and $y(s)$ is the response, i.e., the \textbf{output}. The transfer function is the ratio of output to input (in frequency domain). In this particular example, $\omega_0 = 1/RC = 2\pi f_0$ is the characteristic angular frequency of the low-pass filter. \\

The \textbf{frequency response} of the system is a measure of the magnitude and phase of the output as a function of frequency, in comparison to the input. For example, if a sine wave is injected into a system at a certain frequency, a linear system will respond at that same frequency with a certain amplitude and phase relative to the input. Also, doubling the input amplitude also doubles the output amplitude. Thus, for linear (and time-invariant) systems, the frequency response can be seen as applying the system's transfer function to a purely imaginary number argument representing the frequency of the sinusoidal excitation. \\

In even simpler terms, by evaluating $G(s)$ at $s = i\omega$,  we have
\begin{equation*}
G(i\omega) = \f{y(i\omega)}{u(i\omega)}.
\end{equation*}
Now note that $y(i\omega)$ and $u(i\omega)$ are the Fourier transforms, i.e,. the \textbf{spectra}, of the output and input, respectively. This is what we mean by $G(i\omega)$ being the system's frequency response to the (spectrum) of the input. \\


In any case, back to our example, we have
\begin{equation*}
G(i\omega) = \f{1}{1 + i\omega/\omega_0}. 
\end{equation*}
From here, we can find the \textbf{amplitude} and \textbf{phase} of the frequency response $G(i\omega)$:
\begin{equation*}
\abs{G(i\omega)} = \f{1}{\sqrt{1+ \omega_0^2/\omega^2}} \quad \text{and} \quad \arg(G(i\omega)) = -\tan^{-1} \f{\omega}{\omega_0}.
\end{equation*}
So, the frequency response of a system is simply the transfer function obtained by the Fourier transform. 



\subsection*{Bode magnitude and phase plots}

The \textbf{Bode plot} for a linear, time-invariant system (which is what we're interested in most of the time -- so I will drop this condition from now on) with transfer function $G(s)$ consists of a magnitude and a phase plot.\\


The \textbf{Bode magnitude plot} is the log-log plot of $\abs{G(i\omega)}$ against $\omega$ (or $\omega_0$). Often, the magnitude is given in \textit{decibels}, i.e., the value being plotted is given by $20 \log_{10} \abs{G(i\omega)}$.  



\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/bode-1st-order}
	\caption{From \cite{bechhoefer2005feedback}. Transfer function for a first-order, low pass filter. (a) Bode magnitude plot. The asymptotes intersect at the cutoff frequency $\omega_0$.  (b) Bode phase plot. The phase lag is $-\pi/2$ asymptotically, crossing $-\pi/4$ at $\omega_0$. }
\end{figure}




Similarly, the Laplace/Fourier transfer functions for the damped driven harmonic oscillator are 
\begin{equation*}
G(s) = \f{1}{1 + 2\zeta s + s^2} \quad \text{and}\quad G(i\omega) = \f{1}{1+ 2i\zeta\omega - \omega^2}.
\end{equation*}
With corresponding Bode plots:

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/bode-2nd-order}
	\caption{From \cite{bechhoefer2005feedback}. Transfer function for the damped driven harmonic oscillator. $\zeta = 1$ shows critically damped dynamics, while $\zeta = 0.1$ shows underdamped dynamics. (a) Bode magnitude plot.  (b) Bode phase plot. }
\end{figure}



\subsection*{Convolution theorem and transfer functions}


The convolution theorem says that
\begin{equation*}
\lag[G\ast H] = G(s)H(s),
\end{equation*} 
where $\ast$ denotes the convolution. This is not hard to prove and is left to the reader. \\


The convolution is needed when one wants to describe compound systems where the output of one element is fed into the input of the next element. Consider the following example. Suppose we have a damped driven harmonic oscillator as before: 
\begin{equation*}
\ddot{y} + 2\zeta \dot{y} + y = u(t)
\end{equation*}
with a first-order sensor which reports the position of the oscillator:
\begin{equation*}
\dot{v} + v = y(t). 
\end{equation*}
Here, we see that $u(t)$ drives the oscillator $y(t)$, and $y(t)$ drives the sensor output $v(t)$. Laplace transforming both equations gives
\begin{equation*}
y(s) = G(s)u(s) = \f{1}{1 + 2\zeta s + s^2}u(s) \quad \text{and} \quad v(s) = H(s)y(s) = \f{1}{1+s} y(s),
\end{equation*}
and so $v(s) = H(s) G(s) u(s) \equiv F(s) u(s)$. We see that having two elements in series leads to a transfer function that is the product
of the transfer functions of the individual elements. This motivates the block diagram below. We note that in time domain, the products become convolution products. 


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/block-1}
	\caption{From \cite{bechhoefer2005feedback}}
\end{figure}


\subsection*{Feedback}


Let some system be given whose dynamics is given by $G(s)$. Let the output be $y(t)$. Our goal is to make $y(t)$ follow some \textbf{control signal} $r(t)$ as faithfully as possible. To do this, we measure the \textbf{error signal} $e(t) = r(t) - y(t)$. We now apply some control law $K$ such that $\abs{e(t)}$ or $a\abs{e(t)}^2$ is minimized. The entire feedback system can be represented by the following block diagram:

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/block-2}
	\caption{From \cite{bechhoefer2005feedback}.}
\end{figure}

We ask what the relationship between the control signal and output signal is. To work this out, we first look at the top line. The error signal is used to generate the output signal. This relationship is 
\begin{equation*}
y(s) = G(s)K(s) e(s). 
\end{equation*}
Now, to the bottom line. The output is fed back into the system to generate a new error signal. We have $e(s) = r(s) - y(s)$, so 
\begin{align*}
y(s) = G(s)K(s) e(s) = G(s)K(s) [r(s) - y(s)] \\\implies y(s) = \f{K(s)G(s)}{1 + K(s)G(s)}r(s) \equiv \f{L(s)}{1 + L(s)}r(s) \equiv T(s) r(s). 
\end{align*}

Consider the low pass filter again, but this time with a DC gain $G_0$
\begin{equation*}
G(s) = \f{G_0}{1+ s/\omega_0},
\end{equation*}
and proportional feedback control law $K(s) = K_p$, a constant. Then, we have
\begin{equation*}
T(s) = \f{K_p G_0}{K_p G_0 + 1 + s/\omega_0} = \f{K_p G_0}{K_p G_0 +1} \f{1}{1 + s/[\omega_0(1 + K_p G_0)]}
\end{equation*}
This is nothing but a low pass filter once again, with modified gain and cutoff frequency. The new DC gain is of course $K_p G_0 / (1 + K_p G_0)$, and the new cutoff frequency is $\omega_0(1 + K_p G_0)$. \\


Next, we consider the effects of an output disturbance $d(t)$ and sensor noise $\xi(t)$. The corresponding block diagram is now
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/block-3}
	\caption{From \cite{bechhoefer2005feedback}.}
\end{figure}
Here, we have
\begin{equation*}
e(s) = r(s) - [y(s) + \xi(s)] = r(s) - y(s) - \xi(s) 
\end{equation*}
and
\begin{equation*}
y(s) = G(s) K(s) e(s) + d(s).
\end{equation*}
From here, we find 
\begin{equation*}
y(s) = \f{KG}{1+KG}[r(s) - \xi(s)] + \f{1}{1+ KG}d(s). 
\end{equation*}

Assuming that we're still working with a low pass filter, we make a few remarks:
\begin{itemize}
	\item The disturbances $d(t)$ would have been rejected up to cutoff frequency $\omega' = \omega_0(1+ K_p G_0)$. This is a good thing. 
	
	\item The control signal effectively becomes $r-\xi$, and the system has no way to distinguish the control signal from the measurement noise $\xi(t)$. This means that the higher the gain is (recall that the gain is $KG/(1+KG)$), the noisier the output. 
\end{itemize}


The frequency $\omega'$ is called the \textbf{feedback bandwidth}. From our analysis we see that high feedback bandwidth is \textbf{good} because it allows the system to track rapidly varying control, but also \textbf{bad} because it also noise through which contaminates the output. This tradeoff can be express by rewriting the equation above as 
\begin{align*}
e_0(s) \equiv r(s) - y(s) &= r(s) - \lb \f{KG}{1+KG}[r(s) - \xi(s)] + \f{1}{1+ KG}d(s)  \rb \\
&= \f{1}{1+KG}[r(s) - d(s)] + \f{KG}{1+KG}\xi(s) \\
&\equiv S(s)[r(s) - d(s)] + T(s) \xi(s). 
\end{align*}
Here, we call $e_0(s)$ the \textbf{tracking error signal}, $S(s)$ the \textbf{sensitivity function} and $T(s) = 1- S(s)$ the \textbf{complementary sensitivity function}. The goal is to minimize $e_0$, of course. However, a fundamental obstacle is that $S+T = 1$ at all frequencies: if $S$ is small and disturbances are rejected then the output becomes noisy, and vice versa. There are ways around this, though. 


\subsection*{Feedforward}


Wikipedia has a nice explanation for \textbf{feedforward} so I'll just paste it here: ``With feed-forward or feedforward control, the disturbances are measured and accounted for before they have time to affect the system. In the house example, a feed-forward system may measure the fact that the door is opened and automatically turn on the heater before the house can get too cold. The difficulty with feed-forward control is that the effects of the disturbances on the system must be accurately predicted, and there must not be any unmeasured disturbances. For instance, if a window was opened that was not being measured, the feed-forward-controlled thermostat might let the house cool down.'' \\


To implement this, one may apply a \textbf{prefilter} $F(s)$ to the control signal $r(s)$. In the absence of any feedback, the system response is just $y(s) = F(s)G(s)r(s)$. If we can choose $F = G^{-1}$ then $y$ just follows $r$ directly, which is what we mean by ``feedforward.'' In practice, it is impossible to find $F = G^{-1}$; however, one can approximate $G^{-1}$. \\


Since it is not possible to implement feedforward and because disturbances are usually unknown, in practice one usually combines feedforward with feedback. A block diagram for this scheme has the form

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/block-4}
	\caption{From \cite{bechhoefer2005feedback}}
\end{figure}

The output now becomes
\begin{equation*}
y(s) = \f{KG}{1+KG}[F(s)r(s) - \xi(s) ] + \f{1}{1+KG}d(s).
\end{equation*}

Choose $F$ as close as possible to $1+(KG)^{-1}$ reduces the load on the feedback while simultaneously rejecting disturbances via the feedback loop. 


\subsection*{Case study: Operational Amplifier (OpAmp)}


An op amp is basically a very high gain differential amplifier that uses negative feedback to trade off high gain for reduced sensitivity to drift. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/op-amp}
	\caption{From \cite{bechhoefer2005feedback}}
\end{figure}

The op amp in the configuration above does the following:
\begin{equation*}
V_\text{out} = A(V_\text{in} - V_-),
\end{equation*}
where $A$ is the gain. We want to find the transfer function $G$ which is also the closed-loop gain. The resistors $R_1$ and $R_2$ form a voltage divider:
\begin{equation*}
V_- = V_\text{out} \f{R_2}{R_1 + R_2} \equiv V_\text{out} \beta.
\end{equation*}
From here, we can find 
\begin{equation*}
G = \f{V_\text{out}}{V_\text{in}} = A - A\f{V_-}{V_\text{in}} = A - A\f{V_\text{out}}{V_\text{in}} \beta \implies G = \f{V_\text{out}}{V_\text{in}} = \f{A}{1+A\beta}. 
\end{equation*}
When $A \gg 1$, we have an approximation:
\begin{equation*}
G \approx \f{1}{\beta} = \f{R_1 + R_2}{R_2} = 1 + \f{R_1}{R_2}. 
\end{equation*}








\subsection*{Integral control}

So far, we have only covered proportional control, which suffers from \textbf{proportional droop} -- meaning that there is always some non-zero difference between the set point and the output of the system: only for an infinite gain that $y_\infty = r_\infty$. \\


In integral control, one applies a control $K_i \int_{-\infty}^t e(t')\,dt'$ instead of (just) the proportional term $K_p e(t)$. As long as $e(t) \neq 0$, the integral will build up and eliminates steady state error. This can be seen in the time domain:
\begin{equation*}
\dot{y}(t) = -\omega_0 y(t) + \omega_0^2 K_i \int_{-\infty}^t [r_\infty - y(t')]\,dt'
\end{equation*} 
which after differentiating gives
\begin{equation*}
\ddot{y}(t) = -\omega_0\dot{y}(t) + \omega_0^2 K_i [r_\infty - y(t)]
\end{equation*}
which has steady state solution $y_\infty = r_\infty$. Now we want to go into frequency domain. To do this, we first recall that $y(s) = K(s) G(s) e(s)$. To find $K(s)$, we Laplace-transform $K(t)$, which requires the identity $\lag[\int e(t)\,dt] = e(s)/s$:
\begin{equation*}
K(s) = \lag[K(t)] = K_i \f{e(s)}{s}.
\end{equation*}
So,
\begin{equation*}
\dot{y}(s) = -\omega_0 \lb y(s) + \f{K_i}{s/\omega_0} \rb e(s).
\end{equation*}
Consider the low pass filter again with transfer function 
\begin{equation*}
G(s) = \f{1}{1+ s/\omega_0}.
\end{equation*}
With this $K(s)$, the complementary sensitivity function becomes
\begin{equation*}
T(s) = \f{KG}{1+ KG} = \f{1}{1 + \f{s}{\omega_0 K_i} + \f{s^2}{K_i \omega_0^2}}.
\end{equation*}
We see that this new transfer function resembles a second-order system. Indeed, the integral control law transform a first-order system into a second-order system, with new natural frequency ${\omega_0'}^2 = K_i \omega_0^2$ and $\zeta = 1/2\sqrt{K_i}$. \\


Integral control can be improved by combining with proportional control:
\begin{equation*}
K(s)  = K_p + \f{K_i}{s/\omega_0},
\end{equation*}
which gives faster response while still eliminating steady-state errors. One can write down the closed-loop transfer function and show that $T\to 1$ for $\omega\to 0$ and is asymptotically first-order with time constant $1/K_p \omega_0$. \\


In general, the transfer function for a PID controller is given by 
\begin{equation*}
K(s) = K_p + \f{K_i}{s} + K_d s.
\end{equation*}
In a closed-loop system with transfer function $G(s)$ without any noise or measurement error, the overall transfer function is given by 
\begin{equation*}
T(s) = \f{K(s)G(s)}{1+ K(s)G(s)}. 
\end{equation*}
Adding noise and measurement error follows the same procedure as described above. 








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{E}



\section*{Electromagnetically induced transparency (EIT)}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{F}


\section*{Feshbach Resonance (in ultracold gases)}


To start, let us look at some results in scattering theory. For more information about this topic, the reader may refer to \cite{sakurai1995modern}. Consider a general wave-scattering problem where we have an incident plane wave with wave number $k$ and a radial scattered wave with anisotropy term (partial wve scattering amplitude) $f(\theta)$: 
\begin{equation*}
\phi_\text{inc} = e^{ikz}, \quad \phi_{\text{sc}} = \f{f(\theta)e^{ikr}}{r}
\end{equation*}
and by definition
\begin{equation*}
d\sigma = \abs{f(\theta)}^2\, d\Omega.
\end{equation*}
Consider a particle of mass $m$, energy $\hbar^2k^2/2m$, in a central potential $V(r)$. At large $r$, the wave function is approximated by the sum of the waves above:
\begin{equation*}
\phi_{\text{large } r} = \phi_{\text{inc}} + \phi_{\text{sc}}. 
\end{equation*}
Since $\phi_{\text{large } r}$ must also solve the SE, which is separable due to $V(r)$ being central, we have \cite{bohm2012quantum}
\begin{equation*}
\phi_{\text{large } r} = Y(\theta)R(r) = \lp \sum^\infty_{l=0} C_l P_l(\cos\theta) \rp \f{1}{kr}\sin\lp kr - \f{l\pi}{2} + \delta_l \rp
\end{equation*}
By re-writing $\phi_\text{large $r$}$ like this we find 
\begin{equation*}
f(\theta) = \f{1}{k}\sum^\infty_{l=0} \sin \delta_l P_l(\cos\theta).
\end{equation*}
Here $l$, the angular momentum quantum number, corresponds to different partial waves. \\

Now recall the radial SE:
\begin{equation*}
-\f{\hbar^2}{2m}\f{d^2 u(r)}{dr^2} + V_\text{eff}(r)u(r) = Eu(r)
\end{equation*} 
where
\begin{equation*}
V_\text{eff}(r) = V(r) + \f{\hbar^2l(l+1)}{2mr^2}.
\end{equation*}
The form of $V_\text{eff}$ implies that the particles cannot reach small $r$'s for $l>0$. So, by assuming that $V(r)$ is only significant/nontrivial for small $r$, we find that only $l=0$ matters. So, we only care about $s$-waves in the expansion. With this, we have
\begin{equation*}
\phi_{\text{large } r} = \f{C_0}{kr} \sin(kr - \delta_0) = 
\end{equation*}
This means that
\begin{equation*}
f = -\f{1}{k}e^{i\delta_0 }\sin\delta_0 = \f{1}{k\cot\delta_0 -ik}. 
\end{equation*}

The \textcolor{blue}{$s$-wave scattering length} $a$ is defined as 
\begin{equation*}
a = -\lim_{k\to 0}\f{\tan \delta_0 (k)}{k}
\end{equation*}
\begin{itemize}
	\item For strong and attractive interactions, $a$ is large and negative
	
	\item For strong and repulsive interactions, $a$ is large and positive. 
\end{itemize}
By Taylor expansion we have
\begin{equation*}
k\cos(\delta_0(k)) \approx -\f{1}{a} + \f{1}{2}r_\text{eff} k^2 + \dots
\end{equation*}
where $r_\text{eff}$ is the effective range of the potential. Plugging this back into the equation for $f$ we find 
\begin{equation*}
f(k) = \f{1}{-1/a + r_\text{eff} k^2/2 - ik}.
\end{equation*}
From the Lippmann-Schwinger equation for $s$-waves, we find that 
\begin{equation*}
\f{1}{f(k)} \approx -\f{4\pi}{v_0} + \f{4\pi}{v_0} \int\f{d^3q}{(2\pi)^3} \f{v(\mathbf{q})}{k^2 - q^2 - i\eta}
\end{equation*}
where $v(\cdot)$ is the interatomic potential. \\


Now let us apply some of this to ultracold Fermi gases. Consider two Fermionic atoms in close proximity. The effective interaction between them depends on the angular momentum coupling of their valence electrons, $\ket{s_1,m_1}\otimes \ket{s_2,m_2}$, where $s$ is the total spin quantum number and $m$ is the spin projection quantum number. The valance electrons can be in the singlet or one of the triplet state. If the electrons are in the singlet state with antisymmetric spin wave function 
\begin{equation*}
\ket{0,0} = \f{1}{\sqrt{2}}\lp \ket{\uparrow\downarrow} - \ket{\downarrow \uparrow} \rp
\end{equation*}
then they must have symmetric spatial wave function (since they are fermions -- the total wavefunction must be anti-symmetric), and so they can exist close together. On the other hand, if the spin wave function is one of the triplet states (which are symmetric) 
\begin{equation*}
\ket{1,1} = \ket{\uparrow\uparrow}, \ket{1,0} = \f{1}{\sqrt{2}} \lp \ket{\uparrow\downarrow} + \ket{\downarrow\uparrow} \rp, \text{ or } \ket{1,-1} = \ket{\downarrow\downarrow}
\end{equation*}
then the spatial wave function is anti-symmetric, and thus they cannot be close to each other. \\


As a result, the singlet interatomic potential $V_S$ is deeper than the triplet potential $V_T$. For simplicity, we assume that $V_S$ is just deep enough to contain only one bound state and that $V_T$ contains no bound states.\\


In addition to the interaction potential, there is also the hyperfine interaction
\begin{equation*}
V_\text{hf} = \f{\al}{\hbar^2} \mathbf{I}\cdot \mathbf{S}
\end{equation*}
and the Zeeman shift $\Delta \mu B$ between the singlet and triplet states (which are degenerate), where $\Delta \mu$ is the difference in magnetic moments between the two states. We assume that $0< V_\text{hf} \ll V_T,V_S,\Delta \mu B$. 


\subsection*{Example: Spherical well model}







%\section*{Free spectral range (FSR)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{G}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{H}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{I}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{J}


\section*{Jaynes-Cummings model}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{K}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{L}


\section*{Landau-Zener transition}


This concept was presented in Zener's paper \textit{Non-adiabatic crossing of energy levels} \cite{zener1932non} and describes exactly that. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{M}



\section*{Magic wavelength}



From Wikipedia: Magic wavelength is the wavelength of an optical lattice at which the polarizabilities of two atomic clock states have the same values such that the AC Stark shift caused by the laser \textit{intensity} fluctuation has no effect on the transition frequency between the two clock states.\\


This is really all we need to know about the magic wavelength. Basically, the electric field due to the light for the optical lattice induces an electric dipole moment in the atoms to exert forces on them. This force depends on the location of the atom as well as the intensity of the light. While this is good for trapping, the difference in polarizabilities of the atomic states leads to unequal AC Stark shifts between the two states. To mitigate this problem, we can tune the wavelength of the optical lattice laser so that the polarizabilities of the atomic states are now the same, and so both states experience the same AC Stark shift -- which means that fluctuations in the intensity of the optical trap no longer cause changes to the spacing between the two states. This special wavelength is called the magic wavelength. 



%\section*{Magnetic evaporative cooling}






\section*{Magneto-optical trap (MOT)}

The idea of a MOT is actually quite intuitive. However, there are subtle details that one needs to know in order to truly understand its principle of operation. 

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/MOT}
	\caption{From Wikipedia}
\end{figure}

Magneto-optical traps use a combination of red-detuned, circularly polarized, counter-propagating lasers, and a quadrupole magnetic field to trap neutral atoms with admissible atomic structure (i.e. not all atomic species can be magneto-optically trapped). \\


A theoretical description of a MOT is as follows. First, we require a vacuum and two coils in an anti-Helmholtz configuration. We let the coils be separated along the $z$-axis. To first order approximation (i.e. near the center), the magnetic field takes the form
\begin{equation*}
\vec{B} = B_0 \lp \f{x}{2}, \f{y}{2}, -z \rp, \quad B_0 = -\f{3\mu_0 I a R^2}{2(R^2 + a^2)^{5/2}}
\end{equation*}
where $R$ is the (common) radius of the coils and $2a$ is the distance between two coils. From here, we see that field strength is linear in position, and that the field gradient is twice as strong along the $z$ direction than along $x$ or $y$. 


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.4\textwidth]{images/anti-helmholtz}
	\caption{From \cite{foot2005atomic}.}
\end{figure}

Consider an atom with ground and excited states with $J=0$ and $J=1$, respectively. The Zeeman effect due to the magnetic field lifts the degeneracy in the $m_J$'s. For $J=1$ there are now $2J+1 = 3$ nondegenerate sublevels. For $J=0$ there is no such splitting. Since the magnetic field varies in space, the Zeeman effect on the atoms also varies in space, being more extreme for atoms further away from the center of the trap. \\


Next, we add three pairs of counter-propagating circularly-polarized orthogonal laser beams. The lasers are red-detuned from the $J=0 \to J=1$ transition. Circular polarization is required because we only want to selectively drive transitions from $\ket{0,0}$ to a specific $\ket{1,m_J}$ depending on where the atom is in the magnetic field. \\


When the atoms are the center of the trap, they don't experience any Zeeman shift and don't see the red-detuned light (not very much). They are the coldest and darkest atoms.\\


For an atom moving in the $+z \neq 0$ direction, for instance, the Zeeman effect shifts the energy of $\ket{J=1,m_J=-1}$ down closer to the energy of $\ket{J=0,m_J=0}$. In the $z$ direction, there are also the counter-propagating $\sigma^-$ and $\sigma^+$ in the $- z$ and $+z$ directions, respectively. Due to the Doppler effect, the atom moving in the $+z$ direction gets closer to resonance with the $\sigma^-$ beam traveling in the $-z$ direction, which drives the $\Delta m_J = -1$ transition, and feels a stronger scattering force the further it moves away from the center. When the atom absorbs a $\sigma^-$ photon, it goes to the $\ket{J=1,m_J=-1}$ state and gets a momentum kick opposite to its motion. It will eventually re-emits the photon due to spontaneous emission, but since the process is random and averages out to zero we see the atom affected mostly by the absorption of the $\sigma^-$ photon. The net effect is that the atom is pushed towards the center of the trap. Similarly, the atom moving the $-z$ direction is also pushed towards the center. The same thing happens in the $x-$ and $y-$directions.  
 	
 	
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/foot_1}
	\caption{From \cite{foot2005atomic}}
\end{figure}

\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/foot_2}
	\caption{From \cite{foot2005atomic}}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{N}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{O}



\section*{Optical lattice}



Optical lattice is one of those things which you think you know how they work but turns out that you don't. There are actually a number of technical details experimentalists must overcome in order to successfully trap atoms in a lattice. \\




An optical lattice is formed by the interference of counter-propagating laser beams, creating a spatially periodic \textbf{polarization} (or \textbf{intensity}) pattern. Atoms are cooled and localized in the potential minima (maxima) for red (blue) detuned light. The laser used is often far detuned.  The potential is due to the Stark effect; it is proportional to the laser intensity. \\

The energy shift to (and thus the potential experienced by) an electronic ground state $\ket{g_i}$ is given by second-order time-independent perturbation theory where the rapid time variation of the lattice potential at optical frequencies has been time-averaged:
\begin{equation*}
U(\mathbf{r}) = \Delta E_i = \f{3\pi c^2 \Gamma}{2\omega_0^3}\I(\mathbf{r}) \times  \sum_j \f{c_{ij}^2}{\Delta_{ij}}
\end{equation*}
where $\mu_{ij} = \bra{e_j}\mu \ket{g_i}  = \bra{e_j} \mathbf{d}\cdot \mathbf{E} \ket{g_i} \equiv c_{ij}\norm{\mu}$ are the transition matrix elements $\ket{g_i} \to \ket{e_j}$. For two-level atoms, this simplifies to 
\begin{equation*}
U(\mathbf{r}) = \Delta E = \f{3\pi c^2}{2\omega_0^3}\f{\Gamma}{\Delta} I(\mathbf{r}).
\end{equation*}
Here, $\Gamma$ is the natural linewidth of the excited state transition, i.e. the decay rate of spontaneous emission. 



\begin{framed}
	\noindent \textbf{Derivations of some results:}\\
	
	\noindent To see how we obtained the results above, let us go through the derivation given by \cite{grimm2000optical}. The article focuses on optical dipole trap, but the principle of operation transfer over exactly for optical lattices. The only major difference between the two trapping schemes is that the intensity of an optical lattice has a much more dramatic spatial variation than a standard optical dipole trap.\\
	
	Consider an atom placed in laser light. The laser light has electric field $\mathbf{E}$ oscillating at angular frequency $\omega$ which induces an atomic dipole moment $\mathbf{p}$. $\mathbf{p}$ also oscillates at $\omega$. If $\mathbf{E}(\mathbf{r},t) = \widehat{\epsilon} \widetilde{E}(\mathbf{r}) e^{-i\omega t} + c.c.$ then $\mathbf{\mathbf{p}}(\mathbf{r},t)  = \widehat{\epsilon} \widetilde{p}(\mathbf{r}) e^{-i\omega t} + c.c.$. Here $\widehat{\epsilon}$ denotes the unit polarization vector. The dipole amplitude $\widehat{p}$ is related to the amplitude of the electric field $\widetilde{E}$ by 
	\begin{equation*}
	\widetilde{p} = \al \widetilde{E}
	\end{equation*}
	where $\al$ is the (complex) polarizability, which depends on $\omega$. \\
	
	
	The \textbf{time-averaged interaction (dipole) potential} is given by the time-average of the dot product between $\mathbf{p}$ and $\mathbf{E}$:
	\begin{equation*}
	U_\text{dip} = -\f{1}{2} \langle \mathbf{p} \mathbf{E} \rangle = \dots = -\f{1}{2\epsilon_0 c} \Re(\al) I(\mathbf{r})
	\end{equation*}
	where $I = 2\epsilon_0 c^2 \abs{\widetilde{E}}^2$ is the intensity of the laser. The factor of $1/2$ takes into account the fact that the dipole is induced and not a permanent one. \\
	
	
	The \textbf{dipole force} is simply the negative gradient of the potential. We note now that the force is time-independent, as the time component of $U$ has already been averaged out:
	\begin{equation*}
	F_\text{dip}(\mathbf{r}) = -\nabla U_\text{dip}(\mathbf{r}) = \f{1}{2\epsilon_0 c}\Re(\al) \nabla I(\mathbf{r}). 
	\end{equation*}
	It is easy to see that the force is proportional to the intensity of the light. \\
	
	Next we want to calculate the atomic polarizability. To do this, we consider an atom in Lorentz's model of a classical oscillator and view the electron as a damped, driven harmonic oscillator whose position satisfies the following differential equation:
	\begin{equation*}
	\ddot{\mathbf{r}} + \Gamma_\omega \dot{\mathbf{r}} + \omega_0^2 \mathbf{r} = -\f{eE(t)}{m_e}.
	\end{equation*}
	Here, the electronic is considered to be bounded elastically to the core with a natural oscillation frequency $\omega_0$. The driving field is $E(t)$ with frequency $\omega$. $\Gamma_\omega$ is the damping rate, which results from the dipole radiation of the oscillating electron according charge acceleration (see Larmor's formula and \cite{jackson1999classical}). \\
	
	Solving this equation gives the amplitude of oscillation in terms of the driving frequency $\omega$
	\begin{equation*}
	\mathbf{r}(\omega) = \f{1}{m_e}\f{-eE(t)}{[(\omega_0^2 - \omega^2) + \Gamma_\omega^2 \omega^2]^{1/2}} =?= \f{1}{m_e} \f{-eE(t)}{(\omega_0^2 - \omega^2) - i\Gamma_\omega \omega}
	\end{equation*}
	 The second equality is not really an equality but rather the complex version of the solution. We can ignore that, as it's only a convention thing. The ``real'' solution is given in the first equality. \\
	 
	 Now recall that the dipole moment is related to $\mathbf{r}$ via $\mathbf{p} = -e\cdot \mathbf{r}$. So we can calculate $\mathbf{p}$. And finally, the polarizability $\al(\omega)$ is given by (as above): $\mathbf{p}(\omega) = \al(\omega) \mathbf{E}(\omega)$. So we have
	 \begin{equation*}
	 \al = \f{e^2}{m_e} \f{1}{\omega_0^2 - \omega^2 - i\omega \Gamma_\omega}.
	 \end{equation*}
	 From Larmor's formula for power radiated by an oscillating charge, we find 
	 \begin{equation*}
	 \Gamma_\omega = \f{e^2 \omega^2}{6\pi \epsilon_0 m_e c^3} \implies \f{e^2}{m_e} = \f{6\pi \epsilon_0 c^3 \Gamma_\omega}{\omega^2}.
	 \end{equation*}
	 Further, we introduce the on-resonance damping rate $\Gamma \equiv \Gamma_{\omega_0} =  (\omega_0/\omega)^2 \Gamma_\omega$. Putting everything together, we have
	 \begin{equation*}
	 \al = 6\pi \epsilon_0 c^3 \f{\Gamma/\omega_0^2}{\omega_0^2 - \omega^2 - i(\omega^3 / \omega_0^2) \Gamma}.
	 \end{equation*}
	 
	 
	 In general, the on-resonance damping rate $\omega$ cannot be calculated from Larmor's formula as this is classical theory. From the semi-classical approach, what one finds is that $\Gamma$ corresponds to the decay rate due to spontaneous emission: 
	 \begin{equation*}
	 \Gamma = \f{\omega_0^3}{3\pi \epsilon\hbar c^^3}\abs{\bra{e} \mu \ket{g}}^2.
	 \end{equation*}
	 
	 
	 
	 In any case, with this expression for the polarizability $\al$, we can write down the dipole interaction potential in terms of $\Gamma$ and frequencies:
	 \begin{equation*}
	 U_\text{dip}(\mathbf{r}) = -\f{3\pi c^2}{2\omega_0^3}\lp \f{\Gamma}{\omega_0 - \omega} + \f{\Gamma}{\omega_0 + \omega} \rp I(\mathbf{r}).
	 \end{equation*}
	 In the rotating wave approximation, the potential simplifies to 
	 \begin{equation*}
	 U_\text{dip}(\mathbf{r}) = +\f{3\pi c^2}{2\omega_0^3}\f{\Gamma}{\Delta} I(\mathbf{r}) 
	 \end{equation*}
	 where $\Delta$ is the detuning from the eigenfrequency of the electron in the Lorentz model. It turns out that (we will show this but won't prove) the \textbf{scattering rate} $\Gamma_\text{sc}$ is related to $U_\text{dip}$ via
	 \begin{equation*}
	 \hbar \Gamma_\text{sc} = \f{\Gamma}{\Delta} U_\text{dip}. 
	 \end{equation*}
	 The ``scattering'' process here can be thought of as due to viewing light as a stream of $\hbar \omega$ photons absorbed by the atom and re-emitted via spontaneous emission. \\
	 
	 
	 \noindent \textbf{Dipole trap potential and energy shift correspondence:}\\
	 
	 \noindent To find energy shifts to the oscillating electric field, we can make use of perturbation theory. Now, since the electric field is sinusoidal, the first-order correction to the energies will be zero by time-averaging. However, the second-order term may not be. In fact, we may very well expect the correction to energies to be non-zero in the second-order perturbation because this perturbation is proportional to the field \textit{intensity}, which is the square of the field. In any case, second-order perturbation theory (for non-degenerate states) says that
	 \begin{equation*}
	 \Delta E_i = \sum_{j\neq i} \f{\abs{\bra{j} \ham'\ket{i}}^2}{E_i - E_j}
	 \end{equation*} 
	 where we have perturbation Hamiltonian $\ham' = e \mathbf{r}\cdot \mathbf{E} $ and $E_i - E_j = \hbar \Delta_{ij}$. For a two-level system (which is what we are considering now), the energy shift turns into
	 \begin{equation*}
	 \Delta E = \pm \f{\abs{\bra{e} e\mathbf{r} \ket{g}}^2}{\hbar \Delta_{ij}} \abs{E}^2 = \pm \f{3\pi c^2}{2\omega_0^3} \f{\Gamma}{\Delta}I(\mathbf{r}) = \pm U_\text{dip}(\mathbf{r}).  
	 \end{equation*}
	 Here, the ground state is shifted down and excited state is shifted up, each by $\Delta E$, if $\Delta < 0$ (red-detuned).  \\
	 
	 We see that the optically induced light shift, i.e., the \textbf{ac Stark shift} of the ground-state energy \textbf{corresponds exactly} to the \textbf{dipole potential} for the two-level atom. \\
	 
	 
	 
	 For multi-level atoms, we will have a collection of dipole matrix elements $\mu_{ij} = \bra{e_i}\mu\ket{g_j}$, which can be written in terms of a reduced matrix element $\norm{\mu}$ multiplied by some transition coefficient $c_{ij}$ which one can look up in some table. From standard quantum mechanics, one may find that the fully reduced matrix element only depends on the orbital wave functions and is directly related to the spontaneous emission decay rate. In any case, the energy shift now takes the form
	 \begin{equation*}
	 \Delta E_i = \f{3\pi c^2 \Gamma}{2\omega_0^3} I \times \sum_j \f{c_{ij}^2}{\Delta_{ij}}.
	 \end{equation*}
	 
	 
	 On the basis of this equation  one can derive a general result for the potential of a ground state with total angular momentum F and magnetic quantum number mF , which is valid for both linear and circular polarization as long as all optical detunings stay large compared with the
	 excited-state hyperfine splitting. We won't go into that here, but the reader is welcomed to refer to \cite{grimm2000optical} for more details. 
\end{framed}


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{images/dipole_trap}
	\caption{From \cite{grimm2000optical}}
\end{figure}


Atoms trapped in the potential wells may still move across wells even if the well depth exceeds the kinetic energies of the atoms. This is due to quantum tunneling. However, a \textbf{superfluid-Mott insulator transition} may occur when the interatomic interaction is sufficiently strong. (stronger than the well depth). In the Mott insulator phase, the atoms are trapped at the potential minima/maxima and cannot move freely (hence ``insulator''). \\


There are two parameters for an optical lattice: the potential well depth and the periodicity of the lattice. The potential experienced by the atoms (note that the potential is felt only by the atoms because of the Stark shift -- inherent to the atoms) depends on the intensity of the laser used to generate the optical lattice. The intensity of this laser is usually controlled by an AOM. The periodicity of an optically lattice can be tuned by changing the wavelength of the laser used or by changing the relative angle between the laser beams. 






\section*{Optical dipole trap}


\section*{Optical tweezer}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{P}



\section*{PID Controller (basics)}


\subsection*{OpAmps}


\subsection*{PID controller}




%\section*{Perturbation theory in QM (simple version)}





\section*{Pound-Drever-Hall (PDH) technique}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Q}

\section*{Quantum Harmonic Oscillator}

This is one of the most important concepts in quantum mechanics that is applied almost all the time in AMO physics, so it is \textit{very} good to know. \\


In one dimension, the Hamiltonian is given by 
\begin{equation*}
\widehat{H} = \f{\widehat{p}^2}{2m} + \f{1}{2}k\widehat{x}^2 =  \f{\hat{p}^2}{2m} + \f{1}{2}m\omega^2 \widehat{x}^2,
\end{equation*}
where $m$ is the particle mass, $\omega = \sqrt{k/m}$ is the angular frequency of the particle, and $\widehat{x}$ and $\widehat{p}$ are the position and momentum operators, respectively. The wavefunction of the particle satisfies the time-independent SE says $\widehat{H}\ket{\psi} = E\ket{\psi}$. In position space, the solution space for $\widehat{H}$ is spanned by the following Hermite functions
\begin{equation*}
\bra{x}\ket{\psi} = \psi_n = \f{1}{\sqrt{2^n n!}} \lp \f{m\omega}{\pi \hbar} \rp^{1/4} e^{-m\omega^2 x/2\hbar} H_n\lp \f{\sqrt{m\omega}}{\hbar} x \rp
\end{equation*}
where $n \in \mathbb{N}$ and $H_n$ are Hermite polynomials. The corresponding energy levels are 
\begin{equation*}
E_n = \hbar \omega \lp n + \f{1}{2} \rp,
\end{equation*}
and so the zero-point energy is $\hbar\omega/2$, which is non-zero due to the Heisenberg uncertainty principle. We notice that the energies are quantized and adjacent levels are equally spaced by one unit of $\hbar\omega$. We note also that when the \textcolor{blue}{harmonicity} of the potential is lost, we are no longer guaranteed equal spacings between adjacent levels. \\


One way to solve the QHO problem is by using ladder operators (so that we avoid Hermite functions, etc.). We define the creation and annihilation operators as below:
\begin{equation*}
\widehat{a}^\dagger = \sqrt{\f{m\omega}{2\hbar}} \lp \widehat{x} - \f{i}{m\omega}\widehat{p} \rp, \quad \widehat{a} = \sqrt{\f{m\omega}{2\hbar}} \lp \widehat{x} + \f{i}{m\omega}\widehat{p} \rp
\end{equation*}
from which we can represent the position and momentum operators as 
\begin{equation*}
\widehat{x} = \sqrt{\f{\hbar}{2m\omega}} (\widehat{a}^\dagger + \widehat{a}), \quad \widehat{p} = i\sqrt{\f{\hbar m\omega}{2}}(\widehat{a}^\dagger - \widehat{a}).
\end{equation*}
Creation and annhilation operators are not Hermitian. Rather, they are idempotents. They act on the energy eigenstates the following way:
\begin{equation*}
\widehat{a}\ket{n} = \sqrt{n}\ket{n-1}, \quad a\ket{0} = 0, \quad \widehat{a}^\dagger\ket{n} = \sqrt{n+1}\ket{n+1}.
\end{equation*}
This explains why these operators are called the way they are. The number operator $\widehat{N}$ can be defined from the ladder operators:
\begin{equation*}
\widehat{N}\ket{n} \equiv \widehat{a}^\dagger \widehat{a}\ket{n} = n\ket{n}, 
\end{equation*}
and so 
\begin{equation*}
\widehat{H} = \hbar\omega\lp \widehat{a}^\dagger \widehat{a} + \f{1}{2} \rp = \hbar\omega \lp \widehat{N} + \f{1}{2} \rp.
\end{equation*}
With these definitions, one can express any excited state in terms of the ground state:
\begin{equation*}
\ket{n} = \f{(\widehat{a}^\dagger)^n}{\sqrt{n!}} \ket{0}.
\end{equation*}
The QHO can be easily generalized to an $N$-dimensional isotropic QHO by simply repeating the 1D problem $N$ times. The energy is given by 
\begin{equation*}
E = \hbar\omega\lp n_1 + n_2 + \dots + n_N + \f{N}{2} \rp.
\end{equation*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{R}



\section*{Rabi frequency}


The Rabi frequency is defined as 
\begin{equation*}
\Omega_{ij} = \f{\vec{d}_{ij}\cdot \vec{E}_0}{\hbar}
\end{equation*}
where $\vec{d}_{ij}$ is the transition dipole moment for the $i\to j$ transition, $\vec{E}_0 = \hat{\epsilon} E_0$ is the vector electric field amplitude which includes the polarization. $\Omega_{ij}$ has units of angular frequency. \\


The Rabi frequency is the frequency of fluctuations in the populations of the two atomic levels involved in the atomic transition being considered. It is proportional to the strength of the coupling between the light and the atomic transition (dipole moment) and to the amplitude of the light's electric field. \\


When light is exactly resonant with the transition, Rabi flopping (the fluctuation in populations of the ground and excited states) occurs at the Rabi frequency. When light is detuned from resonance, then Rabi flopping occurs at the generalized Rabi frequency, given by 
\begin{equation*}
\widetilde{\Omega}_{ij} = \sqrt{\abs{\Omega_{ij}}^2 + \Delta^2}
\end{equation*}
where $\Delta$ is the detuning. Further, as light is more detuned from resonance, the amplitude of Rabi oscillations decreases. \\


Everything is characterized by the Rabi formula, which gives the population of the excited state $\ket{1}$ at time $t$, when the system is originally in state $\ket{0}$:
\begin{equation*}
P_{0\to 1}(t) = \f{\abs{\Omega}^2}{\Delta^2 + \abs{\Omega}^2}\sin^2\lp \f{\sqrt{\abs{\Omega}^2 + \Delta^2}}{\hbar} t \rp
\end{equation*}
Here, $\sqrt{\abs{\Omega}^2 + \Delta^2}$ is the generalized Rabi frequency and $\Omega$ is the Rabi frequency, which appears in the two-level Hamiltonian as
\begin{equation*}
\ham = 
\begin{pmatrix}
E + \Delta & \Omega \\ \Omega^* & E - \Delta
\end{pmatrix}.
\end{equation*}




\section*{Rabi oscillation}






\section*{Raman scattering}




\section*{Raman cooling}

Raman cooling is a sub-Doppler/recoil cooling technique. The general mechanism is as follows. Consider a collection of atoms with two hyperfine states. The transition between these two states is triggered by two laser beams: the first beam excites the atoms to some virtual excited state, and the second beam deexcites (via stimulated emission) the atoms to the other hyperfine level. The frequency difference between the two beams is equal to the spacing between the hyperfine levels. This is the general Raman process.


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.25\textwidth]{images/raman}
	\caption{From Wikipedia}
\end{figure}


Now we use the general Raman process to do free-space Raman cooling. In this scheme, pre-cooled atoms experience pulses of Raman-like processes. The two laser beams in this case are now counter-propagating, with $f_2$ being red-detuned with respect to its normal value. This allows for atoms moving towards the $f_2$ beam to be slowed down due to the Doppler effect and end up in the state $\ket{g_2}$. By regularly exchanging the propagation direction of the laser beams and varying the (red) detuning, we can get \textbf{all} atoms whose initial velocities satisfying $\abs{v} > v_\text{max}$ to be in the state $\ket{g_2}$, while atoms with $\abs{v} < v_\text{max}$ remain in the $\ket{g_1}$ state. \\

Now, we need to slow down the atoms in $\ket{g_2}$. To do this, a new beam whose frequency is exactly the spacing between $\ket{g_2}$ and $\ket{e}$ is switched on. This beam optically pumps the atoms from $\ket{g_2}$ to $\ket{g_1}$, and the velocities will be randomized in this process and so inevitably a fraction of the atoms in $\ket{g_2}$ will acquire speeds $\abs{v} < v_\text{max}$. By repeating this process several times, the temperature of the cloud can be reduced (to lower than a microkelvin.)\\


For more information about this technique and its working principle, the reader may refer to \cite{PhysRevLett.69.1741}.








\section*{Raman side-band cooling}

Raman side-band cooling is a sub-technique of Raman cooling. This cooling technique starts from atoms in a MOT, with an optical lattice present. We assume that the optical lattice is sufficiently deep that each site is essentially a harmonic trap. Since the atoms are not in the their ground state, they will be trapped in one of the excited levels of the quantum harmonic oscillator. Raman side-band cooling aims to bring these atoms to the ground state of the harmonic potential at each site, and it requires access to the atomic structure of the atoms. \\


Suppose the atom has two levels with ground state $F=1$, such that there is a three-fold degeneracy $m_F = -1,0,1$. A magnetic field is added to induce Zeeman shifts, lifting the degeneracy. The magnetic field strength is tuned such that the Zeeman splitting between the $\Delta m_F = 1$ states matches the spacing of the two levels in the harmonic potential.


\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/raman_sideband}
	\caption{From Wikipedia}
\end{figure}


By Raman processes, one can transfer an atom to a state whose $m_F$ and vibrational state has changed by $-1$. After all this, the atoms in the lowest vibrational state of the harmonic trap, except for those with $m_F = 1$, are optically pumped to the $m_F = 1$ state using $\sigma_+$ and $\pi$ light. 






\section*{Recoil temperature}

Consider an atom undergoing spontaneous emission of a single photon whose momentum is $p = \hbar k = h/\lambda$. By momentum conservation, the momentum kick that the atom gains is also $p$, and its kinetic energy is $p^2/2m = \hbar^2k^2/2m$. The recoil temperature is given by 
\begin{equation*}
T_r = \f{\hbar^2k^2}{k_Bm} = \f{h^2}{k_B m \lambda^2}.
\end{equation*} 
There is a factor of $1/2$ in an alternative definition, which can be obtained from setting $T_r k_B = p^2/2m$. But I'm following the convention set by \cite{metcalf2007laser}, which does not include the factor of $1/2$.






\section*{Rotating wave approximation}


The rotating wave approximation ignores highly oscillatory terms in the Hamiltonian. This is a valid approximation when the applied electromagnetic radiation is near resonance with an atomic transition, and the intensity is low. The name of the approximation comes from the transformation of the Hamiltonian to the \textbf{interaction picture}. In this picture, the atomic Hamiltonian is absorbed into the system ket, and so the evolution due to the interaction with the atom with the light field is the only thing to consider. \\


Consider a example of a two-level system with resonance frequency $\omega_0$. The atomic Hamiltonian is 
\begin{equation*}
\ham_0 = \f{\hbar \omega_0}{2}\ket{e}\bra{e} - \f{\hbar\omega_0}{2}\ket{g}\bra{g}.
\end{equation*}
Now we introduce a classical electric field $\vec{E}(t) = \vec{E}_0 e^{-i\omega_L t} + \vec{E}_0^* e^{+i \omega_L t}$. Under the \textbf{dipole approximation}, the interaction Hamiltonian is given by 
\begin{equation*}
\ham_I = -\vec{d} \cdot \vec{E},
\end{equation*}
where $\vec{d}$ is the dipole moment operator of the atom. To go further, we have to find the matrix elements of $\vec{d}$. Notice that $\bra{e} \vec{d} \ket{e} = \bra{g}\vec{d}\ket{g} = 0$ due to parity. So, we only have the matrix element $\bra{e} \vec{d} \ket{g}$ and its complex conjugate. Let $\vec{d}_{eg} = \bra{e} \vec{d} \ket{g}$, then $\vec{d}$ is just 
\begin{equation*}
\vec{d} = \vec{d}_{eg} \ket{e}\bra{g} + \vec{d}_{eg}^* \ket{g}\bra{e}. 
\end{equation*}
With this, the interaction Hamiltonian becomes:
\begin{align*}
\ham_1 
&=  -\vec{d}\cdot \vec{E}\\
&= - \lp \vec{d}_{eg} \ket{e}\bra{g} + \vec{d}_{eg}^* \ket{g}\bra{e} \rp
 \cdot \lp \vec{E}_0 e^{-i\omega_L t} + \vec{E}_0^* e^{+i \omega_L t} \rp\\
&= - \lp \vec{d}_{eg}\cdot \vec{E}_0 e^{-i\omega_L t} + \vec{d}_{eg}\cdot \vec{E}_0^* e^{+i\omega_L t} \rp \ket{e}\bra{g}\\
&\quad\quad - \lp \vec{d}_{eg}^*\cdot \vec{E}_0 e^{-i\omega_L t} + \vec{d}_{eg}^*\cdot \vec{E}_0^* e^{+i\omega_L t} \rp \ket{g}\bra{e}\\
&= -\hbar \lp \Omega e^{-i\omega_L t} + \widetilde{\Omega}^*e^{+i\omega_L t} \rp \ket{e}\bra{g} - \lp \widetilde{\Omega}^* e^{-i\omega_L t} + \Omega e^{+i\omega_L t} \rp \ket{g}\bra{e}.
\end{align*}

The Hamiltonian in the interaction picture is obtained by the unitary transform $\ham_{1,I} = U \ham_1 U^\dagger$ where
\begin{equation*}
U = e^{i \ham_0 t/\hbar} = \ket{g}\bra{g} + e^{i\omega_0 t}\ket{e}\bra{e}.
\end{equation*}
So we have
\begin{align*}
\ham_{1,I} &= U \ham_1 U^\dagger\\
&= -\hbar \lp \Omega e^{-i\Delta t} + \widetilde{\Omega}^*e^{+i(\omega_L + \omega_0) t} \rp \ket{e}\bra{g} - \hbar\lp \widetilde{\Omega}^* e^{-i(\omega_L  + \omega_0)t} + \Omega e^{+i\Delta t} \rp \ket{g}\bra{e}
\end{align*}
where $\Delta = \omega_L - \omega_0$ is the detuning and $\omega_L + \omega_0$ is the high frequency which we want to eliminate. Here is where the rotating approximation comes in: 
\begin{equation*}
\ham_{1,I}^{\text{RWA}} = -\hbar \Omega e^{-i\Delta t}  \ket{e}\bra{g} - \hbar\Omega e^{+i\Delta t}  \ket{g}\bra{e}.
\end{equation*} 
Transforming this back into the Schr\"{o}dinger picture we have
\begin{align*}
\ham_1^\text{RWA} &= U^\dagger\ham_{1,I}^{\text{RWA}}U \\
&= -\hbar \Omega e^{-i\Delta t}e^{-i \omega_0 t}\ket{e}\bra{g} - \hbar \Omega^* e^{i\Delta t}\ket{g}\bar{e}e^{i\omega_0 t}\\
&= -\hbar \Omega e^{-i\omega_L t}\ket{e}\bra{g} - \hbar \Omega^* e^{i\omega_L t}\ket{g}\bra{e}.
\end{align*}
The total Hamiltonian in the rotating wave approximation is thus
\begin{align*}
\ham^\text{RWA} &= \ham_0+ \ham_1^\text{RWA} \\
&= \f{\hbar \omega_0}{2}\ket{e}\bra{e} - \f{\hbar \omega_0}{2}\ket{g}\bra{g} -\hbar \Omega e^{-i\omega_L t}\ket{e}\bra{g} - \hbar \Omega^* e^{i\omega_L t}\ket{g}\bra{e}.
\end{align*}
In the standard basis, we have
\begin{equation*}
\ham^\text{RWA} =  \f{\hbar}{2} \begin{pmatrix}
\omega_0 & -\Omega e^{-i\omega_L t} \\ -\Omega^* e^{i\omega_L t} & -\omega_0
\end{pmatrix}.
\end{equation*}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{S}




%\section*{Sisyphus cooling}


%\section*{Stark effect}


%\section*{Standing waves (Energy and Polarization)}






\section*{Superexchange in optical lattice}


Consider a double-well potential with two Bosonic spins, where the relative depths of the wells can be tuned. There are four possible configurations for the system: $\ket{\uparrow, \downarrow}, \ket{\uparrow\downarrow,0}, \ket{0,\uparrow\downarrow}, \ket{\downarrow, \uparrow}$, where the spin on left denotes the spin on the left well, and the spin on the right denotes the spin on the right well. The states $\ket{\uparrow,\downarrow}$ and $\ket{\downarrow,\uparrow}$ are degenerate, and the states $\ket{\uparrow\downarrow,0}$ and $\ket{\downarrow\uparrow,0}$ are separated by $2\Delta$, where $\Delta$ is the difference between the well depths. These two manifolds of states are separated by the interatomic interaction energy $U \gg \Delta$.  Due to quantum tunneling, the manifolds are also coupled, via the tunneling term $J$. We are interested in the \textcolor{blue}{superexchange coupling} $J_{\text{ex}}$ between the states $\ket{\uparrow,\downarrow}$ and $\ket{\downarrow, \uparrow}$. 



\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.75\textwidth]{images/superex_1}
	\caption{From \cite{trotzky2008time}}
\end{figure} 

The system can be described by a two-site version of the Bose-Hubbard Hamiltonian:
\begin{equation*}
\widehat{H}= \sum_{\sigma = \uparrow,\downarrow}\lb -J\lp \widehat{a}^\dagger_{\sigma L}\widehat{a}_{\sigma R} + \widehat{a}^\dagger_{\sigma R}\widehat{a}_{\sigma L} \rp  - \Delta(\widehat{n}_{\sigma L} - \widehat{n}_{\sigma R} )  \rb + U\lp \widehat{n}_{\uparrow L} \widehat{n}_{\downarrow L} + \widehat{n}_{\uparrow R}\widehat{n}_{\downarrow R} \rp
\end{equation*}
This Hamiltonian makes sense: it says that the strength of the hopping (tunneling, i.e., annihilating in one well and appearing in the other) between the wells is $J$. The second term says that the potential bias is $\Delta$: we see that indeed $\ket{\uparrow,\downarrow}$ has the same energy as $\ket{\downarrow,\uparrow}$. But if there are two particles per site, then the difference between the number operators can be $\pm 2$, and the difference in energy between the states $\ket{\uparrow\downarrow,0}$ and $\ket{\downarrow\uparrow,0}$ is $2\Delta$. The last term tells us about the coupling between the $(1,1)$ and $(2,0)$ manifolds. This term assigns an energy of $0$ to the one-particle-per-site states, and assigns $U$ to the $(2,0)$ states. \\




Let's do a quick example. Assume that $\Delta = 0$ for now. In the basis $\{ 
\ket{\uparrow, \downarrow}, 
\ket{\uparrow\downarrow, 0}, 
\ket{0,\uparrow\downarrow}, 
\ket{\downarrow,\uparrow}\}$, the Hamiltonian is of the form
\begin{equation*}
\widehat{H} = \begin{bmatrix}
0 & -J & -J & 0\\
-J & U & 0& -J\\
-J & 0 & U & -J\\
0& -J &-J  & 0
\end{bmatrix}.
\end{equation*}
The eigenvalues associated with this Hamiltonian are 
\begin{equation*}
0, \quad U, \quad \text{ and }\quad \f{1}{2}\lp U \pm \sqrt{16 J^2+U^2} \rp. 
\end{equation*}
Redefining the zero energy (which doesn't change the dynamics of the Hamiltonian), we obtain the following energy diagram for the system (where $\Delta=0$):
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/superex_2}
	\caption{From \cite{trotzky2008time}}
\end{figure}





Here, we have Taylor expanded the last two eigenvalues under the assumption that $U\gg J$. The eigenstates of the system, from lowest to highest in energy, are
\begin{align*} 
\ket{t}+ \al\ket{+} &\propto \f{1}{\sqrt{2}}\lp \ket{\uparrow,\downarrow} + \ket{\downarrow,\uparrow} \rp + \f{J}{2U}\f{1}{\sqrt{2}}\lp \ket{\uparrow\downarrow,0}+ \ket{0,\uparrow\downarrow} \rp  \\
\ket{s} &= \f{1}{\sqrt{2}}\lp -\ket{\uparrow,\downarrow}+ \ket{\uparrow,\downarrow} \rp \\
\ket{-} &=  \f{1}{\sqrt{2}}\lp \ket{\uparrow\downarrow,0} - \ket{0,\uparrow\downarrow} \rp\\
\ket{+} + \al'\ket{t} &= \f{1}{\sqrt{2}}\lp \ket{\uparrow\downarrow,0} + \ket{0,\uparrow\downarrow} \rp - \f{2J}{U}\f{1}{\sqrt{2}}\lp \ket{\uparrow,\downarrow} + \ket{\downarrow,\uparrow} \rp.
\end{align*}

Since $J \ll U$, we see that the new eigenstates are small perturbations of $\ket{t},\ket{s},\ket{-},\ket{+}$, in that order. The states $\ket{t}+\al\ket{+}$ and $\ket{s}$ are spaced by $4J^2/U$, and similarly for $\ket{-}$ and $\ket{+}+ \al'\ket{t}$.  \\



By changing the bias $\Delta$, it is possible to create spin imbalances, spin oscillations, etc. 











%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{T}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{U}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{V}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{W}


\section*{WKB approximation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{X}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Y}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Z}



\section*{Zeeman slower}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\newpage

\bibliography{Bui_AMO} 
\bibliographystyle{ieeetr}


\end{document}