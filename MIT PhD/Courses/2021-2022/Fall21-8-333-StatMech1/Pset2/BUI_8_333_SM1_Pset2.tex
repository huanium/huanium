\documentclass{article}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{xcolor}
%\definecolor{colby}{rgb}{0.0, 0.0, 0.5}
\definecolor{MIT}{RGB}{163, 31, 52}
\usepackage[pdftex]{hyperref}
%\hypersetup{colorlinks,urlcolor=colby}
\hypersetup{colorlinks,linkcolor={MIT},citecolor={MIT},urlcolor={MIT}}  
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{newpxtext,newpxmath}
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\ham}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}
\def\dbar{{\mkern3mu\mathchar'26\mkern-12mu   d}}


\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}



\usepackage{enumitem}


\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}

%\hypersetup{colorlinks,urlcolor=colby}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}






\begin{document}
		\begin{framed}
			\noindent Name: \textbf{Huan Q. Bui}\\
			Course: \textbf{8.333 - Statistical Mechanics I}\\
			Problem set: \textbf{\#2}
		\end{framed}
	



\noindent \textbf{1. Random deposition.}


\begin{enumerate}[label=(\alph*)]
	\item Consider a site. Assume that the gold atoms arrive at this site over time via a Poisson process. The deposition rate is $d$ layers per second, so this Poisson process has rate $d$. Over time $t$, the average number of deposition at a site is $dt$. With this, we have
	\begin{align*}
	\Pr(m \text{ atoms in time } t) = \boxed{\f{(dt)^m e^{-dt}}{m!}}
	\end{align*} 
	Glass is not covered if there is no deposition, i.e., $m=0$. The fraction of the glass not covered by the atoms is the probability of zero deposition:
	\begin{align*}
	\Pr(0 \text{ atoms in time } t) = \boxed{e^{-dt}}
	\end{align*}
	We see that the fraction of the glass not covered decreases exponentially in time. 
	
	
	\item From Part (a), we know that the average thickness is $\langle x \rangle = dt$. To find the variance we need to compute the second moment:
	\begin{align*}
	\langle x^2 \rangle = \sum_{i=0}^\infty x^2 \f{(dt)^x e^{-dt}}{x!}= dt + d^2t^2.
	\end{align*}
	Therefore, the variance in thickness is 
	\begin{align*}
	\sigma^2 = \langle x^2 \rangle - \langle x \rangle^2 = \boxed{dt}
	\end{align*}
\end{enumerate}



\noindent \textbf{2. Semi-flexible polymer in two dimensions. } 

\begin{enumerate}[label=(\alph*)]
	\item It's nicer to work with the $\phi$-dependent $\ham$, so let us write $\mathbf{t}_m \cdot \mathbf{t}_n$ in terms of angles:
	\begin{align*}
	\mathbf{t}_m \cdot \mathbf{t}_n = 	a^2 \cos(\theta_m + \theta_{m+1} + \dots + \theta_{n-1}).
	\end{align*}
	The summed form for the angles is not very convenient to work with, as there is no clear way to find $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ written in this form. Instead, let us find $a^2 \langle \exp(i(\theta_m + \theta_{m+1}+\dots + \theta_{n-1})) \rangle $ and then take the real part. This, written in this form, is still cumbersome. However, we may assume that the angles $\phi_i$ are independent, and therefore 
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = \Re \lb a^2 \langle \exp(i(\theta_m + \theta_{m+1}+\dots + \theta_{n-1})) \rangle \rb = \Re \lb a^2\prod^{n-1}_{j=m} \langle e^{i\phi_j} \rangle \rb = a^2 \prod^{n-1}_{j=m} \langle \cos\phi_j \rangle. 
	\end{align*}
	Moreover, since the angles $\phi_i$'s are independent, the probability for each configuration is simply the product of the individual probabilities:
	\begin{align*}
	\Pr (\phi_1,\dots,\phi_{N-1}) = \exp\lb \f{a^2 \kappa}{k_BT} \sum_{i=1}^{N-1} \cos\phi_i\rb = \prod^{N-1}_{i=1}\exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb .
	\end{align*}
	And so we may write
	\begin{align*}
	\Pr (\phi_i) = \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i \rb
	\end{align*}
	
	With this we have
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = a^2 \prod^{n-1}_{j=m}  \f{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb}{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb} = a^2 \lc \f{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}   \rc^{\abs{n-m}}.
	\end{align*}
	So $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ has the form $a^2 [f(T)]^{\abs{n-m}}$ where $f(T)$ is the fraction in the curly brackets. We may write $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ as an exponential: 
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = a^2 \exp\lb \abs{n-m}\ln f(T) \rb = a^2 \exp\lb \f{\abs{n-m}}{1/\ln f(T)} \rb \equiv a^2 \exp\lb \f{-\abs{n-m}}{\xi} \rb,
	\end{align*}
	as desired. The persistence length is thus
	\begin{align*}
	\boxed{l_p = a\xi = \f{a}{-\ln f(T)} = \f{a}{\ln \lb \f{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}   \rb}}
	\end{align*}
	
	
	
	\item By definition, we have
	\begin{align*}
	\mathbf{R} = \sum_{i=1}^N \mathbf{t}_i \implies \langle R^2 \rangle = \langle \mathbf{R}\cdot \mathbf{R} \rangle = \sum_{m,n=1}^{N} \langle \mathbf{t}_m, \mathbf{t}_n \rangle 
	= \sum_{m,n=1}^{N} a^2 \exp\lb \f{-\abs{n-m}}{\xi} \rb.
	\end{align*}
	Now we consider what happens when $N\to \infty$. We see that $\mathbf{R}$ has the form 
	\begin{align*}
	\langle R^2 \rangle = a^2 \lb N  + N_1e^{-1/\xi} + N_2 e^{-2/\xi} + N_3 e^{-3/\xi} + \dots  \rb 
	\end{align*}   
	where $N_1,N_2,N_3,\dots$ are natural numbers. In the limit $N\to \infty$, we have $N_j\approx 2N$ for small $j$'s (swapping $n,m$ gives an extra factor of $2$), and $N_j$'s for large $j$'s don't really matter because of the exponential decay $e^{-j/\xi}$. So, we may very well write this as 
	\begin{align*}
	\langle R^2 \rangle \approx a^2 \lb N + 2N \lp e^{-1/\xi} + e^{-2/\xi} + e^{-3/\xi} + \dots \rp \rb 
	\end{align*}
	\textcolor{blue}{To see this, make a table with $n$'s on the columns and $m$'s on the rows respectively. The content of the table will be $\abs{n-m}$. Obviously, the diagonal contains all $N$ zeros since that's where $n=m$. The adjacent off-diagonals contain terms with $\abs{n-m}=1$, followed by $\abs{n-m}=2$, and so on. Since there are two such off-diagonals for each value of $\abs{n-m}$, the multiplicity for each of the terms is $2N$. When we are sufficiently far off from the diagonal, the exponential decay tells us that we don't have to worry about them. }\\
	
	
	In any case, we now recall the telescoping sum formula
	\begin{align*}
	\f{x}{1-x} = x+x^2+x^3 + \dots
	\end{align*}
	which gives us a more compact formula for the asymptotic behavior of $\langle R^2 \rangle$:
	\begin{align*}
	\boxed{\langle R^2 \rangle = a^2 N \lp 1 + 2\f{e^{-1/\xi}}{1-e^{-1/\xi}} \rp,\quad\quad N\to \infty }
	\end{align*}
	
	
	\item $\mathbf{R}$ is a sum of iid's $\mathbf{t}_i$. In view of the central limit theorem, $p(\mathbf{R})$ is a Gaussian. To determine the form of $p(\mathbf{R})$, we must find the first and second moments. Since each $\mathbf{t}_i$ is  random, we can conclude that $\langle \mathbf{R} \rangle = 0$. The second moment is given by Part (b), and so the variance of this distribution is $\sigma^2 = \langle R^2 \rangle - 0 = \langle R^2 \rangle $ which is what we found in Part (b). To find the normalization constant, we look at the covariance matrix $C$. Its determinant $|\det(C)|$ will be the product of $\langle R_x^2\rangle$ and $\langle R_y^2 \rangle$, each of which is $\langle R^2 \rangle /2$ (by symmetry, and the fact that variances of independent variables add). With these, 
	\begin{align*}
	p(\mathbf{R}) = \f{1}{\sqrt{(2\pi)^2 |\det(C)|}} \exp \lp -\f{\mathbf{R}^\top C^{-1} \mathbf{R}}{ 2} \rp = \boxed{\f{1}{\pi \langle R^2 \rangle} \exp\lp -\f{\mathbf{R}\cdot \mathbf{R}}{\langle R^2 \rangle} \rp}
	\end{align*}
	where we have used the fact that $C = \langle R^2 \rangle\mathbb{I}/2$. 
	
	\item We shall ``formally'' consider the modified probability weight: 
	\begin{align*}
	\exp(-\ham /k_BT) \to \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT).
	\end{align*} 
	Taking the average of $\mathbf{R}$ under these new weights yields
	\begin{align*}
	\langle \mathbf{R} \rangle = \f{\int \mathbf{R} \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT) }{\int \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT)}.
	\end{align*}
	We may treat $\mathbf{R}\exp(\mathbf{F}\cdot\mathbf{R}/k_BT)$ and $\exp(\mathbf{F}\cdot \mathbf{R}/k_BT)$ as input functions whose averages we wish to find and write
	\begin{align*}
	\langle R \rangle = \f{\langle \mathbf{R} \exp(\mathbf{F}\cdot \mathbf{R}/k_BT) \rangle' }{\langle \exp(\mathbf{F}\cdot \mathbf{R}/k_BT) \rangle' }
	\end{align*}
	where the new average $\langle \cdot \rangle'$ are essentially averages for when $\mathbf{F} = 0$. The denominator becomes unity, while the numerator can be expanded as
	\begin{align*}
	\langle \mathbf{R}_i \exp(\mathbf{F}\cdot \mathbf{R}/k_BT)\rangle' \approx \langle \mathbf{R}_i \rangle' 
	+ \f{\langle \mathbf{R}_i \mathbf{F}\cdot \mathbf{R}\rangle'}{k_BT} + \f{\langle \mathbf{R}_i (\mathbf{F}\cdot \mathbf{R})^2 \rangle' }{2k_BT} + \f{\langle \mathbf{R}_i(\mathbf{F}\cdot \mathbf{R})^3\rangle'}{6k_BT} + \dots 
	\end{align*}
	where $\mathbf{R}_i$'s are the components of $\mathbf{R}$ (i.e., $i$ is $x$ and $y$), and that $\langle R \rangle$ at $\mathbf{F} =0$ is simply $\langle R^2\rangle$ which we already know. Moreover, terms with odd-powered $\mathbf{R}$'s will vanish by symmetry. We are thus interested in the term 
	\begin{align*}
	\langle \mathbf{R}_i \mathbf{F}\cdot \mathbf{R}\rangle' = \langle \mathbf{R}_i \mathbf{F}_j \mathbf{R}_j\rangle' = \mathbf{F}_j \langle \mathbf{R}_i \mathbf{R}_j \rangle' = \mathbf{F}_j \delta_{ij} \langle R^2 \rangle /2 = \mathbf{F}_i \langle R^2 \rangle/2.
	\end{align*}
	With this we have
	\begin{align*}
	\boxed{\langle \mathbf{R} \rangle = \f{\langle R^2 \rangle}{2k_BT} \mathbf{F} + \mathcal{O}(F^3) = K^{-1} \mathbf{F} + \mathcal{O}(F^3) , \quad\quad K = \f{2k_B T}{\langle R^2 \rangle}}
	\end{align*}
\end{enumerate}




\noindent \textbf{3. Foraging.} We have 
\begin{align*}
p(r|t) = \f{r}{2Dt} \exp\lp -\f{r^2}{4Dt} \rp\quad
\text{and}\quad
p(t) \propto \exp\lp- \f{t}{\tau} \rp.
\end{align*}
Normalizing $p(t)$ give the leading factor equal $1/\tau$. So, we can write
\begin{align*}
p(t) = \f{1}{\tau} \exp\lp - \f{t}{\tau} \rp.
\end{align*}
The (unconditional) probability of finding the searcher at a distance $r$ from the nest is 
\begin{align*}
p(r) = \int_0^\infty p(r|t)p(t)\,dt = \int_0^\infty \f{r}{2D\tau t} \exp\lp -\f{r^2}{4Dt} - \f{t}{\tau} \rp\,dt.
\end{align*}
We may compute this using saddle-point approximation. Let $f(t)= r^2/4Dt + t/\tau$. Then we see that $f$ attains a maximum at $t_0 = r\sqrt{\tau}/2\sqrt{D}$. Let $g(t) = r/2D\tau t$. The saddle point approximation reads
\begin{align*}
p(r)\approx e^{-f(t_0)} g(t_0)\sqrt{\f{2\pi}{f''(t_0)}} = \exp\lp \f{-r}{\sqrt{D\tau}} \rp \f{1}{\sqrt{D}\tau^{3/2}} \sqrt{\f{2\pi r t^{3/2}}{4\sqrt{D}}} \implies
\boxed{p(r) \propto \sqrt{r}\exp\lp \f{-r}{\sqrt{D\tau}} \rp}
\end{align*}
We could also say that asymptotically the exponential decay in $r$ dominates over the $\sqrt{r}$ growth, and so in the large $r$ limit, $p(r)\sim \exp(-r/\sqrt{D\tau})$.







\noindent \textbf{4. Jensen's inequality and Kullback–Liebler divergence. }

\begin{enumerate}[label=(\alph*)]
	\item \underline{Claim:} Jensen's inequality: For a convex function $\langle f(x) \rangle \geq f(\langle x \rangle)$. To prove this, we let a probability density function $p(x)$ be given. Then 
	\begin{align*}
	\langle f(x) \rangle = \int p(x) f(x)\,dx \geq \int p(x) \lb f(\langle x \rangle ) + f'(\langle x \rangle)(x-\langle x \rangle) \rb \,dx = f(\langle x\rangle) + f'(\langle x \rangle) \langle x - \langle x \rangle \rangle = f(\langle x \rangle).
	\end{align*}
	And we're done.
	
	
	\item We see that $D(p|q)$ is an expectation taken with respect to $p(x)$. The proof uses Jensen's inequality with the fact that the function $-\ln(x)$ is convex:
	\begin{align*}
	D(p|q) &= \bigg\langle \ln \f{p}{q} \bigg\rangle_p \\
	&= \bigg \langle - \ln \f{q}{p}  \bigg\rangle_p \\
	&\geq -\ln \bigg\langle \f{q}{p} \bigg\rangle_p \\
	&= -\ln \lb \int p(x) \f{q(x)}{p(x)}\,dx \rb = -\ln 1 =  0.
	\end{align*}
\end{enumerate}


\noindent \textbf{5. The book of records.} 

\begin{enumerate}[label=(\alph*)]
	\item Suppose we have picked $n$ entries $\{x_1,\dots,x_n\}$. The probability that $x_n$ is the largest is actually the same as the probability that any other entry $x_i$ is the largest. Therefore, $\boxed{P_n = 1/n}$
	
	\item Since $S_N$ is the number of records after $N$ attempts, we may write $S_N$ as the sum of the indicators $R_i$. As a result,
	\begin{align*}
	\langle S_N \rangle = \sum^N_{m=1} \langle R_m \rangle = \sum^N_{m=1} (1\times P_m + 0\times (1-P_m)) = \sum^N_{m=1} P_m = \sum^N_{m=1}\f{1}{m}.
	\end{align*}
	We may estimate the growth of $\langle S_N \rangle$ by bounding it by two integrals. Choose the integral 
	\begin{align*}
	I = \int_0^N \f{1}{n}\,dn = \ln N.
	\end{align*}
	to be an estimate for $\langle S_N \rangle$. We see that $I$ is an under-estimation. However, we can find how much $\langle S_N \rangle$ differs from $\ln N$ by taking a limit:
	\begin{align*}
	\lim_{N\to \infty} \langle S_N \rangle - \ln N = \texttt{EulerGamma} \approx 0.5775216\dots
	\end{align*}
	where $\texttt{EulerGamma}$ is the output that I got from evaluating this limit in Mathematica. Therefore, we conclude that $\langle S_N \rangle$ grows like $\boxed{\ln N}$ in the $N\gg 1$ limit. If the number of trials double every year, then $N = N(t) = 2^t$. In this case,
	\begin{align*}
	\langle S_N \rangle \sim \ln N = \ln 2^t = \boxed{t\ln 2}
	\end{align*}
	asymptotically. 
	
	
	
	\item  By definition, $\langle R_n R_m \rangle = \langle R_n \rangle_c \langle R_m \rangle_c + \langle R_n R_m \rangle_c = \langle R_n \rangle \langle R_m \rangle + \langle R_n R_m \rangle_c$, so
	\begin{align*}
	\langle R_n R_m \rangle_c = \langle R_n R_m \rangle - \langle R_n \rangle \langle R_m \rangle = P_nP_m - P_nP_m = 0
	\end{align*}
	if $m\neq n$. When $m=n$ the covariance term becomes a variance term which is $P_n$. 
	
	
	\item \textbf{(Optional)} We want to compute all moments and the first three cumulants for $S_N$. To compute all moments, we may use the method of characteristic function. However, since there is no nice way to explicitly write out the expectation function, we will have to proceed as follows: Starting with $S_1 = R_1 = 1$, we have $\langle e^{-ik S_1}\rangle  = e^{-ik}$. Next, using the definition
	\begin{align*}
	\langle e^{-ik S_N} \rangle = \sum_{n=1}^N\Pr(n) e^{-ik n}
	\end{align*}
	we find 
	\begin{align*}
	&\langle e^{-ik S_2} \rangle = \f{1}{2}e^{-ik} + \f{1}{2}e^{-2ik} = \f{1}{2!}e^{-ik}(e^{-ik} + 1)\\
	&\langle e^{-ik S_3} \rangle = \f{1}{2}\f{2}{3} e^{-ik} + \lp \f{1}{2}\f{2}{3} + \f{1}{2}\f{1}{3}\rp e^{-2ik} + \f{1}{2}\f{1}{3}e^{-3ik} = \f{1}{3!}e^{-ik}(e^{-ik}+1)(e^{-ik}+2)\\
	&\langle e^{-ik S_4} \rangle = \f{1}{2}\f{2}{3}\f{3}{4} e^{-ik} + \lp \f{1}{2}\f{2}{3}\f{1}{4} + \f{1}{2}\f{1}{3}\f{3}{4} + \f{1}{2}\f{2}{3}\f{1}{4}\rp e^{-2ik} +\lp \f{1}{2}\f{1}{3}\f{1}{4}+\f{1}{2}\f{2}{3}\f{3}{4}  + \f{1}{2}\f{1}{3}\f{3}{4}\rp e^{-3ik} + \f{1}{2}\f{1}{3}\f{1}{4}e^{-4ik} \\
	&\quad\quad\quad\quad\quad\quad= \f{1}{4!}e^{-ik}(e^{-ik}+1)(e^{-ik}+2)(e^{-ik}+3)\\
	&\quad\quad\vdots
	\end{align*}
	where we have used generated the probabilities by writing out all the record-configurations within the sequence $\{ x_n\}$ and finding the probability for each outcome via $P_n = 1/n$. In any case, we see that a pattern emerges(!) and conclude without proving\footnote{The proof is probably closely related to how the probabilities are generated.} that 
	\begin{align*}
	\langle e^{-ik S_N} \rangle = \f{1}{N!} \prod_{n=1}^N (e^{-ik}+n-1) = \f{1}{N!} \lb e^{-ik}\rb^{\overline{N}},
	\end{align*}
	where the exponent $\overline{N}$ is used in combinatorics to denote the \textit{rising factorial}. \textcolor{purple}{A trip to Wikipedia\footnote{Never knew my undergrad MA355 would be useful for StatMech... I recognized this as something related to \textit{falling factorials} which are related to Stirling numbers. See Kenneth P. Bogart's \textit{Combinatorics Through Guided Discovery} for details about Stirling numbers as well as a change of basis for going from falling to rising factorials.}} tells us that this expression can be expressed in terms of the (unsigned) Stirling numbers of the first kind:
	\begin{align*}
	\langle e^{-ik S_N} \rangle = \f{1}{N!} \prod_{n=1}^N (e^{-ik}+n-1) = \f{1}{N!} \lb e^{-ik}\rb^{\overline{N}} = \sum_{n=0}^N { N\choose{n}}_S e^{-ikn}.
	\end{align*}
	Now it is trivial to find the moments of $S_N$: we simply expand the exponential into powers of $k$:
	\begin{align*}
	\langle e^{-ik S_N} \rangle = \sum_{n=0}^N { N\choose{n}}_S \sum_{m=0}^\infty \f{1}{m!} (-ikn)^m = \sum_{m=0}^\infty \f{(-ik)^m}{m!}\lb \f{1}{N!} \sum_{n=0}^N {N\choose{n}}_S n^m  \rb
	\end{align*}
	So, the $n$th moment of $S_N$ is given by the term in the square brackets:
	\begin{align*}
	\boxed{\langle S^m_N \rangle = \f{1}{N!} \sum_{n=0}^N {N\choose{n}}_S n^m }
	\end{align*}
	
	Next, since $R_i$'s are independent (in the statement of Part (c)), all correlation terms $\langle R_a R_b R_c\dot \rangle$ vanish. As a result, we have 
	\begin{align*}
	\langle S_N^m \rangle = \sum^N_{n=1} \langle R_n^m\rangle \iff  \langle S_N^m \rangle_c = \sum^N_{n} \langle R_n^m\rangle_c. 
	\end{align*}
	We simply compute, using the fact that $\langle R_n \rangle = 1/n$:
	\begin{align*}
	&\boxed{\langle S_N^1\rangle_c} = \sum^N_{n=1} \langle R_n^1\rangle_c = \boxed{\sum^N_{n=1}\f{1}{n}}\\
	&\boxed{\langle S_N^2\rangle_c} = \sum^{N}_{n=1} \langle R_n^2 \rangle_c  = 
	\sum^N_{n=1} (\langle R_n^2 \rangle - \langle R_n\rangle^2) = \sum^N_{n=1}\lb \lp 1^2\times \f{1}{n}+ 0^2\times \f{n-1}{n} \rp - \f{1}{n^2}\rb  = \boxed{\sum^N_{n=1}\lp\f{1}{n} - \f{1}{n^2}\rp}\\
	&\boxed{\langle  S_N^3 \rangle_c} = 
	\sum^{N}_{n=1} \langle R_n^3 \rangle_c  
	= \sum^N_{n=1} (\langle R_n^3 \rangle - 3\langle R_n^2\rangle\langle R_n\rangle+2\langle R_n\rangle^3) = \boxed{\sum^N_{n=1}\lp \f{1}{n} - \f{3}{n^2} + \f{2}{n^3}\rp}\\
	\end{align*}
	where in the last equality we have used
	\begin{align*}
	\langle R_n^3\rangle = 1^3\f{1}{n} + 0^3\f{n-1}{n} = \f{1}{n}.
	\end{align*}
	
	To answer the last point, since $S_N$ is not quite a sum of iid variables (the $R_n$'s have different distributions for different $n$'s.) and that its variance is not finite when $N\to \infty$, the central limit theorem does not/might not apply to $S_N$. \textcolor{red}{I'm not too sure about this.}
	
	
	
	\item \textbf{(Optional)} Suppose the second record occurs at location $m \in \{1,2,\dots,8\}$. Then, the probability for this event (in the total sample space) is 
	\begin{align*}
	P(n_2 = m) &=  P_1 (1-P_2)(1-P_3)\dots (1-P_{m-1})P_m (1+P_{m+1}) \dots P_9 \\
	&= \f{1}{2}\f{2}{3}\f{3}{4}\dots \f{m-2}{m-1}\f{1}{m}\f{m}{m+1}\dots\f{1}{9}\\
	&= \f{1}{8\times 9\times (m-1)}\\
	&= \f{1}{72(m-1)}
	\end{align*}
	where in the third equality we have used the fact that multiplying the second line by $8(m-1)$ gives $1/9$. To use this probability in the $n_2$ sample space, we have to first normalize it:
	\begin{align*}
	\mathcal{N} = \sum^8_{m=2}\f{1}{72(m-1)} = \f{121}{3360}.
	\end{align*}
	Therefore,
	\begin{align*}
	\boxed{\langle{n_2}\rangle} = \f{1}{\mathcal{N}}\sum^N_{m=1} \f{m}{72(m-1)} =  \f{1343}{363}\approx \boxed{3.7}
	\end{align*}
	
	Let the third record to fixed at some value. In order for some number $x_m$ to be the fourth record, it has to be bigger than the third record, and all numbers from $x_{m-1}$ to $x_{10}$ inclusive must be less than the third record. Let $p$ denote the probability that we pick a number bigger than the third record, then the probability that a number $x_m$ is the fourth record is 
	\begin{align*}
	(1-p)^{m-10}p. 
	\end{align*}
	So, we have
	\begin{align*}
	\langle n_4 \rangle_\text{3rd record fixed} = \sum^\infty_{m=10}   (1-p)^{m-10}p = 9 + \f{1}{p}
	\end{align*}
	using Mathematica. We now want to take the average of this quantity over all possible values of the third record. To do this, let us assume that the numbers are picked from some distribution $\psi$, so that we can write $p$ in terms of the cumulative density function associated with $\psi$, evaluated at the value of the third record. Then 
	\begin{align*}
	\langle n_4 \rangle = \langle  \langle n_4 \rangle_\text{3rd record fixed} \rangle_\text{all third records} = \int_{-\infty}^\infty \,dx\,\psi(x)\lp 9 + \f{1}{p(x)} \rp.
	\end{align*}
	Since $p(x) = 1-\int_{-\infty}^x \psi(x)\,dx$, we have $-\psi(x)\,dx = dp(x)$ by the fundamental theorem of calculus. With this, we have
	\begin{align*}
	\langle n_4 \rangle = \int^1_0 dp\lp 9 + \f{1}{p} \rp 
	\end{align*}
	which diverges. So $\langle n_4 \rangle$ is infinite, as expected. 
	
\end{enumerate}


\noindent \textbf{6. Jarzynski equality.} 


\begin{enumerate}[label=(\alph*)]
	\item The new probability density function $p_f(W(\mu))$ is obtained from $p(\mu)$ via a change of variable transformation for which the rule is 
	\begin{align*}
	p_f(W(\mu))\, \abs{\f{dW}{d\mu}} = p(\mu)
	\end{align*}
	where $|dW/d\mu|$ is the Jacobian. Similarly, we have
	\begin{align*}
	p_b(-W(\mu'))\, \abs{\f{-dW}{d\mu'}} = p'(\mu').
	\end{align*}
	Since there is no real thermodynamic reason for $d\mu \neq d\mu'$, we must have
	\begin{align*}
	\f{p_f(W)}{p_b(-W)} = \f{p(\mu)}{p'(\mu')} = \f{\mathcal{Z}'}{\mathcal{Z}} \f{\exp(-\be\ham(\mu))}{\exp(-\be\ham(\mu)) \exp(-\be W(\mu))} = \exp[\be(W + F - F')], 
	\end{align*}
	as desired, where we have used $\ln \mathcal{Z} = -\be F$. 
	
	
	
	\item Let $\Delta F = F'-F$. From Part (a) we have
	\begin{align*}
	p_f(W) e^{-\be W} = p_b(-W)e^{-\be \Delta F} \implies \langle e^{-\be W}\rangle = \int p_f(W) e^{-\be W}\,dW = e^{-\be \Delta F} \int p_b(-W)\,dW = e^{-\be \Delta F}.
	\end{align*}
	As a result, 
	\begin{align*}
	\Delta F = -\f{1}{\be}\ln \langle e^{-\be W} \rangle = -k_BT \ln \langle e^{-\be W} \rangle  = 
	-k_BT \ln \lb \int p_f(W) e^{-\be W}\,dW  \rb,
	\end{align*}
	as desired.
	
	
	\item We use Jensen's inequality and the fact that the function $f(W) = e^{-\be W}$ is convex:
	\begin{align*}
	\Delta F = -k_BT \ln \langle e^{-\be W} \rangle \leq -k_BT \ln e^{-\be \langle W \rangle}=  -k_BT \ln e^{-\langle W \rangle/k_BT}= \langle W \rangle.
	\end{align*}
	
	
	\item To do this problem we have to define a probability density function $\rho(\omega)$ associated with violating the second law. Given a particular $W$, the probability density for second law violation is $p_f(W-\omega)p_b(-W)$. Thus, $\rho(\omega)$ is in some sense a ``quasi-convolution'' of $p_f$ and $p_b$, obtained by integrating out $W$:
	\begin{align*}
	\rho(\omega) = \int p_f(W-\omega)p_b(-W)\,dW.
	\end{align*}  
	We wish to find $\Pr(\omega>0)$, which is given by 
	\begin{align*}
	\Pr(\omega>0) = \int_0^\infty \rho (\omega)\,d\omega.
	\end{align*}
	In order for a factor of $e^{-\be\omega}$ to pop up, we must make use of the fact that 
	\begin{align*}
	\f{p_f(W)}{p_b(-W)} = e^{\be(W+F-F')}
	\end{align*}
	In particular, 
	\begin{align*}
	\lp \f{p_f(W)}{p_b(-W)}\rp^{-1}  \f{p_f(W-\omega)}{p_b(-W+\omega)} = e^{-\be(W+F-F')} e^{\be(W-\omega+F-F')} = e^{-\be\omega}.
	\end{align*}
	As a result,
	\begin{align*}
	p_b(-W) p_f(W-\omega) = e^{-\be\omega} p_f(W) p_b(-W+\omega) \implies \rho(\omega) = e^{-\be\omega} \int p_f(W) p_b(-W+\omega)\,dW.
	\end{align*}
	By shifting the integration $dW \to d(W+\omega)$, we find that
	\begin{align*}
	\rho(\omega) = e^{-\be \omega} \int p_f(W+\omega) p_b(-W)\, d(W+\omega) =  e^{-\be \omega} \int p_f(W+\omega) p_b(-W)\, dW = e^{-\be W} \rho(-\omega).
	\end{align*}
	Therefore, 
	\begin{align*}
	\Pr(\omega>0) = e^{-\be W} \int_0^\infty \rho(-\omega)\,d\omega \leq e^{-\be W} \underbrace{\int_{-\infty}^\infty p(-\omega)\,d\omega}_{=1} \leq e^{-\be W} = e^{-\be W},
	\end{align*}
	as desired, where we have used the fact that the cumulative probability is bounded above by 1. 
	
\end{enumerate}


\noindent \textbf{7. (Optional) Dice. }


\begin{enumerate}[label=(\alph*)]
	\item The unbiased probabilities are such that the entropy is maximized. Since 6 appears twice as many times as 1, and that entropy must be maximized, we have 
	\begin{align*}
	p_6 = 2p_1 = 2a, \quad\quad p_2 = p_3 = p_4 = p_5 = (1- 3a)/4. 
	\end{align*}
	Now we compute:
	\begin{align*}
	S = -\sum_{i=1}^6 p_i \ln p_i = -a\ln a - 2a \ln 2a - 4\f{1-3a}{4} \ln \f{1-3a}{4} = 
	-a\ln a - 2a \ln 2a - (1-3a) \ln \f{1-3a}{4}
	\end{align*}
	To extremize $S$, we find $dS/da$:
	\begin{align*}
	\f{dS}{da} = -8\ln 2 + 3\ln (1-3a) - 3\log a = 3\ln \lb \f{1-3a}{2^{8/3}a} \rb = 0 \iff 1-3a = 2^{8/3}a \implies a = p_1 = \f{1}{2^{8/3}+3}
	\end{align*}
	this is an local maximum because $dS/da$ is monotonically decreasing and crosses zero at $a = 1/(2^{8/3}+3)$.  We can now calculate the rest of the probabilities:
	\begin{align*}
	\boxed{p_6 = 2p_1 = \f{2}{2^{8/3}+3}, \quad\quad p_2 = p_3 = p_4 = p_5 = \f{1}{4}\lb 1-\f{3}{2^{8/3}+3} \rb = \f{2^{2/3} }{2^{8/3}+3} }
	\end{align*}
	
	\item The information content is the difference in entropy of a fair dice and this one. 
	\begin{align*}
	I = S_\text{fair} - S_\text{loaded}= - 6\times \f{1}{6}\log_2 \f{1}{6} - \lb -a_0\log_2 a_0 - 2a_0 \log_2 2a_0 - (1-3a_0) \log_2 \f{1-3a_0}{4} \rb
	\end{align*}
	where we have defined $I$ so that it is positive (the entropy associated with a fair dice is maximal) and $a_0 = 1/(2^{8/3}+3)$. With the help of Mathematica, we find 
	\begin{align*}
	\boxed{I \approx 0.0267\dots}
	\end{align*}
	Mathematica code:
	\begin{lstlisting}
	In[13]:= a0 = 1/(2^(8/3) + 3);
	
	In[17]:= N[-Log2[1/6] - (-a0*Log2[a0] - 
	2*a0*Log2[2*a0] - (1 - 3*a0)*Log2[(1 - 3*a0)/4])]
	
	Out[17]= 0.0267239
	\end{lstlisting}
	
	

\end{enumerate}


\noindent \textbf{8. (Optional) Approach to Equilibrium}
\begin{enumerate}[label=(\alph*)]
	\item By time-translation invariance we have
	\begin{align*}
	C_{ij}(t) = C_{ij}(t+\tau) = \langle x_i(t+\tau) x_j (\tau) \rangle.
	\end{align*}
	Picking $\tau = -t$, we find 
	\begin{align*}
	C_{ij}(t) = \langle x_i(0)x_j(-t)\rangle.
	\end{align*}
	By time-reversal invariance we have
	\begin{align*}
	C_{ij}(t) = C_{ij}(-t) = \langle  x_i (0) x_j(t) \rangle = \langle  x_j (t) x_i(0) \rangle = C_{ji}(t),
	\end{align*}
	as desired. 
	
	\item As appeared in the form 
	\begin{align*}
	\sqrt{\f{\det(K)}{(2\pi)^{n}}} \exp\lb -\f{1}{2}K_{ij} x_i x_j \rb,
	\end{align*}
	the matrix $[K]$ is the inverse of the covariance matrix associated with this Gaussian, i.e., 
	\begin{align*}
	[K^{-1}]_{ij} = \langle (x_i -\langle x_i \rangle)(x_j - \langle x_j \rangle) \rangle 
	\end{align*}
	On the other hand, $C_{ij}(0) = \langle x_i(0)x_j(0) \rangle = \langle x_i x_j \rangle $ forms the autocorrelation matrix. The two matrices are related by the well-known identity in statistics:
	\begin{align*}
	\boxed{[K]^{-1} = [C(0)] - \langle \mathbf{x}\rangle \langle \mathbf{x}\rangle^\top \implies C[0] = [K]^{-1} + \langle \mathbf{x}\rangle \langle \mathbf{x}\rangle^\top}
	\end{align*}
	where $\langle \mathbf{x}\rangle^\top = (\langle x_1 \rangle, \langle x_2 \rangle \dots  )^\top$. 
	
	
	
	\item Given $J_\al = -\p \ln p(\mathbf{x})/\p x_\al$, we may compute:
	\begin{align*}
	J_\al = -\f{\p}{\p x_\al}\ln \lc \sqrt{\f{\det(K)}{(2\pi)^{n}}}  \exp\lb -\f{1}{2}K_{ij} x_i x_j \rb \rc = -\f{\p}{\p x_\al} \lb -\f{1}{2} K_{ij}x_ix_j \rb = K_{\al j} x_j.
	\end{align*}
	So, 
	\begin{align*}
	\langle J_\al x_\be \rangle = \langle K_{\al j} x_j x_\beta\rangle.
	\end{align*}
	In order to compute $\langle x_j x_\be$, we recall the definition for $\langle x_j \rangle$:
	\begin{align*}
	\langle x_j \rangle = \sqrt{\f{\det(K)}{(2\pi)^{n}}}\int \dots \int x_j \exp\lb -\f{1}{2}K_{\al\be}x_\al x_\be \rb  \, \prod_m dx_m.
	\end{align*}
	By the form of the given Gaussian, we see that $\langle x_j \rangle = 0$. However, let us pretend that $\langle x_j \rangle = \eta_j \neq 0$, so that we can parameterize the Gaussian with these $\eta_j$'s as follows:
	\begin{align*}
	\langle x_j \rangle = \eta_j = \sqrt{\f{\det(K)}{(2\pi)^{n}}}\int \dots \int x_j \exp\lb -\f{1}{2}K_{\al\be}(x_\al - \eta_\al)(x_\be  - \eta_\be)\rb  \, \prod_m dx_m.
	\end{align*}
	Now we use Feynman's trick of differentiating under the integral sign with respect to $\eta_k$ and re-setting $\eta_j=0$ for all $j$ to find that 
	\begin{align*}
	\f{\p}{\p \eta_k} \eta_j = \sqrt{\f{\det(K)}{(2\pi)^{n}}}\int \dots 
	\int x_j K_{k \al}x_\al 
	\exp\lb -\f{1}{2}K_{\al\be}(x_\al - \eta_\al)(x_\be  - \eta_\be)\rb  \, \prod_m dx_m = \langle x_j K_{k\al}x_\al\rangle.
	\end{align*}
	Thus, we see that 
	\begin{align*}
	\langle J_\al x_\be\rangle = \langle K_{\al j} x_j x_\be \rangle = \f{\p}{\p \eta_\al} \eta_{\be} = \delta_{\al\be}, 
	\end{align*}
	as desired.
	
	
	\item Starting with $C_{ij}(t) = C_{ji}(t)$, we differentiate both sides with respect to $t$ to find:
	\begin{align*}
	\f{d}{dt}\langle x_i(t)x_j(0)\rangle = \f{d}{dt}\langle x_j(t)x_i(0)\rangle 
	&\implies \langle \dot x_i(t)x_j(0)\rangle = \langle \dot x_j(t)x_i(0)\rangle \\
	&\implies \langle \mu_{i\al}K_{\al\be}x_\be(t)x_j(0) \rangle = \langle \mu_{j\al}K_{\al\be}x_\be(t)x_i(0) \rangle.
	\end{align*}
	Setting $t=0$, we get
	\begin{align*}
	\langle \mu_{i\al}K_{\al\be}x_\be(0)x_j(0) \rangle = \langle \mu_{j\al}K_{\al\be}x_\be(0)x_i(0) \rangle 
	&\implies \langle \mu_{i\al}K_{\al\be}x_\be x_j \rangle = \langle \mu_{j\al}K_{\al\be}x_\be x_i \rangle \\
	\text{Definition} \quad &\implies \mu_{i\al } \langle J_\al x_j \rangle = \mu_{j \al} \langle J_\al x_i\rangle \\
	\text{Part (c)}\quad  &\implies \mu_{i\al}\delta_{\al j} = \mu_{j\al} \delta_{\al i}\\
	&\implies \mu_{ij} = \mu_{ji}.
	\end{align*}
	So, the matrix $\mu$ must be symmetric, as desired. 
\end{enumerate}

\noindent \textbf{9. (Optional) Simpson's Paradox}


\begin{enumerate}[label=(\alph*)]
	\item \textcolor{blue}{Shouldn't the question be: ``Show that the country A
	as a whole can still end up with lower cfrs if its population is \textcolor{red}{younger}.'', as  this would seem paradoxical given that $p_o^A > p_o^B > p_y^A > p_o^B$?}\\


	To answer this modified question, I could just give an example. Let $p_o^A = 0.9$, $p_o^B = 0.7$, $p_y^A = 0.5, p_y^B = 0.3$. We would expect that since $A$ has higher fatality rates than $B$ across all age groups, it would have a higher overall fatality rate. However, this assumption ignores the effect of population. Suppose that $A$ has $10$ olds and $90$ youngs,  while $B$ has $90$ olds and $10$ youngs, then we find that
	\begin{align*}
	p^A = \f{10\times 0.9 + 90\times 0.5}{100} = 54\%
	\end{align*}
	whereas
	\begin{align*}
	p^B = \f{90\times 0.7 + 10\times 0.3}{100} = 66\% > p_A.
	\end{align*}
	\textcolor{purple}{Note that is it impossible for $A$ to have lower overall fatality rate than $B$ if its population is older. Let $\eta_A$ denote the old/total ratio in $A$ and $\eta_B$ in $B$. Then the overal rates for the countries are defined by two lines
	\begin{align*}
	r_A(\eta_A) = \eta_A p_o^A + (1-\eta_A) p_y^A &= p_y^A + \eta_A(p_o^A - p_y^A)\\ 
	r_B(\eta_B) = \eta_B p_o^B + (1-\eta_B) p_y^B &= p_y^B + \eta_B(p_o^B - p_y^B)
	\end{align*}
	defined for $\eta_A,\eta_B\in [0,1]$. It is obvious that the line corresponds to the overall rate of $A$ lies strictly above the line associated with $B$. Therefore, for any $\eta$, we have
	\begin{align*}
	r_A(\eta) - r_B(\eta) = (p_y^A - p_y^B) + \eta[(p_o^A - p_o^B) - (p_y^A - p_y^B)] = (1-\eta)(p_y^A - p_y^B) + \eta(p_o^A -p_o^B) > 0
	\end{align*}
	which means if the populations of $A$ and $B$ are similar (age-wise) then it is impossible for $A$ to have a smaller fatality rate. Now, suppose we make $A$ even older, i.e., $\eta_A > \eta_B$ then the fatality rate for $B$ is $r_B(\eta_B) < r_B(\eta_A) < r_A(\eta_A)$ where we have used the fact that the slope of $r_B(\eta)$ is positive. As a result, it is actually \textbf{not possible} for $A$ to have a lower fatality rate if its population is \textbf{older.}  }
	
	\textcolor{purple}{From this we can also  conclude that $r_A < r_B$ can occur only if $\eta_A < \eta_B  $, or $A$ is younger than $B$. }
	
	
	
	\item For simplicity we may consider two pairs of engines, two Carnot ($C_1,C_2$) and two non-Carnot ($N_1, N_2$). In view of Part (a), let us define the efficiencies for $C_1,C_2,N_1,N_2$ as $\eta_{C_1}, \eta_{C_2}, \eta_{N_1}, \eta_{N_2}$ such that 
	\begin{align*}
	\eta_{C_1} > \eta_{N_1} >  \eta_{C_2} > \eta_{N_2}.
	\end{align*}
	Here, $\eta_{C_i} > \eta_{N_i}$ the Carnot and non-Carnot engines in each pair share the same input and output. The Carnot engines receive some quantity $Q$ of heat, and similarly do the non-Carnot engines. However, since we don't know how $Q$ is shared within the engines, we may call $\lambda_C$ and $\lambda_N$ the distribution ratios within the Carnot and non-Carnot engines respectively. Then, the work done by the Carnot and non-Carnot engines are
	\begin{align*}
	W_C &= Q  [ \lambda_C \eta_{C_1} + (1-\lambda_C) \eta_{C_2}] \\
	W_N &= Q  [ \lambda_N \eta_{N_1} + (1-\lambda_N) \eta_{N_2}]
	\end{align*}
	By virtue of the analys in Part (a), we may consider the case where 
	\begin{align*}
	\eta_{C_1} = 0.9 \quad\quad \eta_{N_1} = 0.7 \quad\quad \eta_{C_2} = 0.5\quad\quad \eta_{N_2} = 0.3
	\end{align*}
	and $\lambda_C = 0.1, \lambda_N = 0.9$. We find
	\begin{align*}
	W_C = 0.54 Q < 0.66 Q = W_N. 
	\end{align*}
	So, the non-Carnot engines actually produce more work! While this seems to violate the second law of thermodynamics, it doesn't. The apparent paradox fails to account for the confounding variable which is the distribution of the input heat $Q$ across the engines. While $Q$ is given to both sets of engines, it is entirely possible that it is simultaneous absorbed by the most efficient non-Carnot engine and least efficient Carnot engine. And since we have made our engines so that $\eta_{C_2} < \eta_{N_1}$, this scenario is thermodynamically allowed. 
\end{enumerate}

 




\end{document}














