\documentclass{article}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{xcolor}
%\definecolor{colby}{rgb}{0.0, 0.0, 0.5}
\definecolor{MIT}{RGB}{163, 31, 52}
\usepackage[pdftex]{hyperref}
%\hypersetup{colorlinks,urlcolor=colby}
\hypersetup{colorlinks,linkcolor={MIT},citecolor={MIT},urlcolor={MIT}}  
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{newpxtext,newpxmath}
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\ham}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}
\def\dbar{{\mkern3mu\mathchar'26\mkern-12mu   d}}


\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}



\usepackage{enumitem}


\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}

%\hypersetup{colorlinks,urlcolor=colby}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}






\begin{document}
		\begin{framed}
			\noindent Name: \textbf{Huan Q. Bui}\\
			Course: \textbf{8.333 - Statistical Mechanics I}\\
			Problem set: \textbf{\#2}
		\end{framed}
	



\noindent \textbf{1. Random deposition.}


\begin{enumerate}[label=(\alph*)]
	\item Consider a site. Assume that the gold atoms arrive at this site over time via a Poisson process. The deposition rate is $d$ layers per second, so this Poisson process has rate $d$. Over time $t$, the average number of deposition at a site is $dt$. With this, we have
	\begin{align*}
	\Pr(m \text{ atoms in time } t) = \boxed{\f{(dt)^m e^{-dt}}{m!}}
	\end{align*} 
	Glass is not covered if there is no deposition, i.e., $m=0$. The fraction of the glass not covered by the atoms is the probability of zero deposition:
	\begin{align*}
	\Pr(0 \text{ atoms in time } t) = \boxed{e^{-dt}}
	\end{align*}
	We see that the fraction of the glass not covered decreases exponentially in time. 
	
	
	\item From Part (a), we know that the average thickness is $\langle x \rangle = dt$. To find the variance we need to compute the second moment:
	\begin{align*}
	\langle x^2 \rangle = \sum_{i=0}^\infty x^2 \f{(dt)^x e^{-dt}}{x!}= dt + d^2t^2.
	\end{align*}
	Therefore, the variance in thickness is 
	\begin{align*}
	\sigma^2 = \langle x^2 \rangle - \langle x \rangle^2 = \boxed{dt}
	\end{align*}
\end{enumerate}



\noindent \textbf{2. Semi-flexible polymer in two dimensions. } 

\begin{enumerate}[label=(\alph*)]
	\item It's nicer to work with the $\phi$-dependent $\ham$, so let us write $\mathbf{t}_m \cdot \mathbf{t}_n$ in terms of angles:
	\begin{align*}
	\mathbf{t}_m \cdot \mathbf{t}_n = 	a^2 \cos(\theta_m + \theta_{m+1} + \dots + \theta_{n-1}).
	\end{align*}
	The summed form for the angles is not very convenient to work with, as there is no clear way to find $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ written in this form. Instead, let us find $a^2 \langle \exp(i(\theta_m + \theta_{m+1}+\dots + \theta_{n-1})) \rangle $ and then take the real part. This, written in this form, is still cumbersome. However, we may assume that the angles $\phi_i$ are independent, and therefore 
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = \Re \lb a^2 \langle \exp(i(\theta_m + \theta_{m+1}+\dots + \theta_{n-1})) \rangle \rb = \Re \lb a^2\prod^{n-1}_{j=m} \langle e^{i\phi_j} \rangle \rb = a^2 \prod^{n-1}_{j=m} \langle \cos\phi_j \rangle. 
	\end{align*}
	Moreover, since the angles $\phi_i$'s are independent, the probability for each configuration is simply the product of the individual probabilities:
	\begin{align*}
	\Pr (\phi_1,\dots,\phi_{N-1}) = \exp\lb \f{a^2 \kappa}{k_BT} \sum_{i=1}^{N-1} \cos\phi_i\rb = \prod^{N-1}_{i=1}\exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb .
	\end{align*}
	And so we may write
	\begin{align*}
	\Pr (\phi_i) = \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i \rb
	\end{align*}
	
	With this we have
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = a^2 \prod^{n-1}_{j=m}  \f{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb}{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi_i\rb} = a^2 \lc \f{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}   \rc^{\abs{n-m}}.
	\end{align*}
	So $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ has the form $a^2 [f(T)]^{\abs{n-m}}$ where $f(T)$ is the fraction in the curly brackets. We may write $\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle$ as an exponential: 
	\begin{align*}
	\langle \mathbf{t}_m \cdot \mathbf{t}_n \rangle = a^2 \exp\lb \abs{n-m}\ln f(T) \rb = a^2 \exp\lb \f{\abs{n-m}}{1/\ln f(T)} \rb \equiv a^2 \exp\lb \f{-\abs{n-m}}{\xi} \rb,
	\end{align*}
	as desired. The persistence length is thus
	\begin{align*}
	\boxed{l_p = a\xi = \f{a}{-\ln f(T)} = \f{a}{\ln \lb \f{\int \,d\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}{\int \,d\phi \cos\phi \exp\lb \f{a^2\kappa}{k_BT} \cos\phi\rb}   \rb}}
	\end{align*}
	
	
	
	\item By definition, we have
	\begin{align*}
	\mathbf{R} = \sum_{i=1}^N \mathbf{t}_i \implies \langle R^2 \rangle = \langle \mathbf{R}\cdot \mathbf{R} \rangle = \sum_{m,n=1}^{N} \langle \mathbf{t}_m, \mathbf{t}_n \rangle 
	= \sum_{m,n=1}^{N} a^2 \exp\lb \f{-\abs{n-m}}{\xi} \rb.
	\end{align*}
	Now we consider what happens when $N\to \infty$. We see that $\mathbf{R}$ has the form 
	\begin{align*}
	\langle R^2 \rangle = a^2 \lb N  + N_1e^{-1/\xi} + N_2 e^{-2/\xi} + N_3 e^{-3/\xi} + \dots  \rb 
	\end{align*}   
	where $N_1,N_2,N_3,\dots$ are natural numbers. In the limit $N\to \infty$, we have $N_j\approx 2N$ for small $j$'s (swapping $n,m$ gives an extra factor of $2$), and $N_j$'s for large $j$'s don't really matter because of the exponential decay $e^{-j/\xi}$. So, we may very well write this as 
	\begin{align*}
	\langle R^2 \rangle \approx a^2 \lb N + 2N \lp e^{-1/\xi} + e^{-2/\xi} + e^{-3/\xi} + \dots \rp \rb 
	\end{align*}
	We now recall that 
	\begin{align*}
	\f{x}{1-x} = x+x^2+x^3 + \dots
	\end{align*}
	So, we have a rather compact formula for $\langle R^2 \rangle$:
	\begin{align*}
	\boxed{\langle R^2 \rangle = a^2 N \lp 1 + 2\f{e^{-1/\xi}}{1-e^{-1/\xi}} \rp,\quad\quad N\to \infty }
	\end{align*}
	
	
	\item $\mathbf{R}$ is a sum of iid's $\mathbf{t}_i$. In view of the central limit theorem, $p(\mathbf{R})$ is a Gaussian. To determine the form of $p(\mathbf{R})$, we must find the first and second moments. Since each $\mathbf{t}_i$ is  random, we can conclude that $\langle \mathbf{R} \rangle = 0$. The second moment is given by Part (b), and so the variance of this distribution is $\sigma^2 = \langle R^2 \rangle - 0 = \langle R^2 \rangle $ which is what we found in Part (b). To find the normalization constant, we look at the covariance matrix $C$. Its determinant $|\det(C)|$ will be the product of $\langle R_x^2\rangle$ and $\langle R_y^2 \rangle$, each of which is $\langle R^2 \rangle /2$ (by symmetry, and the fact that variances of independent variables add). With these, 
	\begin{align*}
	p(\mathbf{R}) = \f{1}{\sqrt{(2\pi)^2 |\det(C)|}} \exp \lp -\f{\mathbf{R}^\top C^{-1} \mathbf{R}}{ 2} \rp = \boxed{\f{1}{\pi \langle R^2 \rangle} \exp\lp -\f{\mathbf{R}\cdot \mathbf{R}}{\langle R^2 \rangle} \rp}
	\end{align*}
	where we have used the fact that $C = \langle R^2 \rangle\mathbb{I}/2$. 
	
	\item We shall ``formally'' consider the modified probability weight: 
	\begin{align*}
	\exp(-\ham /k_BT) \to \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT).
	\end{align*} 
	Taking the average of $\mathbf{R}$ under these new weights yields
	\begin{align*}
	\langle \mathbf{R} \rangle = \f{\int \mathbf{R} \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT) }{\int \exp(\mathbf{F}\cdot \mathbf{R} /k_BT) \exp(-\ham /k_BT)}.
	\end{align*}
	We may treat $\mathbf{R}\exp(\mathbf{F}\cdot\mathbf{R}/k_BT)$ and $\exp(\mathbf{F}\cdot \mathbf{R}/k_BT)$ as input functions whose averages we wish to find and write
	\begin{align*}
	\langle R \rangle = \f{\langle \mathbf{R} \exp(\mathbf{F}\cdot \mathbf{R}/k_BT) \rangle' }{\langle \exp(\mathbf{F}\cdot \mathbf{R}/k_BT) \rangle' }
	\end{align*}
	where the new average $\langle \cdot \rangle'$ are essentially averages for when $\mathbf{F} = 0$. The denominator becomes unity, while the numerator can be expanded as
	\begin{align*}
	\langle \mathbf{R}_i \exp(\mathbf{F}\cdot \mathbf{R}/k_BT)\rangle' \approx \langle \mathbf{R}_i \rangle' 
	+ \f{\langle \mathbf{R}_i \mathbf{F}\cdot \mathbf{R}\rangle'}{k_BT} + \f{\langle \mathbf{R}_i (\mathbf{F}\cdot \mathbf{R})^2 \rangle' }{2k_BT} + \f{\langle \mathbf{R}_i(\mathbf{F}\cdot \mathbf{R})^3\rangle'}{6k_BT} + \dots 
	\end{align*}
	where (1) $\mathbf{R}_i$'s are the components of $\mathbf{R}$ (i.e., $i$ is $x$ and $y$), and that $\langle R \rangle$ at $\mathbf{F} =0$ is simply $\langle R^2\rangle$ which we already know. Moreover, terms with odd-powered $\mathbf{R}$'s will vanish by symmetry. We are thus interested in the term 
	\begin{align*}
	\langle \mathbf{R}_i \mathbf{F}\cdot \mathbf{R}\rangle' = \langle \mathbf{R}_i \mathbf{F}_j \mathbf{R}_j\rangle' = \mathbf{F}_j \langle \mathbf{R}_i \mathbf{R}_j \rangle' = F_j \delta_{ij} \langle R^2 \rangle /2 = F_i \langle R^2 \rangle/2.
	\end{align*}
	With this we have
	\begin{align*}
	\boxed{\langle \mathbf{R} \rangle = \f{\langle R^2 \rangle}{2k_BT} \mathbf{F} + \mathcal{O}(F^3) = K^{-1} \mathbf{F} + \mathcal{O}(F^3) , \quad\quad K = \f{2k_B T}{\langle R^2 \rangle}}
	\end{align*}
\end{enumerate}




\noindent \textbf{3. Foraging.} We have 
\begin{align*}
p(r|t) = \f{r}{2Dt} \exp\lp -\f{r^2}{4Dt} \rp\quad
\text{and}\quad
p(t) \propto \exp\lp- \f{t}{\tau} \rp.
\end{align*}
Normalizing $p(t)$ give the leading factor equal $1/\tau$. So, we can write
\begin{align*}
p(t) = \f{1}{\tau} \exp\lp - \f{t}{\tau} \rp.
\end{align*}
The (unconditional) probability of finding the searcher at a distance $r$ from the nest is 
\begin{align*}
p(r) = \int_0^\infty p(r|t)p(t)\,dt = \int_0^\infty \f{r}{2D\tau t} \exp\lp -\f{r^2}{4Dt} - \f{t}{\tau} \rp\,dt.
\end{align*}
We may compute this using saddle-point approximation. Let $f(t)= r^2/4Dt + t/\tau$. Then we see that $f$ attains a maximum at $t_0 = r\sqrt{\tau}/2\sqrt{D}$. Let $g(t) = r/2D\tau t$. The saddle point approximation reads
\begin{align*}
p(r)\approx e^{-f(t_0)} g(t_0)\sqrt{\f{2\pi}{f''(t_0)}} = \exp\lp \f{-r}{\sqrt{D\tau}} \rp \f{1}{\sqrt{D}\tau^{3/2}} \sqrt{\f{2\pi r t^{3/2}}{4\sqrt{D}}} \implies
\boxed{p(r) \propto \sqrt{r}\exp\lp \f{-r}{\sqrt{D\tau}} \rp}
\end{align*}
We could also say that asymptotically the exponential decay in $r$ dominates over the $\sqrt{r}$ growth, and so in the large $r$ limit, $p(r)\sim \exp(-r/\sqrt{D\tau})$.



\newpage





\noindent \textbf{4. Jensen's inequality and Kullback–Liebler divergence. }

\begin{enumerate}[label=(\alph*)]
	\item \underline{Claim:} Jensen's inequality: For a convex function $\langle f(x) \rangle \geq f(\langle x \rangle)$. To prove this, we let a probability density function $p(x)$ be given. Then 
	\begin{align*}
	\langle f(x) \rangle = \int p(x) f(x)\,dx \geq \int p(x) \lb f(\langle x \rangle ) + f'(\langle x \rangle)(x-\langle x \rangle) \rb \,dx = f(\langle x\rangle) + f'(\langle x \rangle) \langle x - \langle x \rangle \rangle = f(\langle x \rangle).
	\end{align*}
	And we're done.
	
	
	\item We see that $D(p|q)$ is an expectation taken with respect to $p(x)$. The proof uses Jensen's inequality with the fact that the function $-\ln(x)$ is convex:
	\begin{align*}
	D(p|q) &= \bigg\langle \ln \f{p}{q} \bigg\rangle_p \\
	&= \bigg \langle - \ln \f{q}{p}  \bigg\rangle_p \\
	&\geq -\ln \bigg\langle \f{q}{p} \bigg\rangle_p \\
	&= -\ln \lb \int p(x) \f{q(x)}{p(x)}\,dx \rb = 0.
	\end{align*}
\end{enumerate}


\noindent \textbf{5. The book of records.} 

\begin{enumerate}[label=(\alph*)]
	\item Suppose we have picked $n$ entries $\{x_1,\dots,x_n\}$. The probability that $x_n$ is the largest is actually the same as the probability that any other entry $x_i$ is the largest. Therefore, $\boxed{P_n = 1/n}$
	
	\item Since $S_N$ is the number of records after $N$ attempts, we may write $S_N$ as the sum of the indicators $R_i$. As a result,
	\begin{align*}
	\langle S_N \rangle = \sum^N_{m=1} \langle R_m \rangle = \sum^N_{m=1} P_m = \sum^N_{m=1}\f{1}{m}.
	\end{align*}
	We may estimate the growth of $\langle S_N \rangle$ by bounding it by two integrals. Choose the integral 
	\begin{align*}
	I = \int_0^N \f{1}{n}\,dn = \ln N.
	\end{align*}
	to be an estimate for $\langle S_N \rangle$. We see that $I$ is an under-estimation. However, we can find how much $\langle S_N \rangle$ differs from $\ln N$ by taking a limit:
	\begin{align*}
	\lim_{N\to \infty} \langle S_N \rangle - \ln N = \texttt{EulerGamma} \approx 0.5775216\dots
	\end{align*}
	where $\texttt{EulerGamma}$ is the output that I got from evaluating this limit in Mathematica. Therefore, we conclude that $\langle S_N \rangle$ grows like $\boxed{\ln N}$ in the $N\gg 1$ limit. If the number of trials double every year, then $N = N(t) = 2^t$. In this case,
	\begin{align*}
	\langle S_N \rangle \sim \ln N = \ln 2^t = \boxed{t\ln 2}
	\end{align*}
	asymptotically. 
	
	
	
	\item  By definition, $\langle R_n R_m \rangle = \langle R_n \rangle_c \langle R_m \rangle_c + \langle R_n R_m \rangle_c = \langle R_n \rangle \langle R_m \rangle + \langle R_n R_m \rangle_c$
	\begin{align*}
	\langle R_n R_m \rangle_c = \langle R_n R_m \rangle - \langle R_n \rangle \langle R_m \rangle = P_nP_m - P_nP_m = 0.
	\end{align*}
	
	
	
	\item \textbf{(Optional)}
	
	
	\item \textbf{(Optional)}
\end{enumerate}


\noindent \textbf{6. Jarzynski equality.} 


\begin{enumerate}[label=(\alph*)]
	\item The new probability density function $p_f(W(\mu))$ is obtained from $p(\mu)$ via a change of variable transformation for which the rule is 
	\begin{align*}
	p_f(W(\mu))\, \abs{\f{dW}{d\mu}} = p(\mu)
	\end{align*}
	where $|dW/d\mu|$ is the Jacobian. Similarly, we have
	\begin{align*}
	p_b(-W(\mu'))\, \abs{\f{-dW}{d\mu'}} = p'(\mu').
	\end{align*}
	Since there is no real thermodynamic reason for $d\mu \neq d\mu'$, we must have
	\begin{align*}
	\f{p_f(W)}{p_b(-W)} = \f{p(\mu)}{p'(\mu')} = \f{\mathcal{Z}'}{\mathcal{Z}} \f{\exp(-\be\ham(\mu))}{\exp(-\be\ham(\mu)) \exp(-\be W(\mu))} = \exp[\be(W + F - F')], 
	\end{align*}
	as desired, where we have used $\ln \mathcal{Z} = -\be F$. 
	
	
	
	\item Let $\Delta F = F'-F$. From Part (a) we have
	\begin{align*}
	p_f(W) e^{-\be W} = p_b(-W)e^{-\be \Delta F} \implies \langle e^{-\be W}\rangle = \int p_f(W) e^{-\be W}\,dW = e^{-\be \Delta F} \int p_b(-W)\,dW = e^{-\be \Delta F}.
	\end{align*}
	As a result, 
	\begin{align*}
	\Delta F = -\f{1}{\be}\ln \langle e^{-\be W} \rangle = -k_BT \ln \langle e^{-\be W} \rangle  = 
	-k_BT \ln \lb \int p_f(W) e^{-\be W}\,dW  \rb,
	\end{align*}
	as desired.
	
	
	\item We use Jensen's inequality and the fact that the function $f(W) = e^{-\be W}$ is convex:
	\begin{align*}
	\Delta F = -k_BT \ln \langle e^{-\be W} \rangle \leq -k_BT \ln e^{-\be \langle W \rangle}=  -k_BT \ln e^{-\langle W \rangle/k_BT}= \langle W \rangle.
	\end{align*}
	
	
	\item To do this problem we have to define a probability density function $\rho(\omega)$ associated with violating the second law. Given a particular $W$, the probability density for second law violation is $p_f(W-\omega)p_b(-W)$. Thus, $\rho(\omega)$ is in some sense a ``quasi-convolution'' of $p_f$ and $p_b$:
	\begin{align*}
	\rho(\omega) = \int p_f(W-\omega)p_b(-W)\,dW.
	\end{align*}  
	We wish to find $\Pr(\omega>0)$, which is given by 
	\begin{align*}
	\Pr(\omega>0) = \int_0^\infty \rho (\omega)\,d\omega.
	\end{align*}
	In order for a factor of $e^{-\be\omega}$ to pop up, we must make use of the fact that 
	\begin{align*}
	\f{p_f(W)}{p_b(-W)} = e^{\be(W+F-F')}
	\end{align*}
	In particular, 
	\begin{align*}
	\lp \f{p_f(W)}{p_b(-W)}\rp^{-1}  \f{p_f(W-\omega)}{p_b(-W+\omega)} = e^{-\be(W+F-F')} e^{\be(W-\omega+F-F')} = e^{-\be\omega}.
	\end{align*}
	As a result,
	\begin{align*}
	p_b(-W) p_f(W-\omega) = e^{-\be\omega} p_f(W) p_b(-W+\omega) \implies \rho(\omega) = e^{-\be\omega} \int p_f(W) p_b(-W+\omega)\,dW.
	\end{align*}
	By shifting the integration $dW \to d(W+\omega)$, we find that
	\begin{align*}
	\rho(\omega) = e^{-\be \omega} \int p_f(W+\omega) p_b(-W)\, d(W+\omega) =  e^{-\be \omega} \int p_f(W+\omega) p_b(-W)\, dW = e^{-\be W} \rho(-\omega).
	\end{align*}
	Therefore, 
	\begin{align*}
	\Pr(\omega>0) = e^{-\be W} \int_0^\infty \rho(-\omega)\,d\omega \leq e^{-\be W} \underbrace{\int_{-\infty}^\infty p(-\omega)\,d\omega}_{=1} \leq e^{-\be W} = e^{-\be W},
	\end{align*}
	as desired, where we have used the fact that the cumulative probability is bounded above by 1. 
	
\end{enumerate}


\noindent \textbf{7. (Optional) Dice. }


\begin{enumerate}[label=(\alph*)]
	\item The unbiased probabilities are such that the entropy is maximized. Since 6 appears twice as many times as 1, and that entropy must be maximized, we have 
	\begin{align*}
	p_6 = 2p_1 = 2a, \quad\quad p_2 = p_3 = p_4 = p_5 = (1- 3a)/4. 
	\end{align*}
	Now we compute:
	\begin{align*}
	S = -\sum_{i=1}^6 p_i \ln p_i = -a\ln a - 2a \ln 2a - 4\f{1-3a}{4} \ln \f{1-3a}{4} = 
	-a\ln a - 2a \ln 2a - (1-3a) \ln \f{1-3a}{4}
	\end{align*}
	To extremize $S$, we find $dS/da$:
	\begin{align*}
	\f{dS}{da} = -8\ln 2 + 3\ln (1-3a) - 3\log a = 3\ln \lb \f{1-3a}{2^{8/3}a} \rb = 0 \iff 1-3a = 2^{8/3}a \implies a = p_1 = \f{1}{2^{8/3}+3}
	\end{align*}
	this is an local maximum because $dS/da$ is monotonically decreasing and crosses zero at $a = 1/(2^{8/3}+3)$.  We can now calculate the rest of the probabilities:
	\begin{align*}
	\boxed{p_6 = 2p_1 = \f{2}{2^{8/3}+3}, \quad\quad p_2 = p_3 = p_4 = p_5 = \f{1}{4}\lb 1-\f{3}{2^{8/3}+3} \rb = \f{2^{2/3} }{2^{8/3}+3} }
	\end{align*}
	
	\item The information content is the difference in entropy of a fair dice and this one. 
	\begin{align*}
	I = S_\text{fair} - S_\text{loaded}= - 6\times \f{1}{6}\log_2 \f{1}{6} - \lb -a_0\log_2 a_0 - 2a_0 \log_2 2a_0 - (1-3a_0) \log_2 \f{1-3a_0}{4} \rb
	\end{align*}
	where we have defined $I$ so that it is positive (the entropy associated with a fair dice is maximal) and $a_0 = 1/(2^{8/3}+3)$. With the help of Mathematica, we find 
	\begin{align*}
	\boxed{I \approx 0.0267\dots}
	\end{align*}
	Mathematica code:
	\begin{lstlisting}
	In[13]:= a0 = 1/(2^(8/3) + 3);
	
	In[17]:= N[-Log2[1/6] - (-a0*Log2[a0] - 
	2*a0*Log2[2*a0] - (1 - 3*a0)*Log2[(1 - 3*a0)/4])]
	
	Out[17]= 0.0267239
	\end{lstlisting}
\end{enumerate}


\noindent \textbf{8. (Optional) Approach to Equilibrium}
\begin{enumerate}[label=(\alph*)]
	\item By time-translation invariance we have
	\begin{align*}
	C_{ij}(t) = C_{ij}(t+\tau) = \langle x_i(t+\tau) x_j (\tau) \rangle.
	\end{align*}
	Picking $\tau = -t$, we find 
	\begin{align*}
	C_{ij}(t) = \langle x_i(0)x_j(-t)\rangle.
	\end{align*}
	By time-reversal invariance we have
	\begin{align*}
	C_{ij}(t) = C_{ij}(-t) = \langle  x_i (0) x_j(t) \rangle = \langle  x_j (t) x_i(0) \rangle = C_{ji}(t),
	\end{align*}
	as desired. 
	
	\item As appeared in the form 
	\begin{align*}
	\sqrt{\f{\det(K)}{(2\pi)^{n}}} \exp\lb -\f{1}{2}K_{ij} x_i x_j \rb,
	\end{align*}
	the matrix $[K]$ is the inverse of the covariance matrix associated with this Gaussian, i.e., 
	\begin{align*}
	[K^{-1}]_{ij} = \langle (x_i -\langle x_i \rangle)(x_j - \langle x_j \rangle) \rangle 
	\end{align*}
	On the other hand, $C_{ij}(0) = \langle x_i(0)x_j(0) \rangle $ forms the autocorrelation matrix. The two matrices are related by the well-known identity:
	\begin{align*}
	\boxed{[K]^{-1} = [C(0)] - \langle \mathbf{x}\rangle \langle \mathbf{x}\rangle^\top \implies C[0] = [K]^{-1} + \langle \mathbf{x}\rangle \langle \mathbf{x}\rangle^\top}
	\end{align*}
	where $\langle \mathbf{x}\rangle^\top = (\langle x_1 \rangle, \langle x_2 \rangle \dots  )^\top$. 
	
	
	
	\item Given $J_\al = -\p \ln p(\mathbf{x})/\p x_\al$, we may compute:
	\begin{align*}
	J_\al = -\f{\p}{\p x_\al}\ln \lc \sqrt{\f{\det(K)}{(2\pi)^{n}}}  \exp\lb -\f{1}{2}K_{ij} x_i x_j \rb \rc = -\f{\p}{\p x_\al} \lb -\f{1}{2} K_{ij}x_ix_j \rb = K_{\al j} x_j.
	\end{align*}
	So, 
	\begin{align*}
	\langle J_\al x_\be \rangle = K_{\al j}\langle x_j x_\beta\rangle.
	\end{align*}
	
	
	\item 
\end{enumerate}

\noindent \textbf{9. (Optional) Simpson's Paradox}

 




\end{document}














