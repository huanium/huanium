\documentclass{article}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{lemma}{Lemma}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{xcolor}
%\definecolor{colby}{rgb}{0.0, 0.0, 0.5}
\definecolor{MIT}{RGB}{163, 31, 52}
\usepackage[pdftex]{hyperref}
%\hypersetup{colorlinks,urlcolor=colby}
\hypersetup{colorlinks,linkcolor={MIT},citecolor={MIT},urlcolor={MIT}}  
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\usepackage{newpxtext,newpxmath}
\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\ham}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}

\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}


\usepackage{enumitem}



\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}

%\hypersetup{colorlinks,urlcolor=colby}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}






\begin{document}
\begin{framed}
\noindent Name: \textbf{Huan Q. Bui}\\
Course: \textbf{8.321 - Quantum Theory I}\\
Problem set: \textbf{\#2}
\end{framed}
	


\noindent \textbf{1.} Let $A$ be a skew-Hermitian operator, i.e., $A^\dagger = -A$.
\begin{enumerate}[label=(\alph*)]
	\item Let $\lambda$ and $\ket{\lambda}$ be an eigenvalue and eigenvector of $A$, respectively. Then we have
	\begin{align*}
	A\ket{\lambda} = \lambda\ket{\lambda} \implies \lambda\braket{\lambda} = \bra{\lambda}A\ket{\lambda} = -\bra{\lambda}A\ket{\lambda}  = \bra{\lambda}A^*\ket{\lambda} = \lambda^* \braket{\lambda} \implies -\lambda = \lambda^*.
	\end{align*} 
	Since $\lambda \in \mathbb{C}$, the only solution is $\lambda = 0$. Thus, the only real eigenvalue of $A$ (up to multiplicity/degeneracy) is $0$.
	
	  
	
	\item  Let $A,B$ be Hermitian operators. Then
	\begin{align*}
	[A,B] = AB - BA = A^\dagger B^\dagger - B^\dagger A^\dagger = (BA - AB)^\dagger = -(AB-BA)^\dagger = -[A,B]^\dagger.
	\end{align*}
	Thus $[A,B]$ is skew-Hermitian. 
\end{enumerate}



\noindent \textbf{2.} Let $H,K$ be Hermitian operators with non-negative eigenvalues and assume that that the trace defined throughout this problem. Since $H,K$ are Hermitian operators we may assume that there exist complete orthonormal (eigen)bases $\{ \ket{h_i}\}$ and $\ket{k_i}$ for $H,K$ respectively with $H\ket{h_i} = h_i \ket{h_i}$ and $K\ket{k_i} = k_i \ket{k_i}$, and $h_i,k_i \geq 0$ for all $i$. Then we can spectral-decompose $H,K$ in their product as follows
\begin{align*}
HK = \sum_n h_n \ketbra{h_n}\sum_m k_m \ketbra{k_m} = \sum_{n,m} h_n k_m \ket{h_n}\bra{h_n} \ket{k_m}\bra{k_m}.
\end{align*}
Since $\tr(A) = \sum_i \bra{\phi_i} A \ket{\phi_i}$ for any orthonormal basis $\{ \phi_i\}$, we have
\begin{align*}
\tr(HK) &= \sum_j \bra{h_j} \lb  \sum_{n,m} h_n k_m \ket{h_n}\bra{h_n} \ket{k_m}\bra{k_m} \rb    \ket{h_j} \\
&= \sum_{n,m} h_n k_m \bra{h_n} \ket{k_m} \bra{k_m}\ket{h_n}, \quad \text{by orthonormality}\\
&= \sum_{n,m} h_n k_m \abs{\bra{h_n} \ket{k_m}}^2.
\end{align*}
Since $h_i,k_i \geq 0$ for all $i$, and the modulus square is always nonnegative, we see that $\tr(HK) \geq 0$, as desired. \\

Suppose $\tr(HK) = 0$, then by nonnegativity we must have $h_n k_m \abs{\bra{h_n}\ket{k_m}}^2 = 0$ for all $n,m$, or equivalently $h_nk_m \bra{h_n}\ket{k_m} = 0$ for all $n,m$. In view of the first equation for $HK$, we see that $HK= 0$.  \\













\noindent \textbf{3.} Let a Hermitian operator $H$ be given with positive spectrum and a complete orthonormal basis. 

\begin{enumerate}[label=(\alph*)]
	\item We want to prove that for any two vectors $\ket{\al}, \ket{\be}$ 
	\begin{align*}
	\abs{\bra{\al} H \ket{\be}}^2 \leq \bra{\al}H \ket{\al} \bra{\be} H \ket{\be}.
	\end{align*}
	
	There are two ways to go about this proof, in which both approaches are actually the same and only differ by appearance. I will present the notationally ``light'' version first. This goes as follows: Since $H$ is Hermitian with positive spectrum, we may find a complete orthonormal basis in which $H$ is diagonal. The transformation between $H$ and its diagonalization $D$ is given by a unitary operator $U$ as $H  = U^\dagger D U$. Since $D$ is diagonal with positive entries, we can define its square root $\sqrt{D}$. From here, we can also define the square root of $H$, denoted $\sqrt{H}$ by $U^\dagger \sqrt{D} U$. We can check:
	\begin{align*}
	\sqrt{H} \sqrt{H} = U^\dagger \sqrt{D} U U^\dagger \sqrt{D} U = U^\dagger \sqrt{D}\sqrt{D} U = U^\dagger D U = H.
	\end{align*}
	It is easy to show that $\sqrt{H}$ is also Hermitian: 
	\begin{align*}
	\sqrt{H}^\dagger = \lp U^\dagger \sqrt{D} U \rp^\dagger  = U^\dagger \sqrt{D}^\dagger U = U^\dagger \sqrt{D} U = \sqrt{H}, 
	\end{align*}
	where we have used the fact that $\sqrt{D}$ is strictly diagonal and positive, thus Hermitian. The rest of the proof is now a simple application of the Cauchy-Schwarz inequality for inner products:
	\begin{align*}
	\abs{\bra{\al} H \ket{\be}}^2 &= \abs{ \bra{\al} \sqrt{H}\sqrt{H} \ket{\be} }^2 = \abs{ \bra{\al} \sqrt{H}^\dagger \sqrt{H} \ket{\be} }^2 =  \abs{ \bra{\al \sqrt{H}^\dagger}\ket{ \sqrt{H}\be} }^2 \\
	&\leq \braket{\sqrt{H}\al} \braket{\sqrt{H} \be}\\
	&= \bra{\al} \sqrt{H}^\dagger\sqrt{H}\ket{\al} \bra{\be}\sqrt{H}^\dagger\sqrt{H}\ket{ \be}= \bra{\al} {H}\ket{\al} \bra{\be}{H}\ket{ \be}
	\end{align*}
	as desired.\\
	
	
	
	The more notationally heavy approach is to consider a complete orthonormal eigenbasis for $H$, which we may call $\{\ket{\lambda_i} \}$ where $\{\lambda_i\}$ are the eigenvalues of $H$. Under this basis, we have
	\begin{align*}
	\ket{\al} = \sum_i a_i \ket{\lambda_i} \quad\quad\quad \ket{\be} = \sum_i b_i \ket{\lambda_i}
	\end{align*} 
	and so 
	\begin{align*}
	\abs{\bra{\al} H \ket{\be}}^2  = \abs{ \sum_i a_i^* \bra{\lambda_i}\lambda_j b_j \ket{\lambda_j} }^2 = \abs{ \sum_{i} a_i^* \lambda_i b_i }^2 = \abs{ \sum_i \lp a_i \sqrt{\lambda_i}\rp^\dagger \lp b_i\sqrt{\lambda_i}\rp }^2.
	\end{align*}
	Note that $\sqrt{\lambda_i}\in \mathbb{R}^+$, which is possible because $\lambda_i > 0$. Now, call 
	\begin{align*}
	\ket{\al'} = \sum_i a_i \sqrt{\lambda_i} \ket{\lambda_i} \quad\quad\quad \ket{\be'} = \sum_i b_i \sqrt{\lambda_i} \ket{\lambda_i}.
	\end{align*}
	It is clear that 
	\begin{align*}
	\abs{\bra{\al} H \ket{\be}}^2  = \abs{ \bra{\al'}\ket{\be'}}^2.
	\end{align*}
	On the other hand, we have
	\begin{align*}
	&\bra{\al}H \ket{\al} = \sum_{i,j} a_i^* a_j \lambda_j \bra{\lambda_i}\ket{\lambda_j} = \sum_i \abs{a_i}^2 \lambda_i = \braket{\al'}\\
	&\bra{\be}H \ket{\be} = \sum_{i,j} b_i^* b_j \lambda_j \bra{\lambda_i}\ket{\lambda_j} = \sum_i \abs{b_i}^2 \lambda_i = \braket{\be'}.
	\end{align*}
	Applying the Cauchy-Schwarz inequality,
	\begin{align*}
	\abs{\bra{\al} H \ket{\be}}^2 = \abs{ \bra{\al'}\ket{\be'}}^2 \leq \braket{\al'} \braket{\be'} = \bra{\al}H \ket{\al} \bra{\be}H \ket{\be}
	\end{align*}
	we successfully proved the desired result. 
	
	\item The trace of $H$ is simply the sum of its eigenvalues, so $\tr(H) > 0$. To show explicitly, we use the orthonormal basis introduced in Part (a). Since $\lambda_i > 0$ for all $i$, we have
	\begin{align*}
	\tr(H) = \sum_i \bra{\lambda_i} H \ket{\lambda_i} = \sum_i \lambda_i \braket{\lambda_i} = \sum_i \lambda_i > 0.
	\end{align*}
\end{enumerate}



\noindent \textbf{4.} Let a unitary operator $U$ be given which satisfies the eigenvalue equation $U\ket{\lambda} = \lambda\ket{\lambda}$.

\begin{enumerate}[label = (\alph*)]
	\item Since $\braket{\lambda} \neq 0$ (because $\ket{\lambda}$ is an eigenvector), we have
	\begin{align*}
	\braket{\lambda} = \bra{\lambda} U^\dagger U  \ket{\lambda } = \abs{\lambda}^2 \braket{\lambda} \implies  \abs{\lambda}^2 = 1.
	\end{align*}
	Since $\lambda\in \mathbb{C}$, it must be of the form $\lambda = e^{i\theta}$	where $\theta\in \mathbb{R}$. 
	
	
	\item Let distinct  eigenvectors $\ket{\mu}$ and $\ket{\lambda}$ be given with corresponding (distinct) eigenvalues $e^{i\theta_\mu}$ and $e^{i\theta_\lambda}$. We have
	\begin{align*}
	\bra{\mu}\ket{\lambda} = \bra{\mu} U^\dagger U \ket{\lambda} =  e^{-i\theta_\mu}e^{i\theta_\lambda}\bra{\mu}\ket{\lambda}.
	\end{align*}
	Since the eigenvalues are not the same, we have that $e^{-i\theta_\mu}e^{i\theta_\lambda} \neq 1$ (i.e., that the complex conjugate of one is not the complex conjugate of the other). Thus, equality holds only if $\bra{\mu}\ket{\lambda} = 0$. 
\end{enumerate}



\noindent \textbf{5.} 

\begin{enumerate}[label = (\alph*)]
	\item First, we will show that the set of $N\times N$ complex matrices form a vector space (over the complex numbers). 
	\begin{itemize}
		\item The zero matrix $\mathcal{O}$ is the identity for vector (matrix) addition. 
		\item For every matrix $A$, the matrix $-A$ exists and $A+(-A)=\mathcal{O}$, so every matrix has an additive inverse. 
		\item Matrix addition is associative.
		\item Matrix addition is commutative. 
		\item Scalar multiplication: For $a,b\in \mathbb{C}$ and a matrix $A$, we have $a(bA) = (ab)A = (ab)A$, as usual. 
		\item The number $1\in \mathbb{C}$ is the identity for scalar multiplication.
		\item Scalar multiplication is distributive with respect to matrix addition. Given $a\in \mathbb{C}$ and matrices $A,B$ we have $a(A+B) = aA + bB$. 
		\item Finally, for $a,b\in \mathbb{C}$ and a matrix $A$, we have $(a+b)A = aA + bA$. 
	\end{itemize}
	Basically, the rules for matrix addition show that the set of $N\times N$ complex matrices form a vector space. To show that the dimension of this space is $N^2$, we consider the following set of $N^2$ matrices $\{ M(ij)\}_{i,j = 1}^{N}$ where each $M(ij)$ is an $N\times N$ matrix whose entries are all zeros except for a $1$ in the $ij$ position.  It is clear that there exists no non-trivial linear combination of the $M(ij)$'s that gives the zero matrix. Thus, $\{M(ij)\}_{i,j=1}^N$ is a linearly independent set. Moreover, it is also obvious that any $N\times N$ matrix can be written as a linear combination of the $M(ij)$ matrices (i.e., given a matrix $A = [a_{ij}]$ we have $A = \sum a_{ij} M_{ij}$). Therefore, the vector space of $N\times N$ complex matrices is $N^2$-dimensional. 
	
	
	\item Let $(A,B) = \Tr(A^\dagger B)$. We will show that $(\cdot , \cdot)$ defines an inner product over the vector space $\mathcal{V}$ above. 
	\begin{itemize}
		\item Positive semidefinite: Given $A\in \mathcal{V}$. Then 
		\begin{align*}
		\Tr(A^\dagger A) = \lp A^\dagger A \rp_{ii} = A^\dagger_{ij}A_{ji} = A^*_{ji}A_{ji} = \sum_{i,j=1}^N \abs{A_{ij}}^2 \geq 0,
		\end{align*}
		with equality occurring if and only if $A_{ij} = 0$ for all $i,j$, i.e., $A = 0$. 
		
		
		\item Linear in the second argument: For $\be \in \mathbb{C}$ and $A,B\in \mathcal{V}$, we have, by the linearity of the trace function, $\Tr(A^\dagger \be B) =  \be \Tr(A^\dagger B)$. Moreover, given $C\in \mathcal{V}$, we have
		\begin{align*}
		\Tr(A^\dagger (B+C)) = \Tr(A^\dagger B + A^\dagger C) = \Tr(A^\dagger B) + \Tr(A^\dagger C). 
		\end{align*}
	
		\item Conjugate-linear in the first argument (\textcolor{blue}{optional since the previous condition suffices}): For $\al \in \mathbb{C}$ and $A,B\in \mathcal{V}$, we have, by the linearity of the trace function, $\Tr((\al A)^\dagger B) =  \be \Tr(a^* A^\dagger B) = a^* \Tr(A^\dagger B)$. Similarly, given $C\in \mathcal{V}$, 
		\begin{align*}
		\Tr((A+C)^\dagger B) = \Tr(A^\dagger B + C^\dagger B) = \Tr(A^\dagger B) + \Tr(C^\dagger B). 
		\end{align*}
		
		\item Conjugate symmetry: Given $A,B\in \mathcal{V}$, we have $\Tr(B^\dagger A)= \Tr((A^\dagger B)^\dagger) = \overline{\Tr((A^\dagger B)^\top)} = \overline{\Tr(A^\dagger B)}$, using the fact that $\Tr(X) = \Tr(X^\top)$ for a square matrix $X$. 
		
	\end{itemize}





	\item By inspection, we can see that the collection $\{ \mathbb{I}, \sigma_1,\sigma_2,\sigma_3\}$ is linearly independent. It remains to show that it spans the space of $2\times 2$ complex matrices. To this end, let a $2\times 2$ matrix $A$ be given. 
	\begin{align*}
	A = \begin{pmatrix}
	a & b \\ c & d
	\end{pmatrix}.
	\end{align*} 
	We may write $A$ as a linear combination of $\{ \mathbb{I}, \sigma_1, \sigma_2,\sigma_3 \}$ as 
	\begin{align*}
	A = \lp \f{a+d}{2} \rp \mathbb{I} + \lp \f{b+c}{2} \rp \sigma_1 + \lp \f{c-b}{2i} \rp \sigma_3 + \lp \f{a-d}{2} \rp \sigma_3.
	\end{align*}
	Therefore, $\{ \mathbb{I},\sigma_1,\sigma_2,\sigma_3 \}$ also spans the space. Thus, $\{ \mathbb{I}, \sigma_1,\sigma_2,\sigma_3 \}$ is a basis for this space. Now, we claim that this basis, under the normalization factor $1/\sqrt{2}$:
	\begin{align*}
	B = \lc \f{1}{\sqrt{2}}\mathbb{I}, \f{1}{\sqrt{2}}\sigma_1, \f{1}{\sqrt{2}}\sigma_2, \f{1}{\sqrt{2}}\sigma_3 \rc
	\end{align*}
	is orthonormal with respect to the inner product defined in Part (b). To see this, we observe that each element of the basis $B$ is already Hermitian and that $\sigma_i^2 = \mathbb{I} = \mathbb{I}^2$ for $i=1,2,3$. So, we have that $\Tr((\sigma_i^\dagger/\sqrt{2})( \sigma_i/\sqrt{2}) ) = \Tr(\sigma_i^2)/2 = \Tr(\mathbb{I})/2 = \Tr(\mathbb{I}^2)/2 = 1$, for each element of $B$ has unit norm. Moreover, since each of $\sigma_1,\sigma_2,\sigma_3$ is traceless, and that
	\begin{align*}
	\sigma_1\sigma_2 = i\sigma_3 \quad\quad \sigma_2\sigma_3 = i\sigma_1\quad\quad \sigma_3\sigma_1 = i\sigma_2
	\end{align*} 
	all of which are traceless, we have $\Tr(\sigma_i^\dagger \sigma_j)= \Tr(\sigma_i\sigma_j) \propto  \Tr(\sigma_k) = \Tr(\mathbb{I}\sigma_k) = 0$ for all $i\neq j$. Therefore, $B$ is mutually orthogonal collection. In view of the previous result, $B$ is an orthonormal basis. 
	
	\item Let $\Sigma(\cdot)$ denote the spectrum and $\mathcal{E}$ the set of eigenvectors for each matrix. Note: to avoid confusion, we use the capital $\Sigma$ rather than the lowercase. Except for the case of $\mathbb{I}$, the characteristic polynomial for each of $\sigma_1,\sigma_2,\sigma_3$ is $\lambda^2 = 1$, so $\lambda = \pm 1$. 
	\begin{align*}
	&\mathbb{I}:  \,\,\,\,\Sigma(\mathbb{I}) = \{1,1\}\quad\,\, \quad\quad\mathcal{E}(\mathbb{I}) = \{ \vec{v}: \vec{v} \in \mathbb{C}^2 \}\\
	&\sigma_1: \Sigma(\sigma_1) = \{1, -1 \}\quad\quad \mathcal{E}(\sigma_1) = \lc (1\quad 1)^\top, (1\quad -1)^\top \rc \\
	&\sigma_2: \Sigma(\sigma_2) = \{1,-1\}\quad\quad \mathcal{E}(\sigma_2) = \lc (1\quad i)^\top, (1\quad -i)^\top \rc \\
	&\sigma_3: \Sigma(\sigma_3) = \{1,-1\}\quad\quad \mathcal{E}(\sigma_3) = \lc (1\quad 0)^\top, (0\quad 1)^\top \rc,
	\end{align*}
	where the eigenvectors are ordered to match their corresponding eigenvalues. 
	
	
	\item Let $\mathbf{A} = (A_1, A_2, A_3)$ and $\mathbf{B} = (B_1, B_2, B_3)$ be given such that $[A_j, \sigma_i] = 0$ and  $[B_n, \sigma_m] = 0$ for all $i,j,m,n$. Then, using the following identities 
	\begin{align*}
	\sigma_1\sigma_2 = i\sigma_3, \quad \sigma_2\sigma_3 = i\sigma_1,\quad \sigma_3\sigma_1 = i\sigma_2,\quad 
	\sigma_2\sigma_1 = -i\sigma_3, \quad \sigma_3\sigma_2 = -i\sigma_1,\quad \sigma_1\sigma_3 = -i\sigma_2,\quad \sigma_i^2 = \mathbb{I},
	\end{align*} 
	we find
	\begin{align*}
	\lp \sigma \cdot \mathbf{A} \rp \lp \sigma \cdot \mathbf{B} \rp 
	&= (\sigma_1 A_1 + \sigma_2 A_2 + \sigma_3 A_3)(\sigma_1 B_1 + \sigma_2 B_2 + \sigma_3 B_3)\\
	&=  (A_1B_1 + A_2 B_2 + A_3 B_3) + i\sigma_1(A_2 B_3 - A_3B_2) + i\sigma_2(A_3 B_1-A_1 B_3) + i\sigma_3(A_1B_2-A_2B_1)\\
	&= \lp \mathbf{A}\cdot \mathbf{B}\rp \mathbb{I} + i\sigma \cdot \lp \mathbf{A} \times \mathbf{B} \rp
	\end{align*}  
	as desired.
	
	\item We claim: 
	\begin{align*}
	\exp\lp i\theta \sigma \cdot \mathbf{n} \rp = \cos\theta \, \mathbb{I} + i\sigma \cdot \mathbf{n}\sin\theta.
	\end{align*}
	In view of Part (e), we observe that $[\sigma \cdot \mathbf{n}]^{2n} = [(\mathbf{n}\cdot \mathbf{n}) \mathbb{I}]^n = \mathbb{I}$ and thus $[\sigma \cdot \mathbf{n}]^{2n+1} = \sigma \cdot \mathbf{n}$. This will help with simplifying the power series expansion of $\exp(i\theta\sigma \cdot \mathbf{n})$ below by splitting up the odd-powered and even-powered terms:
	\begin{align*}
	\exp\lp i\theta \sigma \cdot \mathbf{n} \rp
	&= \sum_{n=0}^\infty \f{(i\theta)^n}{n!} \lb  \sigma \cdot \mathbf{n} \rb^n \\
	&= \sum_{k=0}^\infty \f{(i\theta)^{2k}}{(2k)!} \mathbb{I} +  [\sigma \cdot \mathbf{n}] \sum_{j=0}^{\infty} \f{(i\theta)^{2j+1}}{(2j+1)!}\\
	&= \sum_{k=0}^\infty \f{(-1)^k\theta^{2k}}{(2k)!} \mathbb{I} +  i[\sigma \cdot \mathbf{n}] \sum_{j=0}^{\infty} \f{(-1)^{j}\theta^{2j+1}}{(2j+1)!}\\
	&= \cos\theta, \mathbb{I} +   i\sigma \cdot \mathbf{n}\sin\theta.
	\end{align*}
	And we're done with the proof. 
	
\end{enumerate}



\noindent \textbf{6.}
\begin{enumerate}[label=(\alph*)]
	\item To see that $R \coloneqq (1/\sqrt{2})(\mathbb{I} + i\sigma_x)$ is a rotation by $-\pi/2$ around the $x$-axis, it suffices to show that (1) $R$ keeps the $\sigma_x$ eigenstates invariant and (2) $R$ rotates clockwise the $\sigma_z$ eigenstates into the $\sigma_y$ eigenstates. \\
	
	It is clear from the definition of  $R$ that $R\ket{\pm ,x} = \ket{\pm,x}$ (since $\ket{\pm, x}$ is a simultaneous eigenket of both $\mathbb{I}$ and $\sigma_x$). So $R$ is an identity function when restricted to the $x$-axis. Now, the matrix representation of this operator in the $z$ basis is 
	\begin{align*}
	R = \f{1}{\sqrt{2}} \begin{pmatrix}
	1 & i \\ i & 1
	\end{pmatrix}
	\end{align*}
	Consider $\ket{+,z} = (1\quad0)^\top$. We see that $R \ket{+,z} =  (1/\sqrt{2})(1 \quad i)^\top = \ket{+,y}$. Applying $R$ one more time, we find $R \ket{+ ,y} =  R^2 \ket{+,z} = i\sigma_x \ket{+,z} = i\ket{-,z} \equiv \ket{-,z}$. The total effect is that $+z$ gets rotated into $+y$ and $+y$ gets rotated into $-z$, all with $x$ fixed. As a result, $R$ is a rotation by $-\pi/2$ about the $x$-axis.  
	
	\textcolor{blue}{To see this even more clearly, plot the $yz$-plane with the $x$-axis pointing out of the paper. Let the state $\ket{+,z}$ represent the $+z$ direction and $\ket{+,y}$ represent the $+y$ direction. Because $R\ket{+,z} = \ket{+,y}$, $R\ket{+,y} \equiv \ket{-,z}$ and $R\ket{\pm ,x} = \ket{\pm,x}$, we have  that $+z$ gets sent to $+y$, and $+y$ gets sent to $-z$. So, the $yz$-plane gets rotated by $-\pi/2$ about the $x$-axis. }
	
	\item We will set $\hbar/2 \equiv 1$ for convenience. The matrix elements of $S_z$ in the $y$-basis are given by $\bra{y_i} S_z \ket{y_j}$. So, in the $y$-basis, $S_z$ is 
	\begin{align*}
	S_z\vert_y = \begin{pmatrix}
	\bra{+,y} S_z \ket{+,y} & \bra{+,y} S_z \ket{-,y} \\
	\bra{-,y} S_z \ket{+,y} & \bra{-,y} S_z \ket{-,y}
	\end{pmatrix} 
	= \begin{pmatrix}
	0 & 1\\ 1 & 0
	\end{pmatrix},
	\end{align*}
	where we have used the fact that $S_z \ket{+,y} = \ket{-,y}$ and $S_z\ket{-,y} = \ket{+,y}$. Alternatively, we could calculate the matrix elements exclusively using known results in the $z$-basis (see Part (d)). 
	
\end{enumerate}


\noindent \textbf{7.} We want to construct a matrix which connects the $z$-basis to the $x$-basis. To do this, we must know how $\ket{\pm,z}$ appears in the $x$-basis:
\begin{align*}
&\ket{+,z} = \f{1}{\sqrt{2}} \ket{+,x} + \f{1}{\sqrt{2}}\ket{-,x}\\
&\ket{+,z} = \f{1}{\sqrt{2}} \ket{+,x} - \f{1}{\sqrt{2}}\ket{-,x}.
\end{align*}
So, to see what vectors in the $z$-basis look like in  the $x$-basis, we applying the following matrix to those vectors:
\begin{align*}
U = \f{1}{\sqrt{2}}\begin{pmatrix}
1 & 1 \\ 1 & -1
\end{pmatrix}.
\end{align*}
Compare this with the formula:
\begin{align*}
U &= \sum_r \ket{x_r}\bra{z_r}\\
&= \ket{+,x}\bra{+,z} + \ket{-,x}\bra{-,z} \\
&= \ket{+,x}\lp \f{1}{\sqrt{2}} \bra{+,x} + \f{1}{\sqrt{2}}\bra{-,x} \rp + \ket{-,x} \lp \f{1}{\sqrt{2}} \bra{+,x} - \f{1}{\sqrt{2}}\bra{-,x} \rp\\
&= \f{1}{\sqrt{2}}\begin{pmatrix}
1 & 1 \\ 1 & -1
\end{pmatrix},
\end{align*}
which is consistent with what we found before. 
	
\end{document}








