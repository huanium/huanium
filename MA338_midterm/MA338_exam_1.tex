\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\usepackage{newpxtext,newpxmath}
\usepackage[left=1.25in,right=1.25in,top=1.5in,bottom=1.5in]{geometry}


%\newcommand{\fig}[1]{figure #1}
%\newcommand{\explain}{appendix?}
%\newcommand{\rat}{\mathbb{Q}}
%
%\newcommand{\mathbb{R}}{\mathbb{R}}
%\newcommand{\nat}{\mathbb{N}}
%\newcommand{\inte}{\mathbb{Z}}
%\newcommand{\M}{{\cal{M}}}
%\newcommand{\sss}{{\cal{S}}}
%\newcommand{\rrr}{{\cal{R}}}
%\newcommand{\uu}{2pt}
%\newcommand{\vv}{\vec{v}}
%\newcommand{\comp}{\mathbb{C}}
%\newcommand{\field}{\mathbb{F}}
%\newcommand{\f}[1]{ \hspace{.1in} (#1) }
%\newcommand{\set}[2]{\mbox{$\left\{ \left. #1 \hspace{3pt}
%\right| #2 \hspace{3pt} \right\}$}}
%\newcommand{\integral}[2]{\int_{#1}^{#2}}
%\newcommand{\ba}{\hookrightarrow}
%\newcommand{\ep}{\varepsilon}
%\newcommand{\limit}{\operatornamewithlimits{limit}}
%\newcommand{\ddd}{.1in}
%\newcommand{\ccc}{2in}
%\newcommand{\aaa}{1.5in}
%\newcommand{\B}{{\cal B}}
%\newcommand{\C}{{\cal C}}
%\newcommand{\D}{{\cal D}}
%\newcommand{\FF}{{\cal F}}


%\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `basename #1 .tif`.png}
%\usepackage{graphics}
%\usepackage{array}
%\def\set#1#2{\left\{\left.\;#1\;\right| #2 \; \right\}}
%\def\Sum{\sum}
%\def\me{.05in}















\begin{document}
\begin{center}
{\Large\bf  Test 1:  Take Home}\\
$\,$\\
{\Large  Huan Q. Bui}
\end{center}


\noindent 
\begin{enumerate}
\item  Let $X$ denote the set of all irrational numbers $x$ with $ \sqrt{2} \leq x \leq 2 \sqrt{2}$,   and with the usual metric $d(x,y) = | x - y |$.  Prove that $X$ is not compact.

\item  Let $(X,d)$ denote any metric space.  The metric space $X$ is called ``totally bounded" when,  for every $\epsilon > 0$,   there exists finitely many neighborhoods $N_\epsilon ( x_i )$  ($i=1,\ldots n$)
such that $X \subseteq \cup_{i=1}^n N_\epsilon ( x_i ) $.  The metric space is ``bounded" when $\{{d(x,y) \vert x,y \in X }\}$ is a bounded subset of $\mathbb{R}$.
\begin{enumerate}
\item  Give an example of a bounded metric space that is not totally bounded.
\item  Prove that every totally bounded metric space is bounded
\item  Prove that a metric space is compact if and only if it is both complete and totally bounded.
\end{enumerate}
\item  Let $\mathbb{R}^n$ denote the usual $n$-dimensional Euclidean space,   with its Euclidean  norm
$$ \norm{x}  = \sqrt{ \sum_{i=1}^n |x_i|^2}$$ and corresponding metric $d(x,y) = \norm{x  - y} $,  with    $ x,y \in \mathbb{R}^n$.
Given an $n \times n$ matrix $T$,  define 
$$ \norm{T}  \equiv \sup  \{ \norm{T x } : \norm{ x } \leq 1\} .$$

\begin{enumerate}
\item  Prove that,   for all $n \times n$ matrices $X$ and $Y$,  that $ \norm{X Y}  \leq  \norm{X}\norm{Y} $.
\item  Prove that
$$  \norm{T}  =  \inf  \{ M \in \mathbb{R} : \norm{Tx} \leq M \norm{x}  \mbox{ for all } x \in \mathbb{R}^n \}. $$
\item  With $x \in \mathbb{R}^n$,   find $ \norm{C_x}  $ when $C_x$ is the $n \times n$ matrix with the coordinates of $x$ in the first column and zeros elsewhere.
\item   With $x \in \mathbb{R}^n$,   find $ \norm{D_x} $ when $D_x$ is the $n \times n$ diagonal matrix with the coordinates of  $x$  on the main diagonal,
and zeros elsewhere.
\item  With $x \in \mathbb{R}^n$,   find $ \norm{R_x} $ when $R_x$ is the $n \times n$ matrix with the coordinates of $x$ in the first row and zeros elsewhere.

\end{enumerate}

\item  Let $T$ be an $n \times n$ matrix,  with $\norm{T}$ defined as in the previous problem.   Prove that
$$ \inf \{  \norm{T^m} ^\frac{1}{m} : m \in \mathbb{N} \} = \sup \{ |\alpha| : \alpha \mbox{ an eigenvalue of } T \} $$

\end{enumerate}


\newpage

\begin{center}
	{\Large\bf  Test 1:  Solution}
\end{center}


\noindent \textbf{1.} Let $X$ denote the set of all irrational numbers $x$ with $ \sqrt{2} \leq x \leq 2 \sqrt{2}$,   and with the usual metric $d(x,y) = | x - y |$.  Prove that $X$ is not compact.\\

\hrule
$\,$\\
\noindent \textit{Proof:} Since $X \subset \mathbb{R}$, it suffices to show $X$ is either not bounded or not closed (or neither). $X$ is evidently bounded, so we will show $X$ is not closed. To this end, we claim $X^c$ is not open, where
\begin{align*}
X^c = \underbrace{\left(\mathbb{R}\setminus [\sqrt{2}, 2\sqrt{2}]\right)}_{A}\cup \underbrace{\left\{ r\in \mathbb{Q} \vert \sqrt{2} < r < 2\sqrt{2} \right\}}_{B}.
\end{align*}
We note that $A \cap B = \emptyset$ and let $\epsilon > 0$ be given. Consider $r\in B \subset X^c$ and $\mathcal{N}_\epsilon(r)$. We want to show that $\mathcal{N}_\epsilon(r) \not\subset X^c$, i.e., $\exists x \in X$ such that $x \in \mathcal{N}_\epsilon(r)$.\\


Because $\mathbb{Q}$ is dense in $\mathbb{R}$, $\exists r' \in B$ such that $ r'\in \mathcal{N}_\epsilon(r)$. Without loss of generality, suppose $r' < r$. Let an irrational number $\bar{x}$ be given. By the denseness of $\mathbb{Q}$, there is a rational number $q\in (r'/\bar{x},r/\bar{x})$ such that $\bar{x}q \in (r',r)$, hence contained in $\mathcal{N}_\epsilon(r)$. Call $x = \bar{x}q$. Since $x$ is a product of an irrational number and a rational number, $x$ is irrational, hence $x\notin B \subset X^c$. Because $\mathcal{N}_\epsilon(r) \not\subset B \subset X^c$ and $A \cap B = \emptyset$, $\mathcal{N}_\epsilon(r) \not\subset X^c$. So, $X^c$ is not open $\iff$ $X$ is not closed, which implies $X$ is not compact. \\

\hfill $\square$




\newpage


\noindent \textbf{2.}  Let $(X,d)$ denote any metric space.  The metric space $X$ is called ``totally bounded" when,  for every $\epsilon > 0$,   there exists finitely many neighborhoods $N_\epsilon ( x_i )$  ($i=1,\ldots n$)
such that $X \subseteq \cup_{i=1}^n N_\epsilon ( x_i ) $.  The metric space is ``bounded" when $\{d(x,y): x,y \in X \}$ is a bounded subset of $\mathbb{R}$.
\begin{enumerate}
	\item  Give an example of a bounded metric space that is not totally bounded.
	\item  Prove that every totally bounded metric space is bounded
	\item  Prove that a metric space is compact if and only if it is both complete and totally bounded.
\end{enumerate}

\hrule

\begin{enumerate}
	\item 
%	Consider $(\mathbb{R},d)$ where $d(x,y) = \min\{1,|x-y|\}$. This is a metric space because:
%	\begin{itemize}
%		\item $d(x,x) = \min\{1,|x-x|\} = \min\{1,0\} = 0$.
%		\item $d(x,y) = \min\{1,|x-y|\} \geq 0\, \forall x,y\in \mathbb{R}$, since $|x-y| \geq 0 \,\forall x,y\in \mathbb{R}$.
%		\item $|x-y| = |y-x| \implies d(x,y) = d(y,x)$.
%		\item $|x-z| \leq |x-y| + |y-z| \implies d(x,z) \leq d(x,y) + d(y,z) $.
%	\end{itemize}
%	$(\mathbb{R},d)$ is a bounded metric space because $d(x,y) \leq 1\, \forall x,y\in \mathbb{R}$. Now, suppose (to get a contradiction) that for any $\epsilon > 0$, there exists $n \in \mathbb{N}$, $n < \infty$ such that $\mathbb{R} \subseteq \cup^n_{i=1}N_{\epsilon}(x_i)$, i.e., $(\mathbb{R},d)$ is not totally bounded. Consider some $x_j\in \cup^n_{i=1}N_{\epsilon}(x_i)$. Since there are only finitely many such $N_\epsilon(x_i)$'s, there is a sufficiently large number $m \in \mathbb{N}$ such that $\forall x\in  \cup^n_{i=1}N_{\epsilon}(x_i)$, $x_j + m\epsilon > x$, which implies $x_j + m\epsilon \notin  \cup^n_{i=1}N_{\epsilon }(x_i)$. On the other hand, $x_j + m\epsilon \in \mathbb{R}$, so $\mathbb{R} \not\subseteq  \cup^n_{i=1}N_{\epsilon}(x_i)$. This is a contradiction, so $(\mathbb{R},d)$ must be totally bounded. \hfill $\square$\\
%	
%	Here's another example: 
	Consider $X = [0,1]$ with the metric:
	\begin{align*}
	d(x,y) = \begin{cases}
	1, \quad x\neq y\\
	0, \quad x = y
	\end{cases}
	\end{align*}
	By Problem 10, Chapter 2, Baby Rudin, $(X,d)$ is a metric space. Clearly $X$ is bounded because $X \subset \mathcal{N}_{r=2}(0)$. However, $X$ is not totally bounded. Set $\epsilon = 1/2$, then for any $x$, $\mathcal{N}_\epsilon(x) = \{x\}$. It follows that for any finite set $\{x_1,\dots,x_n\}$,
	\begin{align*}
	\cup^n_{i=1}\mathcal{N}_\epsilon(x_i) = \{ x_1,\dots,x_n\} \not\supseteq [0,1].
	\end{align*}
	\hfill $\square$

	\item Let a totally bounded metric space $(X,d)$ be given. By definition, $\forall \epsilon > 0, \exists n\in \mathbb{N}$ such that $X \subseteq \cup^n_{i=1}\mathcal{N}_\epsilon(x_i)$. Let $\epsilon > 0$ be given. Consider the points $a,b$ in $X$ where $a\in \mathcal{N}_\epsilon(x_i)$ and $b \in \mathcal{N}_\epsilon(x_j)$. Then we have
	\begin{align*}
	d(a,b) \leq d(a,x_i) + d(x_i,x_j) + d(x_j,b) < \epsilon + d(x_i,x_j) + \epsilon.
	\end{align*}
	Since there are only finitely many values of $d(x_i,x_j)$, $0 \leq d(a,b) < 2\epsilon + \sup\{ d(x_i,x_j)| i,j = 1,\dots,n \}$. Thus, $\{d(a,b)| a,b \in X\}$ is a bounded subset of $\mathbb{R}$, which implies $(X,d)$ is bounded.
	
	\hfill $\square$
	\item  $(\rightarrow)$ Let a metric space $(X,d)$ be given. Suppose $(X,d)$ is compact, i.e., each of its open cover has a finite subcover. We want to show $(X,d)$ is complete and totally bounded. 
	\begin{itemize}
		\item (Completeness) \underline{To prove}: Every Cauchy sequence in $X$ converges. 
		
		Let a Cauchy sequence $\{x_n\} \subset X$ be given. 
		\begin{itemize}
			\item If the set $\Gamma \subset X$ of the terms of $\{x_n\}$ is finite then $\{x_n\}$ converges to some term $x_k\in\Gamma$, because by definition $x_i,x_j \in \{x_n\}$ get arbitrarily close for sufficiently large $i,j$. 
			\item If $\Gamma \subset X$ is infinite then $\Gamma$ contains its limit point $p$ because $X$ is compact (theorem 2.37, Baby Rudin). We want to show $x_n \to p$. To this end, let $\epsilon > 0$ be given and set $\epsilon' = \epsilon/2$. Since $\{x_n\}$ is Cauchy, $\exists N \in \mathbb{N}$ such that whenever $m,n \geq N$, 
			\begin{align}\label{1}
			d(x_m,x_n) < \epsilon' = \frac{\epsilon}{2}.
			\end{align}
			We also know $p$ is a limit point of $\Gamma$, so for $r = \epsilon' = \epsilon/2 > 0$, $\exists x_m \in \Gamma$ where $m \geq N$ such that $x_m \in \mathcal{N}_{\epsilon'}(p)\setminus\{p\} \neq \emptyset$, which means
			\begin{align}\label{2}
			d(x_m,p) \leq \epsilon' = \frac{\epsilon}{2}.
			\end{align}
			From (\ref{1}) and (\ref{2}), if $n \geq N$, we have that
			\begin{align*}
			d(x_n,p) \leq d(x_n,x_m) + d(x_m,p) < \epsilon' + \epsilon' = \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon.
			\end{align*}
			Thus, the Cauchy sequence $\{x_n\}$ in $X$ converges to $p$ in $X$, which implies $X$ is complete.  
		\end{itemize}
		\item (Totally boundedness) \underline{To prove}: $\forall \epsilon > 0, \exists n \in \mathbb{N}, n < \infty$, such that $X \subseteq \cup^{n}_{i=1}\mathcal{N}_\epsilon(x_i)$.  
		
		Let a compact metric space $(X,d)$ be given. Then the collection $\{\mathcal{N}_\epsilon(x) | x\in X\}$ forms an open cover for $X$. Since $X$ is compact, there is a finite subcover, i.e., there are (finitely many) points $x_1,\dots,x_n \in X$ such that 
		\begin{align*} 
		X = \cup^n_{i=1} \mathcal{N}_\epsilon(x_i).
		\end{align*}
		This shows $X$ is totally bounded. 
	\end{itemize}
	
	$(\leftarrow)$ Let $(X,d)$ be given. $(X,d)$ is complete and totally bounded. \underline{To prove}: $(X,d)$ is compact. 
	
	Let the collection $\{\mathcal{N}_\epsilon\}$ be an open cover for $X$. Assume (to get a contradiction) that $\{\mathcal{N}_\epsilon \}$ has no finite subcover for $X$. Let $\alpha = \text{diam}(X)$, which exists because $X$ is (totally) bounded. Since $X$ is totally bounded, $X$ can be covered by finitely many closed ball $\mathcal{B}_{\alpha/4}(x_i)$ with $x_i \in X$. It follows from our assumption that at least one $\mathcal{B}_{\alpha/4}(x_j)$ intersected with $X$ cannot be finitely covered by $\{\mathcal{N}_\epsilon\}$. Call $X_1 = \mathcal{B}_{\alpha/4}(x_j) \cap X$, then $X_1$ is a closed subset of $X$ with $\text{diam}(X_1) \leq \alpha/2$ ($X_1$ closed by theorems 2.24(b) and 2.34, Baby Rudin). Repeating this argument gives us a nested sequence of closed sets $X_n \subset X$ with $\text{diam}(X_n) \leq \alpha/2^n$ where each $X_n$ cannot be finitely covered by $\{\mathcal{N}_\epsilon\}$. \\
	
	Now, for each $n$, consider $x_n \in X_n$. Then $\{x_n\}$ is Cauchy, by the construction of the closed subsets $X_n$. Because $X$ is complete, $\{x_n\}$ converges with some limit $p \in X$. Since each $X_n$ is closed, we have that $p \in \cap^\infty_{n=1}X_n$. Further, because $\text{diam}(X_n) \to 0$ as $n\to \infty$, we must have that $\cap^\infty_{n=1}A_n = \{p\}$, the set with a single element $p$. Consider any $\mathcal{N} \in \{\mathcal{N}_\epsilon\}$ with $p\in \mathcal{N}$. $\mathcal{N}$ is open, so there exists $r > 0$ such that $\mathcal{N}_r(p) \subset \mathcal{N}$. Take $n\in \mathbb{N}$ such that $d(p,x_n) < r/2$ and $\text{diam}(X_n) < r/2$, then $X_n \subset \mathcal{N}_r(p) \subset \mathcal{N} \in\{\mathcal{N}_\epsilon\}$, which contradicts the assumption that $X_n$ cannot be finitely covered by $\{\mathcal{N}_\epsilon\}$. So, $\{\mathcal{N}_\epsilon\}$ has a finite subcover for $X$, so $(X,d)$ is compact.
	
	 \hfill $\square$       
\end{enumerate}


\noindent \textbf{Reference}

For part 3. of this problem, I used the approach given by Anton R. Schep, which is presented in \textit{Compact sets in metric spaces, Notes for Math 703}, link \href{http://people.math.sc.edu/schep/compactmetric.pdf}{\underline{here}}. I also found different versions of this proof which show compactness via the convergence subsequences, but I like Schep's approach best because it uses the definition of compactness.  




\newpage


\noindent \textbf{3.} Let $\mathbb{R}^n$ denote the usual $n$-dimensional Euclidean space,   with its Euclidean  norm
$$ \norm{x} = \sqrt{ \sum_{i=1}^n |x_i|^2}$$ and corresponding metric $d(x,y) = \norm{x-y}$,  with    $ x,y \in \mathbb{R}^n$.
Given an $n \times n$ matrix $T$,  define 
$$ \norm{T} \equiv \sup  \{ \norm{Tx} : \norm{x} \leq 1\} .$$

\begin{enumerate}
	\item  Prove that,   for all $n \times n$ matrices $X$ and $Y$,  that $\norm{XY} \leq \norm{X}  \norm{Y}$.
	\item  Prove that
	$$ \norm{T}=  \inf  \{ M \in \mathbb{R} : \norm{T x } \leq M \norm{ x }   \mbox{ for all } x \in \mathbb{R}^n \}. $$
	\item  With $x \in \mathbb{R}^n$,   find $ \norm{C_x} $ when $C_x$ is the $n \times n$ matrix with the coordinates of $x$ in the first column and zeros elsewhere.
	\item   With $x \in \mathbb{R}^n$,   find $ \norm{D_x} $ when $D_x$ is the $n \times n$ diagonal matrix with the coordinates of  $x$  on the main diagonal,
	and zeros elsewhere.
	\item  With $x \in \mathbb{R}^n$,   find $ \norm{R_x} $ when $R_x$ is the $n \times n$ matrix with the coordinates of $x$ in the first row and zeros elsewhere.
	
\end{enumerate}

\hrule


\begin{enumerate}
	\item \underline{To prove}: $\norm{X Y} \leq \norm{X}\norm{Y}$. 

	We first show that $\norm{Yx} \leq \norm{Y} \norm{x}$. Suppose (to get a contradiction) that $\norm{Yx} > \norm{Y} \norm{x}$, then it follows that
	\begin{align*}
	\frac{1}{\norm{x}} \norm{Y x } > \norm{ Y } \implies \bigg\vert\bigg\vert Y \frac{x}{\norm{x}} \bigg\vert\bigg\vert  > \norm{ Y }.
	\end{align*}
	Because $x/\norm{x}$ is a unit vector, this contradicts the definition of $\norm{Y }$. Thus, $\norm{ Yx} \leq \norm{ Y } \norm{ x}$. It follows that
	\begin{align*}
	\norm{ XY } &= \sup  \{ \norm{ XY x } :\norm{x} \leq 1\}\nonumber\\
	&\leq \sup  \{ \norm{ X } \norm{ Y x } :\norm{x} \leq 1\}\nonumber\\
	&= \norm{ X } \sup  \{ \norm{ Y x } : \norm{x} \leq 1\}\nonumber\\
	&= \norm{ X } \norm{ Y } 
	\end{align*}
	\hfill $\square$
	
	
	
	
	
	\item  \underline{To prove}: $\sup  \{ \norm{ T x } :\norm{x} \leq 1\} = \inf  \{ M \in \mathbb{R} : \norm{ Tx } \leq M \norm{ x  } \forall x \in \mathbb{R}^n \}.$
	
	
	Let 
	\begin{align*}
	a &=  \inf  \{ M \in \mathbb{R} : \norm{ Tx } \leq M \norm{ x  } \forall x \in \mathbb{R}^n \}\nonumber\\
	b &=  \sup  \{ \norm{ T x } : \norm{x} \leq 1\}
	\end{align*}
	We want to show $a\leq b$ and $b \leq a$.
	\begin{itemize}
		\item By definition, $\norm{ Tx } \leq a \norm{x}\, \forall x \in \mathbb{R}^n$. In particular, this holds for $\norm{x} \leq 1$. And so, $b \geq \norm{ T x } \leq a \norm{x} \leq a $, i.e.,  $b \leq a$.  
		\item Consider the quantity
		\begin{align*}
		c = \sup \left\{\frac{\norm{ Tx }}{ \norm{x}}: x \in \mathbb{R}^n \setminus\{ 0\} \right\}.
		\end{align*} 
		Clearly, $\norm{ T x } \leq d\norm{x}$ for all nonzero $x \in \mathbb{R}^n$. So, $a \leq c$, by the definition of $a$. Consider another quantity:
		\begin{align*}
		d = \sup \{\norm{Tx}:\norm{x} = 1\}.
		\end{align*}
		For any nonzero $x\in \mathbb{R}^n$, $x/\norm{x}$ is a unit vector, which means $\norm{Tx}/\norm{x} = \norm{T(x/\norm{x})}  \leq d$. By the definition of $c$, we have that $c \leq d$ and thus $a \leq c \leq d$. Finally, $d \leq b$ clearly because $d$ is a supremum taken over fewer terms than $b$. 
		
		Thus, $a \leq c \leq d \leq b \leq a$, which implies $a = b$.
		
		\hfill $\square$
	\end{itemize}
	
	
	\item Let ${x} = \begin{pmatrix}
	x_1 & \dots & x_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given. Then $C_x$ has the form
	\begin{align*}
	C_x = \begin{pmatrix}
	x_1 & 0 & \dots & 0\\
	\vdots & \vdots &  & \vdots\\
	x_n & 0 & \dots & 0
	\end{pmatrix}_{n\times n}.
	\end{align*} 
	Let ${y} = \begin{pmatrix}
	y_1 & \dots & y_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given, then clearly $C_x {y} = y_1 {x} \implies \norm{ C_xy } = | y_1 | \norm{ x}$. By definition, 
	\begin{align*}
	\norm{ C_x } &= \sup \left\{\norm{ C_x y } : \norm{ y } \leq 1\right\}\nonumber\\
	&= \sup \left\{|y_1| \norm{x} :\norm{y} \leq 1\right\}\nonumber\\
	&= \norm{x}\sup \left\{|y_1|:\norm{y} \leq 1\right\}\nonumber\\
	&= \norm{x}, \quad \text{attained when taking } y = \begin{pmatrix}
	1 & 0 & \dots & 0
	\end{pmatrix}^\top.
	\end{align*}
	Thus, $\norm{C_x} = \norm{x}$. 
	\hfill $\square$
	

	
	
	
	
	\item  Let ${x} = \begin{pmatrix}
	x_1 & \dots & x_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given. Then $D_x$ has the form
	\begin{align*}
	D_x = \text{diag}(x_1,\dots,x_n).
	\end{align*} 
	Let ${y} = \begin{pmatrix}
	y_1 & \dots & y_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given, then clearly 
	\begin{align*}
	\norm{ D_xy} = \norm{ \begin{pmatrix}
	x_1 y_1 & \dots x_ny_n
	\end{pmatrix}^\top}= \sqrt{ \sum^n_{i=1}|x_i y_i|^2  }  = \sqrt{\sum^n_{i=1} x_i^2 y_i^2}.
	\end{align*}
	By definition, 
	\begin{align*}
	\norm{ D_x } &= \sup \left\{\norm{ D_x y} : \norm{y} \leq 1 \right\}\nonumber\\
	&= \sup \left\{\norm{D_x y} : \norm{y} = 1\right\}
	\end{align*}
	where we have used the previous result: $a \leq c \leq d \leq b \leq a$ in the second equality. With this,
	\begin{align*}
	\norm{ D_x } &= \sup \left\{\sqrt{\sum^n_{i=1} x_i^2 y_i^2} : \norm{y} = 1\right\}\nonumber\\
	&\leq \sup \left\{ \sqrt{\sum^n_{i=1} \left(\max_{1\leq i \leq n} |x_i|\right)^2  y_i^2} : \norm{y} = 1 \right\}  \nonumber\\
	&= \sup \left\{\max_{1\leq i \leq n} |x_i| \sqrt{\sum^n_{i=1} y_i^2} : \norm{y} = 1 \right\}  \nonumber\\
	&= \max_{1\leq i \leq n} |x_i| \cdot \underbrace{\sup_{\norm{y}=1}\norm{y}}_{1}\nonumber\\
	&= \max_{1\leq i \leq n} |x_i|,
	\end{align*}
	with equality occurring when $y = e_{(m(i))}$ where $e_{(j)}$ is one of the standard basis vectors with $1$ at the $j$th coordinate and zero elsewhere, and $m(i)$ is the index of the largest coordinate (in magnitude) of $x$. In other words, $||D_x||$ is the absolute value of the largest coordinate of $x$ (in magnitude). Thus, $||D_x|| = \max_{1\leq i \leq n}|x_i|$. \hfill $\square$
	
	
	
	
	\item  Let ${x} = \begin{pmatrix}
	x_1 & \dots & x_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given. Then $C_x$ has the form
	\begin{align*}
	R_x = \begin{pmatrix}
	x_1 & \dots & x_n\\
	\vdots & \ddots  & \vdots\\
	 0 & \dots & 0
	\end{pmatrix}_{n\times n}.
	\end{align*} 
	Let ${y} = \begin{pmatrix}
	y_1 & \dots & y_n
	\end{pmatrix}^\top \in \mathbb{R}^n$ be given, then clearly,
	\begin{align*}
	\norm{ R_xy} &= \norm{ \begin{pmatrix}
	\sum^n_{i=1}x_iy_i & 0 & \dots & 0
	\end{pmatrix}^\top } =  \norm{ \sum^n_{i=1}x_iy_i \begin{pmatrix}
	1 & 0 & \dots & 0
	\end{pmatrix}^\top }= \bigg\vert \sum^n_{i=1}x_i y_i \bigg\vert.
	\end{align*}
	By definition, 
	\begin{align*}
	\norm{R_x} &= \sup \left\{\norm{R_x y} : \norm{y} \leq 1\right\}\nonumber\\
	&= \sup \left\{\norm{R_x y} : \norm{y}  = 1\right\}\nonumber\\
	&= \sup \left\{\norm{R_x y} : \norm{y}  = 1\right\}\nonumber\\
	&= \sup \left\{ \bigg\vert \sum^n_{i=1}x_i y_i \bigg\vert : \norm{y}  = 1\right\}\nonumber\\
	&\leq \sup \left\{ \sqrt{\sum^n_{i=1}x_i^2 {\sum^n_{j=1}y_j^2}} : \norm{y}  = 1\right\},\quad \text{Cauchy-Schwartz}\nonumber\\
	&= \norm{x} ,
	\end{align*}
	where equality occurs if and only if $y$ is a multiple of $x$, under the constraint $\norm{y}  = 1$. This means equality is attained if and only if $y = x/\norm{x} $. Thus, $\norm{R_x}  = \norm{x} $. \hfill $\square$
\end{enumerate}




\noindent \textbf{Reference}



For Part 2. of this problem, I referred to Proposition 2.1, Chapter III: Banach Spaces, in John Conway's \textit{A Course in Functional Analysis}, 2nd Edition, to define the quantities $c,d$ for the proof. 
















\newpage


\noindent \textbf{4.} Let $T$ be an $n \times n$ matrix,  with $||T||$ defined as in the previous problem.   Prove that
$$ \inf \{  || T^m || ^\frac{1}{m} : m \in \mathbb{N} \} = \sup \{ |\alpha| : \alpha \mbox{ an eigenvalue of } T \}. $$

\hrule
$\,$\\
\noindent \underline{Note to Ben}: the proof below is a combination of Internet/book search and my notes from Prof. Livshits's MA353: Matrix Analysis from S'19. The statement of the problem is similar to the statement of the Beurling-Gelfand spectral radius theorem. However, the proof found in Rudin's \textit{Functional Analysis}, section 10.13, is too advanced for me. I found another approach by Joel E. Tropp (Prof. of Mathematics at Caltech),  \href{https://pdfs.semanticscholar.org/0bb8/67b2a2cc8e2711bc3273e21db5acdbc06e4d.pdf}{\underline{here}}, which uses Jordan canonical form (which I learned in MA353) and the fact that all norms on a finite-dimensional vector space are equivalent (which I learned from Prof. Randles) to prove the above statement. However, instead of showing the statement holds for the $\infty$-norm like Joel E. Tropp did, I will be using the $||\cdot||_{\text{HS}}$ norm, since I have done this in MA353.  \\  

\hrule
$\,$\\




\noindent Before getting to the proof, I want to give a lemma which is useful later in the proof.\\


\noindent \textbf{Lemma 4.1.} Suppose that $\{x_{1_n} \}, \{x_{2_n}\}, \dots, \{x_{k_n}\}$ are sequences of positive numbers such that $ \{(x_{i_n})^{1/n}\} \to \alpha_i$ for each $i=1,2,\dots,k$. Then 
\begin{align*}
\left\{ (\alpha_1^n+ \alpha_2^n+ \dots + \alpha_k^n)^{1/n} \right\} \to \sup_{i}\{\alpha_i\}.
\end{align*} 
It follows that
\begin{align*}
\left\{ (x_{1_n}+ x_{2_n}+ \dots + x_{k_n})^{1/n} \right\} \to \sup_{i}\{\alpha_i \}.
\end{align*}
 
\noindent \textit{Proof of Lemma 4.1.}: We assume (without loss of generality) that $\sup_{i} \alpha_i = \alpha_1$. Then, any $\alpha_i$ can be written as $\delta_i \alpha_1$ where $\delta_i$ is some positive number less than or equal to 1. It follows that
\begin{align*}
(\alpha_1^n+ \alpha_2^n+ \dots + \alpha_k^n)^{1/n} = \alpha_1 (1 + \delta_2^n + \dots + \delta_k^n)^{1/n}.
\end{align*}
The number $(1 + \delta_2^n + \dots + \delta_k^n)$ is at most $k$. Thus, when $n\to \infty$, $(1 + \delta_2^n + \dots + \delta_k^n)$ tends to 1. Therefore, $\lim\limits_{n\to\infty} (\alpha_1^n+ \alpha_2^n+ \dots + \alpha_k^n)^{1/n} = \alpha_1 $, i.e., $
\left\{ (\alpha_1^n+ \alpha_2^n+ \dots + \alpha_k^n)^{1/n} \right\} \to \sup_{i}\{\alpha_i\}$. Since $ \{(x_{i_n})^{1/n}\} \to \alpha_i$ for each $i=1,2,\dots,k$, it follows that $\left\{ (x_{1_n}+ x_{2_n}+ \dots + x_{k_n})^{1/n} \right\} \to \sup_{i}\{\alpha_i \}$. \hfill $\Delta$\\

%\noindent From this result, we have that
%\begin{align*}
%\sup_i \alpha_i &= \lim\limits_{n\to\infty} (\alpha_1^n+ \alpha_2^n+ \dots + \alpha_k^n)^{1/n}\\
%&= \lim\limits_{n\to\infty} \left(\lim\limits_{n\to \infty} x_{1_n}^{n/n}+ \lim\limits_{n\to \infty} x_{2_n}^{n/n}+ \dots + \lim\limits_{n\to \infty} x_{k_n}^{n/n} \right)^{1/n}\\
%&=  \lim\limits_{n\to \infty} \left( x_{1_n} + x_{2_n} + \dots + x_{k_n} \right)^{1/n}. 
%\end{align*}

\noindent \underline{\textit{Proof of problem statement}}: \\


I will use (without proving) the fact that the Hilbert-Schmidt norm $\norm{ \cdot }_{\text{HS}}$ and the operator norm $\norm{\cdot}$ are equivalent, i.e., there are positive numbers $a,b> 0$ such that for any $n\times n$ matrix $T$, $a\norm{T}_{\text{HS}} \leq \norm{T} \leq b\norm{T}_{\text{HS}}$. (A general theorem about equivalence of norms on finite-dimensional vector spaces is provided by theorem 2.4-5, Erwin Kreyszig's \textit{Introductory Functional Analysis with Applications}). The fact about ``equivalence of norms'' allows me to translate my result using the Hilbert-Schmidt norm to the operator norm defined in Problem 3. In other words, if I could show that the problem statement holds for the Hilbert-Schmidt norm, then I could argue that it also holds when the operator norm is used.  
$\,$\\


%\noindent \textbf{Lemma 4.1.} The Hilbert-Schmidt $|| \cdot ||_{\text{HS}}$ norm is equivalent to the operator norm $|| \cdot||$, for the finite-dimensional vector space of $n\times n$ matrices. 
%
%\underline{Proof}:  We want to show there are positive numbers $a,b$ such that 
%\begin{align*}
%a|| T||_{\text{HS}} \leq ||T|| \leq b||T||_\text{HS}.
%\end{align*}
%\begin{itemize}
%	\item From problem 3, we know that $||T|| = \sup \set{||Tx||/||x||}{x\neq 0}$. I claim that $||Tx|| \leq ||T||_{\text{HS}} ||x|| $, or equivalently, that $||Tx||^2 \leq ||T||_{\text{HS}}^2 || x||^2$. To see explicitly why this holds, we look at each term:
%	\begin{align*}
%	||Tx||^2 &= \sum^n_{i=1}\sum^n_{j=1} (T_{ij}x_j)^2 = \sum^n_{i=1}\sum^n_{j=1} T_{ij}^2x_j^2 \\
%	||T||^2_{\text{HS}} &= \sum^n_{i=1}\sum^n_{j=1}T_{ij}^2\\
%	||x||^2 &= \sum^n_{j=1} x_j^2.
%	\end{align*} 
%	It is clear that $||Tx||^2 \leq ||T||^2_{\text{HS}} ||x||^2$, because the RHS contains all terms on the LHS and extra nonnegative terms. Thus, $||Tx|| \leq ||T||_{\text{HS}}||x||$. This means all terms in the set $\set{||Tx||/||x||}{x\neq 0}$ is $\leq ||T||_{\text{HS}}$, and thus $||T|| \leq ||T||_{\text{HS}}$.
%	
%	\item 
%\end{itemize}

Let $\rho(T) = \sup \{|\alpha| : \alpha\text{ an eigenvalue of  } T\}$ denote the \textit{spectral radius} of $T$. For any $n\times n $ matrix $T$, we want to first show that
\begin{align*}
\rho(T) = \lim\limits_{n\to \infty} \norm{T^n}^{1/n}_{\text{HS}}.
\end{align*}
Any $n\times n$ matrix $T$ can be written as a direct sum of Jordan blocks following a similarity transformation. Suppose that $\mathcal{J} = S^{-1}TS = \bigoplus^s_{i=1}\mathcal{J}_i$, where each $\mathcal{J}_i$ is a Jordan block. Clearly, $\rho(T) = \rho(\mathcal{J})$ because $T\sim \mathcal{J}$. Now, we want to consider the relationship between $\norm{T^n}^{1/n}$ and $\norm{\mathcal{J}^n}^{1/n}$:
\begin{align*}
\norm{T^n}^{1/n} = \norm{(S^{-1}\mathcal{J}S)^n}^{1/n} = \norm{ S \mathcal{J}^n S^{-1}}^{1/n} \leq \left( \norm{S} \norm{S^{-1}}\right)^{1/n} \norm{\mathcal{J}^n}^{1/n}  
% ,\quad \text{by Problem 3}
\end{align*} 
and
\begin{align*}
\norm{T^n}^{1/n} = \norm{(S^{-1}\mathcal{J}S)^n}^{1/n} = \left( \frac{\norm{S^{-1}}\norm{ S\mathcal{J}^{n}S^{-1}} \norm{S}}{\norm{S} \norm{S^{-1}}}\right)^{1/n} \geq \left( \norm{S} \norm{S^{-1}}\right)^{-1/n} \norm{\mathcal{J}^n}^{1/n} 
\end{align*}
where we have used results from Problem 3 and the fact that $\norm{S^{-1}}\norm{S\mathcal{J}^nS^{-1}}\norm{S} \geq \norm{\mathcal{J}^n}$ when $S$ and $S^{-1}$ are ``absorbed'' into the term in the middle. Further, in each inequality, the term $\left( \norm{S} \norm{S^{-1}}\right)^{\pm 1/n}  \to 1 $  as $n\to \infty$. Thus, it suffices to consider only the behavior of $\norm{\mathcal{J}^n}^{1/n}$ rather than $\norm{T^n}^{1/n}$ itself, i.e., it suffices to show 
\begin{align*}
\rho(T) = \lim\limits_{n\to \infty} \norm{\mathcal{J}^n}^{1/n}_{\text{HS}}.
\end{align*}

Since $\mathcal{J}$ is block-diagonal, $\mathcal{J}^n$ is a direct sum of the powers of the Jordan blocks of $T$, i.e., $\mathcal{J}^n = \bigoplus^s_{i=1} (\mathcal{J}_i)^n$. Consider a Jordan block $\mathcal{J}_i$. Let us write $\mathcal{J}_i \equiv \mathcal{J}_{\lambda,m}$ where $\lambda$ is the associated eigenvalue and $m$ is the size of $\mathcal{J}_i$. Further, we write $\mathcal{J}_{\lambda,m} = \lambda\mathcal{I} + \mathcal{N}$ where $\mathcal{I}$ is the $m\times m$ identity matrix and $\mathcal{N}$ is a nilpotent of order $m$. With these, we can write $(\mathcal{J}_{\lambda,m})^n$ as a sum
\begin{align*}
(\mathcal{J}_{\lambda,m} )^n = (\lambda \mathcal{I} + \mathcal{N})^n = \lambda^n \mathcal{I} + {n \choose 1}\lambda^{n-1}\mathcal{N} + \dots
\end{align*}  
which is truncated at the term with $\mathcal{N}^m = \mathcal{O}$, the zero matrix. Since $\mathcal{N}$ has the form 
\begin{align*}
\mathcal{N} = \begin{bmatrix}
0 & 1 &&\\
& \ddots & \ddots & \\
&&\ddots&1\\
&&&0
\end{bmatrix},
\end{align*}
we recognize that $(\mathcal{J}_{\lambda,m} )^n$ can be written as
\begin{align*}
(\mathcal{J}_{\lambda,m})^n = \begin{bmatrix}
\lambda^n & {n\choose 1}\lambda^{n-1} & & {n\choose {m-1}}\lambda^{n-(m-1)} \\
& \lambda^n & \ddots &\\
&& \ddots & {n\choose 1}\lambda^{n-1}\\
&&&\lambda^{n}
\end{bmatrix}.
\end{align*}

With this, we can write the formula for the Hilbert-Schmidt norm for $(\mathcal{J}_{\lambda,m})^n$ as
\begin{align*}
\norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_\text{HS}^2 =  m(|\lambda|^{2})^n  + (m-1){n\choose 1}^2(|{\lambda}|^{2})^{(n-1)} + \dots + {n\choose m-1}^2(|{  \lambda}|^{2})^{(n-(m-1))} .
\end{align*}
If $\abs{\lambda} = 0$ then $\norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_\text{HS} = 0$, which implies 
\begin{align*}
\lim\limits_{n\to\infty} \left(\norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_2\right)^{\frac{1}{n}} = \lim\limits_{n\to\infty} 0  = 0= \abs{\lambda}.
\end{align*}
If $\abs{\lambda} > 0$, by factoring out $\abs{\lambda}^n$, we get
\begin{align*}
\norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_\text{HS} = \abs{\lambda}^n \left( m  + \frac{(m-1){n\choose 1}^2}{\abs{\lambda}^2} + \dots + \frac{{n\choose m-1}^2}{\abs{\lambda}^{2(m-1)}}  \right)^\frac{1}{2}.
\end{align*}
Therefore,
\begin{align*}
\left(  \norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_\text{HS} \right)^{\frac{1}{n}} &= \abs{\lambda}\left[\left( m  + \frac{(m-1){n\choose 1}^2}{\abs{\lambda}^2} + \dots + \frac{{n\choose m-1}^2}{\abs{\lambda}^{2(m-1)}}  \right)^\frac{1}{2}\right]^{\frac{1}{n}}\\
&= \abs{\lambda}\left[\left( m + \frac{(m-1){n\choose 1}^2}{\abs{\lambda}^2} + \dots + \frac{{n\choose m-1}^2}{\abs{\lambda}^{2(m-1)}}  \right)^\frac{1}{n}\right]^{\frac{1}{2}}.
\end{align*}
Let
\begin{align*}
f(n) = m  + \frac{(m-1){n\choose 1}^2}{\abs{\lambda}^2} + \dots + \frac{{n\choose m-1}^2}{\abs{\lambda}^{2(m-1)}}.
\end{align*}
We recognize that $f(n)$ is a polynomial in $n$. Using logarithms and l'Hopital's rule we find $\lim\limits_{n\to\infty} (f(n))^{\frac{1}{n}} = 1$. Thus, $\lim\limits_{n\to\infty} \sqrt{(f(n))^{\frac{1}{n}}} = 1$, and it follows that 
\begin{align*}
\lim\limits_{n\to\infty} \left(  \norm{\left(\mathcal{J}_{\lambda,m}\right)^n}_\text{HS} \right)^{\frac{1}{n}} = \abs{\lambda}\cdot\lim\limits_{n\to\infty}\sqrt{(f(n))^{\frac{1}{n}}} = \abs{\lambda}\cdot 1 = \abs{\lambda}.
\end{align*}

\noindent Back to $\mathcal{J} = \bigoplus_{i=1}^s \mathcal{J}_i = \bigoplus_{i=1}^s \mathcal{J}_{\lambda_i,m_i}$. We wish to evaluate the limit:
\begin{align*}
\lim_{n\to \infty} \left(\norm{\mathcal{J}^n}_{\text{HS}}\right)^{1/n}.
\end{align*}
We have that
\begin{align*}
\lim\limits_{n\to\infty}\left( \norm{\mathcal{J}^n}_\text{HS} \right)^{\frac{1}{n}} =  \lim\limits_{n\to\infty}\sqrt[n]{ \norm{\bigoplus^{s}_{i=1}\left( \mathcal{J}_{\lambda_i, m_i}\right)^n}_\text{HS}}= \lim\limits_{n\to\infty}\sqrt{  \sum^{s}_{i=1}\left( \norm{\left( \mathcal{J}_{\lambda_i, m_i}\right)^n}_\text{HS}^2 \right)^\frac{1}{n}}.
\end{align*}
From an earlier argument, we know  
$ \lim\limits_{n\to\infty} \left(  \norm{\left(\mathcal{J}_{\lambda_i,m_i}\right)^n}_2 \right)^{\frac{1}{n}}  =  \abs{\lambda_i}$. So,
\begin{align*}
\lim\limits_{n\to\infty} \left(  \norm{\left(\mathcal{J}_{\lambda_i,m_i}\right)^n}_\text{HS} \right)^{\frac{2}{n}}  = \lim\limits_{n\to\infty} \left(\left(  \norm{\left(\mathcal{J}_{\lambda_i,m_i}\right)^n}_\text{HS} \right)^2\right)^{\frac{1}{n}}  =   \abs{\lambda_i}^2.
\end{align*}

If $\norm{\left(\mathcal{J}_{\lambda_j,m_j}\right)^n}_\text{HS}$ is zero for some $j$, then  $\lambda_j = 0$, and  we can drop this term from the direct sum of operators (sum to $\mathcal{J}$). Then, we can treat the positive $\norm{\left(\mathcal{J}_{\lambda_i,m_i}\right)^n}_\text{HS}^2$'s as elements of the sequences $\left\{ \left(\norm{\left(\mathcal{J}_{\lambda_i,m_i}\right)^n}_\text{HS}\right)^2\right\}$, each converging to a corresponding $\abs{\lambda_i}^2$, $i=1,2,\dots,k \leq s$. Using the result from Lemma 4.1., we get
\begin{align*}
\lim\limits_{n\to\infty}\left( \norm{\mathcal{J}^n}_\text{HS} \right)^{\frac{1}{n}}
= \lim\limits_{n\to\infty}\sqrt{ \left( \sum^{s}_{i=1} \norm{ \left( \mathcal{J}_{\lambda_i, m_i}\right)^n}_\text{HS}^2 \right)^\frac{1}{n}}= \sqrt{\sup_{i}(\abs{\lambda_i}^2)}= \sup_{i}(\abs{\lambda_i}) \equiv \rho(\mathcal{J}) = \rho(T).
\end{align*}
We have also argued that $\lim\limits_{n\to\infty}\left( \norm{\mathcal{J}^n}_\text{HS} \right)^{\frac{1}{n}} = \lim\limits_{n\to\infty}\left( \norm{T^n}_\text{HS} \right)^{\frac{1}{n}}$, so we have
\begin{align*}
\lim\limits_{n\to\infty}\left( \norm{T^n}_\text{HS} \right)^{\frac{1}{n}} = \rho(T).
\end{align*}
With this we are done with the first part of the proof. Next, we want to show 
\begin{align*}
\lim\limits_{n\to\infty}\left( \norm{T^n}_\text{HS} \right)^{\frac{1}{n}} = \inf \{  || T^m || ^\frac{1}{m} : m \in \mathbb{N} \}.
\end{align*}
To this end, we first translate our result from using the Hilbert-Schmidt norm to using the operator norm. We do this by the equivalence of norms. Since $\norm{\cdot}$ is equivalent to $\norm{\cdot}_{\text{HS}}$, there exist positive numbers $a,b$ such that
\begin{align*}
a\norm{T^n}_{\text{HS}} \leq \norm{T^n} \leq b\norm{T^n}_{\text{HS}}.
\end{align*}
Taking the $n$th root of this inequality and taking the limit as $n\to \infty$, we have
\begin{align*}
\lim\limits_{n\to \infty}\sqrt[n]{a}\norm{T^n}_{\text{HS}}^{1/n} \leq \lim\limits_{n\to \infty}\norm{T^n}^{1/n} \leq \lim\limits_{n\to \infty}\sqrt[n]{b}\norm{T^n}_{\text{HS}}^{1/n}.
\end{align*}
Of course, $\lim\limits_{n\to\infty} \sqrt[n]{a} = \lim\limits_{n\to\infty} \sqrt[n]{b} = 1$, so we are left with
\begin{align}\label{eq2}
\lim\limits_{n\to \infty}\norm{T^n}_{\text{HS}}^{1/n} \leq \lim\limits_{n\to \infty}\norm{T^n}^{1/n} \leq \lim\limits_{n\to \infty}\norm{T^n}_{\text{HS}}^{1/n} \implies \lim\limits_{n\to\infty} \norm{T^n}_\text{HS}^{1/n} = \lim\limits_{n\to \infty}\norm{T^n}^{1/n} = \rho(T).
\end{align}
To finish the proof, we want to show
\begin{align*}
\lim\limits_{n\to \infty}\norm{T^n}^{1/n} = \inf \{  || T^m || ^\frac{1}{m} : m \in \mathbb{N} \}.
\end{align*}


Consider an eigenvalue $\lambda$ of $T$. $\lambda \in \sigma(T)$, the spectrum of $T$. By the spectral mapping theorem, $\lambda^n \in \sigma(T^n)$. Since $\norm{T^n} = \sup \{ M \in \mathbb{R} : \norm{T^nx} \leq M\norm{x},\,\forall x\in \mathbb{R}^n \}$ (by Problem 3), we see that $\abs{\lambda^n} \leq \norm{T^n}$, which implies $\abs{\lambda} \leq \norm{T^n}^{1/n}$, for all $n\in \mathbb{N}$. This means $\abs{\lambda} \leq \inf \{ \norm{T^n}^{1/n} : n\in \mathbb{N} \}$. Now, with $\rho(T) \equiv \sup_i(\abs{\lambda_i})$, we have 
\begin{align*}
\lim\limits_{n\to \infty}\norm{T^n}^{1/n} =  \rho(T) \leq \inf \{ \norm{T^n}^{1/n} : n\in \mathbb{N} \}
\end{align*}
But of course, we also have by definition
\begin{align*}
\inf \{ \norm{T^n}^{1/n} : n\in \mathbb{N} \} \leq \lim\limits_{n\to \infty}\norm{T^n}^{1/n}.
\end{align*}
So, as desired:
\begin{align}\label{eq1}
\lim\limits_{n\to \infty}\norm{T^n}^{1/n} = \inf \{  || T^m || ^\frac{1}{m} : m \in \mathbb{N} \}
\end{align}
From (\ref{eq2}) and (\ref{eq1}), 
\begin{align*}
\inf \{ \norm{T^m}^{1/m} : m \in \mathbb{N}  \} = \rho(T) \equiv \sup\{ \abs{\alpha} : \alpha \text{ an eigenvalue of } T  \}.
\end{align*}
\hfill $\square$


% https://math.berkeley.edu/~ceur/notes_pdf/Eur_Math206_BanachSpectral_Notes.pdf
% rudin's functional analysis
% krey... introductory functional analysis
% JA Tropp


\noindent \textbf{Reference}

I found the statement of the theorem in section 10.13 of Rudin's \textit{Functional Analysis} (1991), and a less advanced approach to proving it in J.A. Tropp's \textit{An Elementary Proof of the Spectral Radius Formula for Matrices}, link \href{https://pdfs.semanticscholar.org/0bb8/67b2a2cc8e2711bc3273e21db5acdbc06e4d.pdf}{\underline{here}}.  My proof, which I actually did as an exercise in MA353, uses Jordan canonical form (like Tropp's except I used the Hilbert-Schmidt norm). The statement about the equivalence of norms is theorem 2.4-5, Erwin Kreyszig's \textit{Introductory Functional Analysis with Applications}. 



\end{document}




