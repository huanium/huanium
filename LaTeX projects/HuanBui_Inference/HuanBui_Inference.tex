\documentclass{book}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{hyperref}
\usepackage{tensor}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\usepackage{qcircuit}




\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\ham}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}


\newcommand{\Else}{\text{else}}
\newcommand{\N}{\mathcal{N}}


\newcommand{\sig}{\bm\sigma}
\newcommand{\n}{\mathbf{n}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\s}{\mathbf{S}}

\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\Id}{\mathbb{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\had}{\mathbf{H}}
\newcommand{\Y}{\mathbf{Y}}
%\setcounter{chapter}{-1}


\makeatletter
\renewcommand{\@chapapp}{Part}
%\renewcommand\thechapter{$\bf{\ket{\arabic{chapter}}}$}
%\renewcommand\thesection{$\bf{\ket{\arabic{section}}}$}
%\renewcommand\thesubsection{$\bf{\ket{\arabic{subsection}}}$}
%\renewcommand\thesubsubsection{$\bf{\ket{\arabic{subsubsection}}}$}
\makeatother



\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
	\begin{titlepage}\centering
		\clearpage
		\title{{\textsc{\textbf{STATISTICAL INFERENCE}}}\\ \smallskip - A Quick Guide - \\}
		\author{\bigskip Huan Q. Bui}
		\affil{Colby College\\$\,$\\ PHYSICS \& MATHEMATICS\\ Statistics \\$\,$\\Class of 2021\\}
		\date{\today}
		\maketitle
		\thispagestyle{empty}
	\end{titlepage}

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

This guide is based on SC482: Statistical Inference, taught by Professor Liam O'Brien. The guide consists of lecture notes and material from \textit{Introduction to Mathematical Statistics, 8th edition} by Hogg, McKean, and Craig. A majority of the text will be reading notes and solutions to selected problems. \\

As this is intended only to be a reference source, I might not be as meticulous with my explanations as I have been in some other guides. \\

Enjoy! 

\newpage
\tableofcontents
\newpage




\chapter{Special Distributions}
\newpage


\section{The Binomial and Related Distributions}
If we let the random variable $X$ equal the number of observed successes in $n$ independent Bernoulli trials, each with success probability of $p$, then $X$ follows the binomial distribution.s\\



A binomial pmf is given by
\begin{align}
\boxed{p(x) = \begin{cases}
{n\choose x}p^x(1-p)^{n-x} \quad x=0,1,2,\dots \\
0, \quad \text{else}
\end{cases}}
\end{align}
Using the binomial expansion formula, we can easily check that
\begin{align}
\boxed{\sum_x p(x) = 1}
\end{align}
The mgf of a binomial distribution is obtained by:
\begin{align}
\boxed{M_{\text{bin}}(t) = E[e^{tx}] = \sum_x e^{tx}p(x) = \lb (1-p) + pe^t \rb^n \forall t \in \R}
\end{align}
With this, we can find the mean and variance for $p(x)$:
\begin{align}
\boxed{\mu = M'(0) = n, \quad \sigma^2 = M''(0) = np(1-p)}
\end{align}

\noindent \textbf{Theorem:} Let $X_1, X_2, \dots, X_m$ be independent binomial random variables such that $X_i \sim \text{bin}(n_i, p), i = 1,2,\dots,m$. Then 
\begin{align}
\boxed{Y = \sum^m_{i=1} X_i \sim \text{bin}\lp \sum^m_{i=1}n_i, p \rp}
\end{align}
\noindent \textit{Proof:} We prove this via the mgf for $Y$. By independence, we have that
\begin{align}
M_Y(t) = \prod^m_{i=1}(1-p + pe^t)^{n_i} = (1-p + pe^t)^{\sum^m_{i=1} n_i}
\end{align}
The mgf completely determines the distribution which $Y$ follows, so we're done. \qed



\subsection{Negative Binomial \& Geometric Distribution}
Consider a sequence of independent Bernoulli trials with constant probability $p$ of success. The random variable $Y$ which denotes the total number of failures in this sequence before the $r$th success follows the negative binomial distribution.\\


A negative binomial pmf is given by
\begin{align}
\boxed{p_Y(t) = \begin{cases}
{(y+r-1)\choose{r-1}}p^r(1-p)^y \quad y = 0,1,2,\dots\\
0, \quad \text{else}
\end{cases}}
\end{align} 
The mgf of this distribution is 
\begin{align}
\boxed{M(t) = p^r[1-(1-p)e^{t}]^{-r}}
\end{align}
 
 
When $r=1$, $Y$ follows the geometric distribution, whose pmf is given by
\begin{align}
\boxed{p_Y(y) = p(1-p)^y,\quad y = 0,1,2,\dots}
\end{align}
The mgf of this distribution is 
\begin{align}
\boxed{M(t) = p[1-(1-p)e^{t}]^{-1}}
\end{align}




\section{Multinomial Distribution}
We won't worry about this for now.
\section{Hypergeometric Distribution}
We won't worry about this for now.



\section{The Poisson Distribution}

The Poisson distribution gives the probability of observing $x$ occurrences of some rare events characterized by rate $\lambda > 0$. The pmf is given by
\begin{align}
\boxed{p(x) = \begin{cases}
	\f{\lambda^x e^{-\lambda}}{x!}, \quad x = 0,1,2,\dots\\
	0, \quad \text{else}
	\end{cases}}
\end{align}
We say a random parameter with the pmf of the form of $p(x)$ follows the Poisson distribution with parameter $\lambda$.  \\

The mgf of a Poisson distribution is given by 
\begin{align}
\boxed{M(t)= e^{-\lambda (e^t-1)}}
\end{align}
From here, we can find the mean and variance:
\begin{align}
\boxed{\mu = M'(0) = \lambda, \quad \sigma^2 = M''(0) = \lambda}
\end{align}


\noindent\textbf{Theorem:} If $X_1, \dots, X_n$ are independent random variables, each $X_i \sim \text{Poi}(\lambda_i)$, then 
\begin{align}
\boxed{Y = \sum^n_{i=1} X_i \sim \text{Poi}\lp \sum^n_{i=1}\lambda_i \rp}
\end{align}
\noindent \textit{Proof:} We once again prove this via the mgf of $Y$:
\begin{align}
M_Y(t) = \prod^n_{i=1}e^{\lambda_i (e^t-1)} = e^{\sum^n_{i=1} \lambda_i(e^t - 1)}
\end{align}
\qed



\section{The $\Gamma, \chi^2, \beta$ distributions}

The gamma function of $\alpha > 0$ is given by
\begin{align}
\Gamma(\alpha) =\int^\infty_0 y^{\alpha-1}e^{-y}\,dy,
\end{align}
which gives $\Gamma(1) = 1$ and $\Gamma(\alpha) = (\alpha-1)\Gamma(\alpha-1)$. 

\subsection{The $\Gamma$ and exponential distribution}
A continuous random variable $X \sim \Gamma(\alpha,\beta)$ where $\alpha > 0$ and $\beta > 0$ whenever its pdf is
\begin{align}
\boxed{f(x) = \begin{cases}
	\f{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha-1} e^{-x/\beta}, \quad 0 < x < \infty\\
	0, \quad \text{else}
	\end{cases}}
\end{align}
The mgf for $X$ is obtained via the change of variable $y = x(1-\beta t)/\beta$, where $t < 1/\beta$:
\begin{align}
\boxed{M(t) = \int^\infty_0 \f{1}{\Gamma(\alpha)\beta^\alpha} x^{\alpha-1} e^{-x(1-\beta t)/\beta}\,dx = \f{1}{(1-\beta t)^\alpha}}
\end{align}
From here, we can find the mean and variance:
\begin{align}
\boxed{\mu = M'(0) = \alpha\beta, \quad \sigma^2 = \alpha \beta^2}
\end{align}

The $\Gamma(1,\beta)$ distribution is a special case, and it is called the \textbf{exponential distribution} with parameter $1/\beta$.\\

\noindent\textbf{Theorem:} Let $X_1, \dots, X_n$ be independent random variables, with $X_i \sim \Gamma(\alpha_i, \beta)$. Then
\begin{align}
\boxed{Y = \sum^n_{i=1} X_i \sim \Gamma\lp \sum^n_{i=1}\alpha_i, \beta \rp}
\end{align} 
\noindent\textit{Proof:} Can you guess via which device we prove the statement above?\qed


\subsection{The $\chi^2$ distribution}

The $\chi^2$ distribution is a special case of the gamma distribution where $\alpha = r/2, r \in \mathbb{N}^*$ and $\beta = 2$. If a continuous r.v. $X \sim \chi^2(r)$ then its pdf is 
\begin{align}
\boxed{f(x) = \begin{cases}
\f{1}{\Gamma(r/2)2^{r/2}} x^{r/2-1}e^{-x/2}, \quad 0 < x < \infty\\
0, \quad \text{else}
\end{cases}}
\end{align}
Its mgf is 
\begin{align}
\boxed{M(t) = (1-2t)^{-r/2}, \quad t < \f{1}{2}}
\end{align}

\noindent\textbf{Theorem:} Let $X \sim \chi^2(r)$ and $k > -r/2$ be given. Then $ E[X^k] $ exists and is given by
\begin{align}
\boxed{E[X^k] = \f{2^k\Gamma\lp r/2 + k \rp}{\Gamma(r/2)}}
\end{align}
\noindent \textit{Proof:} is proof is purey computational and is left to the reader. \qed\\

From here, we note that all moments of the $\chi^2$ distribution exist.\\

\noindent\textbf{Theorem:} Let $X_1, \dots, X_n$ be r.v. with $X_i \sim \chi^2(r_i)$. Then
\begin{align}
\boxed{Y = \sum^n_{i=1}X_i \sim \chi^2\lp \sum^n_{i=1}r_i \rp}
\end{align}
\noindent \textit{Proof:} we once again find the mgf for $Y$. \qed\\


\subsection{The $\beta$ distribution}

The $\beta$ distribution differs from the other continuous ones we've discussed so far because its support are bounded intervals. \\

I will skip most of the details here, except mentioning that we can derive the beta distribution from the a pair of independent $\Gamma$ random variables. Suppose $Y = X_1/(X_1 + X_2)$ where $X_i \sim \Gamma(\alpha, \beta)$ then the pdf of $Y$ is that of the beta distribution:
\begin{align}
\boxed{g(y) = \begin{cases}
	\f{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}y^{\alpha-1}(1-y)^{\beta-1},\quad 0< y< 1\\
	0,\quad \text{else}
	\end{cases}}
\end{align} 
The mean and variance of $Y$ are
\begin{align}
\boxed{\mu = \f{\alpha}{\alpha+ \beta}, \quad \sigma^2 = \f{\alpha\beta}{(\al + \be + 1)(\al + \be)^2}}
\end{align}





\section{The Normal distribution}

I have dedicated a large chunk in the \href{https://huanqbui.com/LaTeX projects/HuanBui_QM/HuanBui_QM.pdf}{\underline{QFT}} notes to evaluating Gaussian integrals, so I won't go into that here. \\

$X \sim \mathcal{N}(\mu,\sigma^2)$ whenever its pdf is
\begin{align}
\boxed{f(x) = \f{1}{\sqrt{2\pi \sigma^2}} \exp\lp -\f{1}{2}\f{(x-\mu)^2}{\sigma^2} \rp, \quad -\infty < x < \infty}
\end{align}
where $\mu$ and $\sigma^2$ are the mean and variance of $X$, respectively. \\

The mgf of $X$ is can be obtained via the substitution $X = \sigma Z + \mu$:
\begin{align}
\boxed{M(t) = \exp\lp \mu t  + \f{1}{2}\sigma^2 t^2 \rp}
\end{align}
We note the following correspondence for $X = \sigma Z + \mu$:
\begin{align}
X \sim \mathcal{N}(\mu,\sigma^2) \iff {Z \sim \mathcal{N}(0,1)}
\end{align}

\noindent\textbf{Theorem:} $X \sim \mathcal{N}(\mu, \sigma^2) \implies V = (X-\mu)^2/\sigma^2 \sim \chi^2(1)$, i.e. a standardized, squared normal follows a chi-square distribution. \\

\noindent \textit{Proof:} The proof isn't too hard. Let us write $V$ as $W^2$ and so $W \sim \mathcal{N}(0,1)$.  We consider the cdf $G(v)$ for $V$, with $v \geq 0$:
\begin{align}
G(v) = P( W^2 \leq v) = P( -\sqrt{v} \leq  W  \leq \sqrt{v} ) = 2\int^{\sqrt{v}}_0 \f{1}{ \sqrt{2\pi} } e^{-w^2/2}\,dw
\end{align}
with $G(v) = 0$ whenever $v<0$. From here, we can see that the pdf for $v$, under the change of notation $w \to \sqrt{y}$, is
\begin{align}
g(v) = G'(v) = \f{d}{dv}\lc \int^v_0 \f{1}{\sqrt{2\pi}\sqrt{y}}e^{-y/2}\,dy \rc, \quad 0 \geq v
\end{align}
or $0$ otherwise. This means
\begin{align}
\boxed{g(v) = \begin{cases}
\f{1}{\sqrt{\pi}\sqrt{2}} v^{1/2-1}e^{-v/2}, \quad 0 < v< \infty\\
0, \quad \text{else}
\end{cases}}
\end{align}
Using the fact that $\Gamma(1/2) = \sqrt{\pi}$ and by verifying that $g(v)$ integrates to unity we show $V \sim \chi^2(1)$. \qed\\



\noindent\textbf{Theorem:}  Let $X_1, \dots, X_n$ be independent r.v. with $X_i \sim \mathcal{N}(\mu_i, \sigma_i^2)$. Then for constants $a_1,\dots, a_n$
\begin{align}
\boxed{Y = \sum^n_{i=1}a_i X_i \sim \mathcal{N}\lp \sum^n_{i=1}a_i\mu_i, \sum^n_{i=1}a_i^2\sigma^2_i \rp}
\end{align}
\noindent \textit{Proof:} We once again prove this kind of theorems via the mgf for $Y$:
\begin{align}
M(t) &= \prod^n_{i=1}\exp\lp ta_i\mu_i + \f{1}{2}a_i^2\sigma_i^2 \rp \nn\\
&= \exp\lc t\sum^n_{i=1}a_i\mu_i + \f{1}{2}t^2\sum^n_{i=1}a_i^2\sigma_i^2 \rc
\end{align}
which is the mgf for the normal with the corresponding mean and variance above. \qed\\


\textbf{Corollary:} Let $X_1, \dots, X_n \sim \mathcal{N}(\mu,\sigma^2)$. Then 
\begin{align}
\boxed{\bar{X} = \f{\sum^n_{i=1} X_i}{n} \sim \mathcal{N}\lp \mu, \sigma^2/n \rp}
\end{align} 
\noindent \textit{Proof:} the proof is left to the reader.



\subsection{Contaminated Normal}

We won't worry about this for now.


\section{The Multivariate Normal}
I'll just jump straight to the $n$-dimensional generalization. Evaluations of high-dimensional Gaussian integrals and moments can also be found in the \href{https://huanqbui.com/LaTeX projects/HuanBui_QM/HuanBui_QM.pdf}{\underline{QFT}} notes. \\

We say an $n$-dimensional random vector $\X$ has a multivariate normal distribution if its mgf is 
\begin{align}
\boxed{M_{\X}(t) = \exp\lp \mathbf{t}^\top\bm{\mu} + \f{1}{2}\mathbf{t}^\top\mathbf{\Sigma}\mathbf{t} \rp}
\end{align}
for all $\mathbf{t} \in \R^n$, where $\mathbf{\Sigma}$ is a symmetric, positive semi-definite matrix and $\bm{\mu} \in \R^n$. For short, we say $\X \sim \mathcal{N}_n(\bm{\mu}, \mathbf{\Sigma})$.   \\

\noindent\textbf{Theorem:} Suppose $\X \sim \mathcal{N}_n(\bm{\mu}, \mathbf{\Sigma})$ where $\mathbf{\Sigma}$ is positive definite. Then 
\begin{align}
\boxed{\Y = (\X - \bm{\mu})^\top \mathbf{\Sigma}^{-1}(\X - \bm{\mu}) \sim\chi^2(1)}
\end{align}

\noindent\textbf{Theorem:} If $\X \sim \mathcal{N}_n(\bm{\mu},\mathbf{\Sigma})$ then
\begin{align}
\boxed{\Y = \mathbf{A}\X + \mathbf{b} \sim \mathcal{N}_n(\mathbf{A}\bm{\mu} + \mathbf{b})}
\end{align}
\noindent \textit{Proof:} The proof once again uses the mgf for $\Y$, but also some linear algebra manipulations. \qed\\

There are many other theorems and results related to this topic, but I won't go into them for now. \\




\section{The $t$- and $F$-distributions}


These two distributions are useful in certain problems in statistical inference. 

\subsection{The $t$-distribution}
Suppose $W \sim \mathcal{N}(0,1)$ and $V \sim \chi^2(r)$ and that they are independent. Then the joint pdf of $W$ and $V$, called $h(w,v)$, is the product of the pdf's of $W$ and $V$:
\begin{align}
h(w,v) = \begin{cases}
\f{1}{\sqrt{2\pi}}e^{-w^2/2}\f{1}{\Gamma(r/2)2^{r/2}}v^{r/2-1}e^{-v/2}, \quad w\in \R, v >0\\
0, \quad \Else
\end{cases}
\end{align}
Now we define a new variable $T = W/\sqrt{V/r}$ and consider the transformation:
\begin{align}
t = \f{w}{\sqrt{v/r}} \quad u=v
\end{align}
which bijectively maps the parameter space $(w,v) = \R\times \R^+$ to $(t,u)=\R \times \R^+$. The absolute value of the Jacobian of the transformation is given by
\begin{align}
\abs{J} = \abs{\det\begin{pmatrix}
	\p_tw & \p_u w\\
	\p_tv & \p_u v
	\end{pmatrix}} = \f{\sqrt{u}}{\sqrt{r}}.
\end{align} 
With this, the joint pdf of $T$ and $U \equiv V$ is given by
\begin{align}
g(t,u) =  \abs{J}h\lp \f{t\sqrt{u}}{\sqrt{r}},u \rp
= \begin{cases}
\f{u^{r/2-1}}{\sqrt{2\pi} \Gamma(r/2) 2^{r/2}}\exp\lb -\f{u}{2} \lp 1 + \f{t^2}{r} \rp \rb \f{\sqrt{u}}{\sqrt{r}}, \quad t\in \R, u \in \R^+\\
0,\quad \Else
\end{cases}
\end{align}
By integrating out $u$ we obtain the marginal pdf for $T$:
\begin{align}
g_1(t) &= \int^\infty_{-\infty}g(t,u)\,du \nn\\
&= \int^\infty_0 \f{u^{(r+1)/2-1}}{\sqrt{2\pi r} \Gamma(r/2) 2^{r/2}}\exp\lb -\f{u}{2} \lp 1 + \f{t^2}{r} \rp \rb \,du.
\end{align}
Via the substitution $z = u[1 + (t^2/r)]/2$ we can evaluate the integral to find for $t\in \R$
\begin{align}
\boxed{g_1(t) = \f{\Gamma[(r+1)/2]}{\sqrt{\pi r}\Gamma(r/2)}  \f{1}{(1 + t^2/r)^{(r+1/2)}} }
\end{align}

A r.v. $T$ with this pdf is said to follow the $t$-distribution (or the Student's $t$-distribution) with $r$ degrees of freedom. The $t$-distribution is symmetric about 0 and has a unique maximum at 0. As $r\in \infty$, the $t$-distribution converges to $\N(0,1)$. \\

The mean of $T \sim \text{Stu}(r)$ is zero. The variance can be found to be $\text{Var}(T) = E[T^2] = \f{r}{r-2}$, so long as $r>2$. 
   





\subsection{The $F$-distribution}
Let $U \sim \chi^2(r-1), V\sim \chi^2(r_2)$ be given. Then the joint pdf of $U$ and $V$ is once again the product of their pdf's:
\begin{align}
h(u,v) = \begin{cases}
\f{1}{\Gamma(r_1/2)\Gamma(r_2/2) 2^{(r_1+r_2)/2}}u^{r_1/2-1}v^{r_2/2-1}e^{-(u+v)/2}, \quad u,v \in \R^+\\
0,\quad \Else
\end{cases}
\end{align}
Define the new random variable 
\begin{align}
W = \f{U/r_1}{V/r_2}
\end{align}
whose pdf $g_1(w)$ we are interested in finding. Consider the transformation
\begin{align}
w = \f{u/r_1}{v/r_2}, \quad z=v
\end{align}
which bijectively maps $(u,v) = \R^+\times \R^+ \to (w,z) = [\R^+\times \R^+]$. Like last time, the absolute value of the Jacobian can be found to be
\begin{align}
\abs{J} = \f{r_1}{r_2}z.
\end{align}
The joint pdf $g(w,z)$ of the random variables $W$ and $Z=V$ is obtained from by scaling $h(u,v)$ by $\abs{J}$ and applying the variable transformation:
\begin{align}
g(w,z) = 
\f{1}{\Gamma(r_1/2)\Gamma(r_2/2) 2^{\f{r_1+r_2}{2}}} \lp \f{r_1 zw}{r_2} \rp^{\f{r_1-2}{2}}z^{\f{r_2-2}{2}}\exp\lb -\f{z}{2}\lp \f{r_1w}{r_2} +1 \rp\rb \f{r_1z}{r_2}
\end{align}
so long as $(w,z)\in \R^+\times \R^+$ and 0 otherwise. We then proceed to find the marginal pdf $g_1(w)$ of $W$ by integrating out $z$. By considering the change of variables:
\begin{align}
y = \f{z}{2}\lp \f{r_1w}{r_2} + 1 \rp
\end{align}
we can evaluate the integral and find the marginal pdf of $W$ to be 
\begin{align}
g_1(w) = \begin{cases}
\f{\Gamma[(r_1+r_2)/2](r_1/r_2)^{r_1/2}}{\Gamma(r_1/2)\Gamma(r_2/2)}\f{w^{r_1/2-1}}{(1+r_1w/r_2)^{(r_1+r_2)/2}}, \quad w\in \R^+\\
0, \quad \Else
\end{cases}
\end{align}
$W$, which is the ratio of two independent chi-quare variables $U,V$, is said to follow an $F$-distribution with degrees of freedom $r_1$ and $r_2$. We call the ratio $W = (U/r_1)/(V/r_2)$ the ``$F$'' ratio. \\

The mean of $W$ is $E[F] = \f{r_2}{r_2-2}$. When $r_2$ is large, $E[F] \to 1$.  


\subsection{The Student's Theorem}

Here we will create the connection between the normal distribution and the $t$-distribution. This is an important result for the later topics on inference for normal random variables. \\

\noindent\textbf{Theorem:} Let $X_1, \dots, X_n$ be iid r.v. with $X_i \sim \N(\mu,\sigma^2) \forall i$. Define the r.v.'s
\begin{align}
\bar{X} &= \f{1}{n}\sum^n_{i=1}X_i\\
S^2 &= \f{1}{n-1}\sum^n_{i=1}(X_i - \bar{X})^2.
\end{align}
Then
\begin{enumerate}[(a)]
	\item $\bar{X} \sim \N(\mu, \sigma^2/n)$.
	\item $\bar{X}$ and $S^2$ are independent. 
	\item $(n-1)S^2/\sigma^2 \sim \chi^2(n-1)$.
	\item The variable $\bar{T} = (\bar{X} - \mu)/(S/\sqrt{n})$ follows the Student's $t$-distribution with $n-1$ degrees of freedom. 
	\begin{align}
	\bar{T} = \f{\bar{X} - \mu}{S/\sqrt{n}} \sim \text{Stu}(n-1).
	\end{align}
\end{enumerate}

\noindent \textit{Proof:} The proof is good, so I will reproduce it here. Because $X_i \sim \N(\mu,\sigma^2) \forall i$, $\X \sim \N_n\lp \mu \mathbf{1}, \sigma^2\mathbf{1} \rp$, where $\mathbf{1}$ denotes the $n$-vector whose components are all 1. Now, consider $\mathbf{v}^\top = (1/n)\mathbf{1}^\top$. We see that $\bar{X} = \mathbf{v}^\top \X$. Define the random vector $\Y = (X_1 - \bar{X}, \dots, X_2 - \bar{X})^\top$ and consider the (true) equality:
\begin{align}
\mathbf{W} = \begin{pmatrix}
\bar{X} \\ \Y
\end{pmatrix} = \underbrace{\begin{pmatrix}
\mathbf{v}^\top \\ 
\Id - \mathbf{1}\mathbf{v}^\top
\end{pmatrix}}_{\text{the transformation}} \X
\end{align} 
which just restates our definitions nicely. We see that $\mathbf{W}$ is a result of a linear transformation of multivariate normal random vector, and so it follows that $\mathbf{W} \sim \N_{n+1}$ with mean 
\begin{align}
E[\mathbf{W}] = \begin{pmatrix}
\mathbf{v}^\top \\
\Id - \mathbf{1}\mathbf{v}^\top
\end{pmatrix}\mu \mathbf{1} = \begin{pmatrix}
\mu \\ \mathbf{0}_n
\end{pmatrix}
\end{align}
and the covariance matrix
\begin{align}
\mathbf{\Sigma} = \begin{pmatrix}
\mathbf{v}^\top \\
\Id - \mathbf{1}\mathbf{v}^\top
\end{pmatrix}\sigma^2 \Id
\begin{pmatrix}
\mathbf{v}^\top \\
\Id - \mathbf{1}\mathbf{v}^\top
\end{pmatrix}^\top = \sigma^2 \begin{pmatrix}
\f{1}{n} & \mathbf{0}_n^\top \\
\mathbf{0}_n & \Id - \mathbf{1}\mathbf{v}^\top
\end{pmatrix}
\end{align}

From here, part (a) is proven. Next, observe that $\mathbf{\Sigma}$ is diagonal, and so all covariances are zero. This means $\bar{X}$ is independent of $\mathbf{Y}$. But because $S^2 = (n-1)^{-1}\Y^\top \Y$, $\bar{X}$ is independent of $S^2$ as well. So, (b) is proven. \\

Now, consider the r.v. 
\begin{align}
V = \sum^n_{i=1} \lp\f{X_i - \mu}{\sigma}\rp^2
\end{align} 
Each summand of $V$ is a square of an $\N(0,1)$ r.v., and so each follows a $\chi^2(1)$. Because $V$ is a sum of squares of $n$ such $\chi^2(1)$'s, $V \sim\chi^2(n)$. Next, we can rewrite $V$ as
\begin{align}
V = \sum^n_{i=1}\lp \f{(X_i - \bar{X}) + (\bar{X} - \mu)}{\sigma} \rp^2 = \f{(n-1)S^2}{\sigma^2} + \lp \f{\bar{X} - \mu}{\sigma/\sqrt{n}} \rp^2.
\end{align}
By (b), the summands in the last equation is are independent. The second term is a square of a $\N(0,1)$, so it follows a $\chi^2(1)$. Taking mgfs of both sides, we get
\begin{align}
(1-2t)^{-n/2} = \underbrace{E[\exp{t(n-1)S^2/\sigma^2}]}_{M_{\text{(c)}}} (1-2t)^{-1/2}.
\end{align}
Solving for the mgf of $(n-1)S^2/\sigma^2$ we get part $(c)$. Finally, writing $T$ as
\begin{align}
T = \f{(\bar{X} - \mu)/(\sigma/\sqrt{n})}{\sqrt{(n-1)S^2/(\sigma^2(n-1))}}
\end{align}
and using (a)-(c) gives us (d). \textit{Hint}: consider what distributions the numerator and denominator of $T$ follow. \qed
















\chapter{Elementary Statistical Inferences}
\newpage

\section{Sampling \& Statistics}

In statistical inferences, our ignorance about the pdf/pmf of a random variable $X$ can be classified in two ways:
\begin{itemize}
	\item The pdf/pmf is unknown.
	\item The pdf/pmf is assumed/known but its parameter vector $\theta$ is not.
\end{itemize}

We consider the second class of classification for now.\\

\noindent \textbf{Definition:} If the random variables $X_1, X_2, \dots, X_n$ are iid, then these random variables constitute a \textbf{random sample} of size $n$ from the common distribution. \\


\noindent\textbf{Definition:} Let $X_1, \dots, X_n$ denote a sample on a random variable $X$. Let $T = T(X_1,\dots,X_n)$ be a function of the sample. Then $T$ is called a \textbf{statistic}. 





\subsection{Point estimators}

\noindent \textbf{Definition:} (\textit{Unbiasedness}) Let $X_1,\dots,X_n$ denote a sample on a random variable $X$ with pdf $f(x;\theta)$, $\theta\in \Omega$. Let $T = T(X_1,\dots,X_n)$ be a statistic. We say that $T$ is an \textit{unbiased} estimator of $\theta$ if $E[T] = \theta$.  \\

We now introduce the concept of the \textbf{maximum likelihood estimator (mle)}. The information in the sample and the parameter $\theta$ are involved in the joint distribution of the random sample. We write this as
\begin{align}
\boxed{L(\theta) = L(\theta; x_1,\dots,x_n) = \prod^n_{i=1}f(x_i;\theta).}
\end{align}
This is called the \textbf{likelihood function} of the random sample. A measure of the center of $L(\theta)$ seems to be an appropriate estimate of $\theta$. We often use the value of $\theta$ at which $L(\theta)$ is maximized. If this value is unique, then it is called the \textbf{maximum likelihood estimator} (mle), denoted as $\hat{\theta}$:
\begin{align}
\hat{\theta} = \text{Argmax}L(\theta).
\end{align}
We often work with the log of the likelihood in practice, which is the function $l(\theta) = \log(L(\theta))$. The logarithm is a strictly increasing function, so its maximum is obtained exactly when the maximum of $L(\theta)$ is obtained. In most models, the pdf and pmf are differentiable functions of $\theta$, in which cases $\hat{\theta}$ solves the equation:
\begin{align}
\boxed{\p_\theta l(\theta) = 0 }
\end{align}
This is equivalent to saying $\hat{\theta}$ maximizes $l(\theta)$. If $\theta$ is a vector of parameters, this results in a system of equations to be solved simultaneously. These equations are called the \textbf{estimating equations}, (EE).




\subsection{Histogram estimates of pmfs and pdfs}


Let $X_1,\dots,X_n$ be a random sample on a random variable $X$ with cdf $F(x)$. A histogram of the sample is an estimate of the pmf or pdf depending on whether $X$ is discrete of continuous. We make no assumptions on the form of the distribution of $X$. In particular, we don't assume the parametric form of the distribution, hence the histogram is often called the \textbf{nonparametric} estimator. 







\subsubsection{The distribution of $X$ is discrete}

Assume $X$ is a discrete r.v. with pmf $p(x)$. Consider a sample $X_1,\dots,X_n$. Suppose $X \in \mathcal{D} = \{ a_1,\dots, a_n \}$, then intuitively the estimate of $p(a_j)$ is the relative frequency of $a_j$. More formally, for $j = 1,\dots,m$ we define the statistic
\begin{align}
I_j(X_i) = \begin{cases}
1 \quad X_i = a_j\\ 0 \quad X_i \neq a_j
\end{cases}
\end{align} 
Then the estimate of $p(a_j)$ is the average
\begin{align}
{\hat{p}(a_j) = \f{1}{n}\sum^n_{i=1}I_j(X_i)}
\end{align}
The estimators $\{ \hat{p}(a_1),\dots, \hat{p}(a_m) \}$ constitute the nonparametric estimate of the pmf $p(x)$. We note that $I_j(X_i)$ has a Bernoulli distribution with probability $p(a_j)$, and so
\begin{align}
E[\hat{p}(a_j)] = \f{1}{n}\sum^n_{i=1}E[I_j(X_i)] = \f{1}{n}\sum^n_{i=1}p(a_j) = p(a_j),
\end{align}
which means $\hat{p}(a_j)$ is an \textit{unbiased estimator} of $p(a_j)$. \\

Now, suppose that the space of $X$ is infinite, i.e, $\mathcal{D} = \{ a_1, \dots, \}$ then in practice we select a value, say $a_m$, and make the groupings
\begin{align}
\{a_1\}, \{a_2\}, \dots, \{a_m\}, \tilde{a}_{m+1} = \{ a_{m+1}, \dots \}
\end{align}
Let $\hat{p}(\tilde{a}_{m+1})$ be the proportion of the sample items that are greater than or equal to $a_{m+1}$. Then the estimates $\{ \hat{p}(a_1),\dots \hat{p}(a_{m+1}) \}$ form our estimate of $p(x)$. To merge groups, the rule of thumb is to select $m$ so that the frequency of the category $a_m$ exceeds twice the combined frequencies of the categories $a_{m+1}, a_{m+2}, \dots$\\

A histogram is a \textit{barplot} pf $\hat{p}(a_j)$ versus $a_j$. When $a_j$ contains no ordinal information (e.g. hair colors, etc) then such histograms consist of nonabutting bars and are called \textit{bar charts}. When the space $\mathcal{D}$ is ordinal, then the histograms is an abutting bar chart plotted in the natural order of the $a_j$'s. 


















\subsubsection{The distribution of $X$ is continuous}

Assume $X$ is a continuous r.v. with pdf $f(x)$. Consider a sample $X_1,\dots,X_n$. We first sketch an estimate for this pdf at a specified value of $x$. For a given $h > 0$, we consider the interval $(x-h,x+h)$. By MVT, we have for $\xi, \abs{x-\xi} < h$:
\begin{align}
P(\abs{X-x} < h) = \int^{x+h}_{x-h}f(t)\,dt \approx 2hf(x).
\end{align}
The LHS is the porprotion of the sample items that fall in the interval $(x-h,x+h)$. This suggests the use of the estimate of $f(x)$ at a given $x$:
\begin{align}
\hat{f}(x) = \f{\#\{ \abs{X_i - x} < h  \}}{2hn}.
\end{align} 
More formally, the indicator statistic is, for $i = 1,\dots, n$
\begin{align}
I_i(x) = \begin{cases}
1 \quad x-h < X_i < x+h\\
- \quad \Else
\end{cases},
\end{align}
from which we obtain the nonparametric estimator of $f(x)$:
\begin{align}
\hat{f}(x) = \f{1}{2hn}\sum^n_{i=1} I_i(x).
\end{align}
Since the sample items are iid:
\begin{align}
E[\hat{f}(x)] = \f{1}{2hn}nf(\xi)2h = f(\xi) \to f(x) \quad \text{as } h\to 0.
\end{align}
Therefore $\hat{f}(x)$ is \textit{approximately} (as opposed to \textit{exact} in the discrete case) an unbiased estimator of $f(x)$. $I_i$ is called the \textbf{rectangular kernel} with \textbf{bandwidth} $2h$.  \\

Provided realized values $x_1,\dots,x_n$ of the random sample of $X$ with pdf $f(x)$, there are many ways to obtained a histogram estimate of $f(x)$. First, select an integer $m$, an $h > 0$, and a value $a < \min(x_i)$, so that the $m$ intervals cover the range of the sample. These intervals form our classes.  Let $A_j =  ( a+ (2j-3)h, a+ (2j-1)h ] $ for $j = 1,\dots,m$. Let $\hat{f}_h(x)$ denote our histogram estimate. For $a-h < x \leq (2m-1)h$, $x$ is in one and only one $A_j$. Then for $x \in A_j$, we define
\begin{align}
\hat{f}_h(x) = \f{\# \{ x_i \in A_j \}}{2hn} \geq 0.
\end{align}
We see that 
\begin{align}
\int^\infty_{\infty}\hat{f}_h(x)\,dx &= \int^{a+(2m-1)h}_{a-h}\hat{f}_h(x)\,dx \nn\\
&= \sum^m_{j=1}\int_{A_j}\f{\#\{ x_i \in A_j \}}{2hn}\,dx \nn\\
&= \f{1}{2hn}\sum^m_{j=1}\#\{  x_i \in A_j \}[h(2j-1-2j+3)]\nn\\
&= \f{2h}{2hn}n\nn\\
&= 1.
\end{align}
So $\hat{f}_h(x)$ satisfies the properties of a pdf. 








\section{Confidence Intervals}
\noindent \textbf{Definition:} Let $X_1, \dots, X_n$ be a sample on a r.v. $X$ which has the pdf $f(x;\theta), \theta \in \Omega$. Let $0 < \alpha < 1$ be specified. Let $L = L(X_1,\dots,X_n)$ and $U =U(X_1,\dots,X_n)$ be two statistics. We say that the interval $(L,U)$ is a $(1-\alpha)100\%$ \textbf{confidence interval} for $\theta$ if
\begin{align}
1- \alpha \equiv P_\theta[\theta\in (L,U)].
\end{align}  
That is, the probability that the interval includes $\theta$ is $1-\alpha$, which is called the \textbf{confidence coefficient} or the \textbf{confidence level} of the interval.\\

Under normality, the confidence interval for $\mu$ is given by
\begin{align}
\boxed{\lp \bar{x} - t_{\alpha/2,n-1} s/\sqrt{n}, \bar{x}+ t_{\alpha/2, n-1}s/\sqrt{n} \rp}
\end{align}
where $t_{\alpha/2,n-1}$ is the upper $\alpha/2$ critical points of a $t$-distribution with $n-1$ df. This CI is referred to as the $(1-\alpha)100\%$ \textbf{$t$-interval} for $\mu$. $s$ is referred to as the \textbf{standard error} of $\bar{X}$. 

\noindent \textbf{The Central Limit Theorem:} Let $X_1,\dots,X_n$ denote the observations of a random sample from a distribution that has mean $\mu$ and finite variance $\sigma^2$. Then the distribution function of the r.v. $W_n = (\bar{X} - \mu)/(\sigma/\sqrt{n})$ converges to $\Phi$, the distribution function of the $\N(0,1)$ distribution, as $n\ to \infty$. \\


When the sample is large, the CI for $\mu$ can be given by
\begin{align}
\boxed{\lp \bar{x} - z_{\alpha/2}s/\sqrt{n}, \bar{z} + z_{\alpha/a}s/\sqrt{n} \rp}
\end{align}
In general, for the same $\alpha$, the $t$-CI is larger (and hence more conservative) than the $z$-CI. When $\sigma$ is known, we replace $s$ by $\sigma$. \\

The larger sample CI for $p$ is given by
\begin{align}
\boxed{\lp \hat{p} - z_{\alpha/2}\sqrt{\hat{p}(1-\hat{p})/n}, \hat{p}+ z_{\alpha/2}\sqrt{\hat{p}(1-\hat{p})}/n \rp}
\end{align} 
where $\sqrt{\hat{p}(1-\hat{p})/n}$ is called the standard error of $\hat{p}$. 



\subsection{CI for difference in means}

By independence of samples, 
\begin{align}
\text{Var}(\hat{\Delta}) = \f{\sigma_1^2}{n_1} + \f{\sigma^2_2}{n_2}
\end{align}
where $\hat{\Delta} = \bar{X} - \bar{Y}$. We can readily show that $\hat{\Delta}$ is an unbiased estimator of $\Delta = \mu_1 - \mu_2$. Let the sample variances
\begin{align}
S_j^2 = \f{1}{n_j - 1}\sum^{n_j}_{i=1}(X_i - \bar{X})^2
\end{align}
be given. Then th random variable follows the $\N(0,1)$:
\begin{align}
\boxed{Z = \f{\hat{\Delta} - \Delta}{\sqrt{\f{S_1^2}{n_1} + \f{S^2_2}{n_2}}} \sim \N(0,1)}
\end{align}
The approximate $(1-\alpha)100\%$ CI for $\Delta = \mu_1 - \mu_2$ is then given by
\begin{align}
\lp (\bar{x} - \bar{y}) - z_{\alpha/2}\sqrt{\f{S_1^2}{n_1} + \f{S^2_2}{n_2}}, 
(\bar{x} - \bar{y}) + z_{\alpha/2}\sqrt{\f{S_1^2}{n_1} + \f{S^2_2}{n_2}} \rp
\end{align}
This is a large sample $(1-\alpha)100\%$ CI for $\mu_1 - \mu_2$. \\

Now, suppose ${X} \sim \N(\mu_1, \sigma^2)$ and ${Y} \sim \N(\mu_2, \sigma^2)$ (i.e. $X$ and $Y$ are normally distributed with the same variance) are independent. We want to show $\Delta \sim t$-distribution. We know that $\bar{X} \sim \N(\mu_1, \sigma^2/n_1)$ and $\bar{Y} \sim \N(\mu_2, \sigma^2/n_2)$, so it is true that
\begin{align}
\f{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{\sigma\sqrt{1/n_1 + 1/n_2}} \sim \N(0,1).
\end{align}
This quantity will later be the numerator of our $T$-statistic. Now, let
\begin{align}
S_p^2 = \f{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}
\end{align}
then $S_p^2$, the \textbf{pooled estimator} of $\sigma^2$, is also an unbiased estimator of $\sigma^2$. Because $(n_i - 1)S_i^2/\sigma^2 \sim \chi^2(n-1)$, we have that $(n-2)S_p^2/\sigma^2 \sim\chi^2(n-2)$. And so
\begin{align}
\boxed{T = \f{\f{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{\sigma\sqrt{1/n_1 + 1/n_2}}}{\sqrt{(n-2)S_p^2/(n-2)\sigma^2}} = \f{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{S_p\sqrt{1/n_1 + 1/n-2}} \sim t_{n-2}}
\end{align}
From here, it is easy to work out the $(1-\alpha)100\%$ CI for $\mu_1 - \mu_2$:
\begin{align}
\boxed{\lp (\bar{x}-\bar{y})-t_{\alpha/2,n-2}s_p\sqrt{\f{1}{n_1}+ \f{1}{n_2}},  (\bar{x}-\bar{y}) + t_{\alpha/2,n-2}s_p\sqrt{\f{1}{n_1}+ \f{1}{n_2}} \rp}
\end{align}

There is some difficulty when the unknown variances $\sigma$ in the distributions of $X$ and $Y$ are not equal. 


\subsection{CI for difference in proportions}

Our estimator of the difference in proportions $p_1 - p_2$ is $\bar{X} - \bar{Y} \equiv \hat{p}_1 - \hat{p}_2$ where $X \sim b(1,p_1)$ and $Y \sim b(1,p_2)$. Of course, we know that $\sigma_1^2 = p_1(1 - p_1)$ and $\sigma_2^2 = p_2(1 - p_2)$. From here, the approximate $(1-\alpha)100\%$ confidence interval for $p_1 - p_2$ is 
\begin{align}
\boxed{(\hat{p}_1 - \hat{p}_2) \pm z_{\alpha/2}\sqrt{\f{\hat{p}_1(1-\hat{p}_1)}{n_1} + \f{\hat{p}_2(1-\hat{p}_2)}{n_2}}}
\end{align}



















\section{Order Statistics}


Let $X_1, X_2,\dots,X_n$ denote a random sample from a distribution of the continuous type having a pdf $f(x)$ that has support $S = (a, b)$, where $âˆ’\infty \leq a<b \leq \infty$. Let $Y_1 < Y_2 < \dots < Y_n$ represent $X_1, X_2,\dots,X_n$ when the latter are arranged in ascending order of magnitude. We call $Y_i, i = 1, 2,...,n$, the $i$th order statistic of the random sample $X_1, X_2,\dots,X_n$. We have a theorem which gives the joint pdf of $Y_1,Y_2,\dots,Y_n$.\\

\noindent \textbf{Theorem:} The joint pdf of $Y_1, Y_2, \dots, Y_n$ is given by
\begin{align}
g(y_1, y_2, \dots, y_n) = \begin{cases}
n!f(y_1)\dots f(y_n)\quad a < y_1 < \dots < y_n <b\\
0 \quad \Else
\end{cases}.
\end{align}
\textit{Proof:} The support of $X_1,\dots,X_n$ can be partitioned into $n!$ mutually disjoint sets that map onto the support of the $Y_i$'s. Obviously the Jacobian for each transformation is either $1$ or $-1$. 
\begin{align}
g(y_1,y_2,\dots,y_n) &= \sum^{n!}_{i=1}\abs{J_i}f(y_1)\dots f(y_n)\nn\\
&= \begin{cases}
n! f(y_1)\dots f(y_n)\quad a < y_1 \dots < y_n < b\\
0 \quad \Else
\end{cases}
\end{align}




















\subsection{Quantiles}

Let $X$ be a random variable with a continuous cdf $F(x)$. For $0 <p< 1$, define
the $p$th quantile of $X$ to be $\xi_p = F^{-1}(p)$. Let $X_1, X_2,\dots,X_n$ be a random sample from the distribution of $X$ and let $Y_1 < Y_2 < \dots < Y_n$ be the corresponding order statistics. Let $k$ be the greatest integer less than or equal to $p(n + 1)$. We next define an estimator of$\xi_p$
after making the following observation. The area under the pdf $f(x)$ to the left of
$Y_k$ is $F(Y_k)$. The expected value of this area is 
\begin{align}
E[F(Y_k)] = \int^b_a F(y_k)g_k(y_k)\,dy_k
\end{align}
where $g_k(y_k)$ is the pdf of $Y_k$. Consider the transformation $z = F(y_k)$, then the integral becomes
\begin{align}
E[F(Y_k)] = \int^1_0 \f{n!}{(k-1)!(n-k)!}z^k (1-z)^{n-k}\,dz = \dots = \f{k}{n+1}
\end{align} 
where we recognize the similarity between the integral and the integral of a beta pdf. So, on the average, there is $k/(n + 1)$ of the total area to the left of $Y_k$. Because
$p = k/(n + 1)$, it seems reasonable to take $Y_k$ as an estimator of the quantile $\xi_p$. Hence, we call $Y_k$ the \textbf{$p$th sample quantile}. It is also called the \textbf{$100p$th percentile} of the sample.\\

A \textbf{five-number} summary of the data consists of the following five sample quantiles: the minimum ($Y_1$), the first quartile ($Y_{.25(n+1)}$), the median, the third quartile ($Y_{.75(n+1)}$), and the maximum ($Y_n$). For this section, we use the notation $Q_1$, $Q_2$, and $Q_3$ to denote, respectively, the first quartile, median, and third quartile of the sample.\\

The five-number summary is the basis for a useful and quick plot of the data.
This is called a \textbf{boxplot} of the data. In the \textbf{box and whisker} plots, we also define a potential outlier. Let $h = 1.5(Q_3 - Q_1)$. The \textbf{lower/upper fence} is defined by $L/UF = Q_{1/3} \mp h$. Points lying outside the $(LF,UF)$ interval are called \textbf{potential outliers}. 



\subsection{CI for quantiles}

Let $X$ be a continuous random variable with cdf $F(x)$. For $0 <p< 1$, define the
$100p$th distribution percentile to be $\xi_p$, where $F(\xi_p) = p$. For a sample of size $n$ on
$X$, let $Y_1 < Y_2 < \dots < Y_n$ be the order statistics. Let $k = [(n + 1)p]$. Then the
$100p$th sample percentile $Y_k$ is a point estimate of $\xi_p$.\\

Let $i < [(n + 1)p] < j$, and consider the order statistics $Y_i < Y_j$ and the event $Y_i < \xi_p < Y_j$. The event $Y_i < \xi_p < Y_j$ is equivalent to obtaining between $i$ (inclusive) and $j$ (exclusive) successes in $n$ independent trials. So,
\begin{align}
P(Y_i < \xi_p < Y_j) = \sum^{j-1}_{w=i} {n\choose{w}}p^w(1-p)^{n-w}.
\end{align}

For the median, we denote $\xi_{1/2}$ the median of $F(x)$, i.e. $\xi_{1/2}$ solves $F(x) = 1/2$. Let $Q_2$ denote the sample median, which is a point estimator of $\xi_{1/2}$. Take $c_{\alpha/2}$ such that $P[S \leq c_{\alpha/2}] = \alpha/2$ where $S \sim b(n,1/2)$. Then note also that $P[S \leq c_{\alpha/2}] = \alpha/2$. From here we have
\begin{align}
P[Y_{c_{\alpha/2}} < \xi_{1/2} < Y_{n-c_{\alpha/2}}] = 1-\alpha.
\end{align}
So, if $y_{\alpha/2 + 1}$ and $y_{n-\alpha/2}$ are the realized values of the order statistics $Y_{c_{\alpha/2} + 1}$ and $Y_{n-c_{\alpha/2}}$ then the interval 
\begin{align}
\boxed{\lp y_{c_{\alpha/2}}, y_{n - c_{\alpha/2}} \rp  }
\end{align}
is a $(1-\alpha)100\%$ confidence interval for $\xi_{1/2}$. 






\section{Introduction to Hypothesis Testing}

Suppose a r.v. $X \sim f(x;\theta)$, where $\theta \in \Omega$. Suppose that $\theta \in \omega_0$ or $\theta \in \omega_1$ where $\omega_0$ and $\omega_1$ are disjoint subsets of $\Omega$ and $\omega_0 \cup \omega_1 = \Omega$. We label these hypotheses as
\begin{align}
&H_0 : \theta \in \omega_0 \nn\\
&H_1 : \theta \in \omega_1.
\end{align}
$H_0$ is called the \textbf{null hypothesis}. $H_1$ is called the \textbf{alternative hypothesis}. Type I error occurs when we decide that $\theta \in \omega_1$ when in fact $\theta \in \omega_0$. Type II error occurs when we decide the opposite. \\

We require the \textbf{critical region}, $C$, to complete the testing structure for the general problem. Consider the r.v. $X$ and the hypotheses given above. $C$ is such that
\begin{align}
&\text{Reject } H_0 \text{ if } (X_1,\dots, X_n) \in C\nn\\
&\text{Reject } H_1 \text{ if } (X_1,\dots, X_n) \in C^c.
\end{align} 
\textbf{Type I} error occurs if $H_0$ is rejected when it is true. \textbf{Type II} error occurs if $H_0$ is retained when $H_1$ is true. \\

\noindent \textbf{Definition:} We say a critical region $C$ is of \textbf{size} $\alpha$ if 
\begin{align}
\boxed{\alpha = \max_{\theta \in \omega_0} P_\theta[(X_1,\dots,X_n)] \in C}
\end{align}
Over all critical regions of size $\alpha$, we want to consider critical regions that have lower probabilities of Type II error, i.e., for $\theta \in \omega_1$, we want to maximize
\begin{align}
{1 - P_\theta[\text{Type II Error}] = P_\theta[(X_1, \dots, X_n) \in C]}
\end{align}
The probability on the right side of the equation above is called the \textbf{power} of the test at $\theta$. It is the probability that the test detects the alternative when $\theta \in \omega_1$ is the true parameter. Minimizing Type II error requires maximizing the test power. The \textbf{power function} of $C$ is 
\begin{align}
\gamma_C(\theta) = P_\theta[(X_1,\dots,X_n) \in C]; \quad \theta \in \omega_1.
\end{align} 
Given two critical regions $C_1,C_2$ both of size $\alpha$. $C_1$ is better than $C_2$ if $\gamma_{C_1}(\theta) \geq \gamma_{C_2}(\theta)$ for all $\theta\in \omega_1$. \\

A \textbf{simple} hypothesis completely specifies the underlying distribution. A \textbf{composite} hypothesis can be composed of many simple hypotheses and hence do not completely specify the distribution. $\alpha$ is often referred to as the \textbf{significance level} of the test associated with that critical region. 




\section{Additional comments about statistical test}



\subsection{Observed Significance Level, $p$-value}
Suppose $H_0: \mu = \mu_0$ and $H_1: \mu > \mu_0$, where $\mu_0$ is maximized. Then we reject $H_0$ in favor $H_1$ if $\bar{X} \geq k$ where $\bar{X}$ is the sample mean. The p-value is the probability that under $H_0$, $\bar{X} \geq \bar{x}$:
\begin{align}
\boxed{p-\text{value} = P_{H_0}(\bar{X} \geq \bar{x})}
\end{align}
If $\alpha > p$ then we reject $H_0$ in favor of $H_1$. Else, we fail to reject $H_1$. 



\section{Chi-Square Tests}

Let r.v. $X_i \sim \N(\mu_i,\sigma_i^2)$ for $i=1,\dots,n$. Let $X_1,\dots,X_n$ be mutually independent. The joint pdf is then
\begin{align}
\f{1}{\sigma_1\dots\sigma_n(\sqrt{2\pi})^{n}}\exp\lb -\f{1}{2}\sum^n_{i=1}\lp \f{x_i - \mu_i}{\sigma_i} \rp^2 \rb
\end{align}
The r.v. 
\begin{align}
\sum^n_{i=1}\lp \f{x_i - \mu_i}{\sigma_i} \rp^2
\end{align}
has a $\chi^2(n)$ distribution. \\

We will now consider some r.v.s that have approximate $\chi^2$ distribution. Suppose $X_1 \sim b(n,p_1)$. Consider the r.v. defined by
\begin{align}
Y = \f{X_1 - p_1}{\sqrt{np_1(1-p_1)}}
\end{align}
which has, as $n\to\infty$, an approximate $\N(0,1)$. We know from earlier discussions that $Y^2 \sim \N(0,1)$. Now, let $X_2 = n-X_1$ and $p_2 = 1-p_1$. Let $Q_1 = Y^2$. Then we have
\begin{align}
Q_1 = \f{(X_1 - np_1)^2}{np_1(1-p_1)} = \f{(X_1-np_1)^2}{np_1} + \f{(X_1 - np_1)^2}{n(1-p_1)} = \f{(X_1-np_1)^2}{np_1} + \f{(X_2-np_2)^2}{np_2}.
\end{align}
In general, let $X_1,\dots,X_{k-1}$ have a multinomial distribution with the parameters $n$ and $p_{1},\dots,p_{k-1}$. Let $X_k = n-(X_1+\dots+X_{k-1})$ and let $p_k = 1-(p_1+\dots+p_{k-1})$. Define $Q_{k-1}$ by
\begin{align}
Q_{k-1} = \sum^k_{i=1}\f{(X_i - np_i)^2}{np_i}
\end{align}
As $n\to \infty$, $Q_{k-1} \sim \chi^2(k-1)$. This makes the r.v. $Q_{k-1}$ a basis of the tests of certain statistical hypotheses. For instance, when the joint pdf of $X_1,X_2,\dots,X_{k-1}$ (and $X_k = n - X_1 - \dots - X_{k-1}$) is a multinomial pmf with parameters $n$ and $p_1,\dots,p_{k-1}$ (and $p_k = 1- p_1 - \dots - p_{k-1}$), we can consider the simple null hypothesis $H_0: p_1 = p_{10}, \dots, p_{k-1} = p_{(k-1)0}$ where $p_{10},\dots,p_{(k-1)0}$ are specified numbers. Under this null, the r.v.
\begin{align}
Q_{k-1}=\sum^k_{1}\f{(X_i - np_{i0})^2}{np_{i0}}
\end{align} 
has an approximate $\chi^2(k-1)$. Intuitively, when $H_0$ is true, $np_{i0}$ must be the expected value of $X_i$, which means $Q_{k-1}$ is not too large. Thus, we reject $H_0$ if $Q_{k-1} \geq c$. The critical value $c$ is specified by the significance level $\alpha$, \textbf{c = qchisq(1-$\alpha$,k-1)}. This is frequently called the \textbf{goodness-of-fit} test.\\

We can have a chi-square test for \textbf{homogeneity}. Consider two multinomial distributions with parameters $n_j, p_{1j},\dots,p_{kj}$ and $j=1,2$. Let $X_{ij}$ where $i = 1,\dots,k$ and $j=1,2$ be frequencies. Suppose $n_1,n_2$ large and the observations are independent, then the r.v.
\begin{align}
\sum^2_{j=1}\sum^n_{i=1}\f{(X_{ij} - n_jp_{ij})^2}{n_jp_{ij}} \sim \chi^2(2k-2)
\end{align}
because it is the sum of two independent r.v.'s each of which $\sim \chi^2(k-1)$. The null hypothesis we consider is
\begin{align}
H_0: p_{11} = p_{12}; \dots; p_{k1} = p_{k2},
\end{align}
where each $p_{i1} = p_{i2}$ where $i=1,\dots,k$ is unspecified. It turns out that the mle of $p_{i1} = p_{i2}$ is given by
\begin{align}
\theta = \f{X_{i1}+X_{i2}}{n_1+n_2}
\end{align}
which makes intuitive sense. Note that we need only $k-1$ points estimates, and so the r.v.
\begin{align}
Q_{k-1} = \sum^2_{j=1}\sum^k_{i=1}\f{\lc X_{ij} - n_j[(X_{i1}+X_{i2})/(n_1+n_2)] \rc^2}{n_j[(X_{i1}+X_{i2})/(n_1+n_2)]} \sim \chi^2(2k-2-(k-1) = k-1).
\end{align}
With this, we can test if two multinomial distributions are the same. \\

We can also test for \textbf{independence}. Suppose the result of an experiment is classified by only two attributes $A$ (of $a$ possible outcomes) and $B$ (of $b$ possible outcomes). These events are $A_1,\dots,A_a$ for attribute $A$ and $B_1,\dots,B_b$ for attribute $B$. Then consider $p_{ij} = P(A_i\cap A_j)$. Say the experiment is repeated $n$ independent times and $X_{ij}$ denotes the frequency of the event $A_i \cap B_j$. There are $k=ab$ such events, so the r.v.
\begin{align}
Q_{ab-1} = \sum^b_{j=1}\sum^a_{i=1}\f{(X_{ij} - np_{ij})^2}{np_{ij}} \sim \chi^2(ab-1),
\end{align}
provided $n$ is large. To test for independence, $H_0: P(A_i\cap B_j) = P(A_i)P(B_j)$ for all $i,j$. To test $H_0$, we cannot compute $Q_{ab-1}$, but instead compute 
\begin{align}
\sum^b_{j=1}\sum^a_{i=1}\f{[X_{ij} -n(X_{i.}/n)(X_{.j}/n)]^2}{n(X_{i.}/n)(X_{.j}/n)} \sim \chi^2(ab-1-(a+b-2) = (a-1)(b-1))
\end{align}
where
\begin{align}
\hat{p}_{i.} = \f{X_{i.}}{n}, \quad X_{i.} = \sum^b_{j=1}X_{ij}, i=1,\dots,a\\
\hat{p}_{.j} = \f{X_{.j}}{n}, \quad X_{.j} = \sum^a_{i=1}X_{ij}, j=1,\dots,b
\end{align}


Just a sanity check, the chi-square statistic always has the form of $\sum \text{Expected - Observed}^2/\text{Expected}$. All tests' statistics have this form. The differences are subtle and are context-based. 


\section{The Method of Monte Carlo}

\noindent \textbf{Theorem:} Suppose the r.v. $U$ has a uniform $(0,1)$ distribution. Let $F$ be a continuous distribution function (cdf). Then the r.v. $X = F^{-1}(U) \sim F$.\\

\noindent \textit{Proof:} We assume that $F(x)$ is strictly monotone. 
\begin{align}
P[X\leq x] &= P[F^{-1}(U) \leq x]\nn\\
&= P[U \leq F(x)]\nn\\
&= F(x).
\end{align} \qed






\subsection{Accept-Reject Generation Algorithm}

\section{Bootstrap Procedures}

\subsection{Percentile Bootstrap CI}
\subsection{Bootstrap Testing Procedures}





































\newpage


\chapter{Problems}


\newpage

\section{Problem Set 1}

\noindent \textbf{3.6.4}
\begin{enumerate}[(a)]
	\item $X$ has a standard normal distribution:
	\begin{lstlisting}
	x=seq(-6,6,.01); plot(dnorm(x)~x)
	\end{lstlisting}
	\item $X$ has a $t$-distribution with 1 degree of freedom.
	\begin{lstlisting}
	lines(dt(x,1)~x,lty=2)
	\end{lstlisting}
	\item $X$ has a $t$-distribution with 3 degrees of freedom.
	\begin{lstlisting}
	lines(dt(x,3)~x,lty=2)
	\end{lstlisting}
	\item $X$ has a $t$-distribution with 10 degrees of freedom.
	\begin{lstlisting}
	lines(dt(x,10)~x,lty=2)
	\end{lstlisting}
	\item $X$ has a $t$-distribution with 30 degrees of freedom.
	\begin{lstlisting}
	lines(dt(x,30)~x,lty=2)
	\end{lstlisting}
\end{enumerate}

\begin{figure}[!htb]
	\centering
	\subfloat[]{{\includegraphics[width=3.35cm]{364a} }}
	\qquad
	\subfloat[]{{\includegraphics[width=3.35cm]{364b} }}
	\qquad
	\subfloat[]{{\includegraphics[width=3.35cm]{364c} }}\\
	\subfloat[]{{\includegraphics[width=4cm]{364d} }}
	\quad
	\subfloat[]{{\includegraphics[width=4cm]{364e} }}
\end{figure}




\newpage
\noindent\textbf{3.6.5}
\begin{enumerate}[(a)] 
	\item $P(\abs{X} \geq 2) = 2\times[1 - P(X \leq 2)] = \textbf{0.046}$.
	\begin{lstlisting}
	> 2*(1 - pnorm(2))
	[1] 0.04550026
	\end{lstlisting}
	
	\item $P(\abs{X} \geq 2) = 2\times[1 - P(X \leq 2)] = \textbf{0.295}$.
	\begin{lstlisting}
	> 2*(1 - pt(2,1))
	[1] 0.2951672
	\end{lstlisting}
	
	\item $P(\abs{X} \geq 2) = 2\times[1 - P(X \leq 2)] = \textbf{0.139}$.
	\begin{lstlisting}
	> 2*(1 - pt(2,3))
	[1] 0.139326
	\end{lstlisting}
	
	\item $P(\abs{X} \geq 2) = 2\times[1 - P(X \leq 2)] = \textbf{0.073}$.
	\begin{lstlisting}
	> 2*(1 - pt(2,10))
	[1] 0.07338803
	\end{lstlisting}
	
	\item $P(\abs{X} \geq 2) = 2\times[1 - P(X \leq 2)] = \textbf{0.055}$.
	\begin{lstlisting}
	> 2*(1 - pt(2,30))
	[1] 0.05462504
	\end{lstlisting}
\end{enumerate}






\newpage
\noindent\textbf{3.6.11:} Let $T = W/\sqrt{V/r}$, where the independent variables $W \sim \mathcal{N}(0,1)$ and $V \sim \chi^2(r)$. Show that $T^2\sim F(r_1 = 1, r_2 = r)$. \textit{Hint}: What is the distribution of the numerator of $T^2$?\\

\noindent \textit{Solution:} Let the independent random variables $U,V$ be given, with $W\sim \N(0,1)$ and $U\sim \chi^2(r)$. The random variable $T^2$, where $T = W / \sqrt{V/r}$ is given by
\begin{align}
T^2 = \lp \f{W}{\sqrt{V/r}} \rp^2 = \f{W^2}{V/r}.
\end{align}
Because $W \sim \N(0,1)$, we have that $W^2 \sim \chi^2(1)$ (by theorem). Now, $T^2$ has the form 
\begin{align}
T^2 = \f{W^2}{V/r} = \f{W^2/1}{V/r}
\end{align}
where $1$ is the df of $\chi^2(1)$ which $W$ follows, and $r$ is the df of $\chi^2(r)$ which $U$ follows. Thus, $T^2 \sim F(1,r)$, by the definition of the $F$-distribution. \qed






\newpage

\noindent\textbf{3.6.15:} Let $X_1$, $X_2$ be iid with common distribution having the pdf 
\begin{align}
f(x) = \begin{cases}
e^{-x}, \quad 0< x < \infty\\
0, \quad \text{else}
\end{cases}
\end{align}
Show that $Z = X_1/X_2$ has an $F$-distribution.\\

\noindent \textit{Solution:}  It suffices to show that $Z$ can be written as a ratio of two $\chi^2$-distributed independent random variables. To this end, we can consider the mgf $M_X(t)$ of $X_1$, which is also identically that of $X_2$ since $X_1, X_2$ are iid:
\begin{align}
M_X(t) = E[e^{tx}] = \int^\infty_0 e^{tx}e^{-x} \,dx = (1-t)^{-1}.
\end{align}
However, this does not quite match the mgf for a $\chi^2(2)$. To circumvent this problem, we rewrite
\begin{align}
Z = \f{X_1}{X_2} =\f{2X_1/2}{2X_2/2}  = \f{(X_1+X_1)/2}{(X_2+X_2)/2},
\end{align}
as we expect $r=2$. Let $Y_1 = X_1 + X_1$. Then we have trivially $Y_1 =2X_1$, and so $\abs{J} = 2$. With this, $Y_1$ has the pdf 
\begin{align}
\tilde{f}_Y(y) = \abs{J}f(x) = 2f(x) = \begin{cases}
2e^{-y/2}, \quad 0 < y < \infty\\
0, \quad \Else
\end{cases}. 
\end{align}
From here, we find the mgf of $Y_1$ to be 
\begin{align}
M_{Y_1}(t) = E[e^{ty}] = \f{1}{2}\int^\infty_{0}e^{ty}e^{-y/2}\,dy = (1-2t)^{-1} = (1-2t)^{-2/2}, t < \f{1}{2}.
\end{align}
By symmetry, $M_{Y_2}(t)$ is identically $M_{Y_1}(t)$, and both are the mgf for $\chi^2(r=2)$. Because each mgf uniquely determines a pdf, $Y_1, Y_2 \sim \chi^2(r=2)$ identically and independently (for each depends exclusively on $X_1$, $X_2$, respectively). Therefore, 
\begin{align}
Z = \f{(X_1+X_1)/2}{(X_2+X_2)/2} = \f{Y_1/2}{Y_2/2}
\end{align}
follows the $F$-distribution with degrees of freedom $r_1 = r_2 = 2$, by definition. \qed






\newpage
\noindent\textbf{3.6.16:} Let $X_1, X_2, X_3$ be independent r.v. with $X_i \sim \chi^2(r_i)$. 
\begin{enumerate}[(a)]
	\item Show that $Y_1 = X_1/X_2$ and $Y_2 = X_1 + X_2$ are independent and that $Y_2 \sim \chi^2(r_1 + r_2)$. 
	
	\item Deduce that 
	\begin{align}
	\f{X_1/r_1}{X_2/r_2} \text{ and } \f{X_3/r_3}{(X_1 + X_2)/(r_1 + r_2)}
	\end{align}
	are independent $F$-variables. 
\end{enumerate}

\noindent \textit{Solution:}  

\begin{enumerate}[(a)]
	\item We consider the transformation
	\begin{align}
	y_1 &= u(x_1,x_2) = \f{x_1}{x_2}\\
	y_2 &= v(x_1,x_2) = x_1 + x_2.
	\end{align}
	whose inverse is 
	\begin{align}
	x_1 = \bar{u}(y_1, y_2) = \f{y_1y_2}{1+y_1}\nn\\
	x_2 = \bar{v}(y_1, y_2) = \f{y_2}{1+y_1}.
	\end{align}
	The absolute value of the Jacobian is 
	\begin{align}
	\abs{J} = \abs{\det\begin{pmatrix}
		\p_{y_1}\bar{u} & \p_{y_2}\bar{u}\\
		\p_{y_1}\bar{v} & \p_{y_2}\bar{v}\\
		\end{pmatrix}} = \f{y_2}{(1+y_1)^2},
	\end{align}
	which maps one-to-one from the space of $X_1,X_2$ $\R^+\times \R^+$ onto the space of $Y_1, Y_2$ $\R^+\times \R^+$. Since $X_1, X_2$ are independent, we consider the joint pdf of $X_1, X_2$:
	\begin{align}
	h(x_1, x_2) = \begin{cases}
	\f{x_1^{r_1/2-1}x_2^{r_2/2-1}}{\Gamma(r_1/2)\Gamma(r_2/2)2^{(r_1+r_2)/2}}e^{-(x_1+x_2)/2}, \quad 0 < x_1,x_2 < \infty\\
	0, \quad \text{else}
	\end{cases}
	\end{align}
	from which we can deduce the joint pdf for $Y_1, Y_2$:
	\begin{align}
	\tilde{h}(y_1,y_2) &= \abs{J}h\lp \f{y_1y_2}{1+y_1}, \f{y_2}{1+y_1} \rp\nn\\
	&= \begin{cases}
	\f{y_2(y_1y_2)^{r_1/2-1}y_2^{r_2/2-1} (1+y_1)^{-r_1/2-r_2/2 + \cancel{ 2}}}{\cancel{(1+y_1)^2}\Gamma(r_1/2)\Gamma(r_2/2)2^{(r_1+r_2)/2}}e^{-y_2/2}, \,\, 0 < y_1,y_2 < \infty\\
	0, \quad \text{else}
	\end{cases}\nn\\
	&= \begin{cases}
	\f{y_2^{r_1/2+r_2/2-1}y_1^{r_1/2-1}(1+y_1)^{-r_1/2-r_2/2}}{\Gamma(r_1/2)\Gamma(r_2/2)2^{(r_1+r_2)/2}}e^{-y_2/2}, \,\, 0 < y_1,y_2 < \infty\\
	0, \quad \text{else}
	\end{cases}
	\end{align}
	Without further computation we see that $\tilde{h}(y_1,y_2)$ can be written as a product of two nonnegative functions of $y_1$ and $y_2$. In view of Theorem 2.4.1, $Y_1$ and $Y_2$ are independent. \qed \\
	
	Next, we wish to show $Y_2 \sim \chi^2(X_1, X_2)$, to which end we find the marginal pdf $g_2(y_2)$ of $Y_2$:
	\begin{align}
	g_2(y_2) &= \int^\infty_0 \tilde{h}(y_1,y_2)\,dy_1\nn\\
	&= \mathfrak{C}\int^\infty_0 {y_1^{r_1/2-1}(1+y_1)^{-r_1/2-r_2/2}}\,dy_1\nn\\
	&= \mathfrak{C}\f{\Gamma(r_1/2)\Gamma(r_2/2)}{\Gamma[(r_1+r_2)/2]}
	\end{align}
	where $\mathfrak{C}$ contains all the $y_1$-independent elements. From here, via simple back-substitution we obtain the marginal pdf for  $Y_2$:
	\begin{align}
	g_2(y_2) = \begin{cases}
	\f{y_2^{(r_1+r_2)/2 - 1 }}{\Gamma[(r_1+r_2)/2]2^{(r_1+r_2)/2}}e^{-y_2/2}, \quad 0 < y_2 < \infty\\
	0, \quad \Else
	\end{cases},
	\end{align}
	i.e., $Y_2 \sim \chi^2(r_1 + r_2)$. \qed\\
	
	\textbf{Mathematica code:}
	\begin{lstlisting}
	In[20]:= Integrate[
	x^(r1/2 - 1) (1 + x)^(-r1/2 - r2/2), {x, 0, Infinity}]
	
	Out[20]= ConditionalExpression[(Gamma[r1/2] Gamma[r2/2])/
	Gamma[(r1 + r2)/2], Re[r2] > 0 && Re[r1] > 0]
	\end{lstlisting}
	
	
	
	
	
	\item By definition, because $X_1, X_2$ are independent random variables with $X_i \sim \chi^2(r_i)$, 
	\begin{align}
	\Omega = \f{X_1/r_1}{X_2/r_2} \sim F(r_1, r_2).
	\end{align}
	Also, because $X_3 \sim \chi^2(r_3)$ and $(X_1 + X_2) \sim \chi^2(r_1 + r_2)$ (from (a)), we have
	\begin{align}
	\Lambda = \f{X_3/r_3}{(X_1 + X_2)/(r_1 + r_2)} \sim F(r_3, r_1 + r_2)
	\end{align}
	as well. Furthermore, because 
	\begin{align}
	&\Omega = \f{X_1/r_1}{X_2/r_2} = \f{r_2}{r_1}Y_1\\
	&\Lambda = \f{r_1+r_2}{r_3}\f{X_3}{Y_2}
	\end{align}
	and because $X_1, X_2, X_3$ are independent, we have that $Y_1, Y_2, X_3$ are independent. Therefore, it is necessary that $\Omega \sim F(r_1, r_2)$ and $\Lambda \sim F(r_3, r_1 + r_2)$ are independent as well. \qed
	
\end{enumerate}








\newpage




\textbf{4.1.1} Twenty motors were put on test under a high-temperature setting. The
lifetimes in hours of the motors under these conditions are given below. Also, the
data are in the file \textbf{lifetimemotor.rda} at the site listed in the Preface. Suppose
we assume that the lifetime of a motor under these conditions, $X$, has a $\Gamma(1, \theta)$
distribution.\\

\begin{tabular}{c c c c c c c c c c }
	1 &4& 5& 21& 22& 28& 40& 42& 51& 53\\
	58 &67& 95& 124& 124& 160& 202& 260& 303& 363
\end{tabular}


\begin{enumerate}[(a)]
	\item Obtain a histogram of the data and overlay it with a density estimate, using
	the code \textbf{hist(x,pr=T); lines(density(x))} where the R vector\textbf{x} contains
	the data. Based on this plot, do you think that the $\Gamma(1, \theta)$ model is credible?
	
	\item Assuming a $\Gamma(1, \theta)$ model, obtain the maximum likelihood estimate $\hat{\theta}$ of $\theta$ and locate it on your histogram. Next overlay the pdf of a $\Gamma(1, \hat{\theta})$ distribution on the histogram. Use the R function \textbf{dgamma(x,shape=1,scale=$\hat{\theta}$)} to evaluate the pdf.
	
	\item Obtain the sample median of the data, which is an estimate of the median
	lifetime of a motor. What parameter is it estimating (i.e., determine the
	median of $X$)?
	
	\item Based on the mle, what is another estimate of the median of $X$?
\end{enumerate}


\noindent \textit{Solution:}  
\begin{enumerate}[(a)]
	\item For some reason R does not recognize the dataset as of numeric type. Because the dataset is small enough, I recoded and fed it by hand to the data vector $y$:
	\begin{lstlisting}
	> lines(density(y))
	> y <- c(1,4,5,21,22,28,40,42,51,53,58,67,
	95,124,124,160,202,260,303,363)
	> hist(y,pr=T)
	> lines(density(y))
	\end{lstlisting}
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{411a}
	\end{figure}
	The $\Gamma(1,\theta)$, or $\text{Exp}(\theta)$, model seems to be \textbf{credible} as far as the histogram is concerned. However, the overlaying density does not look like a $\Gamma(1,\theta)$. \qed
	
	
	
	\item Assuming the $\Gamma(1,\theta)$ model, then the pdf on the support $\R^+$ is given by
	\begin{align}
	f(y) = \f{1}{\theta}e^{-y/\theta},
	\end{align}
	from which we obtain the logarithm of the likelihood function:
	\begin{align}
	l(\theta) = \log\lp \prod^n_{i=1} \f{1}{\theta}e^{-y_i/\theta} \rp = -n\log\theta - \f{1}{\theta}\sum^n_{i=1}y_i.
	\end{align}
	The first partial derivative wrt $\theta$ is then
	\begin{align}
	\p_\theta l(\theta) = -\f{n}{\theta} + \f{1}{\theta^2}\sum^n_{i=1}y_i.
	\end{align}
	Setting $\p_\theta l(\theta) = 0$, we get (by inspection) that $l(\theta)$ is extremized iff $\theta = (1/n)\sum^n_{i=1}y_i = \bar{y}$. We also have that $\p_{\theta\theta} < 0\, \forall \theta \in \R^+$, which means $l(\theta)$ is maximized globally at $\bar{y}$. From here, the statistic 
	\begin{align}
	\hat{\theta} = \bar{Y} = \mathbf{101.15}
	\end{align} 
	is the mle of $\theta$. (Also note that because $E[Y] = \theta \implies E[\bar{Y}] = \theta$, $\hat{\theta}$ is an unbiased estimator of $\theta$.)
	\begin{lstlisting}
	> mean(y)
	[1] 101.15
	> abline(v = mean(y), lwd=3, lty=2)
	> z=dgamma(y, shape=1, scale=mean(y))
	> lines(z~y,lty=2)
	\end{lstlisting}
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{411b}
	\end{figure}
	
	
	
	
	\item The sample median of the data is \textbf{55.5}
	\begin{lstlisting}
	> median(y)
	[1] 55.5
	\end{lstlisting}
	The median of $Y \sim \Gamma(1,\theta) \equiv \text{Exp}(\theta)$ is the value of $y'$ at which 
	\begin{align}
	0.5 = \int^{y'}_{0}\f{1}{\theta}e^{-y/\theta}\,dy = 1 - e^{-y'/\theta} \implies y' = \theta \ln 2,
	\end{align} 
	which means that the median of $Y \sim \Gamma(1,\theta) \equiv \text{Exp}(1,\theta)$ is the half-life, $\theta \ln 2$. Since the sample median is just $\theta$ multiplied by $\ln 2$, the sample median also estimates the parameter $\theta$. 
	
	
	
	\item From part (a), we know that $\hat{\theta} = \bar{Y}$, the sample mean, is the mle of $\theta$, the population mean. From part (c), we have shown that the median of $Y \sim \Gamma(1,\theta)$ is simply $\theta \ln 2$. By simple inspection we see that $\hat{\theta}\ln 2 = \hat{Y} \ln 2$ is the (\textit{unbiased}) mle of $\theta \ln 2$, the median of $Y$. \qed 
	
	
	
\end{enumerate}








\newpage

\noindent \textbf{4.1.3} Suppose the number of customers $X$ that enter a store between the hours
9:00 a.m. and 10:00 a.m. follows a Poisson distribution with parameter $\theta$. Suppose
a random sample of the number of customers that enter the store between 9:00 a.m.
and 10:00 a.m. for 10 days results in the values\\

\begin{tabular}{c c c c c c c c c c}
	9& 7& 9 &15& 10& 13& 11& 7 &2& 12
\end{tabular}


\begin{enumerate}
	\item Determine the maximum likelihood estimate of $\theta$. Show that it is an unbiased
	estimator.
	
	\item Based on these data, obtain the realization of your estimator in part (a).
	Explain the meaning of this estimate in terms of the number of customers.
	
\end{enumerate}


\noindent \textit{Solution:} 
\begin{enumerate}
	\item Let $X \sim \text{Poi}(\theta)$ be given, then the pmf of $X$ is given by
	\begin{align}
	p(x) = \begin{cases}
	\f{\theta^x e^{-\theta}}{x!}, \quad x\in \mathbb{N}\\ 
	0, \quad \Else
	\end{cases}.
	\end{align}
	Assuming the $X_i$'s $\sim \text{Poi}(\theta)$ are iid, where $i=1,\dots,n$, then the logarithm of the likelihood function is 
	\begin{align}
	l(\theta) &= \log \lp \prod^n_{i=1} \f{\theta^{x_i} e^{-\theta}}{x_i!} \rp\nn\\
	&= \log \lp e^{-n\theta}\theta^{\sum_{i=1}^n x_i}  \prod^n_{i=1}\f{1}{x_i!} \rp\nn\\
	&= -n\theta + \lp \sum^n_{i=1}x_i\rp\log \theta - \sum^n_{i=1}\log x_i!. 
	\end{align}
	Setting $\p_\theta l(\theta) = 0$, we solve for $\theta$:
	\begin{align}
	\p_\theta l(\theta) = -n +  \f{1}{\theta}\sum^n_{i=1}x_i = 0 \iff \theta = \f{1}{n}\sum^n_{i=1} x_i = \bar{x}
	\end{align}
	By inspection, $\p_{\theta\theta}l(\theta) < 0 \forall \theta \in \mathbb{R}^+$, and so the statistic
	\begin{align}
	\hat{\theta} = \bar{Y}
	\end{align}
	is the mle of $\theta$. Further, it is an unbiased estimator of $\theta$ simply because
	\begin{align}
	E[Y] =  \theta \implies E[\bar{Y}] = \theta.
	\end{align}
	\qed
	
	
	\item Part (a) says the sample means is the mle of $\theta$. The means of the given sample is \textbf{9.5}.
	\begin{lstlisting}
	> mean(c(9 ,7, 9, 15, 10, 13, 11, 7, 2, 12))
	[1] 9.5
	\end{lstlisting}
	This says that on average, 9.5, or about 9-10 customers enter the store between the hours 9:00 a.m. and 10:00 a.m.. \qed
	
	
	
\end{enumerate}












\newpage
\noindent \textbf{4.1.8} Recall that for the parameter $\eta = g(\theta)$, the mle of $\eta$ is $g(\hat{\theta})$, where $\hat{\theta}$ is the mle of $\theta$. Assuming that the data in Example 4.1.6 were drawn from a Poisson distribution with mean $\lambda$, obtain the mle of $\lambda$ and then use it to obtain the mle of the pmf. Compare the mle of the pmf to the nonparametric estimate. Note: For the domain value 6, obtain the mle of $P(X \geq 6)$.\\

\noindent \textit{Solution:} Based on the previous problem, the mle of $\lambda$ is the sample means, which has the value \textbf{2.13}.
\begin{lstlisting}
> mean(c(2,1,1,1,1,5,1,1,3,0,2,1,1,3,4,2,1,2,2,6,5,2,3,2,4,1,3,1,3,0))
[1] 2.133333
\end{lstlisting}
Because the sample means $\bar{x}$ is the mle of $\lambda$, and the pmf is given by
\begin{align}
p(x) = \begin{cases}
\f{\lambda^x e^{-\lambda}}{x!}, \quad x \in \mathbb{N}\\ 
0, \quad \Else
\end{cases},
\end{align}
the mle of the pmf is given by
\begin{align}
\tilde{p}(x) = \begin{cases}
\f{\bar{x}^x e^{-\bar{x}}}{x!}, \quad x \in \mathbb{N}\\ 
0, \quad \Else
\end{cases}.
\end{align}
Next, we compare the mle of the pmf to the nonparametric estimate:\\

\begin{tabular}{ c c c c c c c c}
	j & 0 &1& 2& 3& 4& 5& $\geq$ 6 \\
	$\hat{p}(j)$& 0.067 & 0.367 & 0.233 & 0.167 & 0.067 & 0.067 & 0.033\\
	$\tilde{p}(j)$ & 0.118 & 0.253 & 0.270  & 0.192  & 0.102 & 0.044  &  0.022
\end{tabular}\\


\textbf{Mathematica code for $P(j \geq 6)$ for $\tilde{p}(j)$:}
\begin{lstlisting}
P[x_] := (2.1333333)^x*E^(-2.1333333)/x!

N[Sum[P[y], {y, 6, Infinity}]]

0.0218705
\end{lstlisting}






\newpage

\section{Problem Set 2}




\noindent \textbf{4.2.2.} Consider the data on the lifetimes of motors given in Exercise 4.1.1. Obtain
a large sample 95\% confidence interval for the mean lifetime of a motor.\\

\noindent \textit{Solution:} 




\newpage
\noindent\textbf{4.2.6.} $\overline{X}$ is the sample mean of a sample of size $n$ from $\N(\mu,9)$. Find $n$ such that 
\begin{align}
P(\bar{X}-1\leq n \leq \bar{X}+1) = 0.90
\end{align}

\noindent \textit{Solution:} 



\newpage
\noindent\textbf{4.2.18.} $X_i$'s $\sim \N(\mu,\sigma^2)$, with $\mu,\sigma^2$ unknown. A confidence interval for $\sigma^2$ can be found as follows.
We know that $(nâˆ’1)S^2/\sigma^2$ is a random variable with a $\chi^2(nâˆ’1)$ distribution. Thus
we can find constants $a$ and $b$ so that $P((n âˆ’ 1)S^2/Ïƒ\sigma^2 < b)=0.975$ and $P(a <
(n âˆ’ 1)S^2/\sigma^2 < b)=0.95$. In R, $b = qchisq(0.975,n-1)$, while $a = qchisq(0.025,n-1).$


\begin{enumerate}[(a)]
	\item Show that this second probability statement can be written as
	\begin{align}
	P((n âˆ’ 1)S^2/b < \sigma^2 < (n âˆ’ 1)S^2/a)=0.95.
	\end{align}
	\item If $n = 9$ and $S^2 = 7.93$, find a 95\% confidence interval for $\sigma^2$.
	\item If $\mu$ is known, how would you modify the preceding procedure for finding a
	confidence interval for $\sigma^2$?
\end{enumerate}


\noindent \textit{Solution:}

\begin{enumerate}[(a)]
	\item 
\end{enumerate}




 
\newpage
\noindent\textbf{4.2.21.} Let two independent random samples, each of size 10, from two normal
distributions $\N(\mu_1, \sigma^2)$ and $\N(\mu_2, \sigma^2)$ yield $x = 4.8, s^2_1 = 8.64, y = 5.6, s^2_2 = 7.88$. Find a 95\% confidence interval for $\mu_1 âˆ’ \mu_2$.\\

\noindent \textit{Solution:} 


\newpage
\noindent\textbf{4.2.22.} Let two independent random variables, $Y_1$ and $Y_2$, with binomial distributions that have parameters $n_1 = n_2 = 100$, $p_1$, and $p_2$, respectively, be observed
to be equal to $y_1 = 50$ and $y_2 = 40$. Determine an approximate 90\% confidence
interval for $p_1 âˆ’ p_2$.\\



\noindent \textit{Solution:} 


\newpage
\noindent\textbf{4.2.27.} Let $X_1, X_2,\dots,X_n$ and $Y_1, Y_2,\dots,Y_m$ be two independent random samples
from the respective normal distributions $\N(\mu_1, \sigma^2_1)$ and $\N(\mu_2, \sigma^2_2)$, where the four
parameters are unknown. To construct a confidence interval for the ratio, $\sigma^2_1/\sigma^2_2$, of
the variances, form the quotient of the two independent Ï‡2 variables, each divided
by its degrees of freedom, namely,
\begin{align}
F = \f{\f{(m-1)S_2^2}{\sigma^2_2}/(m-1)}{\f{(n-1)S_1^2}{\sigma^2_1}/(n-1)} = \f{S_2^2/\sigma_2^2}{S_1^2/\sigma_1^2}
\end{align}
where $S_1^2, S_2^2$ are respectively sample variances. 

\begin{enumerate}
	\item What kind of distribution does $F$ have?
	\item Rewrite the second probability statement as
	\begin{align}
	P \lb a\f{S_1^2}{S_2^2} < \f{\sigma_1^2}{\sigma_2^2} < b \f{S_1^2}{S^2_2} \rb = 0.95.
	\end{align}
	The observed values, $s^2_1$ and $s_2^2$, can be inserted in these inequalities to provide
	a 95\% confidence interval for $\sigma^2_1/Ïƒ^2_2$. 
\end{enumerate}





\noindent \textit{Solution:} 












\newpage
\noindent\textbf{4.5.1.} Show that the approximate power function given in expression (4.5.12) of
Example 4.5.3 is a strictly increasing function of $\mu$. Show then that the test discussed
in this example has approximate size $\alpha$ for testing
\begin{align}
H_0: \mu \leq \mu_0\quad \text{versus} \quad H_1: \mu > \mu_0.
\end{align}


\noindent \textit{Solution:} 



\newpage
\noindent\textbf{4.5.2.} For the Darwin data tabled in Example 4.5.5, verify that the Student t-test
statistic is 2.15. \\


\noindent \textit{Solution:} 




\newpage
\noindent\textbf{4.5.5.} Let X1, X2 be a random sample of size n = 2 from the distribution having
pdf f(x; Î¸) = (1/Î¸)eâˆ’x/Î¸, 0 <x< âˆž, zero elsewhere. We reject H0 : Î¸ = 2 and
accept H1 : Î¸ = 1 if the observed values of X1, X2, say x1, x2, are such that
\begin{align}
\f{f(x_1;2)f(x_2;2)}{f(x_1;1)f(x_2;1)} \leq \f{1}{2}
\end{align}



Here $\Omega = \{Î¸ : Î¸ = 1, 2\}$. Find the significance level of the test and the power of the
test when $H_0$ is false.\\



\noindent \textit{Solution:} 


\newpage
\noindent\textbf{4.5.12.} Let $X_1, X_2,\dots,X_8$ be a random sample of size $n = 8$ from a Poisson
distribution with mean $\mu$. Reject the simple null hypothesis $H_0 : \mu = 0.5$ and
accept $H_1 : \mu > 0.5$ if the observed $\sum^8_{i=1} x_i \geq 8.$
\begin{enumerate}[(a)]
	\item Show that the significance level is \texttt{1-ppois(7,8*.5).}
	\item Use R to determine $\gamma(0.75)$, $\gamma(1)$, and $\gamma(1.25)$.
	\item Modify the code in Exercise 4.5.9 to obtain a plot of the power function.
\end{enumerate}


\noindent \textit{Solution:} 













\newpage
\noindent \textbf{4.6.4.} (Note that it is fine to make a heuristic argument here. Just make sure that it is clear with
supporting graphs/figures (hand drawn is fine).) Consider the one-sided t-test for $H_0 : \mu = \mu_0$ versus $H_{A1} : \mu>\mu_0$ constructed in Example 4.5.4 and the two-sided t-test for t-test for $H_0 : \mu = \mu_0$ versus
$H_1 : \mu = \mu_0$ given in (4.6.9). Assume that both tests are of size $\alpha$. Show that for
$\mu>\mu_0$, the power function of the one-sided test is larger than the power function of the two-sided test.\\


\noindent \textit{Solution:} 



\newpage
\textbf{4.6.7.} Among the data collected for the World Health Organization air quality
monitoring project is a measure of suspended particles in $\mu$g/m$^3$. Let $X$ and $Y$ equal
the concentration of suspended particles in $\mu$g/m$^3$ in the city center (commercial
district) for Melbourne and Houston, respectively. Using $n = 13$ observations of $X$
and $m = 16$ observations of $Y$ , we test $H_0 : \mu_X = \mu_Y$ against $H_1 : \mu_X < \mu_Y$.

\begin{enumerate}[(a)]
	\item Define the test statistic and critical region, assuming that the unknown variances are equal. Let $\al = 0.05$.
	\item If $\bar{x} = 72.9$, $s_x = 25.6$, $y = 81.7$, and $s_y = 28.3$, calculate the value of the
	test statistic and state your conclusion.
\end{enumerate}


\noindent \textit{Solution:}
\begin{enumerate}[(a)]
	\item 
\end{enumerate}

\newpage
\textbf{4.6.8.} Let $p$ equal the proportion of drivers who use a seat belt in a country that
does not have a mandatory seat belt law. It was claimed that $p = 0.14$. An
advertising campaign was conducted to increase this proportion. Two months after
the campaign, $y = 104$ out of a random sample of $n = 590$ drivers were wearing
their seat belts. Was the campaign successful?
\begin{enumerate}
	\item Define the null and alternative hypotheses.
	\item Define a critical region with an $\al = 0.01$ significance level.
	\item Determine the approximate $p$-value and state your conclusion.
\end{enumerate}


\noindent \textit{Solution:}
\begin{enumerate}[(a)]
	\item 
\end{enumerate}




\newpage







	
	
\end{document}