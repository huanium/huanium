\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage[shortlabels]{enumitem}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\usepackage{newpxtext,newpxmath}
\usepackage[left=1.25in,right=1.25in,top=1.5in,bottom=1.5in]{geometry}


%\newcommand{\fig}[1]{figure #1}
%\newcommand{\explain}{appendix?}
%\newcommand{\rat}{\mathbb{Q}}
%
%\newcommand{\mathbb{R}}{\mathbb{R}}
%\newcommand{\nat}{\mathbb{N}}
%\newcommand{\inte}{\mathbb{Z}}
%\newcommand{\M}{{\cal{M}}}
%\newcommand{\sss}{{\cal{S}}}
%\newcommand{\rrr}{{\cal{R}}}
%\newcommand{\uu}{2pt}
%\newcommand{\vv}{\vec{v}}
%\newcommand{\comp}{\mathbb{C}}
%\newcommand{\field}{\mathbb{F}}
%\newcommand{\f}[1]{ \hspace{.1in} (#1) }
%\newcommand{\set}[2]{\mbox{$\left\{ \left. #1 \hspace{3pt}
%\right| #2 \hspace{3pt} \right\}$}}
%\newcommand{\integral}[2]{\int_{#1}^{#2}}
%\newcommand{\ba}{\hookrightarrow}
%\newcommand{\ep}{\varepsilon}
%\newcommand{\limit}{\operatornamewithlimits{limit}}
%\newcommand{\ddd}{.1in}
%\newcommand{\ccc}{2in}
%\newcommand{\aaa}{1.5in}
%\newcommand{\B}{{\cal B}}
%\newcommand{\C}{{\cal C}}
%\newcommand{\D}{{\cal D}}
%\newcommand{\FF}{{\cal F}}


%\usepackage{epstopdf}
%\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `basename #1 .tif`.png}
%\usepackage{graphics}
%\usepackage{array}
%\def\set#1#2{\left\{\left.\;#1\;\right| #2 \; \right\}}
%\def\Sum{\sum}
%\def\me{.05in}















\begin{document}



\noindent \textbf{Question 1.}
\begin{enumerate}[(a)]
	\item $Y \sim F(v_1,v_2)$. So $Y$ has the form
	\begin{align*}
	Y = \frac{A/v_1}{B/v_2}
	\end{align*}
	where $A \sim \chi^2(v_1)$ and $B \sim \chi^2(v_2)$. So, 
	\begin{align*}
	U = \frac{1}{Y} = \frac{B/v_2}{A/v_1} \sim F(v_2,v_1).
	\end{align*}
	
	
	\item 
	\begin{itemize}
		\item Because for $Y_i \sim \mathcal{N}(0,1)$, $Y_i^2\sim \chi^2(1)$ and $\sum^n Y_i^2 \sim \chi^2(n)$, we have that
		\begin{align*}
		\sum^5_{i=1}Y_i^2 \sim \chi^2(5).
		\end{align*}
		
		\item We know that 
		\begin{align*}
		\frac{n-1}{\sigma^2}S^2 = \frac{n-1}{\sigma^2}\frac{1}{n-1}\sum^n_{i=1}(Y_i-\bar{Y})^2 \sim \chi^2(n-1),
		\end{align*}
		so
		\begin{align*}
		\sum^5_{i=1}(Y_i-\bar{Y})^2 = \frac{5-1}{1^2}\frac{1}{5-1}\sum^5_{i=1}(Y_i - \bar{Y})^2 \sim \chi^2(5-1) = \chi^2(4).
		\end{align*}
		
		\item From (a) and (b) we know that 
		\begin{align*}
		\sum^5_{i=1}(Y_i-\bar{Y})^2 \sim \chi^2(4), \quad Y_6^2 \sim \chi^2(1).
		\end{align*}
		Since these are independent, we have a theorem that says
		\begin{align*}
		\sum^5_{i=1}(Y_i-\bar{Y})^2 + Y_6^2 \sim \chi^2(4+1) = \chi^2(5).
		\end{align*}
	\end{itemize}
\end{enumerate}


\newpage

\noindent \textbf{Question 2.}
\begin{enumerate}[(a)]
	\item We have $H_0 : \mu_\text{sports} = \mu_\text{no sports}$, $H_a : \mu_\text{sports} > \mu_\text{no sports}$, and $\alpha = 0.05$. The test statistic is
	\begin{align*}
	z = \frac{(\bar{x}_\text{sports} - \bar{x}_\text{no sports}) - 0}{\sqrt{\frac{\sigma^2_s}{n_s} + \frac{\sigma^2_{ns}}{n_{ns}}}} = \frac{32.19 - 31.68}{\sqrt{\frac{4.34^2}{37} + \frac{4.56^2}{37}}} \approx 0.4927.
	\end{align*}
	All conditions are met/assumed. The p-value is $1-0.6879 \approx 0.312 > 0.05 = \alpha$. So, there is not enough evidence to reject $H_0$, i.e., there is not enough evidence to indicate that second graders who participated in sports have a higher dexterity score. 
	
	\item Power is the probability of rejecting $H_0$ provided that $H_a$ is true. To find power we want to find the critical value for the test statistic. At $\alpha = 0.05$, single-tail, $z_c = 1.645$. So,
	\begin{align*}
	\text{Power } &= P(z\geq z_c \vert \Delta \mu = 3)\\
	&= P(z' = z-z_c \geq -1.355 \vert \Delta \mu = 0), \quad \text{(shifting)}\\
	&= 0.912.
	\end{align*}
	
	
	\item Bootstrap algorithm to test the scenario in part (a):
	\begin{align*}
	&H_0 : \mu_{ns} = \mu_{s}\\
	&H_a : \mu_{s} > \mu_{ns}. 
	\end{align*}
	\begin{itemize}
		\item Calculate the observed $\bar{X}_s - \bar{X}_{ns} = \bar\Delta$. 
		\item Combine all observations into one sample, called $Z$.
		\item Take a bootstrap sample from $Z$ of size $n_1 = 37.$, then calculate $\bar{X^*}_s$.
		\item Take a bootstrap sample from $Z$ of size $n_2 = 37.$, then calculate $\bar{X^*}_{ns}$.
		\item Calculate $\bar{X^*}_s - \bar{X^*}_{ns} = {\Delta^*}$. 
		\item Repeat steps (3)-(5) and get a distribution for $\Delta^*$.
		\item The p-value is the number of $\Delta^* > 0$ divided by the number of bootstraps.
		\item Compare this p-value to $\alpha = 0.05$ and conclude.
	\end{itemize}
\end{enumerate}
\newpage

\noindent \textbf{Question 3.}
$Y$ is a r.v. with 
\begin{align*}
f_Y(y)= \frac{2(\theta-y)}{\theta}, \quad 0 < y < \theta.
\end{align*}
\begin{enumerate}[(a)]
	\item $U = Y/\theta$ is a function of the sample measurement, and of only one unknown parameter $\theta$. Further, with $h^{-1}(u) = y = u\theta$
	\begin{align*}
	f_U(u) = f_Y(h^{-1}(u))\abs{\partial_u h^{-1}(u)} = \frac{2(\theta-u\theta)}{\theta^2}\cdot \theta = 2(1-u), \quad u \in (0,1)
	\end{align*}
	does not depend on $\theta$ or any unknown parameter. So $U = Y/\theta$ is a pivotal quantity. 
	
	\item We want to find an $a$ such that $P(U < a) = 0.90$. 
	\begin{align*}
	P(U < a) = \int_0^a 2(1-u)\,du = 2a - a^2 = 0.90.
	\end{align*}
	Using the quadratic formula, we have $a = 0.6837$ or $a = 1.316$. We reject the latter because of the condition $a \in (0,1)$. So, $a = 0.6837$. So,
	\begin{align*}
	P(U < 0.6837) = P(Y/\theta < 0.6837) = P(\theta> Y/0.6837) = 0.90. 
	\end{align*}
	
	
	
	\item We want to use the inverse transform to generate r.v. distributed the same as $Y$. We know $f_Y(y)$. So, $F_Y(y)$ is 
	\begin{align*}
	F_Y(y) = \int^y_0 \frac{2(\theta-y')}{\theta^2}\,dy' = \frac{2y}{\theta} - \frac{y^2}{\theta^2}.
	\end{align*}
	Let $u=T^{-1}(y) = T^{-1}(T(u)) = F_Y(y)$ with $u \sim U(0,1)$. Then solving for $y$ using the quadratic formula gives
	\begin{align*}
	y = \theta - \theta\sqrt{1-u}, \quad \text{or} \quad \theta + \theta\sqrt{1-u}.
	\end{align*}
	Since $y \in (0,\theta)$, we reject the second solution. With this, we generate a sample of uniform $u\sim U(0,1)$, then let $y = \theta - \theta\sqrt{1-u} \sim \text{pdf}(Y)$. 
	
\end{enumerate}


\newpage

\noindent \textbf{Question 4.}
\begin{enumerate}[(a)]
	\item To find the mle of $\theta$, $\hat{\theta}$, we write down the log likelihood function:
	\begin{align}
	l(\theta) &= \ln(\mathcal{L}(\theta)) = \ln\left( \prod^n_{i=1}\frac{1}{\theta^2}y_ie^{-y_i/\theta} \right)\\
	&= \ln \left( \frac{1}{\theta^{2n}}e^{-\sum^n_{i=1}y_i/\theta} \prod^n_{i=1}y_i  \right)\\
	&= -2n\ln(\theta) - \frac{1}{\theta}\sum^n_{i=1}y_i + \ln \prod^n_{i=1}y_i.
	\end{align}
	Taking the derivative w.r.t. $\theta$ and setting it to zero gives
	\begin{align}
	\frac{-2n}{\theta} + \frac{1}{\theta^2}\sum^n_{i=1}y_1 = 0 \iff \hat{\theta} = \frac{1}{2n}\sum^n_{i=1}y_i = \frac{\bar{y}}{2}.
	\end{align}
	
	
	\item The mle of $V(Y_i)$, with $Y_i \sim \Gamma(2,\theta)$, is 
	\begin{align*}
	\text{mle}(V(Y_i)) = \text{mle}(2\theta^2) = 2\hat{\theta}^2 = 2\left( \frac{\bar{y}}{2}\right)^2 = \frac{\bar{y}^2}{2}. 
	\end{align*}
	where we have used the invariance property of mle in the second equality. 
	
	
	
	\item 
	\begin{align*}
	&E(\hat{\theta}) = E\left[ \frac{1}{2n}\sum^n_{i=1}Y_i \right] =\frac{1}{2n}\sum^n_{i=1}E(Y_i)= \frac{1}{2n}\sum^n_{i=1}2\theta = \frac{n\theta}{n} = \theta\\
	&V(\hat{\theta}) = V\left[\frac{1}{2n}\sum^n_{i=1}Y_i\right] = \frac{1}{(2n)^2}\sum^n_{i=1}V(Y_i) = \frac{n(2\theta^2)}{(2n)^2} = \frac{\theta^2}{2n}.
	\end{align*}
	
	\item Since $E(\hat{\theta}) = \theta$, $\hat{\theta}$ is an unbiased estimator for $\theta$. Also, 
	\begin{align*}
	\lim_{n\to \infty} V(\hat{\theta}) = \lim_{n\to \infty} \frac{\theta^2}{2n} = 0.
	\end{align*}
	By the handy theorem, $\hat{\theta} \xrightarrow{P} \theta$, i.e., $\hat{\theta}$ is a consistent estimator for $\theta$. 
\end{enumerate}



\newpage

\noindent \textbf{Question 5.}
\begin{enumerate}[(a)]
	\item \begin{align*}
	F_{X_n}(x) = \int_0^x \left(1 - \frac{x'}{n}\right)^{n-1}\,dx' = -\frac{n}{n}\left(1 - \frac{x'}{n}\right)^n\bigg\vert^x_0 = 1 - \left(1 - \frac{x}{n}\right)^n, \quad 0 \leq x \leq n.
	\end{align*}
	
	
	
	\item 
	\begin{align*}
	\lim\limits_{n\to\infty} F_{X_n}(x) = \lim_{n\to \infty} \left[1 - \left(1 - \frac{x}{n}\right)^n\right] = 1 - e^{-x}, \quad 0 \leq x.
	\end{align*}
	This is the cdf of the Exp(1). So, $X_n \xrightarrow{D} X \sim \text{ Exp(1)}$.
	
\end{enumerate}

\newpage

\noindent \textbf{Question 6.}

\begin{enumerate}[(a)]
	\item \textbf{True.} Power is the probability of rejecting $H_0$ when $H_a$ is true. To actually calculate the power we need both $H_0$ and $H_a$ to be in simple form (assigning a specific value to the parameter), or else the power is indeterminate. 
	\item \textbf{False.} $\text{Exp}(\beta) = \Gamma(1,\beta)$. We also know that if iid $X_i \sim \Gamma(1,\beta)$ then $\sum^n X_i \sim \Gamma(n,\beta)$. But $ \Gamma(n,\beta) \neq \text{ Exp}(n\beta)$. So this statement is false.
	\item \textbf{True.} This is true by definition of the p-value and the significance level $\alpha$. We can make such a comparison because these are conditional on the same thing. It is unfair to compare probabilities conditional on different things.
	\item \textbf{True.} 
	\item\textbf{False.} We \underline{do} need to know the density function to run the accept-reject algorithm.
\end{enumerate}




\newpage





\end{document}




