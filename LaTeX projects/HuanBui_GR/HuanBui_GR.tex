\documentclass{article}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage{mathtools}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{exmp}{Example}[section]
\usepackage{empheq}
\usepackage{hyperref}
\usepackage{tensor}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\begin{document}
\begin{titlepage}\centering
 \clearpage
 \title{\textsc{\bf{GENERAL RELATIVITY\\ \& COSMOLOGY}}\\\smallskip A Quick Guide\\}
 \author{\bigskip Huan Bui}
 \affil{Colby College\\Physics \& Statistics\\Class of 2021\\}
 \date{\today}
 \maketitle
 \thispagestyle{empty}
\end{titlepage}

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

\textit{General Relativity \& Cosmology - A Quick Guide to} is compiled from my PH335: General Relativity and Cosmology with professor Robert Bluhm at Colby College. The course is based on \textit{A Short Course in General Relativity}, 3$^{\text{th}}$ Edition, by James Foster and J. David Nightingale.\\

One of my favorite things to do is to transfer my handwritten lecture notes into nice \LaTeX documents. I started working on these documents last year with \textit{Special Relativity - A Quick Guide to}, which was based on (part of) my PH241: Modern Physics I with professor Charles Conover. I am still working on it, along with another one on Linear Algebra (based on MA253 with Otto Bretscher), and of course General Relativity - the one you are reading now. All of these .pdf files can be found on my \href{www.huanqbui.com}{website}, under the ``A Quick Guide to...'' tab. For the ``raw'' content, i.e., my handwritten lecture notes, feel free to visit the ``Lecture Notes'' tab.\\

Today is Dec 22, 2018. In the spring I will be doing an independent study on Classical Fields Theory with prof. Bluhm. I will also be taking Matrix Analysis with prof. Leo Livshits. I'm excited to be taking these advanced physics and mathematics courses, and I look forward to sharing my journey with you through my lecture notes.  \\

Enjoy!

\newpage
\tableofcontents
\newpage

\section{Overview and Review}

General relativity is a theory of gravity, replacing Newton's gravity law for heavy masses to give more precise predictions. However, we should keep in mind that general relativity is not yet compatible with quantum mechanics. There are numerous open problems in physics related to reconciling gravity and quantum mechanics. 

\subsection{Review of Special Relativity}

Special relativity studies the kinematics and dynamics in relatively moving inertial reference frames. Most of special relativity and its consequences are encoded in the Lorentz transformations. A classic example of a Lorentz transformation that is often focused on in introductory special relativity is called the ``Lorentz boost'' in the $x$-direction. Note that there is nothing special about $x$ or $y$ or $z$. The $x$-Lorentz boost is essentially a coordinate transformation, i.e. given coordinates an event $A$ in frame $(S)$, we can calculate the coordinates of $A$ in frame $(S')$.
\begin{align*}
\begin{pmatrix}
ct'\\x'\\y'\\z'
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\gamma\beta & 0 & 0\\
-\gamma\beta & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
ct\\x\\y\\z
\end{pmatrix}
=
\begin{pmatrix}
ct\\x\\y\\z
\end{pmatrix},
\end{align*}
where $\beta = v/c$ and the Lorentz factor 
\begin{align*}
\gamma = \frac{1}{\sqrt{1-\frac{v^2}{c^2}}}.
\end{align*}
Since we are studying relativity, it is important to look at ``invariants'' - quantities that do not change under transformations. In special relativity, one such invariant is the spacetime invariant:
\begin{align*}
(\Delta S)^2 &= (c\Delta \tau)^2\\
&= (c\Delta t)^2 - (\Delta x)^2 - (\Delta y)^2 - (\Delta z)^2\\
&= (c\Delta t')^2 - (\Delta x')^2 - (\Delta y')^2 - (\Delta z')^2\\
&= (\Delta S')^2 = (c\Delta \tau')^2.
\end{align*}
This can be readily shown. In fact, we have verified this in \textit{Special Relativity: A Quick Guide}. This quantity, roughly speaking, is a measure of \textbf{proper} distances and times. If we go to a rest frame of event $A$, such that $\Delta x' = \Delta y' = \Delta z' = 0$, then we get $\Delta t' = \Delta \tau$, where $\tau$ denotes the \textbf{proper time}, which is the time measured in the rest frame. This gives $(\Delta S)^2 = (c\Delta \tau)^2$.\\

In relativity in general and in Minkowski spacetime in particular, we are interested in two types of objects: \textbf{scalars} and \textbf{vectors}. Scalars are invariant under general coordinate transformations. An example of a scalar is the spacetime invariant. Another scalar, which we probably will not see again in this text, is the metric signature (sgn($[\eta_{\mu\nu}] = -2$)). Vectors, on the other hand, are a different type of objects, which transform in the same way under coordinate transformations, but are not invariant under general coordinate transformations in general, i.e., their components are not necessarily the same in different coordinate systems. In 4-dimensional spacetime, we work with 4-vectors, which simply means 4-component vectors.\\

In special relativity, we often talk about position vectors $\mathbf{x} = (ct,x,y,z)^\top = (ct,\vec{x})^\top$ and energy-momentum vectors $\mathbf{p} = (E/c,p_x,p_y,p_z)^\top = (E/c, \vec{p})^\top$. These vectors transform under the Lorentz transformation. 
\begin{exmp}
The $x$-Lorentz boost applied to $\mathbf{p}$ gives
\begin{align*}
\begin{pmatrix}
E'/c\\p_x'\\p_y'\\p_z'
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\gamma\beta & 0 & 0\\
-\gamma\beta & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
E/c\\p_x\\p_y\\p_z
\end{pmatrix}.
\end{align*}
Notice that $E/c$ transforms like $ct$ (time) and $\vec{p}$ transforms like $\vec{x}$ (space). We also have the following invariant
\begin{align*}
\frac{E^2}{c^2} - p^2_x - p^2_y - p^2_z = \frac{E^2}{c^2} - \vec{p}\cdot\vec{p}.
\end{align*}
But recall that $(mc)^2 = E^2/c^2 - \vec{p}\cdot\vec{p}$, so
\begin{align*}
\frac{E^2}{c^2} - p^2_x - p^2_y - p^2_z = (mc)^2.
\end{align*} 
If we go to a rest frame, such that $\vec{p} = \vec{0}$, then we obtain the famous rest mass-energy equivalence: $E = mc^2$.
\end{exmp}
Some quantities associated with a vector can be invariant (scalar), such as their norm. In fact, the spacetime invariant in Minkowski space is nothing but a dot product of a vector with itself:
\begin{align*}
\mathbf{a}\cdot\mathbf{a} = a^0a^0 - a^1a^1 - a^2a^2 - a^3a^3,
\end{align*}
where $0,1,2,3$ are indices, and $\mathbf{a}$ implies a 4-vectors, which should be distinguished from the 3-vector $\vec{a} = (a^1,a^2,a^3)^\top$. Notice that the dot product in Minkowski spacetime is defined differently from that in flat 3-dimensional Cartesian coordinate system. In the following chapters, we will explore how dot products are defined in general. Hint: the metric tensor plays an important role.\\ 


\subsection{The Equivalence Principle}
In 1907, Albert Einstein had the ``happiest thought of his life'' when he realized that in a freely falling frame (non rotating and/or accelerating), the effects of gravity go away, i.e., there is an equivalence between gravity and acceleration such that they can ``undo'' each other. For example, the following two situations are equivalent in terms of the acceleration experienced by the observer: (i) a person standing on Earth, and (ii) a person inside an elevator accelerating upwards at rate $g$ in free space (no gravitational field). Likewise, the following two situations are also equivalent: (iii) a person floating in free space, and (iv) a person inside an airplane free falling towards Earth (this is commercially known as ``zero-G flight''). we arrive at the statement of the Equivalence Principle: \\

\noindent \boxed{``\textit{A small, non-rotating, freely falling frame in a $\vec{g}$ field is an inertial frame.}''}\\

The above statement is a direct result of Galileo's discovery that all objects have the same acceleration due to gravity. This result may seem a little bit \textit{circular}, because it is actually a coincidence that the two roles of \textbf{mass}: (i) to cause gravitational force like charge in an electric field:
\begin{align*}
\vec{F} = \frac{GMm_G}{r^3}\vec{r} = m_G\vec{g}
\end{align*}
and (ii) to measure inertia:
\begin{align*}
\vec{F} = m_I\vec{a}
\end{align*}
are the same, i.e., $m_I = m_G$. It could have been that ``gravitational mass'' $m_G$ and ``inertial mass'' $m_I$ are not the same, in which case $\vec{a} \neq \vec{g}$, and the equivalence principle does not hold. However, the famous E\"{o}tv\"{o}s experiment, which measured the correlation between inertial mass and gravitational mass, has shown that
\begin{align*}
\frac{\vert m_G - m_I\vert}{m_I} \leq 10^{-10}.
\end{align*}

While the observations we have made so far can seem self-apparent, the consequence of the equivalence principle is the bending of light around massive objects. Consider a light ray going horizontally (left to right) pass an upward-accelerating elevator with an observer inside. For the observer, since he is accelerating upwards, he sees the light beam as bent, entering at the top-left of the elevator and exiting at the bottom-right. Now, according to the equivalence principle, because the upward-accelerating scenario is equivalent to the existence of a massive object (such that the observer experiences the same acceleration). This means that the observer, now no longer in the elevator, ``sees'' the light as bent around this massive object. Fig. 1. illustrates this postulate.
\begin{align*}
\text{include figure here!!!}
\end{align*}
General relativity predicts that light going pass Earth's surface will ``fall'' by approximately $1 \text{\AA}$, which is not observable. However, for a much more massive object like the Sun, general relativity predicts a bending of $1.75''$ (arc sec). This prediction was verified by the glorious experiment of Arthur Eddington.\\

Note that we could argue for the bending of light, using Newtonian physics. However, in order to get the correct predictions for the bending of light, we need general relativity. We will explore the reason behind this discrepancy in the following chapter. But roughly speaking, spacetime is assumed to be flat in Newtonian physics, while spacetime is curved by massive objects, according to general relativity.\\ 

One might ask: ``How do we view falling objects on Earth as due to the curvature of spacetime?'' The answer requires bringing back Minkowski spacetime diagrams. 
\begin{align*}
\text{insert figure here!!!}
\end{align*}

\subsection{Versions of the Equivalence Principle}
There are two versions of the equivalence principle, referred to as the \textit{strong equivalence principle}, SEP, and the \textit{weak equivalence principle}, WEP. 
\subsubsection{The Strong Equivalence Principle}
The strong equivalence principle states that \textbf{all} of physics reduces to special relativity in a freely falling frame.
\subsubsection{The Weak Equivalence Principle}
The weak equivalence principle states that all point particles fall at the same rate in a gravitational field ($m_G = m_I$). We notice that WEP only applies to gravity, which makes it sufficient to develop general relativity, but not quantum mechanics. For this remaining of this note, we will rely on the WEP. 

\newpage

\section{Review of Vector Calculus}
\subsection{Operations \& Theorems}
Recall that a path in 3-dimensional space can by parameterized a single variable, which we will call $t$, as
\begin{align*}
\vec{r}(t) = x(t)\hat{i} + y(t)\hat{j} + z(t)\hat{k},
\end{align*}
where the tangent to this path is given by
\begin{align*}
\dot{\vec{r}}(t) = \frac{d \vec{r}}{dt}.
\end{align*}
The length of the underlying curve of this path is the integral over the line element $ds = \vert\vert \vec{r} \vert\vert = \vert\vert \dot{\vec{r}} \vert\vert\,dt$:
\begin{align*}
L = \int_{a}^{b} dS = \int_{a}^{b} \vert\vert \dot{\vec{r}} \vert\vert\,dt.
\end{align*} 
\begin{exmp}
Consider a path in $\mathbb{R}^2$ defined as $\vec{r}(t) = (t, \sin t)$, with $t \in [-2\pi, 2\pi]$. We instantly recognize that the underlying curve of this path is nothing but a sine curve in the $xy$-plane. The length of this curve is
\begin{align*}
L = \int_{-2\pi}^{2\pi}\vert\vert  (1,\cos t)\vert\vert\,dt  = \int_{-2\pi}^{2\pi} \sqrt{1+\cos^2 t}\,dt \approx 15.28.
\end{align*}
\end{exmp}
Next, we consider a vector-valued function $\vec{F} : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined as $\vec{F}(\vec{r}) = (F_x(x,y,z), F_y(x,y,z),F_z(x,y,z))$. Recall the two ``del operators'' from vector calculus: the \textbf{curl} and the \textbf{div}, whose definitions are
\begin{align*}
\text{curl}(\vec{F}) = \vec{\nabla} \times \vec{F} = \det
\begin{pmatrix}
\hat{i} & \hat{j} & \hat{k} \\
\frac{\partial}{\partial x} & \frac{\partial }{\partial y} & \frac{\partial}{\partial z}\\
F_x & F_y & F_z
\end{pmatrix}
\end{align*}
and 
\begin{align*}
\text{div}(\vec{F}) = \vec{\nabla} \cdot \vec{F} = 
\frac{\partial F_x}{\partial x} + \frac{\partial F_y }{\partial y} + \frac{\partial F_z}{\partial z}.
\end{align*}
where the ``nabla'' symbol represents the operator
\begin{align*}
\vec{\nabla} = \left( \frac{\partial }{\partial x} , \frac{\partial }{\partial y} , \frac{\partial }{\partial z} \right). 
\end{align*}
We have also seen the \textbf{gradient} of a scalar field (or potential) $f : \mathbb{R}^3 \rightarrow \mathbb{R}$. The gradient is defined as
\begin{align*}
\vec{\nabla}f = \left( \frac{\partial f}{\partial x} , \frac{\partial f }{\partial y} , \frac{\partial f}{\partial z}\right) .
\end{align*}
There are two kinds of potential functions in vector calculus: scalar potential and vector potential. While we are more familiar with the scalar potential (e.g. the gravitational potential or the electric potential in electromagnetism), there isn't as strong a connection between vector potentials and any physical meaning. However, we should still look at some examples. 
\begin{exmp}
In electromagnetism, an electric field $\vec{E}$ is defined as the negative gradient of the electric potential $\phi$, measured in Volts:
\begin{align*}
\vec{E} = -\vec{\nabla}\phi.
\end{align*}
On the other hand, an magnetic field $\vec{B}$ is defined as the curl of the magnetic potential $\vec{A}$.
\begin{align*}
\vec{B} = \vec{\nabla}\times\vec{A}.
\end{align*}
In this case, $A$ is a vector potential. Note that we don't often talk about a vector potential of an electric field. This is simply because for a point charge, which produces an inverse-square field, the vector potential simply does not exist. The proof is straightforward. For curious readers, this proof can be an interesting exercise (the solution should be a one-liner). Or else, please refer to my vector calculus notes on my website. Hint: we need one of the two following fundamental theorems of vector calculus: Stokes' and Gauss' (or Ostrogradsky's).
\end{exmp}
\textbf{Stokes' theorem} states that, the circulation around a closed curved in $\mathbb{R}^3$ by a vector field $\vec{F}$ is the flux of $\vec{\nabla}\times\vec{F}$ through the surface $S$ whose boundary $C = \partial S$ is the curve $C$.
\begin{align*}\boxed{
\oint_{C = \partial S} \vec{F}\cdot\,d\vec{s} = \iint_{S} \vec{\nabla}\times \vec{F} \cdot\,d\vec{S}.}
\end{align*}

\textbf{Gauss' theorem} states that, if $S$ is closed surface, oriented outward, and if $\vec{F}$ is defined throughout the solid region $W$ enclosed by $S$, then the flux of $\vec{F}$ through $S$ is the total divergence of $S$ through the region $W$.
\begin{align*}
\boxed{
\oiint_{S=\partial W}\vec{F}\cdot\,d\vec{S} = \iiint \vec{\nabla}\cdot\vec{F}\,dV.}
\end{align*}
\textbf{Line integrals} are one of a few important operations in vector calculus. In the context of undergraduate or high school physics, we often use the line integral to calculate the work done by a force $\vec{F}$ over some path $\vec{r}$. The integral in general is simply an accumulation of the component of the vector field along the path. 
\begin{align*}
\int_{a}^{b}\vec{F}\cdot\,d\vec{r}.
\end{align*} 
If $\vec{F}$ is a conservative field, then the line integral along $\vec{F}$ between two point $A$ and $B$ only depends on where $A$ and $B$ are, i.e., the line integral is path-independent. A good illustration of this fact is the way we compute the change in electric potential:
\begin{align*}
W = -\int \vec{E}\cdot\,d\vec{r} = \Delta \phi
\end{align*}

To actually compute a line integral, we often look for symmetry first (or whether $\vec{F}$ is conservative), then parameterize $\vec{r} = \vec{r}(s)$ if we must. Then, the line integral becomes
\begin{align*}
\int_{a}^{b}\vec{F}\cdot\,d\vec{r} = \int_{a}^{b}\vec{F}(\vec{r}(s))\cdot \frac{d\vec{r}}{ds}\,ds
\end{align*}
\textbf{Surface integrals} give the flux of a vector field through a surface:
\begin{align*}
\int \vec{F}\cdot\,d\vec{S} = \int\vec{F}\cdot\vec{n}\,dS = \pm\int\vec{F}\cdot\vec{N}\,d\phi,d\theta,
\end{align*}
where $\vec{F}$ is the vector field, and $\vec{n}$ is the unit normal vector to the surface. The last expression denotes the surface integral in terms of a parameterization $\vec{S}(\phi, \theta) = (x(\phi, \theta),y(\phi, \theta),z(\phi, \theta))$. $\vec{N}$ is the standard normal to the surface, which is not necessarily a unit vector (with norm of 1) and not necessarily in the same direction as $\vec{n}$, which is intrinsic to the surface.
\begin{exmp}
Consider the electric flux through a sphere:
\begin{align*}
\Phi_E = \oiint\vec{E}\cdot\,d\vec{a} = \frac{q}{\epsilon_0}.
\end{align*}
The last expression comes from Gauss' (or Ostrogradsky's) theorem, equivalently the divergence theorem, where $q$ is the charge enclosed.
\end{exmp}
We should introduce the Maxwell's equations as they ``bring together'' the concepts we have touched on so far. To see how, we consider the Maxwell's equations in integral form:
\begin{align*}
\text{Gauss' law:  } \Aboxed{&\oiint\vec{E}\cdot\,d\vec{a} = \frac{q}{\epsilon_0}}\\
\text{No magnetic monopole:  } \Aboxed{&\oint\vec{B}\cdot\,d\vec{a} = 0}\\
\text{Faraday's law:  } 
\Aboxed{&\oint \vec{E}\cdot\,d\vec{s} = -\frac{d}{dt}\Phi_B = \frac{\partial}{\partial t}\int_A\vec{B}\cdot\,d\vec{a}}\\  
\text{Ampere-Maxwell's law:  } 
\Aboxed{&\oiint \vec{B}\cdot\,d\vec{s} = \mu_0I + \mu_0\epsilon_0\frac{\partial}{\partial t}\int_A\vec{E}\cdot\,d\vec{a}}
\end{align*}
Using the theorems and facts we have discussed, we can convert the above equations into differential form. First, we can use Gauss' theorem on Gauss' law, with
\begin{align*}
q = \int_V \rho\,d^3r
\end{align*}
where $\rho$ is the charge volume density. So, Gauss' law becomes
\begin{align*}
\oint \vec{E}\cdot\,d\vec{a} = \int_V \vec{\nabla}\cdot\vec{E}\,d^3r = \frac{1}{\epsilon_0}\int_V\rho\,d^3r,
\end{align*}
which implies that the divergence of any electric field $\vec{E}$ is proportional to the charge enclosed $q$.
\begin{align*}
\Aboxed{\vec{\nabla}\cdot\vec{E} = \frac{\rho}{\epsilon_0}}
\end{align*}
Applying Gauss' theorem to the second equation,
\begin{align*}
\oiint \vec{B}\cdot\d,\vec{a} = \int_V \vec{\nabla}\cdot\vec{B}\,d^3r = 0
\end{align*}
we immediately see that the divergence of any magnetic field $\vec{B}$ is 0, i.e., there is no such thing as a ``magnetic monopole.''
\begin{align*}
\Aboxed{\vec{\nabla}\cdot\vec{B} = 0}
\end{align*}
We can apply Stokes' theorem on the other two equations to turn them into differential form. Converting the third equation is as simple as bookkeeping:
\begin{align*}
\oint \vec{E}\cdot\,d\vec{s} = \int_A\vec{\nabla}\times\vec{E}\cdot\,d\vec{a} = -\frac{\partial}{\partial t}\int_A\vec{B}\cdot\,d\vec{a},
\end{align*}
which says the curl of any electric field $\vec{E}$ is negatively proportional to the change in the magnetic field. 
\begin{align*}
\Aboxed{\vec{\nabla}\times\vec{E} = -\frac{\partial\vec{B}}{\partial t}}
\end{align*}
To convert the fourth equation into differential form, we define a new quantity, $\vec{J}$, as current (area) density, such that
\begin{align*}
I = \int_A \vec{J}\cdot\,d\vec{a}.
\end{align*} 
Again, applying Stokes' theorem to the last equation, 
\begin{align*}
\oint \vec{B}\cdot\,d\vec{s} = \int_A \vec{\nabla}\times\vec{B}\cdot\,d\vec{a} = \mu_0I+\mu_0\epsilon_0\frac{\partial}{\partial t}\int_A\vec{E}\cdot\,d\vec{a} = \mu_0\int_A\left( \vec{J} + \epsilon_0\frac{\partial \vec{E}}{\partial t}\right) \cdot\,d\vec{a}.
\end{align*}
So, in differential form:
\begin{align*}
\Aboxed{\vec{\nabla}\times\vec{B} = \mu_0\vec{J} + \mu_0\epsilon_0\frac{\partial \vec{E}}{\partial t}}
\end{align*}
In a later chapter, we will see how to make these equations fully relativistic. For the curious reader wanting more information regarding this subsection, please refer to a standard textbook on vector calculus, or feel free to use my Vector Calculus lecture notes for a more formal treatment of this subject.

\subsection{Coordinate Systems}
There are numerous coordinate systems to describe 3-dimensional space. However, we are most familiar with the Cartesian coordinates ($x,y,z$), the Spherical coordinates ($r,\theta,\phi$), and the Cylindrical coordinates ($\rho, \phi, z$). The Cartesian coordinate system is the most simple, elegant, and ``nice'' of the three systems, since the $(x,y,z)$ coordinates can also be coefficients of the vector pointing from the origin to any point in space. We will see that this is not the case in general coordinate systems. One such coordinate system is the \textbf{spherical coordinate system} (illustrated in Fig. ???), where
\begin{align*}
x &= r\sin\theta\cos\phi\\
y &= r\sin\theta\sin\phi\\
z &= r\cos\theta
\end{align*}
and the inverse relations are
\begin{align*}
r &= \sqrt{x^2+y^2+z^2}\\
\theta &= \cos^{-1}\left( \frac{z}{r}\right) = \cos^{-1}\left( \frac{z}{\sqrt{x^2+y^2+z^2}}\right) \\
\phi &= \tan^{-1}\left( \frac{y}{x} \right) 
\end{align*}
where $0 \leq r$, $0 \leq \theta \leq \pi$, and $0 \leq \phi \leq 2\pi$.
\begin{align*}
\text{insert figure here!!!!}
\end{align*}
The \textbf{volume element} is given by
\begin{align*}
dV = dx\,dy\,dz = r^2\sin\theta\,dr\,d\theta\,d\phi.
\end{align*}
Consider a sphere of radius $R$. The \textbf{area element} is given by
\begin{align*}
dA = R\,d\theta\,d\phi.
\end{align*}
In \textbf{cylindrical coordinates},
\begin{align*}
x &= \rho\cos\phi\\
y &= \rho\sin\phi\\
z &= z
\end{align*}
and the inversion relations are
\begin{align*}
\rho &= \sqrt{x^2+y^2}\\
z &= z\\
\phi &= \tan^{-1}\left(\frac{y}{x}\right) 
\end{align*}
The \textbf{volume element} is given by
\begin{align*}
dV = r\,dt\,dz\,d\phi.
\end{align*}
Consider a cylinder of radius $R$, the \textbf{area element} is given by 
\begin{align*}
dA = R\,dz\,d\phi.
\end{align*}
\begin{align*}
\text{Insert figure here!!!}
\end{align*}
The purpose of having the area and volume elements is so that we could integrate over regions, parameterized in spherical or cylindrical coordinates. While it seems like these quantities appear out of the blue, there is a general procedure to find them, given any arbitrary coordinate system. This ``procedure'' is called the \textbf{Jacobian}, which essentially gives the scaling factor as we move from one coordinate system into another. We know that the area element in polar coordinate is $dA = r\,dr\,d\theta$. We can see this is true, using the Jacobian. First, the Jacobian matrix, which is characteristic of a transformation from coordinate system $u^i$ into $u^{j'}$ is defined as
\begin{align*}
\boxed{[U^{j'}_{i}] = \left[ \frac{\partial u^{j'}}{\partial u^{i}} \right] = 
\begin{pmatrix}
\frac{\partial u^1}{\partial u^{1'}} & \dots & \frac{\partial u^1}{\partial u^{n'}}\\
\vdots & \ddots & \vdots\\
\frac{\partial u^n}{\partial u^{1'}} & \dots & \frac{\partial u^n}{\partial u^{n'}}
\end{pmatrix}}
\end{align*}
Note that $i$ and $j'$ are indices, not powers. Now, the general \textbf{scaling factor} is defined by the following equality
\begin{align*}
\Aboxed{N = du^{1'}\,du^{2'}\dots\,du^{n'} = \left| \det([U^{j}_{i}])\right| \,du^1\,du^2\dots\,du^{n}}
\end{align*}
\begin{exmp}
The scaling factor as we move from 2-dimensional Cartesian coordinates into polar coordinates is
\begin{align*}
N = \det\begin{pmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta}\\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
\end{pmatrix}
=
\det\begin{pmatrix}
\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{pmatrix}
=
r.
\end{align*}
So, the area element is $dA = dx\,dy = rdr\,d\theta$, as expected. 
\end{exmp}
\begin{exmp}
We can try repeating the same procedure to get the volume element for spherical coordinates, which we have said before to be $dV = r^2\sin\theta\,dr\,d\theta\,d\phi$. First, define the Jacobian matrix:
\begin{align*}
[U^{j'}_{i}] = \begin{pmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \phi}\\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial \phi}\\
\frac{\partial z}{\partial r} & \frac{\partial z}{\partial \theta} & \frac{\partial z}{\partial \phi}
\end{pmatrix}
=
\begin{pmatrix}
\sin\theta\cos\phi & r\cos\theta\cos\phi & -r\sin\theta\sin\phi \\
\sin\theta\sin\phi & r\cos\theta\sin\phi & r\sin\theta\cos\phi \\
\cos\theta & -r\sin\theta & 0
\end{pmatrix}
\end{align*}
After some bookkeeping-type rearrangements and computation, we find that
\begin{align*}
N = \left|\det[U^{j'}_{i}]\right|= r^2\sin\theta,
\end{align*}
as expected. As a check, we can integrate this volume element over a region defined by $x^2+y^2+z^2 = R^2$ to find its volume
\begin{align*}
\int_{0}^{R}\int_{0}^{\pi}\int_{0}^{2\pi}r^2\sin\theta\,dr\,d\theta\,d\phi = \frac{4}{3}\pi R^3,
\end{align*}
which is not surprisingly the volume of the sphere of radius $R$. 
\end{exmp}
\newpage

\section{Flat 3-dimensional space}
Flat, or ``Euclidean'' space are spaces with \textbf{no curvature}. For instance, a flat sheet of paper in $\mathbb{R}^3$ is considered ``flat'' while the surface of a sphere embedded in $\mathbb{R}^3$ is considered curved space. There should also be a distinction between a sphere in $\mathbb{R}^3$ and a sphere \textbf{embedded} into $\mathbb{R}^3$. The former case is simply some subset of flat 3-dimensional space, so it is considered flat. But in the latter case, the sphere is a curved 2-dimensional space, although it lives in flat 3-dimensional space.\\

Our goal in this section is to see how  we can use any arbitrary coordinate system, which specify points in 3-dimensions as an intersection of 3 surfaces. For instance, in Cartesian coordinates, a point ($a^1,a^2,a^3$) is specified by 3 planes: $x = a^1$, $y=a^2$, and $z=a^3$. In spherical coordinates, a point is specified by intersecting a sphere of radius $r$, a cone with azimuth angle $\theta$, and a plane of constant $\phi$. In cylindrical coordinates, a point is specified by intersecting a cylinder of radius $\rho$, vertical plane of constant $\phi$ to the $x-$axis, and the horizontal plane of constant $z$.
\begin{align*}
\text{insert figure here!!! }
\end{align*}

\subsection{Curvilinear coordinates}
Curvilinear coordinates are an arbitrary coordinate system in Euclidean space where coordinate lines can be curved (hence curvilinear). Note that although coordinate lines can be curvy, the space is still flat. We can call $(u,v,w)$ our arbitrary coordinates, which specify a point $(a^1,a^2,a^3)$ by intersecting three surfaces: $u = a^1$, $v=a^2$, $w=a^3$. Let let relations between $(u,v,w)$ and $(x,y,z)$ be established as
\begin{align*}
u &= u(x,y,z); \text{ } x = x(u,v,w)\\
v &= v(x,y,z); \text{ } y = y(u,v,w)\\
w &= w(x,y,z); \text{ } z = z(u,v,w)
\end{align*} 
\begin{align*}
\text{Insert figure here!!!}
\end{align*}

\subsection{Basis vectors}
\subsection{Natural basis}
Next, since we want to be able to describe vectors using arbitrary curvilinear coordinates, we need to know a \textbf{basis set} of vectors that span the space. In Cartesian coordinates, such a basis set is $\{\hat{i},\hat{j},\hat{k}\}$. Similarly, we want to find a set $\{\vec{e}_u, \vec{e}_v, \vec{e}_w\}$ that would give a basis for an arbitrary curvilinear coordinate system. The way we obtain this set is the same as how we obtain $\{\hat{i}, \hat{j}\, \hat{k}\}$ from Cartesian coordinates. We notice that $\hat{i}$ is a vector that follows the change in the $x$-direction with $y$ and $z$ fixed, i.e., $\hat{i}$ is a tangent vector to a vector $\vec{r}$ along the change in $x$. Hence
\begin{align*}
\hat{i} = \frac{\partial \vec{r}}{\partial x}.
\end{align*}
Likewise, $\hat{j} = \frac{\partial \vec{r}}{\partial y}$, and $\hat{k} = \frac{\partial \vec{r}}{\partial z}$. By analogy, we can define the set $\{ \vec{e}_u, \vec{e}_v, \vec{e}_w\}$ with
\begin{align*}
\vec{e}_u = \frac{\partial \vec{r}}{\partial u}, \text{ } \vec{e}_v = \frac{\partial \vec{r}}{\partial v}, \text{ } \vec{e}_w = \frac{\partial \vec{r}}{\partial w},
\end{align*}
In general, given a coordinate system with $(u^1,u^2,\dots,u^n)$, we can define a \textbf{natural basis set} by letting
\begin{align*}
\Aboxed{\vec{e}_{u^i} = \frac{\partial \vec{r}}{\partial u^i}}
\end{align*}
where $i = 0,1,2,\dots,n$ are indices. Note that the set $\{\vec{e}_{u^i} \}$ need not be orthogonal. The only requirement is that they have to be linearly independent and span the space. We also note that they need not be unit vectors. It is possible to make a basis set of only unit vectors by letting
\begin{align*}
\hat{e}_u = \frac{\vec{e}_u}{\vert\vert \vec{e}_u\vert\vert},
\end{align*}
but we will see that this definition is not so useful in general relativity. A question one might ask is: ``What is \textbf{natural} about this basis set?'' The answer, unfortunately, will be provided in the following sections, where we will discuss how they give rise to the \textbf{metric tensor}.\\

We will often use $\{\hat{i}, \hat{j}, \hat{k} \}$ as a reference basis, i.e., we can express $\{\vec{e}_u, \vec{e}_v ,\vec{e}_w\}$ in terms of these:
\begin{align*}
\vec{\epsilon_u} = (e_u)_x\hat{i} + (e_u)_y\hat{j} + (e_u)_z\hat{k}.
\end{align*}
\begin{exmp}
We can look at the basis set for the spherical coordinate system, $\{\vec{e}_r, \vec{e}_\theta, \vec{e}_\phi \}$. First, let the coordinates $(u,v,w)$ be $(r,\theta,\phi)$, the spherical coordinates. A vector $\vec{r}$, expressed in spherical coordinates, has the form:
\begin{align*}
\vec{r} = 
\begin{pmatrix}
r\sin\theta\cos\phi\\
r\sin\theta\sin\phi\\
r\cos\theta
\end{pmatrix},
\end{align*}
where $r \geq 0$, $0 \leq \theta \leq \pi$, and $0 \leq \phi \leq 2\pi$. To obtain the basis vectors, we simply take $\partial \vec{r}/\partial r$, $\partial \vec{r}/\partial \theta$, and $\partial \vec{r}/\partial \phi$. We expect that our resulting vectors should form the column vectors of the Jacobian matrix $[U^{j'}_i]$, which, as we have discussed previously, has the following form:
\begin{align*}
[U^{j'}_i] = 
\begin{pmatrix}
\sin\theta\cos\phi & r\cos\theta\cos\phi & -r\sin\theta\sin\phi \\
\sin\theta\sin\phi & r\cos\theta\sin\phi & r\sin\theta\cos\phi \\
\cos\theta & -r\sin\theta & 0
\end{pmatrix}.
\end{align*}
What we're doing now, in terms of linear algebra, is essentially constructing $[U^{j'}_i]$ - the change-of-basis matrix. Let's see if our described procedure works:
\begin{align*}
\vec{e}_r &= \frac{\partial \vec{r}}{\partial r} = \begin{pmatrix}
\sin\theta\cos\phi\\
\sin\theta\sin\phi\\
\cos\theta
\end{pmatrix}\\
\vec{e}_\theta &= \frac{\partial \vec{r}}{\partial \theta} = 
\begin{pmatrix}
r\cos\theta\cos\phi\\
r\cos\theta\sin\phi\\
-r\sin\theta
\end{pmatrix}\\
\vec{e}_\phi &= \frac{\partial \vec{r}}{\partial \phi}=
\begin{pmatrix}
-r\sin\theta\sin\phi\\
r\sin\theta\cos\phi\\
0
\end{pmatrix}
\end{align*} 
Not surprisingly,
\begin{align*}
[U^{j'}_i] = 
\begin{pmatrix}
\sin\theta\cos\phi & r\cos\theta\cos\phi & -r\sin\theta\sin\phi \\
\sin\theta\sin\phi & r\cos\theta\sin\phi & r\sin\theta\cos\phi \\
\cos\theta & -r\sin\theta & 0
\end{pmatrix}
=
\left( \vec{e}_r \vert \vec{e}_\theta \vert \vec{e}_\phi \right).
\end{align*}
Next, we can check if ${\vec{e}_r, \vec{e}_\theta, \vec{e}_\phi}$ is mutually-perpendicular set. (Note that I avoid the word ``orthogonal'' here, since it carries different meanings in physics and in linear algebra). We can also check whether the three vectors are unit vectors. It suffices to take the dot products of ${\vec{e}_r, \vec{e}_\theta, \vec{e}_\phi}$ among themselves:
\begin{align*}
\vec{e}_r\cdot\vec{e}_r &= \sin^2\theta(\cos^2\phi + \sin^2\phi) + \cos^2\theta = 1\\
\vec{e}_r\cdot\vec{e}_\theta &= r\cos^2\phi\sin\theta\cos\theta + r\sin^2\phi\sin\theta\cos\theta - r\sin\theta\cos\theta = 0\\
\vec{e}_r\cdot\vec{e}_\phi &= -r\sin^2\theta\sin\phi\cos\phi + r\sin^2\theta\sin\phi\cos\phi = 0\\ 
\vec{e}_\theta\cdot\vec{e}_\theta &= r^2\cos^2\phi\cos^2\theta + r^2\cos^2\phi\sin^2\theta + r^2\sin^2\theta = r^2\\ 
\vec{e}_\theta\cdot\vec{e}_\phi &= 0 \\
\vec{e}_\phi\cdot\vec{e}_\phi &= r^2\sin^2\phi\sin^2\theta + r^2\sin^2\theta\cos^2\phi = r^2\sin^2\theta.
\end{align*}
We notice that all dot products of cross-coordinates result in 0, which means the basis vectors we found are mutually perpendicular (we can also see this fat geometrically, based on how the spherical coordinates are defined). But we also find that these basis vectors are not unit vectors, since their norms are not necessarily 1:
\begin{align*}
\vert\vert \vec{e}_r \vert\vert &= 1\\
\vert\vert \vec{e}_\theta \vert\vert &= r\\
\vert\vert \vec{e}_\phi \vert\vert &= r\sin\theta.
\end{align*}
\end{exmp}
\subsection{Dual basis}
The procedure we have described in the previous subsection allows us to find a \textbf{natural basis} set of vectors for a coordinate system. However, basis sets are not unique. In this section, we look at an alternative basis set of vectors, called the \textbf{dual basis} set, denoted $\{ \vec{e}^{\,u}, \vec{e}^{\,v}, \vec{e}^{\,w},\dots \}$. Instead of deriving these from the finding the tangent vectors, we find the normal of surfaces of constant $u,v,w,\dots$\\

Recall that the \textbf{gradient} operator $\vec{\nabla}$, when applied to a scalar field $f$, gives the normal vectors to surfaces of constant $f$. Since curvilinear coordinates are given by coordinate lines where $u,v$, or $w$ is constant, we can let
\begin{align*}
\vec{e}^{\,u} &= \vec{\nabla}u\\
\vec{e}^{\,v} &= \vec{\nabla}v\\
\vec{e}^{\,w} &= \vec{\nabla}w,
\end{align*}  
and so on, where $\vec{e}^{\,u}$ is a normal vector to the surface of constant $u$. Or more generally, 
\begin{align*}
\boxed{\vec{e}^{\,u^i} = \vec{\nabla}u^i}
\end{align*} 
Now, we might wonder what the dual basis in Cartesian coordinates is. It turns out the dual and natural basis sets in Cartesian coordinates are the same:
\begin{align*}
\vec{e}^{\,x} &= \vec{\nabla}x=(1,0,0)^\top=\hat{i} = \vec{e}_x \\
\vec{e}^{\,y} &= \vec{\nabla}y=(0,1,0)^\top=\hat{j} = \vec{e}_y \\
\vec{e}^{\,z} &= \vec{\nabla}z=(0,1,0)^\top=\hat{k} = \vec{e}_z.
\end{align*}
There's actually no magic behind this ``coincidence.'' The reason the basis sets are the same is simply that the direction of increasing/decreasing $x$ is the same as the normal vector to the surface of constant $x$, i.e., the $yz$-plane. 
\begin{align*}
\text{insert figure here!!!}
\end{align*}
However, we suspect that this is not the case in general curvilinear coordinates, since the direction of increasing $u$, for instance, should not necessarily be the same as the normal vector to the surface of constant $u$, as illustrated here:
\begin{align*}
\text{insert figure here!!!}
\end{align*}
\begin{exmp}
We can look at the dual basis set for the spherical coordinate system. Based on what we have established so far, we can compute the dual basis set, using the inverse relations $(x,y,z) \rightarrow (r,\theta,\phi)$.
\begin{align*}
\vec{e}^{\,r} &= \vec{\nabla}r = \vec{\nabla}(x^2+y^2+z^2)^{1/2} = (x^2+y^2+z^2)^{-1/2}\begin{pmatrix}x\\y\\z\end{pmatrix} = \begin{pmatrix}\sin\theta\cos\phi\\ \sin\theta\sin\phi\\ \cos\theta\end{pmatrix}.
\end{align*}
Tedious and painstaking computations give us the other two vectors:
\begin{align*}
\vec{e}^{\,\theta} &= \frac{1}{r}\begin{pmatrix} \cos\theta\cos\phi \\ \cos\theta\sin\phi \\ -\sin\theta \end{pmatrix}\\
\vec{e}^{\,\phi} &= \frac{1}{r}\begin{pmatrix} -\frac{\sin\phi}{\sin\theta} \\ \frac{\cos\phi}{\sin\theta} \\ 0\end{pmatrix}
\end{align*}
We can readily compare $\{ \vec{e}^{\,r}, \vec{e}^{\,\theta}, \vec{e}^{\,\phi} \}$ to $\{ \vec{e}_{r}, \vec{e}_\theta, \vec{e}_\phi \}$ and find that while $\vec{e}^{\,r} = \vec{e}_{r}$, $\vec{e}^{\,\theta} \neq \vec{e}_{\theta}$ and $\vec{e}^{\,\phi} \neq \vec{e}_{\phi}$.
\end{exmp}
\begin{exmp}
Let's consider a non-familiar case of paraboloidal surfaces $(u,v,w)$, a non-orthogonal set, where
\begin{align*}
x &= u+v\\y&=u-v\\z&=2uv+w,
\end{align*}
which gives the inverse relations
\begin{align*}
u&=\frac{1}{2}(x+y)\\
v&=\frac{1}{2}(x-y)\\
w&=z-\frac{1}{2}(x^2-y^2).
\end{align*}
Let $\vec{r} = (x,y,z)^\top = (u+v, u-v, 2uv + w)^\top$. We first obtain the natural basis set:
\begin{align*}
\vec{e}_u &= \frac{\partial \vec{r}}{\partial u} = (1,1,2v)^\top \\
\vec{e}_v &= \frac{\partial \vec{r}}{\partial v} = (1,1,2u)^\top \\
\vec{e}_w &= \frac{\partial \vec{r}}{\partial w} = (0,0,1)^\top, 
\end{align*}
which is an mutually perpendicular set, by inspection (i.e., all cross-dot product terms are zero). Next, we obtain the dual basis set:
\begin{align*}
\vec{e}^{\,u} &= \vec{\nabla}u = \frac{1}{2}\vec{\nabla}(x+y) = \left( \frac{1}{2}, \frac{1}{2},0\right) ^\top\\
\vec{e}^{\,v} &= \vec{\nabla}v = \frac{1}{2}\vec{\nabla}(x-y) = \left( \frac{1}{2}, -\frac{1}{2},0\right) ^\top\\
\vec{e}^{\,w} &= \vec{\nabla}w = \vec{\nabla}\left( z-\frac{1}{2}(x^2-y^2)\right)  = (-x,y,1)^\top = (-u-v,u-v,1)^\top\\
\end{align*}
But notice that we get non-zero cross-dot product terms:
\begin{align*}
\vec{e}^{\,u}\cdot\vec{e}^{\,w} &= -v \\
\vec{e}^{\,u}\cdot\vec{e}^{\,v} &= 0 \\
\vec{e}^{\,v}\cdot\vec{e}^{\,w} &= -u. 
\end{align*}
The reason for observing what value the dot products between basis vectors are will become clear when we talk about the \textbf{metric tensor} in the next subsections.
\end{exmp}
\subsection{Contravariant and covariant vectors}
\subsubsection{The suffix notation (vector calculus)}
In general coordinate systems, there can be many more than three coordinates, and it can be burdensome to keep track of different letters representing these coordinates, especially if they don't follow a convention. It is convenient to change our notation from ``letter-based'' to ``index-based.'' For example, instead of using $(u,v,w)$ as coordinates, we will now use $(u^1,u^2,u^3)$, for $i=1,2,3$. Note that we are using upper indices here. As we will discuss soon, upper and lower indices have different meanings. \\

Similarly, our notation for basis vectors also change. For the natural basis, we ``code'' $\{ \vec{e}_u, \vec{e}_v, \vec{e}_w \}$ as $\{\vec{e}_i \}$, where $i=1,2,3$. For the dual basis, we ``code'' $\{ \vec{e}^{\,u},\vec{e}^{\,v},\vec{e}^{\,w} \}$ as $\{ \vec{e}^{\,i} \}$ for $i=1,2,3$. Our vector notation also changes as a result, but there are additional rules. Let vector $\vec{\lambda}$ be given. In the \textbf{natural basis}, the components of $\lambda$ is written with \textbf{upper} indices,
\begin{align*}
\vec{\lambda} = \lambda^1\vec{e}_1 + \lambda^2\vec{e}_2 + \lambda^3\vec{e}_3 = \sum_{i=1}^{3}\lambda^1\vec{e}_i.
\end{align*} 
and in the \textbf{dual basis}, we use \textbf{lower} indices for the components:
\begin{align*}
\vec{\lambda} = \lambda_1\vec{e}^{\,1} + \lambda_2\vec{e}^{\,2} + \lambda_3\vec{e}^{\,3} = \sum_{i=1}^{3}\lambda_i\vec{e}^{\,i}
\end{align*}

\subsubsection{The Einstein summation notation}
We shall make another ``leap in notation'' as we introduce the Einstein summation convention:
\begin{align*}
\boxed{\text{Any index that appears once up and once down is automatically summed}}
\end{align*}
For example, from now on, we will write $\vec{\lambda}$ as:
\begin{align*}
{\vec{\lambda}} = \sum_{i=1}^{3}\lambda^i\vec{e}_i = {\lambda^i\vec{e}_i}
\end{align*}
Note that this summation convention removes us from indicating the minimum and maximum values for our indices, which allows us to keep the mathematics as applicable to any n-dimensional system as possible. We should also notice that indices can be exchangeable, especially in summed situations. For example, 
\begin{align*}
\boxed{a^ib_i = a^jb_j = \dots = \sum_{n=1}^{N}a^nb_n}
\end{align*}
Keep in mind that $a_ib_i$ does not make sense in the Einstein convention. If we want to denote a sum, we have to specify the range of $i$ as follows:
\begin{align*}
\sum_{i=n}^{N}a_ib_i.
\end{align*} 
Likewise, $a_ib^ic^i$ is not defined either. Remember that only ``1 up index, 1 down index'' is allowed. Finally, it is widely agreed upon that certain letters are reserved for some specific cases. For instance, $i,j,k,l,\dots = 1,2,3$ are reserved for 3-dimensional space. We use Greek characters $\mu, \nu, \alpha, \beta,\dots = 0,1,2,3$ for 4-dimensional spacetime, $A,B,C,\dots = 1,2$ for 2-dimensional spaces, and $a,b,c,\dots = 1,2,\dots,N$ for N-dimensional manifolds. 

\subsubsection{Contravariant and covariant vectors}
Earlier, we have established that any vector $\vec{\lambda}$ can be written in two ``upper-lower index'' ways, depending on whether we are using the natural or the dual basis set:
\begin{align*}
\vec{\lambda} = \lambda^i\vec{e}_i = \lambda_i\vec{e}^{\,i}.
\end{align*}
We shall now call $\lambda^i$ the \textbf{contravariant} components and $\lambda_i$ the \textbf{covariant} components. (Remember: ``co'' is ``low''). Contravariant vectors are simply vectors with contravariant components. Likewise, covariant vectors are vectors with covariant components. Later in this text, we will denote $\vec{\lambda}$ simply as $\lambda^i$ (contravariant vector) or $\lambda_i$ (covariant vector).\\

Putting together what we have established so far about index notation and Einstein summation convention, we can now look at the last interesting dot product: between a vector in the natural basis and one in the dual basis: $\vec{e}^{\,i}\cdot\vec{e}_j$. Before we proceed, keep in mind that this single expression ($\vec{e}^{\,i}\cdot\vec{e}_j$) represents 9 different objects because there is not summation here ($i$ is not necessarily $j$ and $i,j=1,2,3$). Now, recall the definitions of these vectors:
\begin{align*}
\vec{e}^{\,i} &= \vec{\nabla}u^i = \frac{\partial u^i}{\partial x}\hat{i} + \frac{\partial u^i}{\partial y}\hat{j} + \frac{\partial u^i}{\partial z}\hat{k}\\
\vec{e}_j &= \frac{\partial \vec{r}}{\partial u^j} = \frac{\partial x}{\partial u^j}\hat{i} + \frac{\partial y}{\partial u^j}\hat{j} + \frac{\partial z}{\partial u^j}\hat{k}
\end{align*}
Therefore,
\begin{align*}
\vec{e}^{\,i}\cdot\vec{e}_j = \frac{\partial u^i}{\partial x}\frac{\partial x}{\partial u^j} + \frac{\partial u^i}{\partial y}\frac{\partial y}{\partial u^j} + \frac{\partial u^i}{\partial z}\frac{\partial z}{\partial u^j}
\end{align*}
But notice that this is nothing but the chain rule, if we think about $u^i$ as $u^i(x,y,z)$. So,
\begin{align*}
\boxed{\vec{e}^{\,i}\cdot\vec{e}_j = \frac{\partial u^i}{\partial u^j}}
\end{align*}
But also notice that the $u^i$'s are independent variables, i.e.,
\[ \frac{\partial u^i}{\partial u^j} = 
\begin{cases*}
0 & \text{if $i \neq j$} \\
1 & \text{if $i = j$}
\end{cases*}\]
We can introduce a two-index object, the \textbf{Kronecker delta}, defined as
\[ \boxed{\delta^i_j = 
\begin{cases*}
	0 & \text{if $i \neq j$} \\
	1 & \text{if $i = j$}
\end{cases*}}\]
We obtain an elegant relation:
\begin{align*}
\boxed{\vec{e}^{\,i}\cdot\vec{e}_j = \delta^i_j}
\end{align*}
Again, keep in mind that this is not a single equation but rather 9. Six of which are $0=0$ and three $1=1$. We observe that for $u\neq v$, $\vec{e}^{\,u}\cdots\vec{e}_v = 0$, i.e., $\vec{e}^{\,u} \perp \vec{e}_v$, which makes sense, by definition: 
\begin{align*}
\text{insert figure here!!!}
\end{align*}
As a little aside, every time we see an two-index object, we should immediately think of a matrix. In matrix form, the Kronecker delta is our beloved identity matrix $I$:
\begin{align*}
[\delta^i_j] = 
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}
\end{align*}
What about the dot (or inner) products of $\{\vec{e}^{\,i}\}$ and $\{\vec{e}_j \}$ among themselves? We have calculated the terms before, when we introduced spherical coordinates, but we have not thought about their significance. Let us define another two-index object, called the \textbf{metric tensor} by
\begin{align*}
\Aboxed{g_{ij} = \vec{e}_i\cdot\vec{e}_j}
\end{align*}
which in matrix form is:
\begin{align*}
[g_{ij}] = 
\begin{pmatrix}
\vec{e}_1\cdot\vec{e}_1 & \vec{e}_1\cdot\vec{e}_2 & \vec{e}_1\cdot\vec{e}_3\\
\vec{e}_2\cdot\vec{e}_1 & \vec{e}_2\cdot\vec{e}_2 & \vec{e}_2\cdot\vec{e}_3\\
\vec{e}_3\cdot\vec{e}_1 & \vec{e}_3\cdot\vec{e}_2 & \vec{e}_3\cdot\vec{e}_3
\end{pmatrix}.
\end{align*}
And likewise, we define the \textbf{inverse metric tensor} as
\begin{align*}
\Aboxed{g^{ij} &= \vec{e}^{\,i}\cdot\vec{e}^{\,j}}
\end{align*}
which in matrix form is:
\begin{align*}
[g^{ij}] = 
\begin{pmatrix}
\vec{e}^{\,1}\cdot\vec{e}^{\,1} & \vec{e}^{\,1}\cdot\vec{e}^{\,2} & \vec{e}^{\,1}\cdot\vec{e}^{\,3} \\
\vec{e}^{\,2}\cdot\vec{e}^{\,1} & \vec{e}^{\,2}\cdot\vec{e}^{\,2} & \vec{e}^{\,2}\cdot\vec{e}^{\,3} \\
\vec{e}^{\,3}\cdot\vec{e}^{\,1} & \vec{e}^{\,3}\cdot\vec{e}^{\,2} & \vec{e}^{\,3}\cdot\vec{e}^{\,3} 
\end{pmatrix}.
\end{align*}
Since dot products are commutative, i.e, $\vec{e}^{\,i}\cdot\vec{e}^{\,j} = \vec{e}^{\,j}\cdot\vec{e}^{\,i}$ and $\vec{e}_i\cdot\vec{e}_j = \vec{e}_j\cdot\vec{e}_i$, we get an interesting property:
\begin{align*}
\boxed{g_{ij} = g_{ji} \text{ and }  g^{ij} = g^{ji}}
\end{align*}
In other words, the matrices $[g_{ij}]$ and $[g^{ij}]$ are \textbf{symmetric}. Just for sanity check, we can look at $[g_{ij}]$ for Cartesian coordinates, which (not surprisingly), turns out to be the identity matrix, i.e., $[g_{ij}] = [g^{ij}] = [\delta^i_j] = I$, which is symmetric. However, $[g_{ij}]$ are not \textbf{diagonal}, in general. A counter-example is $[g_{ij}]$ in paraboloidal coordinates. \\

We are of course not limited to looking only at dot products of the basis vectors among themselves. Consider the dot product of two vectors $\vec{\mu} = \mu^i\vec{e}_i=\mu_i\vec{e}^{\,i}$ and $\vec{\lambda} = \lambda^i\vec{e}_i = \lambda_i\vec{e}^{\,i}$. We immediately see that there are more than one way to express $\vec{\mu}\cdot\vec{\lambda}$, all of which are equivalent. Now, we have to be careful with writing the dot product with indices, because $\lambda^i\vec{e}^{\,i}\mu^i\cdot\vec{e}^{\,i}$ is not defined in the Einstein summation convention. We have to realize that the indices of $\vec{\mu}$ and $\vec{\lambda}$ should be independent, so it suffices to replace $i$ with $j$ as indices for one of the vectors. We are now safe to proceed. 
\begin{align*}
\boxed{
\vec{\lambda}\cdot\vec{\mu} = \lambda^i\vec{e}_i\cdot\mu^j\vec{e}_j = \lambda_i\vec{e}^{\,i}\cdot\mu_j\vec{e}^{\,j} = \lambda_i\vec{e}^{\,i}\cdot\mu^j\vec{e}_j = \lambda^i\vec{e}_{i}\cdot\mu_j\vec{e}^{\,j}
}
\end{align*}
We have shown that $\vec{e}^{\,i}\cdot\vec{e}_j \delta^i_j$, $\vec{e}^{\,i}\cdot\vec{e}^{\,j} = g^{ij}$, and $\vec{e}_i\cdot\vec{e}_j = g_{ij}$, we obtain a number of relations:
\begin{align*}
\vec{\lambda}\cdot\vec{\mu} &= \lambda^i\vec{e}_i\cdot\mu^j\vec{e}_j \text{ }= g_{ij}\lambda^i\mu^j \nonumber\\
 &= \lambda_i\vec{e}^{\,i}\cdot\mu_j\vec{e}^{\,j} = g^{ij}\lambda_i\mu_j \nonumber\\
 &= \lambda_i\vec{e}^{\,i}\cdot\mu^j\vec{e}_j = \delta^i_j\lambda_i \mu^j = \lambda_i\mu^i = \lambda_j\mu^j \nonumber \\
 &= \lambda^i\vec{e}_{i}\cdot\mu_j\vec{e}^{\,j} = \delta^j_i\lambda^i \mu_j = \lambda^i\mu_i = \lambda^j\mu_j \nonumber
\end{align*}
We can summarize the statements above into five equivalent statements:
\begin{align*}
\boxed{\vec{\lambda}\cdot\vec{\mu} = g_{ij}\lambda^i\mu^j = g^{ij}\lambda_i\mu_j = \lambda_i\mu^i = \lambda^i\mu_i}
\end{align*}
From the second and fourth expressions, we get
\begin{align*}
\boxed{g_{ij}\mu^j = \mu_i}
\end{align*}
Likewise, if we look at the third and fifth expressions:
\begin{align*}
\boxed{g^{ij}\lambda_i = \lambda^j}
\end{align*}
This means we can use the metric tensor $g_{ij}$ and the inverse metric tensor $g^{ij}$ to go back and forth between covariant and covariant vector components. Or, in a more ``bookkeeping'' sense, $g^{ij}$ raises an index, while $g_{ij}$ lowers an index. These operations are useful to remember, since they will become especially handy when we have objects with more than one upper and lower indices. \\

But we can extract even more information from the above two equalities by combining them:
\begin{align*}
\mu^i = g^{ij}\mu_j = g^{ij}\left(g_{jk}\mu^k\right). 
\end{align*}
Note that we need an extra index $k$ here since the indices of the expansion of $\mu_j$ are independent of the indices of $g^{ij}$, only except for $j$. Now, since it is also true that $\mu^i = \delta^i_j\mu^j$, the following has to be true as well:
\begin{align*}
\boxed{g^{ij}g_{jk} = \delta^i_k}
\end{align*}
We obtain a similar equality if we start with the covariant component $\mu_i$:
\begin{align*}
\boxed{g_{ij}g^{jk} = \delta^k_i}
\end{align*}
The above two equalities imply that the matrices $[g^{ij}]$ and $[g_{ij}]$ are \textbf{inverses} of each other. Hence, we call $g_{ij}$ the \textbf{metric tensor} and $g^{ij}$ the \textbf{inverse metric tensor}. 
\begin{exmp}
Consider $\vec{a} = \vec{e}_\theta$ in spherical coordinates. Express $\vec{a}$ with contravariant components, covariant components, and find the norm of $\vec{a}$, with and without the metric $g_{ij}$.\\

Since $\vec{a} = \vec{e}_\theta$, a single natural basis vector, we can easily write $\vec{a}$ as a contravariant vector as 
\begin{align*}
a^i = (a^1, a^2, a^3)^\top = (0,1,0)^\top. 
\end{align*}
We can calculate the covariant components of $\vec{a}$ using the metric tensor $g_{ij}$:
\begin{align*}
a_1 &= g_{1j}a^j = g_{11}a^1 = 0\\
a_2 &= g_{2j}a^j = g_{22}a^2 = r^2\\
a_3 &= g_{3j}a^j = g_{33}a^3 = 0.
\end{align*}
So, $\vec{a}$, with covariant components, is
\begin{align*}
a_i = (0, r^2, 0)^\top
\end{align*}
Next, we can calculate $\vert\vert \vec{a} \vert\vert$ in two ways, and both will give the same answer:
\begin{align*}
\vert\vert \vec{a} \vert\vert^2 &= a^ia_i = a^1a_1 + a^2a_2+a^3a_3 = r^2\\
\vert\vert \vec{a} \vert\vert^2 &= g_{ij}a^ia^j = g_{11}a^1a^1 +g_{22}a^2a^2 + g_{33}a^3a^3 = r^2.
\end{align*}
So, $\vert\vert \vec{a} \vert\vert  = r$.

\end{exmp}
\subsection{Metric tensor}
In the previous subsection we have defined the metric tensor and the inverse metric tensor. In this subsection, we will discuss their physical significance and related properties.\\

To start, the metric tensor contains information about physical lengths and the geometry of the space. To illustrate this point, let us consider a path $\vec{r} = \vec{r}(t)$ in flat 3-dimensional space (Euclidean). For vector calculus, we know that the length of the underlying curve, denoted as $L$, is given by
\begin{align*}
L = \int_{a}^{b}\vert\vert \dot{\vec{r}}\vert\vert\,dt.
\end{align*}
Let this curve be expressed in a general curvilinear coordinate system with coordinates $(u,v,w)$, i.e., $\vec{r}(t) = (u(t),v(t),w(t))$, it follows that
\begin{align*}
\dot{\vec{r}} &= \frac{\partial \vec{r}}{\partial u}\frac{d u}{d t} + \frac{\partial \vec{r}}{\partial v}\frac{d v}{d t} + \frac{\partial \vec{r}}{\partial w}\frac{d w}{d t}\\
&= \vec{e}_u\frac{d u}{d t} + \vec{e}_v\frac{d v}{d t} + \vec{e}_w\frac{d w}{d t}\\
&= \vec{e}_i\frac{d u^i}{d t},
\end{align*} 
in Einstein's notation. So, 
\begin{align*}
\left| \left|  \dot{\vec{r}\,}\right| \right|  = \sqrt{\dot{\vec{r}}\cdot\dot{\vec{r}}} = \sqrt{\vec{e}_i\frac{d u^i}{d t}\cdot \vec{e}_j\frac{d u^j}{d t}} = \sqrt{g_{ij}\frac{du^i}{dt}\frac{du^j}{dt}}
\end{align*} 
Putting everything (so far) together, we get
\begin{align*}
\boxed{L = \int_{a}^{b}\left| \left|  \frac{d\vec{r}}{dt} \right| \right| \,dt = \int_{a}^{b}\sqrt{g_{ij}\frac{du^i}{dt}\frac{du^j}{dt}}\,dt}
\end{align*}
Let $ds$ the infinitesimal distance, we get
\begin{align*}
ds = \sqrt{g_{ij}\frac{du^i}{dt}\frac{du^j}{dt}}\,dt.
\end{align*}
Bringing the $dt$ into the square root, and squaring both sides, we get
\begin{align*}
\boxed{ds^2 = g_{ij}\,du^i\,du^j}
\end{align*}
Even though $ds^2$ has units of length squared, we still shall refer to this quantity as the \textbf{line element}. Let us look at a few examples to see how the above equality manifests in geometries we are familiar with.
\begin{exmp}
In Cartesian coordinates, the natural basis vectors are $\{\vec{e}_i\} \{ \hat{i}, \hat{j}, \hat{k}\}$. Observe that $g_{ij} = \vec{e}_i \cdot \vec{e}_j = \delta_{ij}$, since the $\vec{e}_i$'s are both unit vectors and mutually perpendicular. So, as a matrix:
\begin{align*}
[g_{ij}] = 
\begin{pmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{pmatrix}.
\end{align*}
The line element, $ds^2 = g_{ij}\,du^i\,du^j$, is a sum of nine terms. However, we notice that $g_{ij} = 0 $ if $i\neq j$ and $1$ otherwise. Therefore the line element is:
\begin{align*}
ds^2 = g_{ij}\,du^i\,du^j = dx^2+dy^2+dz^2,
\end{align*}
which we are very familiar with. Indeed, the moral of this example is to show that while the form of the line element can be derived geometrically, a rigorous way to obtain the line element is actually through the metric. For Cartesian coordinates, deriving the line element is more or less trivial, because we realize that the above equality is simply Pythagorean theorem in the three dimensions. However, as we will see in the next two examples, it is more difficult to obtain the line element by ``inspection,'' and that realizing the line element from the metric is more reliable.   
\end{exmp}
\begin{exmp}
We have shown that, in spherical coordinates:
\begin{align*}
\vec{e}_r\cdot\vec{e}_r &= 1\\
\vec{e}_\theta\cdot\vec{e}_\theta &= r^2\\
\vec{e}_\phi\cdot\vec{e}_\phi &= r^2\sin^2\theta\\
\vec{e}_r\cdot\vec{e}_\theta &= \vec{e}_r\cdot\vec{e}_\phi = \vec{e}_\theta\cdot\vec{e}_\phi = 0, 
\end{align*}
which give the metric, in matrix form, as:
\begin{align*}
[g_{ij}] = 
\begin{pmatrix}
1 & 0 & 0\\
0 & r^2 & 0 \\
0 & 0 & r^2\sin^2\theta
\end{pmatrix}.
\end{align*}
Therefore, the line element in spherical coordinates is:
\begin{align*}
ds^2 = g_{ij}\,du^i\,du^j = dr^2 + r^2\,d\theta^2 + r^2\sin^2\theta\,d\phi^2.
\end{align*}
While one can certainly derive this expression geometrically, it is no doubt more challenging to do than in Cartesian coordinates.
\end{exmp}
\begin{exmp}
Find the length of a curve in spherical coordinates with the parameterization $\vec{r}(t) = (r(t), \theta(t),\phi(t)) = (t,\pi/4,4t)$, where $0\leq t \leq \pi$.\\

It is possible to proceed without knowing what the underlying curve of this path is. However, it certainly does not harm to know what length we are trying to find here. First, notice that $\theta$ is fixed, so we are limited to a cone, oriented vertically along the $z$-axis. Now, since $0 \leq t \leq \pi$, and that $\phi = 4t$, we know that the curve makes two revolutions around the $z$-axis. So we can imagine the curve looking something like this:
\begin{align*}
\text{Insert figure here!!!}
\end{align*}
Find the length of a curve is only ``mechanical'' once we already know the line element $ds^2$, which we have derived in the previous example. Plus, we know that $d\theta = 0$ since $\theta$ is constant. So,
\begin{align*}
L &= \int\,ds\\ 
&= \int_{0}^{\pi}\sqrt{\frac{dr^2}{dt^2} + r^2\frac{d\theta^2}{dt^2} + r^2\sin^2\theta\frac{d\phi^2}{dt^2}}\,dt\\
&= \int_{0}^{\pi}\sqrt{1 + 0 + \left( 4t\sin\left(\frac{\pi}{4} \right) \right) ^2}\,dt\\
&= \int_{0}^{\pi}\sqrt{1 + 8t^2}\,dt\\
&\approx 14.55. 
\end{align*}
\end{exmp}
As we have seen before in the earlier sections, the metric tensor does not just encode physical lengths. $g_{ij}$ are also ubiquitous in calculating norms of vectors and inner products (or dot products). As a reminder:
\begin{align*}
&\text{\textbf{Norm:} } \vert\vert \vec{\lambda} \vert\vert^2 = \vec{\lambda}\cdot\vec{\lambda} = g_{ij}\lambda^i\lambda^j\\
&\text{\textbf{Inner product:} } \vec{\lambda}\cdot\vec{\mu} = g_{ij}\lambda^i\mu^j.
\end{align*}
They can also be used to raise/lower indices, as we have seen. 
\subsubsection{Summations as matrix products}
As a little ``aside,'' writing vectors and 2-component tensors (such as $g_{ij}$) as matrices can be more convenient for us. But note that more general tensors (with three or more indices) can no longer be written as matrices.\\

Suppose that we have two square matrices $\tilde{A} = [a_{ij}]$ and $\tilde{B} = [b_{ij}]$. Let matrix $\tilde{C} = \tilde{A}\tilde{B}$. In matrix multiplication,
\begin{align*}
\boxed{
\tilde{C} = 
\begin{pmatrix}
&\vdots&\\
\dots&c_{ij}&\dots\\
&\vdots&
\end{pmatrix}
=
\begin{pmatrix}
\vdots&&\vdots\\
a_{i1} & \dots & a_{in}\\
\vdots&&\vdots
\end{pmatrix}
\begin{pmatrix}
\dots&b_{1j}&\dots\\
&\vdots&\\
\dots&b_{nj}&\dots
\end{pmatrix}
}
\end{align*} 
i.e., the $ij^{th}$ element of $\tilde{C}$ is 
\begin{align*}
\boxed{c_{ij} = \sum_{k=1}^na_{ik}b_{kj}}
\end{align*}
where $k$ on the $a$ term indicates a column, while $k$ on the $b$ term indicates a row. Notice that the summed index ($k$) goes as ``column-row,'' whereas $i-j$ go as row-column. Note that I do not have a restriction on $n$, because the above relations are true in any $n$-dimensional space. \\

Similarly, we can multiply vectors in matrix notations. Let two vectors $\vec{F}, \vec{G} \in \mathbb{R}^n$ be given. Assuming that the metric is the identity, their inner product is given by:
\begin{align*}
\boxed{
\vec{F}\cdot\vec{G} = 
\begin{pmatrix}
f_1\\
\vdots\\
f_n
\end{pmatrix}\cdot
\begin{pmatrix}
g_1\\
\vdots\\
g_n
\end{pmatrix}
=
\vec{F}^\top\vec{G} 
=
\sum_{k=1}^{n}f_kg_k.
}
\end{align*}
With a general metric $g_{ij}$, we want to express $\vec{\lambda}\cdot\vec{\mu} = g_{ij}\lambda^i\mu^j$ as a matrix multiplication. We have to make sure that the matrix multiplication is defined, i.e., the rows of the right-multiplying element is equal to the columns of the left-multiplying element. There are two things we can do: ordering the $ij$ indices, and transposing. Let $\tilde{\lambda}$ denote $\vec{\lambda} = [\lambda^i]$, and $\tilde{\mu}$ denote $\vec{\mu} = [\mu^i]$.
\paragraph{For contravariant vectors:\\}
Let $\tilde{G} = [g_{ij}]$, where $i$ is the row indicator and $j$ is the column indicator. This means that the index of $\vec{\lambda}$ has to be a column indicator, so we transpose $\vec{\lambda}$. The index of $\mu$ is a row indicator, which matches with $j$-the column indicator of $\tilde{G}$, so we are good to proceed. Again, note that $n$ is not necessarily 3.  
\begin{align*}
\boxed{
\vec{\lambda}\cdot\vec{\mu} = 
\lambda^ig_{ij}\mu^j
=
\begin{pmatrix}
\lambda_1 & \dots & \lambda_n
\end{pmatrix}
\begin{pmatrix}
g_{11} & \dots & g_{1n}\\
\vdots & \ddots & \vdots \\
g_{n1} & \dots & g_{nn}
\end{pmatrix}
\begin{pmatrix}
\mu_1 \\
\vdots\\
\mu_n
\end{pmatrix}
=
\tilde{\lambda}^\top \tilde{G} \tilde{\mu}}
\end{align*}
\paragraph{For covariant vectors:\\}
We can find express the inner product of covariant vectors in matrix multiplication, using the form we have found for contravariant products. Consider two covariant vectors $\tilde{\lambda}^* = [\lambda_i]$ and $\tilde{\mu}^* = [\mu_i]$ and the inverse metric tensor $\tilde{G}^{-1} = [g_{ij}]^{-1} = [g^{ij}]$ (recall that $g_{ij}g^{jk} = \delta^k_i$). Similar to the contravariant vector case, but now with $\vec{\lambda}\cdot\vec{\mu} = g^{ij}\lambda_i\mu_j$:
\begin{align*}
\boxed{
\tilde{\lambda}^*\cdot\tilde{\mu}^* = 
\begin{pmatrix}
\lambda_1 & \dots & \lambda_n
\end{pmatrix}
=
\begin{pmatrix}
g^{11} & \dots & g^{1n}\\
\vdots & \ddots & \vdots \\
g^{n1} & \dots & g^{nn}
\end{pmatrix}
\begin{pmatrix}
\mu_1 \\
\vdots\\
\mu_n
\end{pmatrix}
=
\tilde{\lambda}^{*\top}\tilde{G}^{-1}\tilde{\mu}
}
\end{align*}
As an little aside, to write $\tilde{\lambda}^*$ in terms of the metric tensor matrix and the contravariant vector, we simply remember that $\lambda_i = g_{ij}\lambda^j$. So,
\begin{align*}
&\tilde{\lambda}^* = \tilde{G}\tilde{\lambda}\\
&\tilde{\lambda} = \tilde{G}^{-1}\tilde{\lambda}^*
\end{align*}
In matrix form, the above two equalities can be written as:
\begin{align*}
\begin{pmatrix}
\lambda_1 \\
\vdots\\
\lambda_n
\end{pmatrix}
&=
\begin{pmatrix}
g_{11} & \dots & g_{1n}\\
\vdots & \ddots & \vdots\\
g_{n1} & \dots & g_{nn}
\end{pmatrix}
\begin{pmatrix}
\lambda^1\\
\vdots\\
\lambda_n
\end{pmatrix}
\end{align*}
and
\begin{align*}
\begin{pmatrix}
\lambda^1 \\
\vdots\\
\lambda^n
\end{pmatrix}
&=
\begin{pmatrix}
g^{11} & \dots & g^{1n}\\
\vdots & \ddots & \vdots\\
g^{n1} & \dots & g^{nn}
\end{pmatrix}
\begin{pmatrix}
\lambda_1\\
\vdots\\
\lambda_n
\end{pmatrix}
\end{align*}
respectively.\\
\paragraph{Metric tensor and inverse metric tensor\\}
As mentioned, $[g^{ij}] = [g_{ij}]^{-1}$. In matrix form:
\begin{align*}
\boxed{
\tilde{G}^{-1}\tilde{G} = 
\begin{pmatrix}
g^{11} & \dots & g^{1n}\\
\vdots & \ddots & \vdots \\
g^{n1} & \dots & g^{nn}
\end{pmatrix}
\begin{pmatrix}
g_{11} & \dots & g_{1n}\\
\vdots & \ddots & \vdots\\
g_{n1} & \dots & g_{nn}
\end{pmatrix}
=
\text{diag}(1,\dots,1) 
=
\tilde{I}
}
\end{align*}
\begin{exmp}
Find the inverse metric tensor $g^{ij}$ in spherical coordinates, given the metric tensor $g_{ij}$ is:
\begin{align*}
[g_{ij}] = 
\begin{pmatrix}
1 & 0 & 0\\
0 & r^2 & 0\\ 
0 & 0 & r^2\sin^2\theta 
\end{pmatrix}
\end{align*}
By definition, $[g^{ij}] = [g_{ij}]^{-1}$. So taking the inverse of $[g_{ij}]$ will give us $[g^{ij}]$. But notice that $[g_{ij}]$ is diagonal, so $g^{ij}$ is simply the reciprocal of $g_{ij}$, if $g_{ij} \neq 0$, and $0$ otherwise. This gives
\begin{align*}
[g^{ij}] = 
\begin{pmatrix}
1 & 0 & 0 \\
0 & r^{-2} & 0\\
0 & 0 & r^{-2}\sin^{-2}\theta
\end{pmatrix}
\end{align*}
\end{exmp}
\subsection{Coordinate transformation}
We have already seen the usage of multiple (more or less conventional) coordinate systems in the three-dimensional space. In general $n$-dimensional spaces, the options for coordinate systems are almost unlimited. It is crucial for us, as relativists, to develop for rigorous procedure to transform vector and tensor components as we move from one coordinate system to another. Note that vector and tensor \textbf{components} change under general coordinate transformation. It is common in physics to associate a vector or a tensor with a physical quantity, and coordinate system, or reference frame, etc., with perspectives. It makes sense that what we see might differ at different perspectives, but the physical quantity itself is independent of how we view it. In other words, vectors and tensors do not change under general coordinate transformations, only their components do.\\

In this subsection, we will learn how to transform between arbitrary coordinate systems. We will also learn how vector and tensor components transform, as well as what vectors and tensors are.
\subsubsection{Vectors}
We as physicists often think of vectors as an object with magnitude and direction. But this definition is incomplete, as it disregards a reference frame in which the vector is viewed. As we have mentioned in the introduction to this section, under general coordinate transformations, vectors do not change, but their components do, since the basis set of the coordinate system changes. For instance,
\begin{align*}
\text{insert figure here!!!}
\end{align*}
\begin{align*}
\text{insert figure here!!!}
\end{align*}
In the above two figures, the vector $\vec{\lambda}$ is the same. However, one frame is rotated by $\theta$ from another. In frames $(S)$ and $(S')$, $\vec{\lambda}$ will have different components, but the ``physical object'' $\vec{\lambda}$ does not change. For instance, we can calculate the norm of $\vec{\lambda}$, $\vert\vert \vec{\lambda} \vert\vert$, and find that it is the same in both frames. In fact, \textbf{scalars are invariant under general coordinate transformations}.\\

Next, let's talk \textbf{notations}. Let's assume, without loss of generality, that we are only dealing with contravariant vectors here (the rules we will establish shortly will apply to covariant vectors as well). Consider $\vec{\lambda}$ in frame $(S)$. In index-notation, we can write $\vec{\lambda} = \{\lambda^i\}$. Consider a different frame (a ``primed'' frame), denoted $(S')$. We will denote the components of $\vec{\lambda}$ in $(S')$ as $\lambda^{i'}$. Just to get us familiar with the new notation, let's go through an example:
\begin{exmp}
Suppose that $\vec{\lambda}$ is described two coordinate systems: spherical and cylindrical. Let the ``unprimed'' coordinate system be the spherical coordinate system. Then,
\begin{align*}
\{u^i\} &= (r,\theta,\phi)\\
\{u^{i'}\} &= (\rho, \phi, z).
\end{align*}
As simple as that!
\end{exmp}

Back to $(S)$ and $(S')$. Since these are two different coordinate systems, the basis sets and metric tensors are also different:
\begin{align*}
(S): &\text{ }\vec{e}_i = \frac{\partial \vec{r}}{\partial u^i}, \text{ }\vec{e}^{\,i} = \vec{\nabla}u^i, \text{ }g_{ij} = \vec{e}_i\cdot\vec{e}_j\\
(S'): &\text{ }\vec{e}_{i'} = \frac{\partial \vec{r}}{u^{i'}}, \text{ }\vec{e}^{\,i'} = \vec{\nabla} u^{i'}, \text{ }g_{i'j'} = \vec{e}_{i'}\cdot\vec{e}_{j'}.
\end{align*}
Now, since $\vec{\lambda}$ is the same in both $(S)$ and $(S')$, it must be true that"
\begin{align*}
\vec{\lambda} = \lambda^i\vec{e}_i = \lambda^{i'}\vec{e}_{i'},
\end{align*}
i.e., $\lambda^i$ and $\vec{e}_i$ must transform in a way that leaves $\vec{\lambda}$ unchanged. Consider $\vec{r} = \vec{r}(u^{i'}) = \vec{r}(u^{i'}(u^j))$. By the chain rule:
\begin{align*}
\vec{e}_j = \frac{\partial \vec{r}}{\partial u^j} = \frac{\partial \vec{r}}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial u^j} = \vec{e}^{\,i'}\frac{\partial u^{i'}}{\partial u^j}.
\end{align*}
Let us define, for simplicity sake,
\begin{align*}
\boxed{U^{i'}_j = \frac{\partial u^{i'}}{\partial u^j}}
\end{align*}
Note that there is nothing new here. $U^{i'}_j$ simply generalizes 9 partial derivatives (because we are in 3-dimensional space). Together as a matrix, $[U^{i'}_j]$ is nothing but the Jacobian matrix ``representing'' the transformation $(S) \rightarrow (S')$:
\begin{align*}
[U^{i'}_j] = 
\begin{pmatrix}
\frac{\partial u^{1'}}{\partial u^1} & \frac{\partial u^{1'}}{\partial u^2} & \frac{\partial u^{1'}}{\partial u^3}\\
\frac{\partial u^{2'}}{\partial u^1} & \frac{\partial u^{2'}}{\partial u^2} & \frac{\partial u^{2'}}{\partial u^3}\\
\frac{\partial u^{3'}}{\partial u^1} & \frac{\partial u^{3'}}{\partial u^2} & \frac{\partial u^{3'}}{\partial u^3}
\end{pmatrix}
\end{align*}
So, the chain rule for $\vec{e}_j$ becomes the general coordinate transformation rule for the natural basis sets $\vec{e}_{i'} \rightarrow \vec{e}_i$. 
\begin{align*}
\boxed{\vec{e}_j = U^{i'}_j\vec{e}_{i'}}
\end{align*}
Next, since $\vec{\lambda} = \lambda^{i'}\vec{e}_{i'} = \lambda^j\vec{e}_j = \lambda^jU^{i'}_j\vec{e}_{i'}$, we obtain the general coordinate transformation rule for contravariant vector components $\lambda^{i} \rightarrow \lambda^{i'}$: 
\begin{align*}
\boxed{\lambda^{i'} = U^{i'}_j\lambda^j}
\end{align*}
We can, of course, conjecture that the transformation rule for contravariant vector components $\lambda^{i'} \rightarrow \lambda^{i}$ is:
\begin{align*}
\boxed{\lambda^{i'} = U^{j}_{i'}\lambda^{i'}}
\end{align*}
and we would be correct. The ``proof'' follows here. First, we look at the Jacobian matrix representing the transformation $(S') \rightarrow (S)$:
\begin{align*}
[ U^{j}_{i'}]  = \left[ \frac{\partial u^j}{\partial u^{i'}}\right], 
\end{align*}
which is simply the inverse of $\left[ U^{i'}_j\right] $. So, it follows that:
\begin{align*}
[ U^{j'}_k] [ U^{k}_{i'}]  &= \text{diag}(1,\dots,1) = [\delta^{j'}_{i'}] = [ \delta^{j}_{i}] = \tilde{I},
\end{align*}
and 
\begin{align*}
[U^k_{i'}][U^{i'}_j] &= \text{diag}(1,\dots,1) = [\delta^k_{j}] = \tilde{I},
\end{align*}
because matrices that are inverses of each other are commutative. Element-wise:
\begin{align*}
\boxed{U^k_{i'} U^{i'}_j = \delta^k_j = \delta^{k'}_{j'}= U^{k'}_i U^i_{j'}}
\end{align*}
So we can now verify our little ``conjecture:''
\begin{align*}
\lambda^{i'} &= U^{i'}_j\lambda^j\\
U^k_{i'}\lambda^{i'} &= U^{i'}_jU^k_{i'}\lambda^j\\
U^k_{i'}\lambda^{i'} &= \delta^k_j\lambda^j\\
U^k_{i'}\lambda^{i'} &= \lambda^k.
\end{align*}
We notice that nothing makes $U^{i'}_j$ more special than $U^{i}_{j'}$. Also, while we use contravariant vectors more often than covariant vectors, nothing makes contravariant vectors more special than covariant vectors. So if transformation rules apply to contravariant vectors, we should also expect them to work for covariant vectors:
\begin{empheq}[box=\fbox]{align}
\lambda_{i'} &= U^{j}_{i'}\lambda_j \nonumber\\
\lambda_{i} &= U^{j'}_i\lambda_{j'} \nonumber
\end{empheq}
At this point it looks like we are inventing new mathematics by playing a little game of swapping indices, and it is reasonable for us to grow a little bit suspicious of what we are doing here. But we can show that our ``index-swapping intuition'' is correct. Let us consider (again) a vector $\vec{\lambda} = \lambda_j\vec{e}^{\,j} = \{ \lambda_j\}$, where $\vec{e}^{\,j}$ is a vector in the dual basis:
\begin{align*}
\vec{e}^{\,j} = \vec{\nabla}u^j = \frac{\partial u^j}{\partial x}\hat{i} + \frac{\partial u^j}{\partial y}\hat{j} + \frac{\partial u^j}{\partial z}\hat{k}.
\end{align*}
Let us apply to chain rule:
\begin{align*}
\frac{\partial u^j}{\partial x} = \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial x}.
\end{align*}
Therefore,
\begin{align*}
\vec{e}^{\,j} &= \vec{\nabla}u^j\\
&= \frac{\partial u^j}{\partial x}\hat{i} + \frac{\partial u^j}{\partial y}\hat{j} + \frac{\partial u^j}{\partial z}\hat{k}\\
&= \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial x}\hat{i} + \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial y}\hat{j} + \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial z}\hat{k}\\
&= \sum_{i=1}^{3}\frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial x}\hat{i} + \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial y}\hat{j} + \frac{\partial u^j}{\partial u^{i'}}\frac{\partial u^{i'}}{\partial z}\hat{k}\\
&= \sum_{i=1}^{3}\frac{\partial u^j}{\partial u^{i'}}\left( \frac{\partial u^{i'}}{\partial x}\hat{i} + \frac{\partial u^{i'}}{\partial y}\hat{j} + \frac{\partial u^{i'}}{\partial z}\hat{k}\right) \\
&= \sum_{i=1}^{3}\frac{\partial u^j}{\partial u^{i'}}\vec{\nabla}u^{i'}\\
&= \sum_{i=1}^{3}U^{j}_{i'}\vec{e}^{\,i'}.
\end{align*}
Or, in Einstein's summation notation, the transformation rule for the dual basis vectors is:
\begin{align*}
\boxed{\vec{e}^{\,j} = U^j_{i'}\vec{e}^{\,i'}}
\end{align*}
And the inverse transformation follows:
\begin{align*}
\boxed{\vec{e}^{\,j'} = U^{j}_{i'}\vec{e}^{\,i'}}
\end{align*}
Note that, the above relation is in fact true for any $n$-dimensional space. Our ``proof'' here is restricted to only 3-dimensional space. However, it is not so difficult to fix this slight issue. If we redefine the $\vec{\nabla}$ operator for $n$-dimensional space, then we can easily see that the new proof is identical to the proof above, except for the maximal value of $n$. \\

We easily obtain the transformation rules for the covariant vector components using the results above:
\begin{align*}
\vec{\lambda} = \lambda_{i'}\vec{e}^{\,i'} = \lambda_j\vec{e}^{\,j} = \lambda_jU^j_{i'}\vec{e}^{\,i'} = \lambda_{i'}U^{i'}_j\vec{e}^{\,j}.
\end{align*}
Therefore,
\begin{empheq}[box=\fbox]{align}
\lambda_{i'} &= U^j_{i'}\lambda_j \nonumber\\
\lambda_j &= U^{i'}_j\lambda_{i'}
\end{empheq}
Below is a summary of what we have learned so far about general coordinate transformations:
\[
\boxed{
\begin{matrix}
\text{\textbf{Natural basis:} }& \vec{e}_j = U^{i'}_j\vec{e}_{i'} & \vec{e}_{j'} = U^{i}_{j'}\vec{e}_{i} \\
\text{\textbf{Dual basis:} }& \vec{e}^{\,j} = U^{j}_{i'}\vec{e}^{\,i'} & \vec{e}^{\,j'} = U^{j'}_i\vec{e}^{\,i}\\
\text{\textbf{Contravariants:} } & \vec{\lambda}^{j'} = U^{j'}_i\vec{\lambda}^{i} & \vec{\lambda}^j = U^{j}_{i'}\vec{\lambda}^{i'}\\
\text{\textbf{Covariants:} } & \vec{\lambda}_j = U^{i'}_j\vec{\lambda}_{i'} & \vec{\lambda}_{j'} = U^{i}_{j'}\vec{\lambda}^{j'}\\
\end{matrix}
}
\]
Since components of a vector must transform according to the rules above under general coordinate transformations, we can turn these rules into a more general definition of a vector.
\begin{defn}
	A vector is a quantity whose components transform contravariantly as $\lambda^{i'} = U^{i'}_j\lambda^j$ under a general coordinate transformation $u^{i'} = u^{i'}(u^j)$.
\end{defn}
\begin{rmk}
	We are often interested in vector fields, a collection of vectors at different points in space whose components depend on the coordinates: $\lambda^i = \lambda^i(u^j)$. Since vectors have to transform correctly, we require that at any point $P$, we require that $\lambda^{i'} = U^{i'}_j\lambda^j$ be true for a vector field (defined on $\lambda^i$) to be defined. 
\end{rmk}
\begin{rmk}
	Not all 3-tuple of functions are vectors. Consider a 3-tuple of coordinates. Let $\lambda^i = u^i$ and $\lambda^{j'} = u^{j'}$, where $u^{j'} = u^{j'}(u^i)$. For $\lambda^i$ to be make a vector field, it is required that $\lambda^{i'} = U^{i'}_j\lambda^j$. This implies that $u^{i'} = U^{i'}_ju^j$, with $U^{i'}_j = \partial u^{i'}/\partial u^j$. However, we immediately see that this is \textbf{not true in general}:
	\begin{align*}
	\boxed{u^{i'} \neq \frac{\partial u^{i'}}{\partial u^j}u^j}
	\end{align*}
	But rather, $u^{i'} = u^{i'}(u^j) = f(u^j)$. So, coordinates themselves do not make vectors in general, because they, as components, do not transform correctly under general coordinate transformations. This is why we never use $g_{ij}$ or $g^{ij}$ to lower or raise the index of $u^i$ or $u_i$, i.e., $u^i \neq g^{ij}u_j$, and $u_j \neq g_{ij}u^i$, in general. \\
	
	There are exemptions to the ``general rule,'' of course. For instance, consider \textbf{linear transformations}, where $u^{j'} = u^{j'}(u^i) = C^{j'}_iu^i$, where $C^{j'}_i$ are constants. What this means is that the new coordinates are simply linear combinations of the old coordinates. It follows that
	\begin{align*}
	\frac{\partial u^{j'}}{\partial u^k} = C^{j'}_i\frac{\partial u^i}{\partial u^k} = C^{j'}_i\delta^i_k=C^{j'}_k.
	\end{align*}
	If we let $k=i$ (we're just swapping dummy indices here), then $C^{j'}_i = \partial u^{j'}/\partial u^i = U^{j'}_i$, i.e., $u^{j'} = U^{j'}_iu^i$ under linear transformations. Therefore, $u^i$ transform correctly, and in this case the coordinates $u^i$ can form a vector, $\{u^i\}$.
\end{rmk}
\begin{rmk}
	Properly speaking we can define vectors with respect to a particular class of transformations. It is possible for something to be a vector with respect to a class of transformation, but not a vector with respect to another. Such a class of transformation (default) is the general coordinate transformations. 
\end{rmk}
\begin{exmp}
	Can differentials of coordinates ($du^i$) form a vector?\\
	
	The solution is actually quite simple. Without loss of generality, let us consider a hypothetical vector $\{ du^i \}$ in 3-dimensional space. $\{du^1, du^2, du^3 \}$. Applying the chain rule:
	\begin{align*}
	du^{i'} = \frac{\partial u^{i'}}{\partial u^{j}}\,du^j = U^{i'}_j\,du^j.
	\end{align*}
	We see that $du^i$ transform correctly under general coordinate transformations. Therefore, $du^i$ can make a vector. 
\end{exmp}
\begin{exmp}
	Find $[U^{j'}_i]$ for a coordinate transformation from Cartesian to spherical in flat 3-dimensional space. Let $\{u^j\} = \{ x,y,z\}$ and $\{u^{j'} \} = \{ r, \theta, \phi\} $. We find the Jacobian matrix:
	\begin{align*}
	[U^{j'}_i] = 
	\begin{pmatrix}
	\frac{\partial r}{\partial x} & \frac{\partial r}{\partial y} & \frac{\partial r}{\partial z}\\
	\frac{\partial \theta}{\partial x} & \frac{\partial \theta}{\partial y} & \frac{\partial \theta}{\partial z}\\
	\frac{\partial \phi}{\partial x} & \frac{\partial \phi}{\partial y} & \frac{\partial \phi}{\partial z}
	\end{pmatrix}
	=\frac{1}{r}
	\begin{pmatrix}
	r\sin\theta\cos\phi & r\sin\theta\sin\phi & r\cos\theta\\
	\cos\theta\cos\phi & \cos\theta\sin\phi & -\sin\theta\\
	\frac{-\sin\phi}{\sin\theta} & \frac{\cos\phi}{\sin\theta} & 0
	\end{pmatrix}
	\end{align*}
\end{exmp}
\begin{exmp}
	Consider $\vec{\lambda} = (1,0,0)^\top = \{\lambda^{i'}\}$ in Cartesian coordinates. What are the components of $\vec{\lambda}$ in spherical coordinates? \\
	
	With our developed procedure, we should be able to obtain the answer with little thinking:
	\begin{align*}
	\vec{\lambda} = \{ \lambda^{i'} \} = 
	\begin{pmatrix}
	\lambda^{1'} \\ \lambda^{2'} \\ \lambda^{3'}
	\end{pmatrix}
	=
	[U^{i'}_j\lambda^j]
	= 
	\frac{1}{r}
	\begin{pmatrix}
	r\sin\theta\cos\phi & r\sin\theta\sin\phi & r\cos\theta\\
	\cos\theta\cos\phi & \cos\theta\sin\phi & -\sin\theta\\
	\frac{-\sin\phi}{\sin\theta} & \frac{\cos\phi}{\sin\theta} & 0
	\end{pmatrix}
	\begin{pmatrix}
	1 \\ 0 \\ 0
	\end{pmatrix},
	\end{align*}
	which gives
	\begin{align*}
	\begin{pmatrix}
	\lambda^{1'} \\ \lambda^{2'} \\ \lambda^{3'}
	\end{pmatrix}
	=
	\frac{1}{r}
	\begin{pmatrix}
	r\sin\theta\cos\phi\\
	\cos\theta\cos\phi\\
	\frac{-\sin\phi}{\sin\theta}
	\end{pmatrix}.
	\end{align*}
	In the natural basis of spherical coordinates, 
	\begin{align*}
	\vec{\lambda} = \sin\theta\cos\phi\,\vec{e}_r + \frac{1}{r}\cos\theta\cos\phi\,\vec{e}_\theta - \frac{\sin\phi}{r\sin\theta}\,\vec{e}_\phi.
	\end{align*}
	We have said before that the ``physical quantity'' $\vec{\lambda}$ does not change. We can test this by finding $\vert\vert \vec{\lambda} \vert\vert$ in Cartesian and spherical coordinates and compare the results. In Cartesian, the metric tensor is $g_{ij} = \delta^j_i$, so
	\begin{align*}
	\vert\vert \vec{\lambda} \vert\vert_{\text{Cartesian}} = \sqrt{g_{ij}\lambda^i\lambda^j} = \sqrt{1^2 + 0^2 + 0^2} = 1,
	\end{align*}
	as expected. The metric tensor $g_{i'j'}$ of spherical coordinate, in matrix form, is:
	\begin{align*}
	[g_{i'j'}] = \begin{pmatrix}
	1 & 0 & 0\\
	0 & r^2 & 0\\
	0 & 0 & r^2\sin^2\theta
	\end{pmatrix}.
	\end{align*}
	It follows that
	\begin{align*}
	\vert\vert \vec{\lambda} \vert\vert_{\text{Spherical}} &= \sqrt{g_{i'j'}\lambda^{i'}\lambda^{j'}} = g_{1'1'}\left( \lambda^{1'}\right) ^2 + g_{2'2'}\left( \lambda^{2'}\right) ^2 + g_{3'3'}\left( \lambda^{3'}\right) ^2\\
	&= \sqrt{\left( \sin\theta\cos\phi\right)^2 + r^2\left(\frac{1}{r}\cos\theta\cos\phi \right)^2 + r^2\sin^2\theta\left(-\frac{\sin\phi}{r\sin\theta} \right)^2  }\\
	&= \sqrt{\left( \sin\theta\cos\phi\right)^2 + \left(\cos\theta\cos\phi \right)^2 + \sin^2\phi}\\
	&= \sqrt{\cos^2\phi + \sin^2\phi}\\
	&= 1.
	\end{align*}
	Not surprisingly, $\vert\vert \vec{\lambda}_{\,\text{Cartesian}} \vert\vert = \vert\vert \vec{\lambda}_{\,\text{Spherical}} \vert\vert = 1$.
\end{exmp}
\begin{exmp}
	Find $U^{j'}_i$ for a rotation by an angle $\phi$ around the $z$-axis in Cartesian coordinates. Suppose $\vec{a} = \{a^i \}= (1,1,0)^\top$. What is $\vec{a} = \{ a^{i'}\}$ after a rotation by $\phi$.
	\begin{align*}
	\text{insert figure here!!!}
	\end{align*}
	From linear algebra, we know that a rotation by $\phi$ around the $z$-axis is given by:
	\begin{align*}
	\begin{pmatrix}
	x'\\y'\\z'
	\end{pmatrix}
	=
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}
	\begin{pmatrix}
	x\\y\\z
	\end{pmatrix}.
	\end{align*}
	So,
	\begin{align*}
	[U^{j'}_i] = 
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}
	\end{align*}
	Given that $\vec{a} = \{a^i \}= (1,1,0)^\top$, 
	\begin{align*}
	\begin{pmatrix}
	\lambda^{1'}\\\lambda^{2'}\\\lambda^{3'}
	\end{pmatrix}
	=
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}
	\begin{pmatrix}
	1\\1\\0
	\end{pmatrix}
	=
	\begin{pmatrix}
		\cos\phi - \sin\phi\\
		\sin\phi + \cos\phi\\
		0
	\end{pmatrix}
	\end{align*}
	Notice that because the coefficients of $[U^{j'}_i]$ are constants, the coordinates $\{ x,y,z\}$ can make a vector. But note (again) that this is not true in general.\\
	
	Next, we can find $\vert\vert \vec{\lambda} \vert\vert$ in both coordinate systems again and show that they are equal. In Cartesian coordinates, $\{\lambda^i\} = (1,1,0)^\top$. So, $\vert\vert \vec{\lambda} \vert\vert = \sqrt{2}$. In the rotated frame:
	\begin{align*}
	\vert\vert\vec{\lambda}\vert\vert = \sqrt{g_{i'j'}\lambda^{i'}\lambda^{j'}}.
	\end{align*}
	We seem to be stuck here, since we have yet found the form of the new metric $g_{i'j'}$. However, we observe that $g_{i'j'} = e^{i'}\cdot\vec{e}_{j'}$, which is just either 1 (if $i=j$) or 0 (if $j\neq j$), because the new coordinate system is just the ``tilted'' version of the old one. So we expect that new metric tensor is the same as the old metric tensor, which is nothing but $g_{i'j'}=\text{diag}(1,1,1)$. We can proceed to calculate the norm:
	\begin{align*}
	\vert\vert\vec{\lambda}\vert\vert &= \sqrt{\left( \cos\phi - \sin\phi \right)^2 + \left( \sin\phi + \cos\phi \right)^2 + 0^2}\\
	&= \sqrt{2\left(\sin^2\phi + \cos^2\phi\right) }\\
	&= \sqrt{2},
	\end{align*}
	which is of course what we found earlier. 
\end{exmp}
The above example motivates what we will discuss next. We have seen how vector components transform under general coordinate transformations, and how their norms (scalar) remain invariant. However, we start to question how the metric tensor transforms, in general. In the above example, the only reason we was able to calculate the norm of $\vec{\lambda}$ was that we could guess the form of the metric tensor, using some cleverness. In general, though, we will not be able to solve this problem without knowing exactly the form of the metric. Our next task, as apparent as it is now, is to find out how metric tensors in particular and tensors in general transform under general coordinate transformations. 
\subsubsection{Tensors}
We should first ask ourselves what tensors are. While a rigorous definition of a tensor will come shortly in this section (hint: it will resemble that of a vector, i.e., based on how their components transform under general coordinate transformations), we can first think of tensors as generalizations of vectors, i.e., with more than one index and hence multi-dimensional. Consider a physical object like a balloon which stretches and compresses multi-dimensionally when some force is applied to it. We call this ``response'' the stress tensor, which has 9 components in 3-dimensional space: $F_{xx}, F_{xy}, F_{xz}, F_{yx}, F_{yy}, F_{yz}, F_{zx}, F_{zy}, F_{zz}$.  
\begin{defn}
	A tensor is a multi-component quantity whose components transform as contravariant or covariant vector components.
\end{defn}
\begin{exmp}
	Find the condition for $\tensor{\tau}{^{ij}_k^l}$ to be a tensor.\\
	
	Well, a tensor has to transform correctly, so we can play a little game of index-replacing:
	\begin{align*}
	\boxed{\tensor{\tau}{^{i'j'}_{k'}^{l'}} = U^{i'}_mU^{j'}_nU^p_{k'}U^{l'}_p\tensor{\tau}{^{mn}_{p}^{q}}}
	\end{align*}
	under a general coordinate transformation $u^{i'} \rightarrow u^{i'}(u^j)$. 
\end{exmp}
\begin{exmp}
	Show that $g_{ij}$ is a tensor.\\
	
	$g_{ij}$ is a tensor, in fact, by definition:
	\begin{align*}
	\boxed{g_{i'j'} = U^k_{i'}\vec{e}_k \cdot U^l_{j'}\vec{e}_l = U^k_{i'}U^l_{j'}\vec{e}_k \cdot\vec{e}_l = U^k_{i'}U^l_{j'}g_{kl}}
	\end{align*} 
	Similarly, the inverse $g^{ij}$ is also a tensor:
	\begin{align*}
	\boxed{g^{k'l'} = U^{k'}_{i}\vec{e}^{\,i} \cdot U^{l'}_{j}\vec{e}^{\,j} = U^{k'}_iU^{l'}_{j}\vec{e}^{\,i} \cdot\vec{e}^{\,j} = U^{k'}_{i}U^{l'}_{j}g^{ij}}
	\end{align*}
\end{exmp}
\begin{defn}
	A tensor $\tensor{\tau}{^{abc\dots}_{def\dots}}$ is of type $(r,s)$ when it has $r$ contravariant components and $s$ covariant components. 
\end{defn}
\begin{exmp}
	$g_{ij}$ is a type $(0,2)$ tensor. $g^{ij}$ is a type $(2,0)$ tensor. 
\end{exmp}
Vectors are just a special type of tensors. Contravariant vectors $\lambda^i$ are type $(1,0)$ tensors, while covariant vectors $\lambda_i$ are type $(0,1)$ tensors. 
\begin{rmk}
	The transformation $U^{i'}_j$ is NOT a tensor. Indeed, it does not make sense to talk about how a transformation transforms. 
\end{rmk}
\begin{exmp}
	Write $g_{i'j'} = U^k_{i'}U^l_{j'}g_{kl}$ as a matrix equation.\\
	
	We let $\tilde{G} = [g_{ij}]$, $\tilde{G}'= [g_{i'j'}]$, and $\hat{U} = \tilde{U}^{-1} = [\partial u^k/\partial u^{i'}]$. To create a matrix equation, we have to be aware of when an index indicate a row or column. The correct ``row-column-sensitive'' order expression is:
	\begin{align*}
	[g_{i'j'}] &= [U^{k}_{i'}][g_{kl}][U^l_{j'}],
	\end{align*}
	or equivalently,
	\begin{align*}
	\boxed{\tilde{G}' = \hat{U}^{\top}\tilde{G}\hat{U}}
	\end{align*}
\end{exmp}
\begin{exmp}
	Using the transformation rules for tensors, obtain $g_{i'j'}$ for a rotation by $\phi$ around the $z$-axis in 3-dimensional space. \\
	
	Recall that
	\begin{align*}
	[U^{j'}_i] = 
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix},
	\end{align*}
	and that $g_{ij} = \text{diag}(1,1,1)$ in 3-dimensional Cartesian coordinate system. To find $\tilde{G}'$, we first have to find $U^k_{i'}$, the inverse transformation. We expect that $U^k_{i'}$ is simply a rotating by $-\phi$ (since this intuitively ``reverses'' what $U^{k'}_{i}$ does). But we, as always, find $U^k_{i'}$ mechanically:
	\begin{align*}
	\hat{U} = \left[\frac{\partial u^k}{\partial u^{i'}} \right]  =\tilde{U}^{-1} = 
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}^{-1}
	=
	\begin{pmatrix}
	\cos\phi & \sin\phi & 0\\
	-\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix},
	\end{align*} 
	as anticipated. Using the result of the previous example,
	\begin{align*}
	\tilde{G}' &= \hat{U}^{\top}\tilde{G}\hat{U}\\
	&= \begin{pmatrix}
	\cos\phi & \sin\phi & 0\\
	-\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}
	\begin{pmatrix}
	1 & 0 & 0\\
	0 & 1 & 0\\
	0 & 0 & 1
	\end{pmatrix}
	\begin{pmatrix}
	\cos\phi & -\sin\phi & 0\\
	\sin\phi & \cos\phi & 0 \\
	0 & 0 & 1
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
	1 & 0 & 0\\
	0 & 1 & 0\\
	0 & 0 & 1
	\end{pmatrix},
	\end{align*}
	just as guessed. So our intuition was correct after all. But we realize an important fact: the metric tensor can remain the same after a non-trivial transformation (in this case the transformation is a rotating around the $z$-axis). We also notice that the metric tensors are the same under a transformation if the transformation matrix is \textbf{orthogonal}, i.e., its inverse the equal to its transpose:
	\begin{align*}
	\hat{U} = \tilde{U}^{-1} = \tilde{U}^\top.
	\end{align*}
\end{exmp}

\begin{rmk}
	Only tensors of type $(r,s)$ with $r+s \leq 2$ can be displayed as matrices and/or matrix multiplications. It is not possible to write $\tensor{\tau}{^{ij}_{kl}}$ as a matrix. 
\end{rmk}

\subsection{Scalars}
As we have seen from time to time, scalars are invariant under general coordinate transformations. As tensors, scalars are of type $(0,0)$, i.e., they have open indices.
\begin{exmp}
	Show that the magnitude of a vector is a scalar.\\
	
	Consider $\vec{a} = {a^i} = {a^{i'}}$ in two different coordinate systems. The magnitude of $\vec{a} = \vert\vert \vec{a} \vert\vert$ is a scalar if:
	\begin{align*}
	\vec{a}\cdot\vec{a} = a^ia_i = a^{i'}a_{i'}.
	\end{align*} 
	This can be readily shown, since we already know the transformation rules for vector components:
	\begin{align*}
	a^{i'}a_{i'} &= \left( U^{i'}_j a^j \right) \left( U^{k}_{i'}a_k\right) \\
	&= U^{i'}_j a^j U^{k}_{i'} a_k a^j\\
	&= \delta^{k}_j a_k a^j\\
	&= a^k a_k\\
	&= a^i a_i.
	\end{align*}
	Therefore, $\vert\vert \vec{a} \vert\vert$ is a scalar.
\end{exmp}
\begin{exmp}
	Show that the line element $ds^2 = g_{ij}\,du^i\,du^j$ is a scalar. \\
	
	Similar to the previous example, we have to show that:
	\begin{align*}
	g_{i'j'}\,du^{i'}\,du^{j'} = g_{ij}\,du^i\,du^j.
	\end{align*}
	We simply apply the transformation rule to each factor and play a little game of index-arranging:
	\begin{align*}
	g_{i'j'}\,du^{i'}\,du^{j'} &= \left( U^k_{i'} U^l_{j'} g_{kl}\right) \left(U^{i'}_m \,du^m \right) \left(U^{j'}_n \,du^n\right) \\
	&= U^k_{i'} U^l_{j'} U^{i'}_m U^{j'}_n g_{kl}\,du^m\,du^n\\
	&= \delta^k_m\delta^l_n \,g_{kl}\,du^m\,du^n\\
	&= g_{kl}\,du^k\,du^l\\
	&= g_{ij}\,du^i\,du^j.
	\end{align*}
	Therefore, $ds^2 = g_{ij}\,du^i\,du^j$ is a scalar.
\end{exmp}

\newpage

\section{Flat spacetime}
Recall that in space, we require 3 coordinates. It makes sense that in order to describe spacetime, we need an extra coordinate to describe time, in which case, we will switch from Roman-character indices ($i,j,k,\dots = 1,2,3$) to Greek-character indices $(\mu, \nu, \sigma, \dots) = 0,1,2,3$. We also tend to (by an unwritten convention and of course not mandatory) write vectors as capital letters to distinguish 4-vectors from 3-vectors. For example, below are a number of ways we express a 4-vector:
\begin{align*}
X^\mu = \left\{ X^0, X^1, X^2, X^3\right\} = \left( X^0, \vec{X} \right)  = \left( X^0, X^i\right) .
\end{align*}   
Notice that the ``hat'' notation is reserved for 3-vectors. 
\subsection{Special Relativity}
Coordinate transformations in special relativity are Lorentz transformations under which the spacetime interval (in Cartesian coordinates)
\begin{align*}
ds^2 = c^2\,dt^2 + dx^2 +dy^2+dz^2 = \left(dx^0 \right)^2 + \left(dx^1 \right)^2 + \left(dx^2 \right)^2 + \left(dx^3 \right)^2,
\end{align*}
which gives ``physical distances'' in spacetime, is invariant. We also recall that
\begin{align*}
ds^2 = g_{ij}\,du^i\,du^j,
\end{align*}
we can ``read off'' the metric from the spacetime interval as $\text{diag} = (1,-1,-1,-1)$. This is referred to as the Minkowski metric (flat 4-dimensional spacetime in Cartesian coordinates), denoted as $\eta_{\mu\nu}$:
\begin{align*}
[\eta_{\mu\nu}] = 
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{pmatrix}
\end{align*}
Because any frame in special relativity can be connected to the original frame by a Lorentz transformation, i.e.,
\begin{align*}
ds^2 = {ds'}^2 = \left(dx^{0'} \right)^2 + \left(dx^{1'} \right)^2  + \left(dx^{2'} \right)^2 + \left(dx^{3'} \right)^2, 
\end{align*}
The Minkowski metric (in Cartesian coordinates) is the same for any reference frame:
\begin{align*}
[\eta_{\mu'\nu'}] = [\eta_{\mu\nu}] = 
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{pmatrix}.
\end{align*}
This says that:
\begin{align*}
\boxed{ds^2 = \eta_{\mu\nu}\,dX^\mu\,dX^\nu = \eta_{\mu'\nu'}\,dX^{\mu'}\,dX^{\nu'}}
\end{align*}
For general coordinate systems in three dimensions, the metric $g_{ij} \neq \text{diag}(1,1,1)$. In these cases, the Minkowski metric has the following form:
\begin{align*}
[\eta_{\mu\nu}] = 
\begin{pmatrix}
1 & 0\\
0 & -[g_{ij}]
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & & &\\
0 & & -[g_{ij}] &\\
0 & & &
\end{pmatrix}.
\end{align*} 
More generally, however, spacetime can be curved, and the metric tensor of the three-dimensional space can be non-Cartesian. In these cases, we simply replace $\eta_{\mu\nu}$ by $g_{\mu\nu}$. It follows that:
\begin{align*}
\boxed{ds^2 = g_{\mu\nu}\,du^\mu\,du^\nu}
\end{align*}
in general. The ``moral'' here is that $\eta_{\mu\nu}$ is reserved for flat Minkowski spacetime. \\

Same as before, the metric tensor encodes all ``physical properties'' of the Minkowski spacetime and allows us to transform contravariant components into covariant components and vice versa. For instance, consider a contravariant vector $\lambda^\mu = (\lambda^0, \lambda^1, \lambda^2, \lambda^3) = (\lambda^0, \vec{\lambda})$. The covariant vector is given by simply ``lowering'' the index of the contravariant vector using the Minkowski metric tensor:
\begin{align*}
\lambda_\mu = \eta_{\mu\nu}\lambda^\nu = (\lambda_0, \lambda_1, \lambda_2, \lambda_3).
\end{align*}
The inverse transformation has the same form:
\begin{align*}
\lambda^\mu = \eta^{\mu\nu}\lambda_\nu.
\end{align*}
Note that if we use the Minkowski metric tensor in Cartesian coordinates, then
\begin{align*}
\lambda_\mu = \eta_{\mu\nu}\lambda^\nu = (\lambda_0, \lambda_1, \lambda_2, \lambda_3) = (\lambda^0, -\lambda^1, -\lambda^2, -\lambda^3).
\end{align*}
We observe that
\begin{align*}
\lambda^0 &= \lambda_0\\
\lambda^i &= -\lambda_i.
\end{align*}
We also observe that the inverse Minkowski metric tensor is Minkowski metric tensor itself:
\begin{align*}
\boxed{
[\eta^{\mu\nu}] = [\eta_{\mu\nu}]^{-1} 
= 
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{pmatrix}^{-1}
=
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & -1 & 0 & 0\\
0 & 0 & -1 & 0\\
0 & 0 & 0 & -1
\end{pmatrix}}
\end{align*}
Inner products of vectors follow the same rule as before:
\begin{align*}
\boxed{
\mathbf{a}\cdot\mathbf{b} = a^\mu b_\mu = a_\mu b^\mu = \eta_{\mu\nu}a^\mu b^\nu = \eta^{\mu\nu}a_\mu b_\nu}
\end{align*}
On a more practical side of things, we have to be careful with the signs that go with the terms when we solve problems:
\begin{align*}
\mathbf{a}\cdot\mathbf{b} &= a^1b_1 + a^2b_2 + a^2b_2 + a^3b_3\\
&= a_1b_1 - a_2b_2 - a_2b_2 - a_3b_3\\
&= a^1b^1 - a^2b^2 - a^2b^2 - a^3b^3.
\end{align*}
At this point, we might wonder what the basis vectors $\vec{e}_\mu$ in the flat, 4-dimensional Minkowski spacetime in Cartesian coordinates are. We can, of course, find them, from the metric tensor $\eta_{\mu\nu}$. However, one thing for sure: they are not going to be the basis vectors $\hat{i}, \hat{j}, \hat{k}$. In fact, the basis vectors that make up $\eta_{\mu\nu}$ will likely contain imaginary parts, since $\vec{e}_i\cdot\vec{e}_j = -1$ for $i = j$. The point is: we don't really need the basis vectors once we already have the metric, because after all, almost all of our computations involve the metric tensor. 
\subsubsection{The Lorentz Transformations, revisited} 
Recall that Lorentz transformations are (not necessarily strictly linear) transformations from one \textbf{inertial frame} $(S)$ to another \textbf{inertial frame} $(S')$.\\

The most general Lorentz transformations, also referred to collectively as ``Poincar\'e transformations'' include:
\begin{itemize}
	\item Lorentz boost: inertial frames with relative velocity $v$.
	\item Translation: origins don't coincide at $t'=t=0$.
	\item Spatial rotation
	\item Spatial inversion: or parity transformations ($x' = -x$)
	\item Time reversal: $t'=-t$.
\end{itemize} 
We can also make further distinctions among the listed transformations:
\begin{itemize}
	\item Inhomogeneous Lorentz transformations: have non-trivial translation.
	\item Homogeneous Lorentz transformations: no translations.
	\item Improper Lorentz transformations: with time reversal or parity.
	\item Proper Lorentz transformation: no parity or time reversal.
\end{itemize}
We can first look at \textbf{homogeneous, proper Lorentz transformations with no rotations}, i.e., Lorentz boosts. These are probably the simplest of Lorentz transformations that we encountered in introductory special relativity. A classic example is the Lorentz boost along the $x$-axis:
\begin{align*}
\begin{pmatrix}
a^{0'}\\a^{1'}\\a^{2'}\\a^{3'}
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\beta\gamma & 0 & 0\\
-\beta\gamma & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
a^{0}\\a^{1}\\a^{2}\\a^{3}
\end{pmatrix}
\end{align*}
where the transformation matrix element $U^{i'}_j$ in 3-dimensional space is now generalized to 4-dimensional spacetime:
\begin{align*}
\boxed{X^{\mu'}_\nu = \frac{\partial a^{\mu'}}{\partial a^\nu}}
\end{align*}
Specifically for Lorentz boosts, we let the general $X^{\mu'}_\nu$ be $\Lambda^{\mu'}_\nu$. This slight change in notation gives:
\begin{align*}
\boxed{[\Lambda^{\mu'}_\nu] = \left[ \frac{\partial a^{\mu'}}{\partial a^{\nu}}\right] = 
\begin{pmatrix}
\gamma & -\beta\gamma & 0 & 0\\
-\beta\gamma & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}}
\end{align*}
We notice immediately that $\Lambda^{\mu'}_\nu$ is constant for all $\mu, \nu$. This means (i) Lorentz transformations are linear transformations, and (ii) Cartesian coordinates $X^{\mu}$ can form the components of a vector under Lorentz transformations because
\begin{align*}
X^{\mu'} = \Lambda^{\mu'}_\nu X^{\nu}
\end{align*} 
is obeyed. While this concept might seem foreign, we realize we have seen this many times before in special relativity, only in a slightly different form:
\begin{align*}
\begin{pmatrix}
ct'\\x'\\y'\\z'
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\beta\gamma & 0 & 0\\
-\beta\gamma & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
ct\\x\\y\\z
\end{pmatrix},
\end{align*}
which gives us the very first Lorentz-Einstein equations that we are familiar with:
\begin{align*}
\begin{cases}
ct'= \gamma(ct-\beta x)\\
x'= \gamma(x - c\beta t)\\
y'= y\\
z'= z.
\end{cases}
\end{align*}
Keep in mind that the equalities above are only true because $\Lambda^{\mu'}_\nu \Lambda^{\nu}_{\sigma'} = \delta^\mu_\sigma$, and are not true in general, because coordinates don't form vectors in general.\\

We can find the inverse of $\Lambda^{\mu'}_\nu$ in two ways: the intuitive way, and the mechanical way. Intuitively, $\Lambda^{\mu}_{\nu'}$, or the inverse transformation, undoes what $\Lambda^{\mu'}_\nu$, so it has to be a boost in the opposite direction ($v \rightarrow -v$ therefore $\beta \rightarrow -\beta$). So if $\Lambda^{\mu'}_\nu$ is an $x$-boost, then its inverse is simply:
\begin{align*}
[\Lambda^{\mu}_{\nu'}] = 
\begin{pmatrix}
\gamma & \beta\gamma & 0 & 0\\
\beta\gamma & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix},
\end{align*}
which not surprisingly gives us the inverse Lorentz-Einstein transformation equations:
\begin{align*}
\begin{cases}
ct= \gamma(ct'+\beta x)\\
x= \gamma(x+c\beta t)\\
y= y\\
z= z.
\end{cases}
\end{align*}
A reasonable next step is to look at \textbf{proper homogeneous Lorentz transformation with rotation(s)}. Recall from linear algebra that such a transformation is simply a composition of multiple, more elementary transformations (pure boost and/or pure rotation). For example, a pure rotation an angle $\phi$ about the $z$-axis has the following matrix:
\begin{align*}
[\Lambda^{\mu'}_\nu] = 
\begin{pmatrix}
1 & 0 & 0 & 0\\
0 & \cos\phi & \sin\phi & 0\\
0 & -\sin\phi & \cos\phi & 0\\
0 & 0 & 0 & 1
\end{pmatrix}.
\end{align*}
To think how to do about how to construct a transformation matrix has includes both a boost and a rotation, we can first do a gentle example.
\begin{exmp}
	 Find the transformation matrix for a Lorentz boost in the $y$-axis in two ways: intuition and composition of rotation by $\pi/2$ and boost along the $x$-direction.\\
	 
	 We can easily predict that the matrix for a boost in $y$ should be almost the same as that for a boost $x$, except for the locations of the terms in the matrix. We know this because the $x$-direction is no more special than the $y$-direction. So,
	 \begin{align*}
	 [\Lambda^{\mu'}_\nu] = 
	 \begin{pmatrix}
	 \gamma&0&-\beta\gamma&0\\
	 0&0&0&0\\
	 -\beta\gamma&0&\gamma&0\\
	 0&0&0&0
	 \end{pmatrix}
	 \end{align*}
	 We should get the same matrix by doing the ``composition method.'' We can simply think of a boost in the $y$-direction as a rotating by $\pi/2$, followed by a boost along the $x$-direction, then followed by a rotation by $-\pi/2$. A final transformation matrix is a product of the list matrix, in right-to-left order:
	 \begin{align*}
	 [\Lambda^{\mu'}_\nu] &= 
	 \begin{pmatrix}
	 1 & 0 & 0 & 0 \\
	 0 & 0 & -1 & 0\\
	 0 & 1 & 0 & 0\\
	 0 & 0 & 0 & 1
	 \end{pmatrix}
	 \begin{pmatrix}
	 \gamma & \beta\gamma & 0 & 0\\
	 \beta\gamma & \gamma & 0 & 0\\
	 0 & 0 & 1 & 0\\
	 0 & 0 & 0 & 1
	 \end{pmatrix}
	\begin{pmatrix}
	1 & 0 & 0 & 0\\
	0 & 0 & 1 & 0\\
	0 & -1 & 0 & 0\\
	0 & 0 & 0 & 1
	\end{pmatrix}\\
	&=
	\begin{pmatrix}
	\gamma&0&-\beta\gamma&0\\
	0&0&0&0\\
	-\beta\gamma&0&\gamma&0\\
	0&0&0&0
	\end{pmatrix},
	 \end{align*}
	 as expected.
\end{exmp}
The moral of the above example is to show that any Lorentz boost along an arbitrary direction can be found as a combination of a boost along the $x$-direction and a spatial rotation:
\begin{align*}
\boxed{
[\Lambda^{\mu'}_\nu] = 
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\phi & -\sin\phi & 0\\
0 & \sin\phi & \cos\phi & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
\gamma & \beta\gamma & 0 & 0\\
\beta\gamma & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\phi & \sin\phi & 0\\
0 & -\sin\phi & \cos\phi & 0\\
0 & 0 & 0 & 1
\end{pmatrix}}
\end{align*}
\subsubsection{A curiosity about the Lorentz boosts}
We can actually make Lorentz boosts look like rotation using hyperbolic functions: 
\begin{align*}
\sinh(\alpha) &= \frac{e^{\alpha} - e^{-\alpha}}{2}\\
\cosh(\alpha) &= \frac{e^\alpha + e^{-\alpha}}{2}\\
\sech(\alpha) &= \frac{1}{\cosh(\alpha)}\\
\csch(\alpha) &= \frac{1}{\sinh(\alpha)}\\
\tanh(\alpha) &= \frac{\sinh(\alpha)}{\cosh(\alpha)}\\
\coth(\alpha) &= \frac{1}{\tanh(\alpha)},
\end{align*}
which have the following identities:
\begin{align*}
\cosh^2(\alpha) - \sinh^2(\alpha) &= 1\\
1 - \tanh^2(\alpha) &= \sech^2(\alpha).
\end{align*}
If we let $\beta = \tanh(\phi)$, where the angle $\phi$ is called ``rapidity,'' then it follows that:
\begin{align*}
\gamma = \frac{1}{\sqrt{1-\beta^2}} = \frac{1}{\sqrt{1-\tanh^2(\phi)}} = \left( \sech(\phi)  \right)^{-1} = \cosh(\phi), 
\end{align*}
and
\begin{align*}
\gamma\beta = \sinh(\phi).
\end{align*}
So the Lorentz boost $[\Lambda^{\mu'}_\nu]$ becomes:
\begin{align*}
[\Lambda^{\mu'}_\nu] =
\begin{pmatrix}
\cosh(\phi) & -\sinh(\phi) & 0 & 0\\
-\sinh(\phi) & \cosh(\phi) & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\end{align*}

\subsubsection{The Poincar\'e transformations}

\begin{align*}
\boxed{a^{\mu'} = \Lambda^{\mu'}_\nu a^{\nu} + b^{\mu'}}
\end{align*}
\begin{exmp}
	stuff
\end{exmp}
\begin{exmp}
	stuff
\end{exmp}
\begin{exmp}
	stuff
\end{exmp}
\subsubsection{Velocity, momentum, and force}

\subsection{Relativistic Electrodynamics}
\begin{align*}
\text{Gauss' law:  } \Aboxed{&\oiint\vec{E}\cdot\,d\vec{a} = \frac{q}{\epsilon_0}}\\
\text{No magnetic monopole:  } \Aboxed{&\oint\vec{B}\cdot\,d\vec{a} = 0}\\
\text{Faraday's law:  } 
\Aboxed{&\oint \vec{E}\cdot\,d\vec{s} = -\frac{d}{dt}\Phi_B = \frac{\partial}{\partial t}\int_A\vec{B}\cdot\,d\vec{a}}\\  
\text{Ampere-Maxwell's law:  } 
\Aboxed{&\oiint \vec{B}\cdot\,d\vec{s} = \mu_0I + \mu_0\epsilon_0\frac{\partial}{\partial t}\int_A\vec{E}\cdot\,d\vec{a}}
\end{align*}
\begin{defn}
	The electromagnetic field strength tensor $F^{\mu\nu}$ is given by:
	\begin{align*}
	[F^{\mu\nu}] = 
	\begin{pmatrix}
	0 & E^1/c & E^2/c & E^3/c\\
	-E^1/c & 0 & B^3 & -B^2\\
	-E^2/c & -B^3 & 0 & B^1\\
	-E^3/c & B^2 & -B^1 & 0
	\end{pmatrix}.
	\end{align*}
\end{defn}
The Maxwell equations, in index notation become:
\begin{align*}
stuff
\end{align*}

\newpage

\section{Curved spaces}
\subsection{2-dimensional curved spaces}
\subsection{Manifolds}
\subsection{Tensors on manifolds}

\newpage

\section{Gravitation and Curvature}
\subsection{Geodesics and Affine connections $\Gamma^{\sigma}_{\mu\nu}$}
\subsection{Parallel transport}
\subsection{Covariant differentiation}
\subsection{Newtonian limit}

\newpage

\section{Einstein's field equations}
\subsection{The stress-energy tensor $T^{\mu\nu}$}
\subsection{Riemann curvature tensor $R^{\lambda}_{\text{ }\mu\nu\sigma}$}
\subsection{The Einstein equations}
\subsection{Schwarzschild solution}

\newpage

\section{Predictions and tests of general relativity}
\subsection{Gravitational redshift}
\subsection{Radar time-delay experiments}
\subsection{Black Holes}

\newpage

\section{Cosmoslogy}
\subsection{Large-scale geometry of the universe}
\subsubsection{Cosmological principle}
\subsubsection{Robertson-Walker (flat, open, closed) geometries}
\subsubsection{Expansion of the universe}
\subsubsection{Distances and speeds}
\subsubsection{Redshifts}
\subsection{Dynamical evolution of the universe}
\subsubsection{The Friedmann equations}
\subsubsection{The cosmological constant $\Lambda$}
\subsubsection{Equations of state}
\subsubsection{A matter-dominated universe ($\Lambda = 0$) [Friedmann models]}
\subsubsection{A flat, metter-dominated universe ($\Lambda = 0$) [old favorite model]}
\subsection{Observational cosmology}
\subsubsection{Hubble law}
\subsubsection{Acceleration of the universe}
\subsubsection{Matter densities and dark matter}
\subsubsection{The flatness and horizon problems}
\subsubsection{Cosmic Microwave Backgrond (CMB) anisotropy}
\subsection{Modern cosmology}
\subsubsection{Inflation}
\subsubsection{Dark energy (The cosmological constant problem)}
\subsubsection{Concordance model [new favorite model]}
\subsubsection{Open questions}

\newpage
 
\end{document}
