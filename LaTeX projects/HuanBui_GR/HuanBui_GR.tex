\documentclass{article}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage{mathtools}

\begin{document}
\begin{titlepage}\centering
 \clearpage
 \title{\textsc{\bf{GENERAL RELATIVITY\\ \& COSMOLOGY}}\\\smallskip A Quick Guide\\}
 \author{\bigskip Huan Bui}
 \affil{Colby College\\Physics \& Statistics\\Class of 2021\\}
 \date{\today}
 \maketitle
 \thispagestyle{empty}
\end{titlepage}

\tableofcontents
\newpage

\section{Overview and Review}

General relativity is a theory of gravity, replacing Newton's gravity law for heavy masses to give more precise predictions. However, we should keep in mind that general relativity is not yet compatible with quantum mechanics. There are numerous open problems in physics related to reconciling gravity and quantum mechanics. 

\subsection{Review of Special Relativity}

Special relativity studies the kinematics and dynamics in relatively moving inertial reference frames. Most of special relativity and its consequences are encoded in the Lorentz transformations. A classic example of a Lorentz transformation that is often focused on in introductory special relativity is called the ``Lorentz boost'' in the $x$-direction. Note that there is nothing special about $x$ or $y$ or $z$. The $x$-Lorentz boost is essentially a coordinate transformation, i.e. given coordinates an event $A$ in frame $(S)$, we can calculate the coordinates of $A$ in frame $(S')$.
\begin{align*}
\begin{pmatrix}
ct'\\x'\\y'\\z'
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\gamma\beta & 0 & 0\\
-\gamma\beta & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
ct\\x\\y\\z
\end{pmatrix}
=
\begin{pmatrix}
ct\\x\\y\\z
\end{pmatrix},
\end{align*}
where $\beta = v/c$ and the Lorentz factor 
\begin{align*}
\gamma = \frac{1}{\sqrt{1-\frac{v^2}{c^2}}}.
\end{align*}
Since we are studying relativity, it is important to look at ``invariants'' - quantities that do not change under transformations. In special relativity, one such invariant is the spacetime invariant:
\begin{align*}
(\Delta S)^2 &= (c\Delta \tau)^2\\
&= (c\Delta t)^2 - (\Delta x)^2 - (\Delta y)^2 - (\Delta z)^2\\
&= (c\Delta t')^2 - (\Delta x')^2 - (\Delta y')^2 - (\Delta z')^2\\
&= (\Delta S')^2 = (c\Delta \tau')^2.
\end{align*}
This can be readily shown. In fact, we have verified this in \textit{Special Relativity: A Quick Guide}. This quantity, roughly speaking, is a measure of \textbf{proper} distances and times. If we go to a rest frame of event $A$, such that $\Delta x' = \Delta y' = \Delta z' = 0$, then we get $\Delta t' = \Delta \tau$, where $\tau$ denotes the \textbf{proper time}, which is the time measured in the rest frame. This gives $(\Delta S)^2 = (c\Delta \tau)^2$.\\

In relativity in general and in Minkowski spacetime in particular, we are interested in two types of objects: \textbf{scalars} and \textbf{vectors}. Scalars are invariant under general coordinate transformations. An example of a scalar is the spacetime invariant. Another scalar, which we probably will not see again in this text, is the metric signature (sgn($[\eta_{\mu\nu}] = -2$)). Vectors, on the other hand, are a different type of objects, which transform in the same way under coordinate transformations, but are not invariant under general coordinate transformations in general, i.e., their components are not necessarily the same in different coordinate systems. In 4-dimensional spacetime, we work with 4-vectors, which simply means 4-component vectors.\\

In special relativity, we often talk about position vectors $\mathbf{x} = (ct,x,y,z)^\top = (ct,\vec{x})^\top$ and energy-momentum vectors $\mathbf{p} = (E/c,p_x,p_y,p_z)^\top = (E/c, \vec{p})^\top$. These vectors transform under the Lorentz transformation. For example, for the $x$-Lorentz boost applied to $\mathbf{p}$ gives
\begin{align*}
\begin{pmatrix}
E'/c\\p_x'\\p_y'\\p_z'
\end{pmatrix}
=
\begin{pmatrix}
\gamma & -\gamma\beta & 0 & 0\\
-\gamma\beta & \gamma & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 1
\end{pmatrix}
\begin{pmatrix}
E/c\\p_x\\p_y\\p_z
\end{pmatrix}.
\end{align*}
Notice that $E/c$ transforms like $ct$ (time) and $\vec{p}$ transforms like $\vec{x}$ (space). We also have the following invariant
\begin{align*}
\frac{E^2}{c^2} - p^2_x - p^2_y - p^2_z = \frac{E^2}{c^2} - \vec{p}\cdot\vec{p}.
\end{align*}
But recall that $(mc)^2 = E^2/c^2 - \vec{p}\cdot\vec{p}$, so
\begin{align*}
\frac{E^2}{c^2} - p^2_x - p^2_y - p^2_z = (mc)^2.
\end{align*} 
If we go to a rest frame, such that $\vec{p} = \vec{0}$, then we obtain the famous rest mass-energy equivalence: $E = mc^2$.\\

Some quantities associated with a vector can be invariant (scalar), such as their norm. In fact, the spacetime invariant in Minkowski space is nothing but a dot product of a vector with itself:
\begin{align*}
\mathbf{a}\cdot\mathbf{a} = a^0a^0 - a^1a^1 - a^2a^2 - a^3a^3,
\end{align*}
where $0,1,2,3$ are indices, and $\mathbf{a}$ implies a 4-vectors, which should be distinguished from the 3-vector $\vec{a} = (a^1,a^2,a^3)^\top$. Notice that the dot product in Minkowski spacetime is defined differently from that in flat 3-dimensional Cartesian coordinate system. In the following chapters, we will explore how dot products are defined in general. Hint: the metric tensor plays an important role.\\ 


\subsection{The Equivalence Principle}
In 1907, Albert Einstein had the ``happiest thought of his life'' when he realized that in a freely falling frame (non rotating and/or accelerating), the effects of gravity go away, i.e., there is an equivalence between gravity and acceleration such that they can ``undo'' each other. For example, the following two situations are equivalent in terms of the acceleration experienced by the observer: (i) a person standing on Earth, and (ii) a person inside an elevator accelerating upwards at rate $g$ in free space (no gravitational field). Likewise, the following two situations are also equivalent: (iii) a person floating in free space, and (iv) a person inside an airplane free falling towards Earth (this is commercially known as ``zero-G flight''). we arrive at the statement of the Equivalence Principle: \\

\noindent \boxed{``\textit{A small, non-rotating, freely falling frame in a $\vec{g}$ field is an inertial frame.}''}\\

The above statement is a direct result of Galileo's discovery that all objects have the same acceleration due to gravity. This result may seem a little bit \textit{circular}, because it is actually a coincidence that the two roles of \textbf{mass}: (i) to cause gravitational force like charge in an electric field:
\begin{align*}
\vec{F} = \frac{GMm_G}{r^3}\vec{r} = m_G\vec{g}
\end{align*}
and (ii) to measure inertia:
\begin{align*}
\vec{F} = m_I\vec{a}
\end{align*}
are the same, i.e., $m_I = m_G$. It could have been that ``gravitational mass'' $m_G$ and ``inertial mass'' $m_I$ are not the same, in which case $\vec{a} \neq \vec{g}$, and the equivalence principle does not hold. However, the famous E\"{o}tv\"{o}s experiment, which measured the correlation between inertial mass and gravitational mass, has shown that
\begin{align*}
\frac{\vert m_G - m_I\vert}{m_I} \leq 10^{-10}.
\end{align*}

While the observations we have made so far can seem self-apparent, the consequence of the equivalence principle is the bending of light around massive objects. Consider a light ray going horizontally (left to right) pass an upward-accelerating elevator with an observer inside. For the observer, since he is accelerating upwards, he sees the light beam as bent, entering at the top-left of the elevator and exiting at the bottom-right. Now, according to the equivalence principle, because the upward-accelerating scenario is equivalent to the existence of a massive object (such that the observer experiences the same acceleration). This means that the observer, now no longer in the elevator, ``sees'' the light as bent around this massive object. Fig. 1. illustrates this postulate.
\begin{align*}
\text{include figure here!!!}
\end{align*}
General relativity predicts that light going pass Earth's surface will ``fall'' by approximately $1 \text{\AA}$, which is not observable. However, for a much more massive object like the Sun, general relativity predicts a bending of $1.75''$ (arc sec). This prediction was verified by the glorious experiment of Arthur Eddington.\\

Note that we could argue for the bending of light, using Newtonian physics. However, in order to get the correct predictions for the bending of light, we need general relativity. We will explore the reason behind this discrepancy in the following chapter. But roughly speaking, spacetime is assumed to be flat in Newtonian physics, while spacetime is curved by massive objects, according to general relativity.\\ 

One might ask: ``How do we view falling objects on Earth as due to the curvature of spacetime?'' The answer requires bringing back Minkowski spacetime diagrams. 
\begin{align*}
\text{insert figure here!!!}
\end{align*}

\subsection{Versions of the Equivalence Principle}
There are two versions of the equivalence principle, referred to as the \textit{strong equivalence principle}, SEP, and the \textit{weak equivalence principle}, WEP. 
\subsubsection{The Strong Equivalence Principle}
The strong equivalence principle states that \textbf{all} of physics reduces to special relativity in a freely falling frame.
\subsubsection{The Weak Equivalence Principle}
The weak equivalence principle states that all point particles fall at the same rate in a gravitational field ($m_G = m_I$). We notice that WEP only applies to gravity, which makes it sufficient to develop general relativity, but not quantum mechanics. For this remaining of this note, we will rely on the WEP. 

\newpage

\section{Review of Vector Calculus}
\subsection{Operations \& Theorems}
Recall that a path in 3-dimensional space can by parameterized a single variable, which we will call $t$, as
\begin{align*}
\vec{r}(t) = x(t)\hat{i} + y(t)\hat{j} + z(t)\hat{k},
\end{align*}
where the tangent to this path is given by
\begin{align*}
\dot{\vec{r}}(t) = \frac{d \vec{r}}{dt}.
\end{align*}
The length of the underlying curve of this path is the integral over the line element $ds = \vert\vert \vec{r} \vert\vert = \vert\vert \dot{\vec{r}} \vert\vert\,dt$:
\begin{align*}
L = \int_{a}^{b} dS = \int_{a}^{b} \vert\vert \dot{\vec{r}} \vert\vert\,dt.
\end{align*} 
For example, consider a path in $\mathbb{R}^2$ defined as $\vec{r}(t) = (t, \sin t)$, with $t \in [-2\pi, 2\pi]$. We instantly recognize that the underlying curve of this path is nothing but a sine curve in the $xy$-plane. The length of this curve is
\begin{align*}
L = \int_{-2\pi}^{2\pi}\vert\vert  (1,\cos t)\vert\vert\,dt  = \int_{-2\pi}^{2\pi} \sqrt{1+\cos^2 t}\,dt \approx 15.28.
\end{align*}
Next, we consider a vector-valued function $\vec{F} : \mathbb{R}^3 \rightarrow \mathbb{R}^3$ defined as $\vec{F}(\vec{r}) = (F_x(x,y,z), F_y(x,y,z),F_z(x,y,z))$. Recall the two ``del operators'' from vector calculus: the \textbf{curl} and the \textbf{div}, whose definitions are
\begin{align*}
\text{curl}(\vec{F}) = \vec{\nabla} \times \vec{F} = \det
\begin{pmatrix}
\hat{i} & \hat{j} & \hat{k} \\
\frac{\partial}{\partial x} & \frac{\partial }{\partial y} & \frac{\partial}{\partial z}\\
F_x & F_y & F_z
\end{pmatrix}
\end{align*}
and 
\begin{align*}
\text{div}(\vec{F}) = \vec{\nabla} \cdot \vec{F} = 
\frac{\partial F_x}{\partial x} + \frac{\partial F_y }{\partial y} + \frac{\partial F_z}{\partial z}.
\end{align*}
where the ``nabla'' symbol represents the operator
\begin{align*}
\vec{\nabla} = \left( \frac{\partial }{\partial x} , \frac{\partial }{\partial y} , \frac{\partial }{\partial z} \right). 
\end{align*}
We have also seen the \textbf{gradient} of a scalar field (or potential) $f : \mathbb{R}^3 \rightarrow \mathbb{R}$. The gradient is defined as
\begin{align*}
\vec{\nabla}f = \left( \frac{\partial f}{\partial x} , \frac{\partial f }{\partial y} , \frac{\partial f}{\partial z}\right) .
\end{align*}
There are two kinds of potential functions in vector calculus: scalar potential and vector potential. While we are more familiar with the scalar potential (e.g. the gravitational potential or the electric potential in electromagnetism), there isn't as strong a connection between vector potentials and any physical meaning. However, we should still look at some examples. In electromagnetism, an electric field $\vec{E}$ is defined as the negative gradient of the electric potential $\phi$, measured in Volts:
\begin{align*}
\vec{E} = -\vec{\nabla}\phi.
\end{align*}
On the other hand, an magnetic field $\vec{B}$ is defined as the curl of the magnetic potential $\vec{A}$.
\begin{align*}
\vec{B} = \vec{\nabla}\times\vec{A}.
\end{align*}
In this case, $A$ is a vector potential. Note that we don't often talk about a vector potential of an electric field. This is simply because for a point charge, which produces an inverse-square field, the vector potential simply does not exist. The proof is straightforward. For curious readers, this proof can be an interesting exercise (the solution should be a one-liner). Or else, please refer to my vector calculus notes on my website. Hint: we need one of the two following fundamental theorems of vector calculus: Stokes' and Gauss' (or Ostrogradsky's).\\

\textbf{Stokes' theorem} states that, the circulation around a closed curved in $\mathbb{R}^3$ by a vector field $\vec{F}$ is the flux of $\vec{\nabla}\times\vec{F}$ through the surface $S$ whose boundary $C = \partial S$ is the curve $C$.
\begin{align*}\boxed{
\oint_{C = \partial S} \vec{F}\cdot\,d\vec{s} = \iint_{S} \vec{\nabla}\times \vec{F} \cdot\,d\vec{S}.}
\end{align*}

\textbf{Gauss' theorem} states that, if $S$ is closed surface, oriented outward, and if $\vec{F}$ is defined throughout the solid region $W$ enclosed by $S$, then the flux of $\vec{F}$ through $S$ is the total divergence of $S$ through the region $W$.
\begin{align*}
\boxed{
\oiint_{S=\partial W}\vec{F}\cdot\,d\vec{S} = \iiint \vec{\nabla}\cdot\vec{F}\,dV.}
\end{align*}
\textbf{Line integrals} are one of a few important operations in vector calculus. In the context of undergraduate or high school physics, we often use the line integral to calculate the work done by a force $\vec{F}$ over some path $\vec{r}$. The integral in general is simply an accumulation of the component of the vector field along the path. 
\begin{align*}
\int_{a}^{b}\vec{F}\cdot\,d\vec{r}.
\end{align*} 
If $\vec{F}$ is a conservative field, then the line integral along $\vec{F}$ between two point $A$ and $B$ only depends on where $A$ and $B$ are, i.e., the line integral is path-independent. A good illustration of this fact is the way we compute the change in electric potential:
\begin{align*}
W = -\int \vec{E}\cdot\,d\vec{r} = \Delta \phi
\end{align*}

To actually compute a line integral, we often look for symmetry first (or whether $\vec{F}$ is conservative), then parameterize $\vec{r} = \vec{r}(s)$ if we must. Then, the line integral becomes
\begin{align*}
\int_{a}^{b}\vec{F}\cdot\,d\vec{r} = \int_{a}^{b}\vec{F}(\vec{r}(s))\cdot \frac{d\vec{r}}{ds}\,ds
\end{align*}
\textbf{Surface integrals} give the flux of a vector field through a surface:
\begin{align*}
\int \vec{F}\cdot\,d\vec{S} = \int\vec{F}\cdot\vec{n}\,dS = \pm\int\vec{F}\cdot\vec{N}\,d\phi,d\theta,
\end{align*}
where $\vec{F}$ is the vector field, and $\vec{n}$ is the unit normal vector to the surface. The last expression denotes the surface integral in terms of a parameterization $\vec{S}(\phi, \theta) = (x(\phi, \theta),y(\phi, \theta),z(\phi, \theta))$. $\vec{N}$ is the standard normal to the surface, which is not necessarily a unit vector (with norm of 1) and not necessarily in the same direction as $\vec{n}$, which is intrinsic to the surface. An example of surface integrals we often see in physics is the electric flux through a sphere:
\begin{align*}
\Phi_E = \oiint\vec{E}\cdot\,d\vec{a} = \frac{q}{\epsilon_0}.
\end{align*}
The last expression comes from Gauss' (or Ostrogradsky's) theorem, equivalently the divergence theorem, where $q$ is the charge enclosed. \\

We should introduce the Maxwell's equations as they ``bring together'' the concepts we have touched on so far. To see how, we consider the Maxwell's equations in integral form:
\begin{align*}
\text{Gauss' law:  } \Aboxed{&\oiint\vec{E}\cdot\,d\vec{a} = \frac{q}{\epsilon_0}}\\
\text{No magnetic monopole:  } \Aboxed{&\oint\vec{B}\cdot\,d\vec{a} = 0}\\
\text{Faraday's law:  } 
\Aboxed{&\oint \vec{E}\cdot\,d\vec{s} = -\frac{d}{dt}\Phi_B = \frac{\partial}{\partial t}\int_A\vec{B}\cdot\,d\vec{a}}\\  
\text{Ampere-Maxwell's law:  } 
\Aboxed{&\oiint \vec{B}\cdot\,d\vec{s} = \mu_0I + \mu_0\epsilon_0\frac{\partial}{\partial t}\int_A\vec{E}\cdot\,d\vec{a}}
\end{align*}
Using the theorems and facts we have discussed, we can convert the above equations into differential form. First, we can use Gauss' theorem on Gauss' law, with
\begin{align*}
q = \int_V \rho\,d^3r
\end{align*}
where $\rho$ is the charge volume density. So, Gauss' law becomes
\begin{align*}
\oint \vec{E}\cdot\,d\vec{a} = \int_V \vec{\nabla}\cdot\vec{E}\,d^3r = \frac{1}{\epsilon_0}\int_V\rho\,d^3r,
\end{align*}
which implies that the divergence of any electric field $\vec{E}$ is proportional to the charge enclosed $q$.
\begin{align*}
\Aboxed{\vec{\nabla}\cdot\vec{E} = \frac{\rho}{\epsilon_0}}
\end{align*}
Applying Gauss' theorem to the second equation,
\begin{align*}
\oiint \vec{B}\cdot\d,\vec{a} = \int_V \vec{\nabla}\cdot\vec{B}\,d^3r = 0
\end{align*}
we immediately see that the divergence of any magnetic field $\vec{B}$ is 0, i.e., there is no such thing as a ``magnetic monopole.''
\begin{align*}
\Aboxed{\vec{\nabla}\cdot\vec{B} = 0}
\end{align*}
We can apply Stokes' theorem on the other two equations to turn them into differential form. Converting the third equation is as simple as bookkeeping:
\begin{align*}
\oint \vec{E}\cdot\,d\vec{s} = \int_A\vec{\nabla}\times\vec{E}\cdot\,d\vec{a} = -\frac{\partial}{\partial t}\int_A\vec{B}\cdot\,d\vec{a},
\end{align*}
which says the curl of any electric field $\vec{E}$ is negatively proportional to the change in the magnetic field. 
\begin{align*}
\Aboxed{\vec{\nabla}\times\vec{E} = -\frac{\partial\vec{B}}{\partial t}}
\end{align*}
To convert the fourth equation into differential form, we define a new quantity, $\vec{J}$, as current (area) density, such that
\begin{align*}
I = \int_A \vec{J}\cdot\,d\vec{a}.
\end{align*} 
Again, applying Stokes' theorem to the last equation, 
\begin{align*}
\oint \vec{B}\cdot\,d\vec{s} = \int_A \vec{\nabla}\times\vec{B}\cdot\,d\vec{a} = \mu_0I+\mu_0\epsilon_0\frac{\partial}{\partial t}\int_A\vec{E}\cdot\,d\vec{a} = \mu_0\int_A\left( \vec{J} + \epsilon_0\frac{\partial \vec{E}}{\partial t}\right) \cdot\,d\vec{a}.
\end{align*}
So, in differential form:
\begin{align*}
\Aboxed{\vec{\nabla}\times\vec{B} = \mu_0\vec{J} + \mu_0\epsilon_0\frac{\partial \vec{E}}{\partial t}}
\end{align*}
In a later chapter, we will see how to make these equations fully relativistic. For the curious reader wanting more information regarding this subsection, please refer to a standard textbook on vector calculus, or feel free to use my Vector Calculus lecture notes for a more formal treatment of this subject.

\subsection{Coordinate Systems}
There are numerous coordinate systems to describe 3-dimensional space. However, we are most familiar with the Cartesian coordinates ($x,y,z$), the Spherical coordinates ($r,\theta,\phi$), and the Cylindrical coordinates ($\rho, \phi, z$). The Cartesian coordinate system is the most simple, elegant, and ``nice'' of the three systems, since the $(x,y,z)$ coordinates can also be coefficients of the vector pointing from the origin to any point in space. We will see that this is not the case in general coordinate systems. One such coordinate system is the \textbf{spherical coordinate system} (illustrated in Fig. ???), where
\begin{align*}
x &= r\sin\theta\cos\phi\\
y &= r\sin\theta\sin\phi\\
z &= r\cos\theta
\end{align*}
and the inverse relations are
\begin{align*}
r &= \sqrt{x^2+y^2+z^2}\\
\theta &= \cos^{-1}\left( \frac{z}{r}\right) = \cos^{-1}\left( \frac{z}{\sqrt{x^2+y^2+z^2}}\right) \\
\phi &= \tan^{-1}\left( \frac{y}{x} \right) 
\end{align*}
where $0 \leq r$, $0 \leq \theta \leq \pi$, and $0 \leq \phi \leq 2\pi$.
\begin{align*}
\text{insert figure here!!!!}
\end{align*}
The \textbf{volume element} is given by
\begin{align*}
dV = dx\,dy\,dz = r^2\sin\theta\,dr\,d\theta\,d\phi.
\end{align*}
Consider a sphere of radius $R$. The \textbf{area element} is given by
\begin{align*}
dA = R\,d\theta\,d\phi.
\end{align*}
In \textbf{cylindrical coordinates},
\begin{align*}
x &= \rho\cos\phi\\
y &= \rho\sin\phi\\
z &= z
\end{align*}
and the inversion relations are
\begin{align*}
\rho &= \sqrt{x^2+y^2}\\
z &= z\\
\phi &= \tan^{-1}\left(\frac{y}{x}\right) 
\end{align*}
The \textbf{volume element} is given by
\begin{align*}
dV = r\,dt\,dz\,d\phi.
\end{align*}
Consider a cylinder of radius $R$, the \textbf{area element} is given by 
\begin{align*}
dA = R\,dz\,d\phi.
\end{align*}
\begin{align*}
\text{Insert figure here!!!}
\end{align*}
The purpose of having the area and volume elements is so that we could integrate over regions, parameterized in spherical or cylindrical coordinates. While it seems like these quantities appear out of the blue, there is a general procedure to find them, given any arbitrary coordinate system. This ``procedure'' is called the \textbf{Jacobian}, which essentially gives the scaling factor as we move from one coordinate system into another. For example, we know that the area element in polar coordinate is $dA = r\,dr\,d\theta$. We can see this is true, using the Jacobian. First, the Jacobian matrix, which is characteristic of a transformation from coordinate system $u^i$ into $u^{j'}$ is defined as
\begin{align*}
\boxed{[U^{j'}_{i}] = \left[ \frac{\partial u^{j'}}{\partial u^{i}} \right] = 
\begin{pmatrix}
\frac{\partial u^1}{\partial u^{1'}} & \dots & \frac{\partial u^1}{\partial u^{n'}}\\
\vdots & \ddots & \vdots\\
\frac{\partial u^n}{\partial u^{1'}} & \dots & \frac{\partial u^n}{\partial u^{n'}}
\end{pmatrix}}
\end{align*}
Note that $i$ and $j'$ are indices, not powers. Now, the general \textbf{scaling factor} is defined by the following equality
\begin{align*}
\Aboxed{N = du^{1'}\,du^{2'}\dots\,du^{n'} = \left| \det([U^{j}_{i}])\right| \,du^1\,du^2\dots\,du^{n}}
\end{align*}
For example, the scaling factor as we move from 2-dimensional Cartesian coordinates into polar coordinates is
\begin{align*}
N = \det\begin{pmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta}\\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}
\end{pmatrix}
=
\det\begin{pmatrix}
\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{pmatrix}
=
r.
\end{align*}
So, we get $dA = dx\,dy = rdr\,d\theta$, as expected. We can try repeating the same procedure to get the volume element for spherical coordinates, which we have said before to be $dV = r^2\sin\theta\,dr\,d\theta\,d\phi$. First, define the Jacobian matrix:
\begin{align*}
[U^{j'}_{i}] = \begin{pmatrix}
\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} & \frac{\partial x}{\partial \phi}\\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta} & \frac{\partial y}{\partial \phi}\\
\frac{\partial z}{\partial r} & \frac{\partial z}{\partial \theta} & \frac{\partial z}{\partial \phi}
\end{pmatrix}
=
\begin{pmatrix}
\sin\theta\cos\phi & r\cos\theta\cos\phi & -r\sin\theta\sin\phi \\
\sin\theta\sin\phi & r\cos\theta\sin\phi & r\sin\theta\cos\phi \\
\cos\theta & -r\sin\theta & 0
\end{pmatrix}
\end{align*}
After some bookkeeping-type rearrangements and computation, we find that
\begin{align*}
N = \left|\det[U^{j'}_{i}]\right|= r^2\sin\theta,
\end{align*}
as expected. As a check, we can integrate this volume element over a region defined by $x^2+y^2+z^2 = R^2$ to find its volume
\begin{align*}
\int_{0}^{R}\int_{0}^{\pi}\int_{0}^{2\pi}r^2\sin\theta\,dr\,d\theta\,d\phi = \frac{4}{3}\pi R^3,
\end{align*}
which is not surprisingly the volume of the sphere of radius $R$. 

\newpage

\section{Flat 3-dimensional space}
Flat, or ``Euclidean'' space are spaces with \textbf{no curvature}. For instance, a flat sheet of paper in $\mathbb{R}^3$ is considered ``flat'' while the surface of a sphere embedded in $\mathbb{R}^3$ is considered curved space. There should also be a distinction between a sphere in $\mathbb{R}^3$ and a sphere \textbf{embedded} into $\mathbb{R}^3$. The former case is simply some subset of flat 3-dimensional space, so it is considered flat. But in the latter case, the sphere is a curved 2-dimensional space, although it lives in flat 3-dimensional space.\\

Our goal in this section is to see how  we can use any arbitrary coordinate system, which specify points in 3-dimensions as an intersection of 3 surfaces. For instance, in Cartesian coordinates, a point ($a^1,a^2,a^3$) is specified by 3 planes: $x = a^1$, $y=a^2$, and $z=a^3$. In spherical coordinates, a point is specified by intersecting a sphere of radius $r$, a cone with azimuth angle $\theta$, and a plane of constant $\phi$. In cylindrical coordinates, a point is specified by intersecting a cylinder of radius $\rho$, vertical plane of constant $\phi$ to the $x-$axis, and the horizontal plane of constant $z$.
\begin{align*}
\text{insert figure here!!! }
\end{align*}

\subsection{Curvilinear coordinates}
Curvilinear coordinates are an arbitrary coordinate system in Euclidean space where coordinate lines can be curved (hence curvilinear). Note that although coordinate lines can be curvy, the space is still flat. We can call $(u,v,w)$ our arbitrary coordinates, which specify a point $(a^1,a^2,a^3)$ by intersecting three surfaces: $u = a^1$, $v=a^2$, $w=a^3$. Let let relations between $(u,v,w)$ and $(x,y,z)$ be established as
\begin{align*}
u &= u(x,y,z); \text{ } x = x(u,v,w)\\
v &= v(x,y,z); \text{ } y = y(u,v,w)\\
w &= w(x,y,z); \text{ } z = z(u,v,w)
\end{align*} 
\begin{align*}
\text{Insert figure here!!!}
\end{align*}

\subsection{Basis vectors}
\subsection{Natural basis}
Next, since we want to be able to describe vectors using arbitrary curvilinear coordinates, we need to know a \textbf{basis set} of vectors that span the space. In Cartesian coordinates, such a basis set is $\{\hat{i},\hat{j},\hat{k}\}$. Similarly, we want to find a set $\{\vec{e}_u, \vec{e}_v, \vec{e}_w\}$ that would give a basis for an arbitrary curvilinear coordinate system. The way we obtain this set is the same as how we obtain $\{\hat{i}, \hat{j}\, \hat{k}\}$ from Cartesian coordinates. We notice that $\hat{i}$ is a vector that follows the change in the $x$-direction with $y$ and $z$ fixed, i.e., $\hat{i}$ is a tangent vector to a vector $\vec{r}$ along the change in $x$. Hence
\begin{align*}
\hat{i} = \frac{\partial \vec{r}}{\partial x}.
\end{align*}
Likewise, $\hat{j} = \frac{\partial \vec{r}}{\partial y}$, and $\hat{k} = \frac{\partial \vec{r}}{\partial z}$. By analogy, we can define the set $\{ \vec{e}_u, \vec{e}_v, \vec{e}_w\}$ with
\begin{align*}
\vec{e}_u = \frac{\partial \vec{r}}{\partial u}, \text{ } \vec{e}_v = \frac{\partial \vec{r}}{\partial v}, \text{ } \vec{e}_w = \frac{\partial \vec{r}}{\partial w},
\end{align*}
In general, given a coordinate system with $(u^1,u^2,\dots,u^n)$, we can define a \textbf{natural basis set} by letting
\begin{align*}
\Aboxed{\vec{e}_{u^i} = \frac{\partial \vec{r}}{\partial u^i}}
\end{align*}
where $i = 0,1,2,\dots,n$ are indices. Note that the set $\{\vec{e}_{u^i} \}$ need not be orthogonal. The only requirement is that they have to be linearly independent and span the space. We also note that they need not be unit vectors. It is possible to make a basis set of only unit vectors by letting
\begin{align*}
\hat{e}_u = \frac{\vec{e}_u}{\vert\vert \vec{e}_u\vert\vert},
\end{align*}
but we will see that this definition is not so useful in general relativity. A question one might ask is: ``What is \textbf{natural} about this basis set?'' The answer, unfortunately, will be provided in the following sections, where we will discuss how they give rise to the \textbf{metric tensor}.\\

We will often use $\{\hat{i}, \hat{j}, \hat{k} \}$ as a reference basis, i.e., we can express $\{\vec{e}_u, \vec{e}_v ,\vec{e}_w\}$ in terms of these:
\begin{align*}
\vec{\epsilon_u} = (e_u)_x\hat{i} + (e_u)_y\hat{j} + (e_u)_z\hat{k}.
\end{align*}
For example, we can look at the basis set for the spherical coordinate system, $\{\vec{e}_r, \vec{e}_\theta, \vec{e}_\phi \}$.

\subsection{Dual basis}

\subsection{Contravariant and covariant vectors}
\subsection{Metric tensor}
\subsection{Coordinate transformation}
\subsection{Tensors}

\newpage

\section{Flat spacetime}
\subsection{Special Relativity}
\subsection{Relativistic Electrodynamics}

\newpage

\section{Curved spaces}
\subsection{2-dimensional curved spaces}
\subsection{Manifolds}
\subsection{Tensors on manifolds}

\newpage

\section{Gravitation and Curvature}
\subsection{Geodesics and Affine connections $\Gamma^{\sigma}_{\mu\nu}$}
\subsection{Parallel transport}
\subsection{Covariant differentiation}
\subsection{Newtonian limit}

\newpage

\section{Einstein's field equations}
\subsection{The stress-energy tensor $T^{\mu\nu}$}
\subsection{Riemann curvature tensor $R^{\lambda}_{\text{ }\mu\nu\sigma}$}
\subsection{The Einstein equations}
\subsection{Schwarzschild solution}

\newpage

\section{Predictions and tests of general relativity}
\subsection{Gravitational redshift}
\subsection{Radar time-delay experiments}
\subsection{Black Holes}

\newpage

\section{Cosmoslogy}
\subsection{Large-scale geometry of the universe}
\subsubsection{Cosmological principle}
\subsubsection{Robertson-Walker (flat, open, closed) geometries}
\subsubsection{Expansion of the universe}
\subsubsection{Distances and speeds}
\subsubsection{Redshifts}
\subsection{Dynamical evolution of the universe}
\subsubsection{The Friedmann equations}
\subsubsection{The cosmological constant $\Lambda$}
\subsubsection{Equations of state}
\subsubsection{A matter-dominated universe ($\Lambda = 0$) [Friedmann models]}
\subsubsection{A flat, metter-dominated universe ($\Lambda = 0$) [old favorite model]}
\subsection{Observational cosmology}
\subsubsection{Hubble law}
\subsubsection{Acceleration of the universe}
\subsubsection{Matter densities and dark matter}
\subsubsection{The flatness and horizon problems}
\subsubsection{Cosmic Microwave Backgrond (CMB) anisotropy}
\subsection{Modern cosmology}
\subsubsection{Inflation}
\subsubsection{Dark energy (The cosmological constant problem)}
\subsubsection{Concordance model [new favorite model]}
\subsubsection{Open questions}

\newpage
 
\end{document}
