\documentclass{article}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Properties}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\p}{\partial}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\V}{\mathbf{V}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\X}{\mathbf{X}}

\newcommand{\xpan}{\text{span}}


\begin{document}
	\begin{titlepage}\centering
		\clearpage
		\title{\textsc{\bf{MATRIX ANALYSIS}}\\\smallskip A Quick Guide\\}
		\author{\bigskip Huan Bui}
		\affil{Colby College\\Physics \& Statistics\\Class of 2021\\}
		\date{\today}
		\maketitle
		\thispagestyle{empty}
	\end{titlepage}

\newpage

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

\textit{Matrix Analysis: A Quick Guide to} is compiled based on my MA353: Matrix Analysis notes with professor Leo Livshits. The sections are based on a number of resources: \textit{Linear Algebra Done Right} by Axler, \textit{A Second Course in Linear Algebra} by Horn and Garcia, \textit{Matrices and Linear Transformations} by Cullen, \textit{Matrices: Methods and Applications} by Barnett, \textit{Problems and Theorems in Linear Algebra} by Prasolov, \textit{Matrix Operations} by Richard Bronson, and professor Leo Livshits' own textbook (in the making). Prerequisites: some prior exposure to a first course in linear algebra.\\

The development of this text will come in layers. The first layer, one that I am working on during the course of S'19 MA353, will be an overview of the key topics listed in the table of contents. As the semester progresses, I will be constantly updating the existing notes, as well as adding prof. Livshits' problems and my solutions to the problems. The second layer will come after the course is over, when concepts will have hopefully ``come together.''\\ 

I will decide how much narrative I should put into the text as text is developed over the semester. I'm thinking that I will only add detailed explanations wherever I find fit or necessary for my own studies. I will most likely keep the text as condensed as I can.\\

Enjoy!


\newpage
\tableofcontents
\newpage

\section{List of Special Matrices \& Their Properties}
\begin{enumerate}
	\item \textbf{Hermitian/Self-adjoint}: $H = H^\dagger$. A Hermitian matrix is matrix that is equal to its own conjugate transpose:
	\begin{align*}
	H \text{ is Hermitian } \iff H_{ij} = \bar{H}_{ji}
	\end{align*} 
	\begin{prop}
	$\,$
	\begin{enumerate}
		\item $H$ is Hermitian $ \iff \langle w, Hv\rangle = \langle Hw, v \rangle$, where $\langle, \rangle$ denotes the inner product.
		\item $H$ is Hermitian $ \iff \langle v, Hv\rangle \in \R$.
		\item $H$ is Hermitian $\iff$ it is \textit{unitarily diagonalizable} with \textit{real eigenvalues}.
	\end{enumerate}
	\item \textbf{Unitary}: $U^*U = UU^* = I = U^\dagger U = UU^\dagger$. The real analogue of a unitary matrix is an orthogonal matrix. The following list contain the properties of $U$:
	\begin{enumerate}
		\item $U$ preserves the inner product:
		\begin{align*}
		\langle Ux, Uy \rangle = \langle x,y\rangle.
		\end{align*}
		
		\item $U$ is normal: it commutes with $U^* = U^\dagger$.
		
		\item $U$ is diagonalizable:
		\begin{align*}
		U = VDV^*,
		\end{align*}
		where $D$ is diagonal and unitary, and $V$ is unitary.
		
		\item $\vert \det(U) \vert = 1 $ (hence the real analogue to $U$ is an orthogonal matrix)
		
		\item Its eigenspaces are orthogonal.
		
		\item $U$ can be written as
		\begin{align*}
		U = e^{iH},
		\end{align*}
		where $H$ is a Hermitian matrix. 
		
		\item Any square matrix with unit Euclidean norm is the average of two unitary matrices.
	\end{enumerate}
	\end{prop}
	\item \textbf{Idempotent:} $M$ idempotent $\iff M^2 = M$.
	\begin{enumerate}
		\item Singularity: its number of independent rows (and columns) is less than its number of rows (and columns). 
		\item When an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. 
		\begin{proof}[``Proof'']
			\begin{align*}
			[I-M][I-M] = I - M - M + M^2 = I-M - M + M = I - M.
			\end{align*}
		\end{proof}
		\item $M$ is idempotent $\iff \forall n \in \mathbb{N}$, $A^n = A$. 
		\item Eigenvalues: an idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1. (think ``projection")
		\item Trace: the trace of an idempotent matrix equals the rank of the matrix and thus is always an integer. So
		\begin{align*}
		\tr(A) = \dim(\Im A).
		\end{align*}
	\end{enumerate}
	\item \textbf{Nilpotent:} a nilpotent matrix is a square matrix $N$ such that
	\begin{align*}
	N^k = 0
	\end{align*}
	for some positive integer $k$. The smallest such $k$ is sometimes called the \textbf{index} of $N$.\\
	
	The following statements are equivalent:
	\begin{enumerate} 
		\item $N$ is nilpotent.
		\item The minimal polynomial for $N$ is $x^k$ for some positive integer $k\leq n$.
		\item The characteristic polynomial for $N$ is $x^n$.
		\item The only complex eigenvalue for $N$ is 0.
		\item $\tr N^k = 0$ for all $k > 0$.
	\end{enumerate}
	\begin{prop}
		$\,$
		\begin{enumerate}
			\item The degree of an $n\times n$ nilpotent matrix is always less than or equal to $n$. 
			\item $\det N = \tr(N) = 0$.
			\item Nilpotent matrices are not invertible.
			\item The only nilpotent diagonalizable matrix is the zero matrix. 
		\end{enumerate}
	\end{prop}
\end{enumerate}

\newpage
\section{List of Operations}
\begin{enumerate}
	\item \textbf{Conjugate transpose} is what its name suggests.
	\item \textbf{Classical adjoint/Adjugate/adjunct} of a square matrix is the transpose of its cofactor matrix. 
\end{enumerate}
\newpage

\section{List of Algorithms}
\newpage

\section{Complex Numbers}
\subsection{A different point of view}
We often think of complex numbers as
\begin{align*}
a + ib
\end{align*}
where $a,b \in \R$ and $i = \sqrt{-1}$. While there is nothing ``bad'' about this way of thinking - in fact thinking of complex numbers as $a+ib$ allows us to very quickly and intuitively do arithmetics operations on them - a ``matrix representation'' of complex numbers can give us some insights on ``what we actually do'' when we perform complex arithmetics.\\

Let us think of 
\begin{align*}
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\end{align*}
as a different representation of the same object - the same complex number ``$a+ib$.'' Note that it does not make sense to say the matrix representation \textbf{equals} the complex number itself. But we shall see that a lot of the properties of complex numbers are carried into this matrix representation under interesting matricial properties.
\begin{align*}
\boxed{\begin{pmatrix}
	a & -b\\
	b & a
	\end{pmatrix}
\sim a + ib}
\end{align*}

First, let us break the the matrix down:
\begin{align*}
a+ib = a\times 1 + i \times b \sim \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix} 
= 
a\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
+
b\begin{pmatrix}
0 & -1\\
1 & 0
\end{pmatrix}
= aI + b\mathcal{I}.
\end{align*}
Right away, we can make some ``mental connections'' between the representations:
\begin{align*}
I &\sim 1\\
\mathcal{I} \sim i.
\end{align*}

Now, we know that complex number multiplications commute:
\begin{align*}
(a+ib)(c+id) = (c+id)(a+ib).
\end{align*}
Matrix multiplications are not commutative. So, we might wonder whether commutativity holds under the this new representation of complex numbers. Well, the answer is yes. We can readily verify that
\begin{align*}
(aI+b\mathcal{I})(cI+b\mathcal{I}) = (cI+b\mathcal{I})(aI+b\mathcal{I}).
\end{align*}
How about additions? Let's check:
\begin{align*}
(a + ib) + (c+ id) = (a+c) + i(b+d) \sim \begin{pmatrix}
a+c & -(b+d)\\
(b+d) & a+c 
\end{pmatrix}=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}+
\begin{pmatrix}
c & -d\\
d & c
\end{pmatrix}.
\end{align*}
Ah! Additions work. So, the new representation of complex numbers seems to be working flawlessly. However, we have yet to gain any interesting insights into the connections between the representations. To do that, we have to look into changing the form of the matrix. First, let's see what conjugation does:
\begin{align*}
(a+ib)^* = a-ib \sim \begin{pmatrix}
a & b \\
-b & a
\end{pmatrix}
=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}^{\top}
\end{align*}
Ah, so conjugation to a complex number in the traditional representation is the same as transposition in the matrix representations. What about the amplitude square? Let us call 
\begin{align*}
M = \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix}.
\end{align*}
We have
\begin{align*}
(a+ib)(a-ib) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
a & b \\
-b & a
\end{pmatrix} =
MM^\top= (a^2+b^2)I = \det(M)I
\end{align*}
Interesting. But observe that if $\det(M) \neq 0$
\begin{align*}
\frac{1}{\det(M)}MM^\top = I.
\end{align*}
This tells us that
\begin{align*}
M^\top = M^{-1},
\end{align*}
where $M^{-1}$ is the inverse of $M$, and, not surprisingly, it corresponds to the reciprocal to the complex number $a+ib$. We can readily show that
\begin{align*}
M^{-1} \sim (a+ib)^{-1} = \frac{1}{a^2+b^2}(a-ib).
\end{align*} 
Remember that we can also think of a complex number as a column vector:
\begin{align*}
c + id \sim \begin{pmatrix}
c\\d
\end{pmatrix}.
\end{align*}
Let us look back at complex number multiplication under matrix representation:
\begin{align*}
(a+ib)(c+id) = (ac-bd) + i(bc + ad) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
c\\d
\end{pmatrix}=
\begin{pmatrix}
ac-bd\\
bc +ad
\end{pmatrix}.
\end{align*}
Multiplication actually works in this ``mixed'' way of representing complex numbers as well. Now, observe that what we just did was performing a linear transformation on a vector in $\R^2$. It is always interesting to look at the geometrical interpretation of this transformation. To do this, let us call $N$ the ``normalized'' version of $M$:
\begin{align*}
N = \frac{1}{\sqrt{a^2 + b^2}}\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}.
\end{align*}
We immediately recognize that $N$ is an orthogonal matrix. This means $N$ is an orthogonal transformation (length preserving). Now, it is reasonable to define
\begin{align*}
\cos\theta &= \frac{a}{\sqrt{a^2 + b^2}}\\
\sin\theta &= \frac{b}{\sqrt{a^2+b^2}}.
\end{align*}
We can write $N$ as
\begin{align*}
N = \begin{pmatrix}
\cos\theta & -\sin\theta
\sin\theta & \cos\theta
\end{pmatrix},
\end{align*}
which is a physicists' favorite matrix: the rotation by $\theta$. So, let us write $M$ in terms of $N$:
\begin{align*}
M = \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix} = \sqrt{a^2 + b^2}N = \sqrt{a^2 + b^2}\begin{pmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{pmatrix}.
\end{align*}
We can interpret $M$ as a rotation by $\theta$, followed by a scaling by $\sqrt(a^2 + b^2)$. But what $\sqrt{a^2 + b^2}$ exactly is just the ``length'' or the ``amplitude'' of the complex number $a+ib$, if we think of it as an arrow in a plane. 
\subsection{Relevant properties and definitions}
\begin{enumerate}
	\item The \textit{modulus} of $z = a+ib$ is the ``amplitude'' of $z$, denoted by $\vert z \vert = \sqrt{a^2 + b^2} = z\bar{z}$. 
	\item The modulus is \textit{multiplicative}, i.e. 
	\begin{align*}
	\vert wz \vert = \vert w \vert \vert z \vert.
	\end{align*}
	\item Triangle inequality:
	\begin{align*}
	\vert z + w \vert \leq \vert z \vert + \vert w \vert.
	\end{align*}
	We can readily show this geometrically, or algebraically. 
	\item The \textit{argument} of $z = a+ib$ is $\theta$, where
	\begin{align*}
	\theta = 
	\begin{cases}
	\tan^{-1}\left( \frac{b}{a}\right), \text{ if } a > 0\\
	\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b > 0\\
	-\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b < 0\\
	\text{Undefined if } a=b=0.
	\end{cases}
	\end{align*} 
	\item The \textit{conjugate} of $a+ib$ is $a-ib$. Conjugation is \textit{additive} and \textit{multiplicative}, i.e.
	\begin{align*}
	\bar{z+w} &= \bar{z} + \bar{w}\\
	\bar{wz} &= \bar{w}\bar{z}.
	\end{align*}
	Note that we can also show the multiplicative property with the matrix representation as well:
	\begin{align*}
	\bar{wz} \sim (WZ)^\top = Z^\top W^\top \sim \bar{z}\bar{w} = \bar{w}\bar{z}.
	\end{align*}
	\item Euler's identity, generalized to de Moivre's formula:
	\begin{align*}
	z^n = r^ne^{in\theta}.
	\end{align*}
\end{enumerate}

\newpage
\section{Vector Spaces \& Linear Functions}
\subsection{Review of Linear Spaces and Subspaces}
\begin{prop} of linear spaces:
\begin{enumerate}
	\item Commutativity and associativity of addition
	\item Existence of an additively neutral element (null element). Zero multiples of elements give the null element: $0\cdot V = \mathbf{0}$\\
	\item Every element has an (unique) additively antipodal element
	\item Scalar multiplication distributes over addition
	\item Multiplicative identity: 
	\item $ab\cdot V = a\cdot(bV)$
	\item $(a+b)V = aV + bV$
\end{enumerate}
\end{prop}
$W$ is a subspace of $V$ if
\begin{enumerate}
	\item $S \subseteq V$ 
	\item $S$ is non-empty
	\item $S$ is closed under addition and scalar multiplication
\end{enumerate}
\begin{prop} that are interesting/important/maybe-not-so-obvious:
\begin{enumerate}
	\item If $S$ is a subspace of $V$ and $S \neq V$ then $S$ is a proper subspace of $V$.
	\item If $X$ in a subspace of $Y$ and $Y$ is a subspace of $Z$, then $X$ is a subspace of $W$.
	\item Non-trivial linear (non-singleton) spaces are infinite. 
\end{enumerate}
\end{prop}
\subsection{Review of Linear Maps}
Consider linear spaces $V$ and $W$ and elements $v \in V$ and $w \in W$ and scalars $\alpha, \beta \in \R$, a function $F : V \rightarrow W$ is a linear map if
\begin{align*}
F[\alpha v + \beta w] = \alpha F[v] + \beta F[w].
\end{align*}
\begin{prop}
	$\,$
\begin{enumerate}
	\item $F[\mathbf{0}_V] = \mathbf{0}_W$
	\item $G[w] = (\alpha \cdot F)[w] = \alpha\cdot F[w]$
	\item Given $F : V \rightarrow W$ and $G : V \rightarrow W$, $H[v] = F[v] + G[v] = (F+G)[v]$ is call the sum of the functions $F$ and $G$.
	\item Linear combinations of linear maps are linear.
	\item Compositions of linear maps are linear. 
	\item Compositions distributes over linear combinations of linear maps.
	\item Inverses of linear functions (if they exist) are linear.
	\item Inverse of a bijective linear function is a bijective linear function
\end{enumerate}
\end{prop}
\subsection{Review of Kernels and Images}
\begin{defn}
	Let $F : V \rightarrow W$ be given. The kernel of $F$ is defined as
	\begin{align*}
	\ker(F) = \{v\in V \vert F[v] = \mathbf{0}_W\}.
	\end{align*}
\end{defn}
\begin{prop}
	Let $F : V \rightarrow W$ a linear map be given. Also, consider a linear map $G$ such that $F\circ G$ is defined 
	\begin{enumerate}
		\item $F$ is null $\iff$ $\ker(F) = V \iff \Im(F) = \mathbf{0}_W $
		\item $\ker(F)$ is a subspace of $V$
		\item $\Im(F)$ is a subspace of $W$
		\item $F$ is injective $\iff \ker(F) = \mathbf{0}_V$
		\item $\ker(F) \subseteq \ker(F\circ G)$.
		\item $F$ injective $\implies \ker(F) = \ker(F\circ G)$
	\end{enumerate}
\end{prop}


\begin{defn}
	Let $F : V \rightarrow W$ be given. The image of $F$ is defined as
	\begin{align*}
	\Im(F) = \{w\in W \vert \exists v\in V, F[v] = w \}. 
	\end{align*}
\end{defn}
\subsection{Atrices}
\begin{defn}
	\textbf{Atrix functions:} Let $V_1,\dots,V_m \in \mathbf{V}$ be given. Consider $f : \R^m \rightarrow \mathbf{V}$ be defined by
	\begin{align*}
	f\begin{pmatrix}
	a_1\\
	a_2\\
	\vdots\\
	a_m
	\end{pmatrix}
	=
	\sum_{i=1}^m a_i V_i.
	\end{align*} 
	We denote $f$ by
	\begin{align*}
	\begin{pmatrix}
	V_1 & V_2 &\dots& V_m
	\end{pmatrix}.
	\end{align*}
	We refer to the $V_i$'s as the \textbf{columns} of $f$, even though there doesn't have to be any columns. Basically, $f$ is simply a function that takes in an ordered list of coefficients and returns a linear combination of $V_i$ with the respective coefficients. A matrix is a special atrix. Not every atrix is a matrix. 
\end{defn}
\begin{prop} of atrices
	$\,$
	\begin{enumerate}
	\item The $V_i$'s - the columns of an atrix - are the images of the standard basis tuples. 
	\item $\mathbf{e}_j \in \ker(V_1\dots V_m) \iff V_j = \mathbf{0}_V$, where $\mathbf{e}_j$ denotes a standard basis tuple with a 1 at the j$^\text{th}$ position. To put in words, a kernel of an atrix contains a standard basis if and only if one of its columns in a null element. 
	\item $\Im(f) \equiv \Im(V_1\dots V_m) = \xpan(V_1\dots V_m)$
	\item $B$ is a null atrix $\iff \ker(B) = \R^m \iff \Im(B) = \mathbf{0}_V \iff V_j = \mathbf{0}_V \forall j=1,2,\dots,m$.
	\item  $f$ is a linear function $\R^m \rightarrow V \iff f$ is an atrix function $\R^m \rightarrow V$.
	\item An atrix $A$ is bijective/invertible, then its inverse $A^{-1}$ is a linear function, but is an atrix only if $A$ is a matrix. 
	\item Linear combinations of atrices are atrices:
	\begin{align*}
	\alpha \cdot\begin{pmatrix}
	V_1 & \cdots & V_m
	\end{pmatrix}
	+ 
	\beta \cdot \begin{pmatrix}
	W_1 & \dots & W_m
	\end{pmatrix}
	\\= \begin{pmatrix}
	\alpha V_1 + \beta W_1 &\dots& \alpha V_m + \beta W_m
	\end{pmatrix}
	\end{align*}  
	\item Compositions of two atrices are NOT defined unless the atrix going first is a matrix. Consider $F : \R^m \rightarrow W$ and $G : \R^n \rightarrow T$. $F\circ G$ is only defined if $T = \R^m$. This make $G$ an $n\times m$ matrix. It follows that the atrix $F\circ G$ has the form
	\begin{align*}
	\begin{pmatrix}
	f(g_1)&f(g_2)&\dots&f(g_m)
	\end{pmatrix}.
	\end{align*}
	\item Consider $F : \R^m \rightarrow V$ and $G : \R^k \rightarrow V$. $\Im(F) \subseteq \Im(G) \iff F = G \circ C$, with $C \in \mathbb{M}_{k\times m}$, i.e. $C$ is an $k \times m$ matrix. 
	\item Consider an atrix $A : \R^m \rightarrow V$. $A = (V_1\dots V_m)$. $A$ is NOT injective.\\
	$\iff \exists$ a non-trivial linear combination of the columns of $A$ that gives $\mathbf{0}_V$\\
	$\iff$ $A = [\mathbf{0}_v]$ or $\exists j \vert V_j$ is linear combination of other columns of $A$\\
	$\iff$ The first column of $A$ is $\mathbf{0}_V$ or $\exists j \vert V_j$ is a linear combination some of $V_i, i < j$.
	\item If atrix $A: \R \rightarrow V$ has a single column then it is injective if the column is not $\mathbf{0}_V$.
	\end{enumerate}
\end{prop}
\begin{prop}
	of elementary column operations for atrices. Elementary operations on the columns of $F$ can be expressed as a composition of $F$ and an appropriate elementary matrix $E$, $F\circ E$. 
	\begin{enumerate}
		\item Swapping $i^{th}$ and $j^{th}$ columns: $F\circ E^{[i]\leftrightarrow[j]}$.
		\item Scaling the $j^{th}$ column by $\alpha$: $F\circ E^{\alpha\cdot[j]}$.
		\item Adjust the $j^{th}$ column by adding to it $\alpha\times i^{th}$ column: $F\circ E^{[i]\stackrel{+}\leftarrow \alpha\cdot[j]}$.
		\item Elementary column operations do not change the 'jectivity nor image of $F$.
		\item If a column of $F$ is a linear combination of some of the other columns then elementary column operations can turn it into $\mathbf{0}_V$.
		\item Removing/Inserting null columns or columns that are linear combinations of other columns does not change the image of $F$.
		\item Given atrix $A$, it is possible to eliminate (or not) columns of $A$ to end up with an atrix $B$ with $\Im(B) = \Im(A)$.
		\item If $B$ is obtained from insertion of columns into atrix $A$, then $\Im(B) = \Im(A) \iff$ the insert columns $\in \Im(A)$.
		\item Inserting columns to a surjective $A$ does not destroy surjectivity of $A$. ($A$ is already having extra or just enough columns) 
		\item $A$ surjective $\iff$ $A$ is obtained by inserting columns (or not) into an invertible atrix $\iff$ deleting some columns of $A$ (or not) gives an invertible matrix.
		\item If $A$ is injective, then column deletion does not destroy injectivity. ($A$ is already ``lacking'' or having just enough columns)
		\item The new atrix obtained from inserting columns from $\Im(A)$ into $A$ is injective $\iff$ $A$ is injective.
	\end{enumerate}
\end{prop}
\subsection{Linear Independence, Span, and Bases}
\subsubsection{Linear Independence}
$\,\,\,\,\,\,\,\,\,\,\,\,\,\,X_1\dots X_m$ are linearly independent\\
$\iff F = [X_1 \dots X_m]$ injective\\
$\iff \sum a_iX_i = 0 \iff a_i = 0 \forall i$\\
$\iff \mathbf{0}_V \notin \{X_i\}$ and none are linear combinations of some of the others.\\
$\iff X_1 \neq \mathbf{0}_V$ and $X_j$ is not a linear combination of any of $X_i$'s for $i < j$. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item The singleton list is linearly independent if its entry is not the null element
		\item Sublists of a linearly independent list are linearly independent
		\item List operations cannot create/destroy linearly independence. 
		\item If a linear map $L : X \rightarrow W$ injective, then $X_1\dots X_m$ linearly independent $\iff L(X_1)\dots L(X_m)$ linearly independent.  
	\end{enumerate}
\end{prop}
\subsubsection{Span}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Spans are subspaces. $\xpan(X_1\dots X_m), X_j \in V$ is a subspace of $V$.
		\item $\xpan(X_1\dots X_j) \subseteq \xpan(X_1\dots X_k)$ if $j\leq k$.
		\item Adding elements to a list that spans $V$ produces a list that spans $V$.
		\item The following list operations do not change the span of $V$: removing the null/linearly dependent element, inserting a linearly dependent element, scaling element(s), adding (multiples) of an element to another element.
		\item It is possible to reduce a list that spans to a list that spans AND have linearly independent elements.
		\item Consider $A : V \rightarrow W$. If $X_i$ span $W$ then $A(X_i)$ span $\Im(A)$. $i=1,2,\dots,m$.
		\item If $A : V \rightarrow W$ invertible, then $X_i$ span $V \iff A(X_i)$ span $W$, $i=1,2,\dots,m$. 
	\end{enumerate}
\end{prop}
\subsubsection{Bases}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A list is a basis of $V$ if the elements are linearly independent and they span $V$.
		\item A singleton linear space has no basis.
		\item Re-ordering the elements of a basis gives another basis.
		\item $\{X_1\dots X_m\}$ is a basis of $V$ if $(X_1\dots X_m)$ is injective AND $\Im(X_1\dots X_m) = V$, i.e. $(X_1\dots X_m)$ invertible.
		\item If $\{X_i\}$ is a basis of $V$ and $\{Y_i \}$ is a list of elements in $W$, then there exists a unique linear function $L : V \rightarrow W$ satisfying 
		\begin{align*}
		L[X_i] = Y_i
		\end{align*} 
		\item If $A: V \rightarrow W$ bijective, then $\{ V_i \}$ forms a basis of $V$ and $\{A(X_i) \}$ forms a basis of $W$.
		\item Elementary operations on bases give bases.  
	\end{enumerate}
\end{prop}
\subsection{Linear Bijections and Isomorphisms}
\begin{defn}
	Let linear spaces $V, W$ be given. $V$ is \textbf{isomorphic} to $W$ if $\exists F : V \rightarrow W$ bijective. We say $V \sim W$. 
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A non-zero scalar multiple of an isomorphism is an isomorphism.
		\item A composition of isomorphism is an isomorphism.
		\item ``Isomorphism'' behaves like an equivalence relation:
		\begin{enumerate}
			\item Reflexivity: $V \sim V$.
			\item Symmetry: if $V\sim W$ then $W\sim V$.
			\item Transitivity: if $V\sim W$ and $W \sim Z$ then $V\sim Z$.
		\end{enumerate}
		\item Consider $F : V \rightarrow W$ an isomorphism.
			\begin{enumerate}
				\item Isomorphisms preserve linear independence. $V_i$'s are linearly independent in $V$ $\iff$ $F(V_i)$'s are linearly independent in $W$.
				\item isomorphisms preserve spanning. $V_i$'s span $V$ $\iff$ $F(V_i)$'s span $W$.
				\item Isomorphisms preserve bases. $\{V_i\}$ is a basis of $V$ $\iff$ $F\{(V_i)\}$ is a basis of $W$.
			\end{enumerate}
		\item If $V\sim W$ then $\dim(V) = \dim(W)$ (finite or infinite).
		\item If a linear map $A : V \rightarrow W$ is given and $A(X_i)$'s are linearly independent, then $A_i$'s are linearly independent.  
		\item If a linear map $A : V \rightarrow W$ is injective and $\{ X_i\}$ is a basis of $V$ then $\{ A(X_i)\}$ is a basis of $\Im(A)$
	\end{enumerate}
\end{prop}
\subsection{Finite-Dimensional Linear Spaces}
\subsubsection{Dimension}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item $\R^m \sim R^n \iff m=n$.
		\item Isomorphisms $F : \R^n \rightarrow V$ are bijective atrices.
		\item Isomorphisms $G : W \rightarrow \R^m$ are inverses of bijective atrices.
		\item Consider a non-singleton linear space $V$
		\begin{enumerate}
			\item $V$ has a basis with $n$ elements.
			\item $V \sim \R^n$.
			\item $V \sim W$, where $W$ is any linear space with a basis of $n$ elements.
		\end{enumerate}
		\item If $V$ is a linear space with a basis with $n$ elements, then any basis of $V$ has $n$ elements.
		\item Linear space $V \sim W$ where $W$ is $n$-dimensional if $V$ is $n$-dimensional.
		\item For a non-singleton linear space $V$, $V\sim \R^n \iff \dim(V) = n$.
		\item $V \sim W \iff \dim(V) = \dim(W)$.
		\item If $W$ is a subspace of $V$, then $\dim(W)\leq \dim(V)$. Equality holds when $W = V$.
	\end{enumerate}
\end{prop}
\subsubsection{Rank-Nullity Theorem}
Let finite-dimensional linear space $V$ and linear map $F : V \rightarrow W$ be given. Then $\Im(F)$ is finite-dimensional and 
\begin{align*}
\dim(\Im(F)) + \dim(\ker(F)) = \dim(V)
\end{align*}

A stronger statement: If a linear map $F : V\rightarrow W$ has finite rank and finite nullity $\iff$ $V$ is finite-dimensional, then
\begin{align*}
\Im(F) + \ker(F) = \dim(V).
\end{align*}

\subsection{Infinite-Dimensional Spaces}
Consider a non-singleton linear space $V$. The following statements are equivalent:
\begin{enumerate}
	\item $V$ is infinite-dimensional.
	\item Every linearly independent list in $V$ can be enlarged to a strictly longer linearly independent set in $V$.
	\item Every linearly independent list in $V$ can be enlarged to an arbitrarily long (finite) linearly independent set in $V$.
	\item There are arbitrarily long (finite) linearly independent lists in $V$.
	\item There are linearly independent lists in $V$ of any (finite) length.
	\item No list of finitely many elements of $V$ spans $V$.
\end{enumerate}
\newpage
\section{Sums of Subspaces \& Products of vector spaces}
\subsection{Direct Sums}
\begin{defn}
	Let $U_j$, $j=1,2,\dots m$ are subspaces of $V$. $\sum_1^m U_j$ is a \textit{direct sum} if each $u \in \sum U_j$ can be written in only one way as $u = \sum_1^m u_j$. The direct sum $\sum^m_i U_j$ is denoted as $U_1 \oplus\dots\oplus U_m$.
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Condition for direct sum: If all $U_j$ are subspaces of $V$, then $\sum_1^m U_j$ is a direct sum $\iff$ the only way to write 0 as $\sum_1^m u_j$, where $u_j \in U_j$ is to take $u_j = 0$ for all $j$.  
		\item If $U, W$ are subspaces of $V$ and $U \bigcap W = \{ 0\}$ then $U+W$ is a direct sum. 
	\end{enumerate}
\end{prop}


\subsection{Products of Vector Spaces}
\begin{defn}
	Product of vectors spaces
	\begin{align*}
	V_1 \times\dots\times V_m = \{ (v_1,\dots,v_m): v_j \in V_j, j=1,2,\dots,m\}.
	\end{align*}
\end{defn}
\begin{defn}
	Addition on $V_1 \times\dots\times V_m$:
	\begin{align*}
	(u_1,\dots,u_m) + (v_1,\dots,v_m) = (u_1+v_1,\dots,v_m+u_m).
	\end{align*}
\end{defn}
\begin{defn}
	Scalar multiplication on $V_1 \times\dots\times V_m$:
	\begin{align*}
	\lambda(v_1,\dots,v_m) = (\lambda v_1,\dots,\lambda v_m).
	\end{align*}
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Product of vectors spaces is a vector space.
		\begin{align*}
		V_j \text{ are vectors spaces over } \F \implies V_1\times\dots\times V_m \text{ is a vector space over } \F.
		\end{align*}
		\item Dimension of a product is the sum of dimensions: 
		\begin{align*}
		\dim(V_1\times\dots\times V_m) = \sum_1^m\dim(V_j)
		\end{align*}
		\item Vector space products are NOT commutative:
		\begin{align*}
		W\times V \neq V\times W.
		\end{align*}
		However, 
		\begin{align*}
		V\times W \sim W \times V.
		\end{align*}
		\item Vector space products are NOT associative:
		\begin{align*}
		V\times(W\times Z) \neq (V\times W)\times Z
		\end{align*}
	\end{enumerate}
\end{prop}
\subsection{Products \& Direct Sums}
\begin{prop}
	$\,$
	\begin{enumerate}
	\item Let $U_1,\dots,U_m$ be subspaces of $V$. Define a linear map $\Gamma : U_1\times\dots\times U_m \rightarrow U_1 + \dots + U_m$ by:
	\begin{align*}
	\Gamma(u_1,\dots,u_m) = \sum_1^m u_j.
	\end{align*}
	$U_1 + \dots + U_m$ is a direct sum $\iff \Gamma$ is injective. 
	\item Let $U_j$ be finite-dimensional and are subspaces of $V$. 
	\begin{align*}
	U_1 \oplus \dots\oplus U_m \iff \dim(U_1+\dots+U_m) = \sum_1^m\dim(U_j)
	\end{align*}
	\end{enumerate}
\end{prop}

\subsection{Rank-Nullity Theorem}
Suppose $Z_1$ and $Z_2$ are subspaces of a finite-dimensional vector space $W$. Consider $z_1\in Z_1$, $z_2\in Z_2$, and a function $\phi : Z_1 \times Z_2 \to Z_1 + Z_2 \prec W$ defined by
\begin{align*}
\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
=
z_1 + z_2.
\end{align*}
First, $\phi$ is a linear function, as it satisfies the linearity condition:
\begin{align*}
\phi\left(\alpha
\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
+
\beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}\right) = \alpha\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix} + \beta \phi \beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}.
\end{align*}
By rank-nullity theorem,
\begin{align*}
\dim(Z_1\times Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi)). 
\end{align*}
But this is equivalent to
\begin{align*}
\dim(Z_1) + \dim(Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi))
\end{align*}
The kernel of $\phi$ is:
\begin{align*}
\ker(\phi) = 
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1, z\in Z_2
\right\}
=
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1 \cap Z_2
\right\}
\end{align*}
We can readily verify that $Z_1 \cap Z_2$ is a subspace of $W$. With this, $\dim(\ker(\phi)) = \dim(Z_1\cap Z_2)$. So we end up with
\begin{align*}
\dim(Z_1 + Z_2) = \dim(Z_1) + \dim(Z_2) - \dim(Z_1\cap Z_2).
\end{align*}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item When $Z_1\cap Z_2$ is trivial, then $Z_1 + Z_2$ is direct. 
		\item When $\dim(\ker(\phi)) = 0$, $\phi$ is injective. But $\phi$ is also surjective by definition, this implies $\phi$ is a bijection, in which case
		\begin{align*}
		Z_1 \oplus Z_2 \sim Z_1 + Z_2.
		\end{align*} 
	\end{enumerate}
\end{prop}


\subsection{Nullspaces \& Ranges of Operator Powers}

\begin{enumerate}
	\item Sequence of increasing null spaces: Suppose $T \in \lag(V)$, i.e., $T$ is some linear function mapping $V\to V$, then
	\begin{align*}
	\{ 0 \} = \ker(T^0) \subset \ker(T^1) \subset \ker(T^2) \subset \dots \subset \ker(T^k) \subset \ker(T^{k+1}) \subset\dots.
	\end{align*} 
	
	\begin{proof}[Proof Outline]
		Let $k$ be a nonnegative integer and $v\in \ker(T^k)$. Then $T^kv = 0$, so $T^{k+1}v = T(T^kv) = T(0) = 0$, so $v\in \ker T^{k+1}$. So $\ker(T^k) \subset \ker(T^{k+1})$. 
	\end{proof}
	\item Equality in the sequence of null spaces: Suppose $m$ is a nonnegative integer such that $\ker(T^m) = \ker(T^{m+1})$, then
	\begin{align*}
	\ker(T^m) = \ker(T^{m+1}) = \ker(T^{m+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		We want to show
		\begin{align*}
		\ker(T^{m+k}) = \null(T^{m+k+1}).
		\end{align*}
		We know that $\ker T^{m+k} \subset \ker T^{m+k+1}$. Suppose $v\in \ker T^{m+k+1}$, then
		\begin{align*}
		T^{m+1}(T^kv) = T^{m+k+1}v = 0.
		\end{align*}
		So
		\begin{align*}
		T^kv \in \ker T^{m+1} = \ker T^m.
		\end{align*}
		So
		\begin{align*}
		0 = T^m(T^kv) = T^{m+k}v,
		\end{align*}
		i.e., $v\in \ker T^{m+k}$. So $\ker T^{m+k+1} \subset \ker T^{m+k}$. This completes the proof. 
	\end{proof}
	\item Null spaces stop growing: If $n = \dim(V)$, then
	\begin{align*}
	\ker(T^n) = \ker(T^{n+1}) = \ker(T^{n+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n = \ker T^{n+1}.
		\end{align*}
		Suppose this is not true. Then the dimension of the kernel has to increase by at least 1 every step until $n+1$. Thus $\dim \ker T^{n+1} \geq n+1 > n = \dim(V)$. This is a contradiction. 
	\end{proof}
	\item $V$ is the direct sum of $\ker(T^{\dim(V)})$ and $\Im(T^{\dim(V)})$: If $n = \dim(V)$, then
	\begin{align*}
	V = \ker(T^n) \oplus \Im(T^n).
	\end{align*}
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n \cap \Im T^n = \{0 \}. 
		\end{align*}
		Suppose $v\in \ker T^n \cap \Im T^n$. Then $T^n v = 0$ and $\exists u\in V$ such that $v = T^n u$. So
		\begin{align*}
		T^n v = T^{2n}u = 0.
		\end{align*}
		So
		\begin{align*}
		T^n u = 0.
		\end{align*}
		But this means $v = 0$. 
	\end{proof}
	\item IT IS NOT TRUE THAT $V = \ker(T) \oplus \Im(T)$ in general. 
\end{enumerate}

\subsection{Generalized Eigenvectors and Eigenspaces}
\begin{defn}
	Suppose $T\in \lag(V)$ and $\Lambda$ is an eigenvalue of $T$. A vector $v\in V$ is called a \textbf{generalized eigenvector} of $T$ corresponding to $\lambda$ if $v\neq 0$ and
	\begin{align*}
	(T-\lambda I)^j v = 0
	\end{align*}
	for some positive integer $j$. 
\end{defn}

\begin{defn}
	\textbf{Generalized Eigenspace}: Suppose $T \in \lag(V)$ and $\lambda \in \mathbf{F}$. The \textbf{generalized eigenspace} of $T$ corresponding to $\lambda$, denoted $G(\lambda,T)$, is defined to be the set of all generalized eigenvectors of $T$ corresponding to $\lambda$, along with the 0 vector.
\end{defn}

\begin{prop}
	\begin{enumerate}
		\item Suppose $T\in\lag(V)$ and $\lambda \in \mathbf{F}$. Then 
		\begin{align*}
		G(\lambda,T) = \ker(T-\lambda I)^{\dim(V)}.
		\end{align*}
		\begin{proof}[Proof Outline]
			Suppose $v\in \ker (T_\lambda I)^{\dim(V)}$. Then $v\in G(\lambda,T)$. So, $\ker(T-\lambda I)^{\dim V} \subset G(\lambda,T)$. Next, suppose $v\in G(\lambda,T)$. Then these is a positive integer $j$ such that
			\begin{align*}
			v\ in \ker (T-\lambda I)^j.
			\end{align*}
			But if this is true, then 
			\begin{align*}
			v\ in \ker (T-\lambda I)^{\dim V},
			\end{align*}
			since $\ker(T-\lambda I)^{\dim V}$ is the largest possible kernel, in a sense. 
		\end{proof}
		\item Linearly independent generalized eigenvectors: Let $T\in\lag(V)$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $v_1,\dots,v_m$ are corresponding generalized eigenvectors. Then $v_1,\dots,v_m$ is linearly independent.
	\end{enumerate}
\end{prop}

\subsection{Nilpotent Operators}
\begin{defn}
	An operator is called \textbf{nilpotent} if some power of it equals 0.
\end{defn}
\begin{prop}
	\item Nilpotent operator raised to dimension of domain is 0: Suppose $N\in \lag(V)$ is nilpotent. Then \begin{align*}
	N^{\dim(V)} = 0.
	\end{align*}
	\item Matrix of a nilpotent operator: Suppose $N$ is a nilpotent operator on $V$. Then there is a basis of $V$ with espect to which the matrix of $N$ has the form
	\begin{align*}
	\begin{pmatrix}
	0 & & *\\
	&\ddots&\\
	0&&0
	\end{pmatrix};
	\end{align*}
	here all entries on and below the diagonal are 0's. 
\end{prop}

\subsection{Weyr Characteristic}

\newpage
\section{Idempotents \& Resolutions of Identity}




\newpage
\section{Block-representations of operators}
\subsection{Direct sums of operators}
\subsection{Coordinatization \& matricial representation of linear functions}
Consider a finite-dimensional linear space $V$ with basis $\{V_i \}$, $i=1,2,\dots,m$. An element $\tilde{V}$ in $V$ can be expressed in exactly one way:
\begin{align*}
\tilde{V} = \sum_{i=1}^m a_iV_i,
\end{align*}
where $\{ a_i\}$ is unique. We call $\{ a_i \}$ the coordinate tuple of $\tilde{V}$ and $a_i$'s the coordinates. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Inverse of a bijective atrix outputs the coordinates. Suppose $A = [V_i]$. Then
		\begin{align*}
		\tilde{V} = \sum_{i=1}^ma_iV_i \iff A^{-1}(Z) = \begin{pmatrix}
		a_1&\dots&a_m
		\end{pmatrix}^\top
		\end{align*}
	\end{enumerate}
\end{prop}
\subsection{Equality of rank and trace for idempotents; Resolution of Identity Revisited}



\newpage
\section{Invariant subspaces}
\subsection{Reducing subspaces}
\newpage 
\section{Polynomials applied to operators}
\subsection{Minimal polynomials of block-$\Delta^r$ operators}
\subsection{Minimal polynomials at a vector}
\newpage 
\section{Eigentheory}
\subsection{Spectral Mapping Theorem}
\newpage 
\section{Triangularization}
\subsection{Compression to invariant subspaces}
\subsection{Simultaneously $\Delta$-ity of commuting families}
\newpage 
\section{Diagonalization}
\subsection{Spectral resolutions}
\subsection{Compressions to reducing subspaces}
\subsection{Simultaneous diagonalizability for commuting families}
\newpage
\section{Primary decomposition over $\mathbb{C}$ and generalized eigenspaces}
\newpage 
\section{Cyclic decomposition and Jordan form}
\subsection{Square roots of operators}
\subsection{Similarity of a matrix and its transpose}
\subsection{Similarity of a matrix and its conjugate}
\subsection{Jordan forms of $\mathcal{AB}$ and $\mathcal{BA}$}
\subsection{Power-convergent operators}
\subsection{Power-bounded operators}
\subsection{Row-stochastic matrices}
\newpage 
\section{Determinant \& Trace}
\subsection{Classical adjoints}
\subsection{Cayley-Hamilton theorem}

\newpage

\section{Inner products and norms} 
\newpage 
\subsection{Riesz representation theorem}
\subsection{Adjoints}
\subsection{Grammians}
\subsection{Orthogonal complements and orthogonal decompositions}
\subsection{Ortho-projections}
\subsection{Closest point solutions}
\subsection{Gram-Schmidt and orthonormal bases}

\newpage

\section{Isometries and unitary operators}
\newpage 

\section{Ortho-triangularization}
\newpage 

\section{Spectral resolutions}
\newpage 

\section{Ortho-diagonalization; self-adjoint and normal operators; spectral theorems}
\newpage 

\section{Positive (semi-)definite operators}
\subsection{Classification of inner products}
\subsection{Positive square roots}
\newpage 

\section{Polar decomposition}
\newpage 

\section{Single value decomposition}
\subsection{Spectral/operator norm}
\subsection{Singular values and approximation}
\subsection{Singular values and eigenvalues}







































\newpage
\section{Problems and Solutions}
\subsection{Problem set 1}
\begin{prob*} Prelim. 1.1.
	Suppose that $\V$ is a finite-dimensional vector space. Consider the following subspace $\W$ of the vector space $\V\times \V$:
	\begin{align*}
	\W = \left\{ \begin{pmatrix}
	v\\-v
	\end{pmatrix}\bigg\vert\, v \in \V \right\}.
	\end{align*}
	Argue that
	\begin{align*}
	\dim(\V) = \dim(\W).
	\end{align*}
	\begin{sln*}
		$\,$\\\\
		We can equivalently argue that $\dim(V) = \dim(W)$ by showing $\V$ is isomorphic to $\W$, i.e., there exists a bijective linear function $\lag : \V \rightarrow \W \prec \V\times\V $.\\
		
		\noindent Let $\lag : \V \rightarrow \W$ be defined as 
		\begin{align*}
		\lag[v] = \begin{pmatrix}
		v\\-v
		\end{pmatrix},
		\end{align*} 
		where $v\in \V$. Let $v_1,v_2\in\V$ and $a,b \in \mathbb{C}$ be given, $\lag$ is a linear function$^{\dagger}$ because it satisfies the linearity condition because
		\begin{align*}
		\lag[av_1 + bv_2] = \begin{pmatrix}
		av_1 + bv_2\\
		-(av_1 + bv_2)
		\end{pmatrix}
		= 
		a\begin{pmatrix}
		v_1\\-v_1
		\end{pmatrix}+
		b\begin{pmatrix}
		v_2\\-v_2
		\end{pmatrix}
		=
		a\lag[v_1] + b\lag[v_2],
		\end{align*}
		where the second equality follows from the fact that $\W$ is a subspace.\\
		
		\noindent Now, $\lag$ is surjective$^{\Delta}$, by definition:
		\begin{align*}
		\Im(\lag) = \left\{ \begin{pmatrix}
		v\\-v
		\end{pmatrix}\bigg\vert\, v \in \V \right\} = \W.
		\end{align*}
		$\lag$ is also injective$^{\square}$, since $\ker(\lag) = \mathbf{0}_V$:
		\begin{align*}
		\lag[v] = \mathbf{0}_\W \iff 
		\begin{pmatrix}
		v\\-v
		\end{pmatrix}
		= \mathbf{0}_\W
		=
		\begin{pmatrix}
		\mathbf{0}_\V\\
		\mathbf{0}_\V
		\end{pmatrix}
		\iff v = \mathbf{0}_\V.
		\end{align*}
		By ${\dagger}, {\Delta}$, and ${\square}$, $\lag$ is an isomorphism. It follows that $\V \sim \W \iff \dim(V) = \dim(W)$.
	\end{sln*}
\end{prob*} 

\newpage





\begin{prob*} Prelim. 1.2.
	Argue that $\V + \{ \mathbf{0}_\W \} = \V$, for any subspace $\V$ of a vector space $\W$. 
	\begin{sln*}
		$\,$\\\\
		We shall first argue that $\mathbf{0}_\W = \mathbf{0}_\V$. Let $v_+ \in \V$ be given, and let the unique additive antipodal element of $v_+$ in $\V$ be $v_-$. Because $\V$ is a subspace of $\W$, $v_+\in \W$ and $v_- \in \W$,
		\begin{align*}
		\mathbf{0}_\V = v_+ \stackrel{\tiny\V}{+} v_- = v_+ \stackrel{\tiny\W}{+} v_- = \mathbf{0}_\W.
		\end{align*}
		Therefore, $\{\mathbf{0}_\W \} = \{ \mathbf{0}_\V\}$. It follows that 
		\begin{align}
		\V + \{ \mathbf{0}_\W\} = \V + \{\mathbf{0}_\V \}. 
		\end{align}
		Now, it suffices to show: (i) $\V \subseteq \V + \{ \mathbf{0}_\V\}$, and (ii) $\V + \{\mathbf{0}_\V\}\subseteq \V$.\\
		
		\noindent Let $v \in \V$ be given, then $v \in \V + \{\mathbf{0}_\V \}$, so $\V \subseteq \V + \{\mathbf{0}_\V \}$. But because both $\V$ and $\V + \{\mathbf{0}_\V \}$ are subspaces, $\V$ is a subspace of $\V + \{\mathbf{0}_\V \}$. Hence, (i) is true.\\
		
		\noindent Consider $v_0 \in \V + \{\mathbf{0}_\V \}$, then there are some $v_\V \in \V$ and $v_\mathbf{0} \in \{\mathbf{0}_\V\}$ such that
		\begin{align*}
		v_0 = v_\V + v_\mathbf{0}. 
		\end{align*} 
		But $v_\mathbf{0}$ is identically $\mathbf{0}_\V$ since $v_\mathbf{0} \in \{\mathbf{0}_\V \}$, so
		\begin{align*}
		v_0 = v_\V + \mathbf{0}_\V = v_\V \in \V, 
		\end{align*}
		which implies $\V+\{ \mathbf{0}_\V\} \subseteq \V$. Therefore $\V+\{ \mathbf{0}_\V\}$ is a subspace of $\V$ because they are subspaces. Hence, (ii) is true. \\
		
		\noindent Therefore, by Eq. (1), facts (i), and (ii), 
		\begin{align*}
		\V + \{\mathbf{0}_\V\} = \V.
		\end{align*}
	\end{sln*}
\end{prob*}


\newpage





\begin{prob*} \textbf{1. Equivalent formulations of directness of a subspace sum:} Suppose that $\V_1, \V_2\,\dots,\V_{315}$ are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum $\V_1 + \V_2 + \dots + \V_{315}$ is direct. 
		\item If $x_i \in \V_i$ and $x_1 + x_2 + \dots + x_{315} = \mathbf{0}_\W$, then $x_i = \mathbf{0}_\W$, for every $i$.
		\item If $x_i, y_i \in \V_i$ and $x_1 + x_2 + x_3 + \dots + x_{315} = y_1 + y_2 + y_3 + \dots + y_{315}$, then $x_i = y_i$, for every $i$.
		\item For any $i$, no non-null element of $\V_i$ can be express as a sum of the elements of the other $V_j$'s.
		\item For any $i$, no non-null element of $\V_i$ can be expressed as a sum of the elements of the preceding $V_j$'s. 
	\end{enumerate}
	\begin{sln*}
		It suffices to show $[1] \implies [2]\implies [3]\implies [1]$ and $[2]\iff[4]\iff[5]$.
		\begin{itemize}
			\item $[1]$ implies $[2]:$ If $\V_1 + \V_2 + \dots + \V_{315}$ is direct, then by definition there is a unique way to write 
			\begin{align*}
			\mathbf{0}_\W = x_1 + x_2 + \dots + x_{315},
			\end{align*}
			where $x_i \in \V_i$. Since taking $x_i = \mathbf{0}_\W$ for every $i$ is one such way, it is also \textit{the only way} to express $\mathbf{0}_\W$ as $x_1 + x_2 + \dots + x_{315}$, $x_i \in \V_i$. Therefore $[1]$ implies $[2]$.
			
			\item $[2]$ implies $[3]:$ Let $x_i,y_i\in \V_i$ be given such that
			\begin{align*}
			x_1 + x_2 + x_3 + \dots + x_{315} = y_1 + y_2 + y_3 + \dots + y_{315}.
			\end{align*}
			Then rearranging gives
			\begin{align*}
			(x_1 - y_1) + (x_2 - y_2) + (x_3 - y_3) + \dots + (x_{315} - y_{315}) =\mathbf{0}_\W.
			\end{align*}
			By $[2]$, $x_i - y_i = \mathbf{0}_\W$ for every $i$, i.e., $x_i = y_i$ for every $i$. Therefore, $[2]$ implies $[3]$. 
			
			\item $[3]$ implies $[1]:$ $[3]$ implies that any expression of $h \in \V$ as $x_1 + x_2 + \dots + x_{315}$ is unique, which implies $\V_1 + \V_2 + \dots + \V_{315}$ is direct by definition.
			
			\item $[4]$ implies $[2]$: Suppose $[2]$ is false. Without loss of generality, suppose that for $x_i \in \V_i$ and $x_1 \neq \mathbf{0}_\W$,
			\begin{align*}
			x_1 + x_2 + \dots + x_{315} = \mathbf{0}_\W
			\end{align*}
			still holds. It follows that
			\begin{align*}
			x_1 = -(x_2 + x_3+\dots + x_{315})\neq \mathbf{0}_\W,
			\end{align*}
			which implies $[4]$ is false. By contraposition, $[4]$ implies $[2]$.
			
			\item $[2]$ implies $[4]:$ Suppose $[4]$ is false. Without loss of generality, suppose that $x_1 = -\sum_{j=2}^{315}x_j \neq \mathbf{0}_\W$, where no $x_j \in \V_j$ is necessarily non-null. Then 
			\begin{align*}
			x_1 + x_2 + \dots x_{315} = -\sum_{j=2}^{315}x_j + \sum_{j=2}^{315}x_j  = \mathbf{0}_\W.
			\end{align*}
			But $x_1 \neq \mathbf{0}_\W$ implies $[2]$ is false. By contraposition, $[2]$ implies $[4]$.
			\item $[4]$ and $[5]$ are equivalent: $[4]$ implies $[5]$ evidently since any ``sum of the elements of the preceding $\V_j$'s'' is a ``sum of the elements of the other $\V_j$'s.'' Conversely, the non-existence of the former (one that expresses an $x_i\in\V_i$) implies the non-existence of the latter, so $[5]$ implies $[4]$. 
		\end{itemize}
	\end{sln*}
\end{prob*}

\newpage

\begin{prob*} \textbf{2.} \textbf{Sub-sums of direct sums are direct: }\\
	Suppose that $\V_1, \V_2,\dots, \V_{315}$ are subspaces of a vector space $\W$, and the subspace sum $\V_1+\V_2+\dots+\V_{315}$ is direct.
	\begin{enumerate}
		\item Suppose that for each $i$, $\Z_i$ is a subspace of $\V_i$. Argue that the subspace sum $\Z_1 + \Z_2 + \dots \Z_{315}$ is direct. 
		\item Argue that the sum $\V_2 + \V_5 + \V_7 + \V_{12}$ is also direct. 
	\end{enumerate}
	\begin{sln*}
		$\,$
		\begin{enumerate}
			\item By Problem 1., because $\V_1+\V_2+\dots+\V_{315}$ is direct, for any $i$, no non-null element of $\V_i$ can be expressed as a sum of the elements of the other $\V_j$'s. Since $\Z_i$ is a subspace of $\V_i$, no $z_i\in \Z_i \prec \V_i$, $z_i \neq \mathbf{0}_\W $, can be expressed as a sum of the elements of the other $\Z_j$'s $\prec \V_j$'s. Hence, the subspace sum $\Z_1 + \Z_2 + \dots \Z_{315}$ is also direct.
			\item This is true by $[1]$. Since each $\V_j$ is a subspace of itself, the subspace sum of distinct $\V_j$'s is direct. Therefore, the subspace sum $\V_2 + \V_5 + \V_7 + \V_{12}$ is direct.
		\end{enumerate}
	\end{sln*}
\end{prob*}

\newpage

\begin{prob*} \textbf{3.} \textbf{Associativity of directness of subspace sums:} Suppose that 
	\begin{align*}
	\Y, \V_1, \V_2, \dots, \V_5, \U_1, \U_2, \dots, \U_{12},\Z_1,\Z_2,\dots,\Z_4, \X_1, \X_2, \dots,\X_{53}
	\end{align*}
	are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum 
		\begin{align*}
		\Y+ \V_1+ \V_2+\dots+ \V_5+ \U_1+ \U_2+ \dots+ \U_{12}\\+\Z_1+\Z_2+\dots+\Z_4+ \X_1+ \X_2+ \dots+\X_{53}
		\end{align*}
		is direct. 
		\item The subspace sums
		\begin{align*}
		&\left[\V :=  \right] \V_1 +\V_2 + \dots + \V_5\\
		&\left[\U :=  \right] \U_1 +\U_2 + \dots + \U_{12}\\
		&\left[\Z :=  \right] \Z_1 +\Z_2 + \dots + \Z_4\\
		&\left[\X :=  \right] \X_1 +\X_2 + \dots + \X_{53}\\
		&\Y + \V + \U + \X + \Z
		\end{align*}
		are all direct. 
	\end{enumerate}
	\begin{sln*}
		$\,$
		\begin{enumerate}
			\item $[1]$ implies $[2]:$ If the subspace sum 
			\begin{align*}
			\Y+ \V_1+ \V_2+\dots+ \V_5+ \U_1+ \U_2+ \dots+ \U_{12}\\+\Z_1+\Z_2+\dots+\Z_4+ \X_1+ \X_2+ \dots+\X_{53}
			\end{align*}
			is direct, then by part 2 of Problem 2., the subspace sums
			\begin{align*}
			&\left[\V :=  \right] \V_1 +\V_2 + \dots + \V_5\\
			&\left[\U :=  \right] \U_1 +\U_2 + \dots + \U_{12}\\
			&\left[\Z :=  \right] \Z_1 +\Z_2 + \dots + \Z_4\\
			&\left[\X :=  \right] \X_1 +\X_2 + \dots + \X_{53}\\
			&\Y + \V + \U + \X + \Z
			\end{align*}
			are all direct.
			
			\item $[2]$ implies $[1]:$ The subspace sum
			\begin{align*}
			\Y \oplus \V \oplus \U \oplus \X \oplus \Z
			\end{align*}
			can be written as 
			\begin{align*}
			\Y\oplus \left(\V_1\oplus \V_2\oplus\dots\oplus \V_5\right)\oplus \left(\U_1\oplus \U_2\oplus \dots\oplus \U_{12}\right)\\\oplus\left(\Z_1\oplus\Z_2\oplus\dots\oplus\Z_4\right)\oplus \left(\X_1\oplus \X_2\oplus \dots\oplus\X_{53}\right),
			\end{align*}
			which is equivalent to
			\begin{align*}
			\Y\oplus \V_1\oplus \V_2\oplus\dots\oplus \V_5\oplus \U_1\oplus \U_2\oplus \dots\oplus \U_{12}\\\oplus\Z_1\oplus\Z_2\oplus\dots\oplus\Z_4\oplus \X_1\oplus \X_2\oplus \dots\oplus\X_{53}
			\end{align*}
			by associativity of vector space addition, i.e., the subspace sum in the first statement is direct. 
		\end{enumerate}
	\end{sln*}
\end{prob*}

\newpage

\begin{prob*} \textbf{4.} \textbf{Direct sums preserve linear independence:} Suppose that $\U,\V,\W$ are subspaces of a vector space $\Z$, and the sum $\U,\V,\W$ is direct. 
	\begin{enumerate}
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a linearly independent list in $\U$, $V_1,V_2,\dots,V_{6}$ is a linearly independent list in $\V$, and $W_1,W_2,\dots,W_{134}$ is a linearly independent list in $\W$. Argue that the concatenated list 
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is linearly independent. 
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a basis of $\U$, $V_1,V_2,\dots,V_{6}$ is a basis of $\V$, and $W_1,W_2,\dots,W_{134}$ is a basis of $\W$. Argue that the concatenated list
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is a basis of $\U \oplus \V \oplus \W$.
	\end{enumerate}
	\begin{sln*}
		$\,$
		\begin{enumerate}
			\item By Problem 1., because $\U \oplus \V \oplus \W = \Z$, for any $U\in\U,V\in\V$, and $W\in\W$, 
			\begin{align*}
			U+V+W=\mathbf{0}_\Z \iff U=V=W=\mathbf{0}_\Z.
			\end{align*}
			Therefore $U,V,W$ are linearly independent. It follows, that if $U_1,U_2,\dots,U_{13}$ is a linearly independent list in $\U$, $V_1,V_2,\dots,V_{6}$ is a linearly independent list in $\V$, and $W_1,W_2,\dots,W_{134}$ is a linearly independent list in $\W$, then the list
			\begin{align*}
			U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
			\end{align*}
			is linearly independent.
			\item Since $U_1,U_2,\dots,U_{13}$ is a basis of $\U$, $V_1,V_2,\dots,V_{6}$ is a basis of $\V$, $W_1,W_2,\dots,W_{134}$ is a basis of $\W$, and that any list $U\in\U$, $V\in\V$, and $W\in\W$ is linearly independent (as shown in 1.), the list
			\begin{align*}
			U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
			\end{align*}
			is linearly independent. It, then, suffices to show that this list also spans $\U \oplus \V \oplus \W$.\\
			
			Consider $H \in \U \oplus \V \oplus \W$, which can be expressed uniquely as
			\begin{align*}
			H = U_h + V_h + W_h,
			\end{align*}
			where $U_h \in \U, V_h\in\V, W_h\in\W$. $U_h$ can be expressed as a linear combination of $U_1,U_2,\dots,U_{13}$ because $U_1,U_2,\dots,U_{13}$ form a basis of $\U$. Similarly, $V_h, W_h$ can be expressed uniquely as linear combinations of basis elements of their respective subspaces. It follows that $H$ can be expressed as a linear combination of the given elements of the concatenated list, i.e, the concatenated list spans $\U \oplus \V \oplus \W$.\\
			
			\noindent Therefore,
			\begin{align*}
			U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
			\end{align*}
			is a basis of $\U \oplus \V \oplus \W$.
		\end{enumerate}
	\end{sln*}
\end{prob*}

\newpage

\begin{prob*} \textbf{5.} Suppose that $x_1, x_2,\dots,x_{14}$ are non-null elements of a linear space $\W$. 
	\begin{enumerate}
		\item Argue that the following claims are equivalent. 
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$ are linearly independent. 
			\item The subspace sum 
			\begin{align*}
			\xpan(x_1) + \xpan(x_2) + \dots + \xpan(x_{14})
			\end{align*}
			is direct. 
		\end{enumerate}
		\item Argue that the following claims are equivalent.
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$  is a basis of $\W$.
			\item $\W = \xpan(x_1) \oplus \xpan(x_2) \oplus \dots \oplus \xpan(x_{14})$
		\end{enumerate}
	\end{enumerate}
	\begin{sln*}
		$\,$
		\begin{enumerate}
			\item 
			\begin{enumerate}
				\item $[a]$ implies $[b]:$ If $x_1, x_2,\dots,x_{14}$ are linearly independent, then any $x_i$ is neither null nor can be expressed as a linear combination of the other $x_j$'s. Therefore, no non-null $x_i \in \xpan(x_i)$ can be written as a sum of the other $x_j$'s $\in \xpan(x_j)$, i.e.,
				\begin{align*}
				\xpan(x_1) + \xpan(x_1) + \dots + \xpan(x_{14})
				\end{align*}
				is direct, by Problem 1.  
				\item $[b]$ implies $[a]:$ By problem 1., if
				\begin{align*}
				\xpan(x_1) + \xpan(x_2) + \dots + \xpan(x_{14})
				\end{align*}
				is direct, then no non-null $x_i \in \xpan(x_i)$ can be written as a sum of the other $x_j$'s $\in \xpan(x_j)$'s. Therefore, the list $x_1, x_2, \dots, x_{14}$ is linearly independent.  
			\end{enumerate}
			\item Because $x_i$ is a basis of $\xpan(x_i)$ and the subspace sum $\xpan(x_1) + \xpan(x_2) + \dots + \xpan(x_{14})$ is direct (by part 1 of Problem 5), the concatenated list $x_1,x_2,\dots,x_{14}$ is a basis of $\xpan(x_1)\oplus\xpan(x_2)\oplus\dots\oplus\xpan(x_{14})$, by Problem 4. (${\dagger}$)
			\begin{enumerate}
				\item $[a]$ implies $[b]:$ By ($\dagger$) and assumption, $\W$ and $\xpan(x_1)\oplus\xpan(x_2)\oplus\dots\oplus\xpan(x_{14})$ have a common basis, namely the list $x_1,x_2,\dots,x_{14}$. Therefore, $\W = \xpan(x_1)\oplus\xpan(x_2)\oplus\dots\oplus\xpan(x_{14})$.
				\item $[b]$ implies $[a]:$ It follows from fact ($\dagger$) that if $\W = \xpan(x_1) \oplus \xpan(x_2) \oplus \dots \oplus \xpan(x_{14})$, then $x_1, x_2,\dots,x_{14}$  is a basis of $\W$.
			\end{enumerate}
		\end{enumerate}
	\end{sln*}
\end{prob*}

\newpage

\subsection{Problem set 2}
\begin{prob*}\textbf{1.}
	\textbf{Finite unions of subspaces are rarely a subspace}\\
	Suppose that $\W_1,\W_2,\dots,\W_n$ are subspaces of a vector space $\V$. Prove that the following are equivalent:
	\begin{enumerate}
		\item $\W_1 \cup \W_2\cup\dots\cup\W_n$ is a subspace of $\V$.
		\item One of the $\W_i$'s contains all the others. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{1.}
		$\,$
		\begin{itemize}
			\item $[1 \implies 2]:$ Suppose $2.$ is false. Then there exists a smallest $n_0$ such that the implication $1. \implies 2.$ fails to hold. If $n=1$, then the implication is true because $\W_1$ contains itself. Therefore, $n_0 \geq 2$.\\
			
			\noindent Let $\W_1, \W_2\,\dots,\W_{n_0}$ be a collection of subspaces of $\V$ such that $\W_1 \cup \W_2 \cup \dots \cup \W_{n_0}$ is a subspace of $\V$ but none of the $\W_i$'s contains all the others. ???????\\
			
			\noindent Therefore there exists $w_i \in \W_i$ such that $w_i \notin \W_j$, for $j\neq i$. \\
			
			\noindent Consider $w_1 \in \W_1$ and $w_2 \in \W_2$. Because $\W_1 \cup \W_2\cup\dots\cup\W_n$ is a subspace, $w_1 + w_2 \in \W_1 \cup \W_2\cup\dots\cup\W_n$. 
		\end{itemize}
	\end{sln*}
	
	
\end{prob*}
\newpage
\begin{prob*}\textbf{2.}
	\textbf{Images of pre-images and pre-images of images:} Is there a $3\times 3$ matrix $\mathcal{A}$ such that
	\begin{align*}
	\W := \left\{ \begin{pmatrix}
	x\\y\\0
	\end{pmatrix}\bigg\vert \,x,y\in \mathbb{C} \right\}
	\end{align*}
	is an invariant subspace for $\mathcal{A}$, and
	\begin{align*}
	\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \subsetneq \W \subsetneq \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]?
	\end{align*}
	Justify your answer.
	
	
	\begin{sln*}\textbf{2.}
		$\,$\\
		
		\noindent $\W$ is invariant under $\mathcal{A}$ when $\mathcal{A}[\W] \subseteq \W$. 
	\end{sln*}
	
	
\end{prob*}









\newpage

\begin{prob*}
	\textbf{Invariant subspaces form a ``lattice'':}
	\begin{enumerate}
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under intersection; i.e., that the intersection of any two invariant subspaces for $\lag$ is also an invariant subspace for $\lag$.
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under subspace sums; i.e., that a subspace sum of two invariant subspaces for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that an image under $\lag$ of an invariant subspace for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that a pre-image under $lag$ of an invariant subspace for $lag$ is again an invariant subspace for $lag$. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{3.}
		$\,$
	\end{sln*}
	
\end{prob*}














\newpage

\begin{prob*}\textbf{4. Cyclic invariant subspaces}
	For a given $\lag\in\mathfrak{L}(\V)$ and a fixed $v_0 \in \V$, define
	\begin{align*}
	\mathbf{P}(\lag,v_0) := \left\{ (a_0 \lag^0 + a_1\lag^1 + a_2\lag^2 + \dots + a_k \lag^k)(v_0)\bigg\vert k \geq 0, a_i \in \mathbb{C} \right\}.
	\end{align*}
	\begin{enumerate}
		\item Argue that $\mathbf{P}(\lag,v_0)$ is a subspace of $\V$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is invariant under $\lag$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is the smallest invariant subspace for $\lag$ that contains $v_0$. We refer to $\mathbf{P}(\lag,v_0)$ as \textbf{the cyclic invariant subspace for $\lag$ generated by $v_0$}.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is 1-dimensional exactly when $v_0$ is an eigenvector of $\lag$.
		\item Argue a subspace $\W$ of $\V$ is invariant under $\lag$ exactly when it is a union of cyclic invariant subspaces for $\lag$. 
	\end{enumerate}
	
	\begin{sln*}\textbf{4.}
		$\,$
	\end{sln*}
\end{prob*}














\newpage


\begin{prob*} \textbf{5. Invariant subspaces of commuting operators:} Suppose that linear functions $\lag, \mathcal{M} \in \mathfrak{L}(\V)$ commute; i.e.,
	\begin{align*}
	\lag \circ \mathcal{M} = \mathcal{M}\circ \lag.
	\end{align*}
	\begin{enumerate}
		\item Argue that for any non-negative integer $k$, $\Im(\mathcal{M}^k)$ and $\ker(\mathcal{M}^k)$ are invariant subspace for $\lag$.
		\item Argue that every eigenspace of $\mathcal{M}$ is an invariant subspace for $\lag$.
		\item By giving a general example (with justification, of course!) show that for each $n>1$ there are commuting matrices $\mathcal{A}$ and $\mathcal{B}$ in $\mathbb{M}_n$ such that
		\begin{align*}
		\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B}).
		\end{align*}
	\end{enumerate}
	\begin{sln*}\textbf{5.}
		$\,$
	\end{sln*}
\end{prob*}








\newpage

\begin{prob*}\textbf{6. Invariant subspace chains:} Suppose that $\lag \in \mathfrak{L}(\V)$ and $\W$ is an invariant subspace of $\lag$. 
	\begin{enumerate}
		\item Argue that the following inclusions hold:
		\begin{align*}
		\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq \lag^{-3}[\W]\subseteq\dots
		\end{align*}
		Note that by Problem 3, all of the subspaces listed in the chain are invariant under $\lag$.
		\item Argue that the following implications hold:
		\begin{align*}
		\lag^{k+1}[\W] = \lag^{k}[\W] \implies \lag^m[\W] = \lag^k[\W]\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and 
		\begin{align*}
		\lag^{-k}[\W] = \lag^{-(k+1)}[\W] \implies \lag^{-k}[\W] = \lag^{-m}[\W]\,\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and then explain why the chain in part 1 stabilizes in both directions, and has at most $\dim(\V)$ proper inclusions. 
		\item Argue that any subspace $\mathbf{M}$ that falls between two consecutive subspaces in the chain shown in part 1$^*$, is also invariant under $\lag$. 
		
		
	\end{enumerate}
	\begin{sln*}\textbf{6.}
		$\,$
	\end{sln*}
\end{prob*}







\newpage
\subsection{Problem set 3}
\newpage


\end{document}
