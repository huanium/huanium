\documentclass{article}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Properties}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\p}{\partial}

\newcommand{\V}{\mathbf{V}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\X}{\mathbf{X}}

\newcommand{\xpan}{\text{span}}

\newcommand{\lag}{\mathcal{L}}

\newcommand{\J}{\mathbf{J}}

\newcommand{\M}{\mathcal{M}}

\newcommand{\K}{\mathcal{K}}

\newcommand{\N}{\mathcal{N}}

\newcommand{\E}{\mathcal{E}}

\newcommand{\ima}{\text{Im}}
\newcommand{\lin}{\overset{\text{linear}}{\longrightarrow}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\poly}{\mathbb{P}}
\newcommand{\s}{\mathcal{S}}

\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
	\begin{titlepage}\centering
		\clearpage
		\title{\textsc{\bf{MATRIX ANALYSIS}}\\\smallskip A Quick Guide\\}
		\author{\bigskip Huan Bui}
		\affil{Colby College\\Physics \& Statistics\\Class of 2021\\}
		\date{\today}
		\maketitle
		\thispagestyle{empty}
	\end{titlepage}

\newpage

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

\textit{Matrix Analysis: A Quick Guide to} is compiled based on my MA353: Matrix Analysis notes with professor Leo Livshits. The sections are based on a number of resources: \textit{Linear Algebra Done Right} by Axler, \textit{A Second Course in Linear Algebra} by Horn and Garcia, \textit{Matrices and Linear Transformations} by Cullen, \textit{Matrices: Methods and Applications} by Barnett, \textit{Problems and Theorems in Linear Algebra} by Prasolov, \textit{Matrix Operations} by Richard Bronson, and professor Leo Livshits' own textbook (in the making). Prerequisites: some prior exposure to a first course in linear algebra.\\

The development of this text will come in layers. The first layer, one that I am working on during the course of S'19 MA353, will be an overview of the key topics listed in the table of contents. As the semester progresses, I will be constantly updating the existing notes, as well as adding prof. Livshits' problems and my solutions to the problems. The second layer will come after the course is over, when concepts will have hopefully ``come together.''\\ 

I will decide how much narrative I should put into the text as text is developed over the semester. I'm thinking that I will only add detailed explanations wherever I find fit or necessary for my own studies. I will most likely keep the text as condensed as I can.\\

Enjoy!


\newpage
\tableofcontents
\newpage

\section{List of Special Matrices \& Their Properties}
\begin{enumerate}
	\item \textbf{Hermitian/Self-adjoint}: $H = H^\dagger$. A Hermitian matrix is matrix that is equal to its own conjugate transpose:
	\begin{align*}
	H \text{ is Hermitian } \iff H_{ij} = \bar{H}_{ji}
	\end{align*} 
	\begin{prop}
	$\,$
	\begin{enumerate}
		\item $H$ is Hermitian $ \iff \langle w, Hv\rangle = \langle Hw, v \rangle$, where $\langle, \rangle$ denotes the inner product.
		\item $H$ is Hermitian $ \iff \langle v, Hv\rangle \in \R$.
		\item $H$ is Hermitian $\iff$ it is \textit{unitarily diagonalizable} with \textit{real eigenvalues}.
	\end{enumerate}
	\item \textbf{Unitary}: $U^*U = UU^* = I = U^\dagger U = UU^\dagger$. The real analogue of a unitary matrix is an orthogonal matrix. The following list contain the properties of $U$:
	\begin{enumerate}
		\item $U$ preserves the inner product:
		\begin{align*}
		\langle Ux, Uy \rangle = \langle x,y\rangle.
		\end{align*}
		
		\item $U$ is normal: it commutes with $U^* = U^\dagger$.
		
		\item $U$ is diagonalizable:
		\begin{align*}
		U = VDV^*,
		\end{align*}
		where $D$ is diagonal and unitary, and $V$ is unitary.
		
		\item $\vert \det(U) \vert = 1 $ (hence the real analogue to $U$ is an orthogonal matrix)
		
		\item Its eigenspaces are orthogonal.
		
		\item $U$ can be written as
		\begin{align*}
		U = e^{iH},
		\end{align*}
		where $H$ is a Hermitian matrix. 
		
		\item Any square matrix with unit Euclidean norm is the average of two unitary matrices.
	\end{enumerate}
	\end{prop}
	\item \textbf{Idempotent:} $M$ idempotent $\iff M^2 = M$.
	\begin{enumerate}
		\item Singularity: its number of independent rows (and columns) is less than its number of rows (and columns). 
		\item When an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. 
		\begin{proof}[``Proof'']
			\begin{align*}
			[I-M][I-M] = I - M - M + M^2 = I-M - M + M = I - M.
			\end{align*}
		\end{proof}
		\item $M$ is idempotent $\iff \forall n \in \mathbb{N}$, $A^n = A$. 
		\item Eigenvalues: an idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1. (think ``projection")
		\item Trace: the trace of an idempotent matrix equals the rank of the matrix and thus is always an integer. So
		\begin{align*}
		\tr(A) = \dim(\Im A).
		\end{align*}
	\end{enumerate}
	\item \textbf{Nilpotent:} a nilpotent matrix is a square matrix $N$ such that
	\begin{align*}
	N^k = 0
	\end{align*}
	for some positive integer $k$. The smallest such $k$ is sometimes called the \textbf{index} of $N$.\\
	
	The following statements are equivalent:
	\begin{enumerate} 
		\item $N$ is nilpotent.
		\item The minimal polynomial for $N$ is $x^k$ for some positive integer $k\leq n$.
		\item The characteristic polynomial for $N$ is $x^n$.
		\item The only complex eigenvalue for $N$ is 0.
		\item $\tr N^k = 0$ for all $k > 0$.
	\end{enumerate}
	\begin{prop}
		$\,$
		\begin{enumerate}
			\item The degree of an $n\times n$ nilpotent matrix is always less than or equal to $n$. 
			\item $\det N = \tr(N) = 0$.
			\item Nilpotent matrices are not invertible.
			\item The only nilpotent diagonalizable matrix is the zero matrix. 
		\end{enumerate}
	\end{prop}
\end{enumerate}

\newpage
\section{List of Operations}
\begin{enumerate}
	\item \textbf{Conjugate transpose} is what its name suggests.
	\item \textbf{Classical adjoint/Adjugate/adjunct} of a square matrix is the transpose of its cofactor matrix. 
\end{enumerate}
\newpage

\section{List of Algorithms}
\newpage

\section{Complex Numbers}
\subsection{A different point of view}
We often think of complex numbers as
\begin{align*}
a + ib
\end{align*}
where $a,b \in \R$ and $i = \sqrt{-1}$. While there is nothing ``bad'' about this way of thinking - in fact thinking of complex numbers as $a+ib$ allows us to very quickly and intuitively do arithmetics operations on them - a ``matrix representation'' of complex numbers can give us some insights on ``what we actually do'' when we perform complex arithmetics.\\

Let us think of 
\begin{align*}
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\end{align*}
as a different representation of the same object - the same complex number ``$a+ib$.'' Note that it does not make sense to say the matrix representation \textbf{equals} the complex number itself. But we shall see that a lot of the properties of complex numbers are carried into this matrix representation under interesting matricial properties.
\begin{align*}
\boxed{\begin{pmatrix}
	a & -b\\
	b & a
	\end{pmatrix}
\sim a + ib}
\end{align*}

First, let us break the matrix down:
\begin{align*}
a+ib = a\times 1 + i \times b \sim \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix} 
= 
a\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
+
b\begin{pmatrix}
0 & -1\\
1 & 0
\end{pmatrix}
= aI + b\mathcal{I}.
\end{align*}
Right away, we can make some ``mental connections'' between the representations:
\begin{align*}
I &\sim 1\\
\mathcal{I} \sim i.
\end{align*}

Now, we know that complex number multiplications commute:
\begin{align*}
(a+ib)(c+id) = (c+id)(a+ib).
\end{align*}
Matrix multiplications are not commutative. So, we might wonder whether commutativity holds under the this new representation of complex numbers. Well, the answer is yes. We can readily verify that
\begin{align*}
(aI+b\mathcal{I})(cI+b\mathcal{I}) = (cI+b\mathcal{I})(aI+b\mathcal{I}).
\end{align*}
How about additions? Let's check:
\begin{align*}
(a + ib) + (c+ id) = (a+c) + i(b+d) \sim \begin{pmatrix}
a+c & -(b+d)\\
(b+d) & a+c 
\end{pmatrix}=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}+
\begin{pmatrix}
c & -d\\
d & c
\end{pmatrix}.
\end{align*}
Ah! Additions work. So, the new representation of complex numbers seems to be working flawlessly. However, we have yet to gain any interesting insights into the connections between the representations. To do that, we have to look into changing the form of the matrix. First, let's see what conjugation does:
\begin{align*}
(a+ib)^* = a-ib \sim \begin{pmatrix}
a & b \\
-b & a
\end{pmatrix}
=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}^{\top}
\end{align*}
Ah, so conjugation to a complex number in the traditional representation is the same as transposition in the matrix representations. What about the amplitude square? Let us call 
\begin{align*}
M = \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix}.
\end{align*}
We have
\begin{align*}
(a+ib)(a-ib) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
a & b \\
-b & a
\end{pmatrix} =
MM^\top= (a^2+b^2)I = \det(M)I
\end{align*}
Interesting. But observe that if $\det(M) \neq 0$
\begin{align*}
\frac{1}{\det(M)}MM^\top = I.
\end{align*}
This tells us that
\begin{align*}
M^\top = M^{-1},
\end{align*}
where $M^{-1}$ is the inverse of $M$, and, not surprisingly, it corresponds to the reciprocal to the complex number $a+ib$. We can readily show that
\begin{align*}
M^{-1} \sim (a+ib)^{-1} = \frac{1}{a^2+b^2}(a-ib).
\end{align*} 
Remember that we can also think of a complex number as a column vector:
\begin{align*}
c + id \sim \begin{pmatrix}
c\\d
\end{pmatrix}.
\end{align*}
Let us look back at complex number multiplication under matrix representation:
\begin{align*}
(a+ib)(c+id) = (ac-bd) + i(bc + ad) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
c\\d
\end{pmatrix}=
\begin{pmatrix}
ac-bd\\
bc +ad
\end{pmatrix}.
\end{align*}
Multiplication actually works in this ``mixed'' way of representing complex numbers as well. Now, observe that what we just did was performing a linear transformation on a vector in $\R^2$. It is always interesting to look at the geometrical interpretation of this transformation. To do this, let us call $N$ the ``normalized'' version of $M$:
\begin{align*}
N = \frac{1}{\sqrt{a^2 + b^2}}\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}.
\end{align*}
We immediately recognize that $N$ is an orthogonal matrix. This means $N$ is an orthogonal transformation (length preserving). Now, it is reasonable to define
\begin{align*}
\cos\theta &= \frac{a}{\sqrt{a^2 + b^2}}\\
\sin\theta &= \frac{b}{\sqrt{a^2+b^2}}.
\end{align*}
We can write $N$ as
\begin{align*}
N = \begin{pmatrix}
\cos\theta & -\sin\theta
\sin\theta & \cos\theta
\end{pmatrix},
\end{align*}
which is a physicists' favorite matrix: the rotation by $\theta$. So, let us write $M$ in terms of $N$:
\begin{align*}
M = \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix} = \sqrt{a^2 + b^2}N = \sqrt{a^2 + b^2}\begin{pmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{pmatrix}.
\end{align*}
We can interpret $M$ as a rotation by $\theta$, followed by a scaling by $\sqrt(a^2 + b^2)$. But what $\sqrt{a^2 + b^2}$ exactly is just the ``length'' or the ``amplitude'' of the complex number $a+ib$, if we think of it as an arrow in a plane. 
\subsection{Relevant properties and definitions}
\begin{enumerate}
	\item The \textit{modulus} of $z = a+ib$ is the ``amplitude'' of $z$, denoted by $\vert z \vert = \sqrt{a^2 + b^2} = z\bar{z}$. 
	\item The modulus is \textit{multiplicative}, i.e. 
	\begin{align*}
	\vert wz \vert = \vert w \vert \vert z \vert.
	\end{align*}
	\item Triangle inequality:
	\begin{align*}
	\vert z + w \vert \leq \vert z \vert + \vert w \vert.
	\end{align*}
	We can readily show this geometrically, or algebraically. 
	\item The \textit{argument} of $z = a+ib$ is $\theta$, where
	\begin{align*}
	\theta = 
	\begin{cases}
	\tan^{-1}\left( \frac{b}{a}\right), \text{ if } a > 0\\
	\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b > 0\\
	-\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b < 0\\
	\text{Undefined if } a=b=0.
	\end{cases}
	\end{align*} 
	\item The \textit{conjugate} of $a+ib$ is $a-ib$. Conjugation is \textit{additive} and \textit{multiplicative}, i.e.
	\begin{align*}
	\bar{z+w} &= \bar{z} + \bar{w}\\
	\bar{wz} &= \bar{w}\bar{z}.
	\end{align*}
	Note that we can also show the multiplicative property with the matrix representation as well:
	\begin{align*}
	\bar{wz} \sim (WZ)^\top = Z^\top W^\top \sim \bar{z}\bar{w} = \bar{w}\bar{z}.
	\end{align*}
	\item Euler's identity, generalized to de Moivre's formula:
	\begin{align*}
	z^n = r^ne^{in\theta}.
	\end{align*}
\end{enumerate}

\newpage
\section{Vector Spaces \& Linear Functions}
\subsection{Review of Linear Spaces and Subspaces}
\begin{prop} of linear spaces:
\begin{enumerate}
	\item Commutativity and associativity of addition
	\item Existence of an additively neutral element (null element). Zero multiples of elements give the null element: $0\cdot V = \mathbf{0}$\\
	\item Every element has an (unique) additively antipodal element
	\item Scalar multiplication distributes over addition
	\item Multiplicative identity: 
	\item $ab\cdot V = a\cdot(bV)$
	\item $(a+b)V = aV + bV$
\end{enumerate}
\end{prop}
$W$ is a subspace of $V$ if
\begin{enumerate}
	\item $S \subseteq V$ 
	\item $S$ is non-empty
	\item $S$ is closed under addition and scalar multiplication
\end{enumerate}
\begin{prop} that are interesting/important/maybe-not-so-obvious:
\begin{enumerate}
	\item If $S$ is a subspace of $V$ and $S \neq V$ then $S$ is a proper subspace of $V$.
	\item If $X$ in a subspace of $Y$ and $Y$ is a subspace of $Z$, then $X$ is a subspace of $W$.
	\item Non-trivial linear (non-singleton) spaces are infinite. 
\end{enumerate}
\end{prop}
\subsection{Review of Linear Maps}
Consider linear spaces $V$ and $W$ and elements $v \in V$ and $w \in W$ and scalars $\alpha, \beta \in \R$, a function $F : V \rightarrow W$ is a linear map if
\begin{align*}
F[\alpha v + \beta w] = \alpha F[v] + \beta F[w].
\end{align*}
\begin{prop}
	$\,$
\begin{enumerate}
	\item $F[\mathbf{0}_V] = \mathbf{0}_W$
	\item $G[w] = (\alpha \cdot F)[w] = \alpha\cdot F[w]$
	\item Given $F : V \rightarrow W$ and $G : V \rightarrow W$, $H[v] = F[v] + G[v] = (F+G)[v]$ is call the sum of the functions $F$ and $G$.
	\item Linear combinations of linear maps are linear.
	\item Compositions of linear maps are linear. 
	\item Compositions distributes over linear combinations of linear maps.
	\item Inverses of linear functions (if they exist) are linear.
	\item Inverse of a bijective linear function is a bijective linear function
\end{enumerate}
\end{prop}
\subsection{Review of Kernels and Images}
\begin{defn}
	Let $F : V \rightarrow W$ be given. The kernel of $F$ is defined as
	\begin{align*}
	\ker(F) = \{v\in V \vert F[v] = \mathbf{0}_W\}.
	\end{align*}
\end{defn}
\begin{prop}
	Let $F : V \rightarrow W$ a linear map be given. Also, consider a linear map $G$ such that $F\circ G$ is defined 
	\begin{enumerate}
		\item $F$ is null $\iff$ $\ker(F) = V \iff \Im(F) = \mathbf{0}_W $
		\item $\ker(F)$ is a subspace of $V$
		\item $\Im(F)$ is a subspace of $W$
		\item $F$ is injective $\iff \ker(F) = \mathbf{0}_V$
		\item $\ker(F) \subseteq \ker(F\circ G)$.
		\item $F$ injective $\implies \ker(F) = \ker(F\circ G)$
	\end{enumerate}
\end{prop}


\begin{defn}
	Let $F : V \rightarrow W$ be given. The image of $F$ is defined as
	\begin{align*}
	\Im(F) = \{w\in W \vert \exists v\in V, F[v] = w \}. 
	\end{align*}
\end{defn}
\subsection{Atrices}
\begin{defn}
	\textbf{Atrix functions:} Let $V_1,\dots,V_m \in \mathbf{V}$ be given. Consider $f : \R^m \rightarrow \mathbf{V}$ be defined by
	\begin{align*}
	f\begin{pmatrix}
	a_1\\
	a_2\\
	\vdots\\
	a_m
	\end{pmatrix}
	=
	\sum_{i=1}^m a_i V_i.
	\end{align*} 
	We denote $f$ by
	\begin{align*}
	\begin{pmatrix}
	V_1 & V_2 &\dots& V_m
	\end{pmatrix}.
	\end{align*}
	We refer to the $V_i$'s as the \textbf{columns} of $f$, even though there doesn't have to be any columns. Basically, $f$ is simply a function that takes in an ordered list of coefficients and returns a linear combination of $V_i$ with the respective coefficients. A matrix is a special atrix. Not every atrix is a matrix. 
\end{defn}
\begin{prop} of atrices
	$\,$
	\begin{enumerate}
	\item The $V_i$'s - the columns of an atrix - are the images of the standard basis tuples. 
	\item $\mathbf{e}_j \in \ker(V_1\dots V_m) \iff V_j = \mathbf{0}_V$, where $\mathbf{e}_j$ denotes a standard basis tuple with a 1 at the j$^\text{th}$ position. To put in words, a kernel of an atrix contains a standard basis if and only if one of its columns in a null element. 
	\item $\Im(f) \equiv \Im(V_1\dots V_m) = \xpan(V_1\dots V_m)$
	\item $B$ is a null atrix $\iff \ker(B) = \R^m \iff \Im(B) = \mathbf{0}_V \iff V_j = \mathbf{0}_V \forall j=1,2,\dots,m$.
	\item  $f$ is a linear function $\R^m \rightarrow V \iff f$ is an atrix function $\R^m \rightarrow V$.
	\item An atrix $A$ is bijective/invertible, then its inverse $A^{-1}$ is a linear function, but is an atrix only if $A$ is a matrix. 
	\item Linear combinations of atrices are atrices:
	\begin{align*}
	\alpha \cdot\begin{pmatrix}
	V_1 & \cdots & V_m
	\end{pmatrix}
	+ 
	\beta \cdot \begin{pmatrix}
	W_1 & \dots & W_m
	\end{pmatrix}
	\\= \begin{pmatrix}
	\alpha V_1 + \beta W_1 &\dots& \alpha V_m + \beta W_m
	\end{pmatrix}
	\end{align*}  
	\item Compositions of two atrices are NOT defined unless the atrix going first is a matrix. Consider $F : \R^m \rightarrow W$ and $G : \R^n \rightarrow T$. $F\circ G$ is only defined if $T = \R^m$. This make $G$ an $n\times m$ matrix. It follows that the atrix $F\circ G$ has the form
	\begin{align*}
	\begin{pmatrix}
	f(g_1)&f(g_2)&\dots&f(g_m)
	\end{pmatrix}.
	\end{align*}
	\item Consider $F : \R^m \rightarrow V$ and $G : \R^k \rightarrow V$. $\Im(F) \subseteq \Im(G) \iff F = G \circ C$, with $C \in \mathbb{M}_{k\times m}$, i.e. $C$ is an $k \times m$ matrix. 
	\item Consider an atrix $A : \R^m \rightarrow V$. $A = (V_1\dots V_m)$. $A$ is NOT injective.\\
	$\iff \exists$ a non-trivial linear combination of the columns of $A$ that gives $\mathbf{0}_V$\\
	$\iff$ $A = [\mathbf{0}_v]$ or $\exists j \vert V_j$ is linear combination of other columns of $A$\\
	$\iff$ The first column of $A$ is $\mathbf{0}_V$ or $\exists j \vert V_j$ is a linear combination some of $V_i, i < j$.
	\item If atrix $A: \R \rightarrow V$ has a single column then it is injective if the column is not $\mathbf{0}_V$.
	\end{enumerate}
\end{prop}
\begin{prop}
	of elementary column operations for atrices. Elementary operations on the columns of $F$ can be expressed as a composition of $F$ and an appropriate elementary matrix $E$, $F\circ E$. 
	\begin{enumerate}
		\item Swapping $i^{th}$ and $j^{th}$ columns: $F\circ E^{[i]\leftrightarrow[j]}$.
		\item Scaling the $j^{th}$ column by $\alpha$: $F\circ E^{\alpha\cdot[j]}$.
		\item Adjust the $j^{th}$ column by adding to it $\alpha\times i^{th}$ column: $F\circ E^{[i]\stackrel{+}\leftarrow \alpha\cdot[j]}$.
		\item Elementary column operations do not change the 'jectivity nor image of $F$.
		\item If a column of $F$ is a linear combination of some of the other columns then elementary column operations can turn it into $\mathbf{0}_V$.
		\item Removing/Inserting null columns or columns that are linear combinations of other columns does not change the image of $F$.
		\item Given atrix $A$, it is possible to eliminate (or not) columns of $A$ to end up with an atrix $B$ with $\Im(B) = \Im(A)$.
		\item If $B$ is obtained from insertion of columns into atrix $A$, then $\Im(B) = \Im(A) \iff$ the insert columns $\in \Im(A)$.
		\item Inserting columns to a surjective $A$ does not destroy surjectivity of $A$. ($A$ is already having extra or just enough columns) 
		\item $A$ surjective $\iff$ $A$ is obtained by inserting columns (or not) into an invertible atrix $\iff$ deleting some columns of $A$ (or not) gives an invertible matrix.
		\item If $A$ is injective, then column deletion does not destroy injectivity. ($A$ is already ``lacking'' or having just enough columns)
		\item The new atrix obtained from inserting columns from $\Im(A)$ into $A$ is injective $\iff$ $A$ is injective.
	\end{enumerate}
\end{prop}
\subsection{Linear Independence, Span, and Bases}
\subsubsection{Linear Independence}
$\,\,\,\,\,\,\,\,\,\,\,\,\,\,X_1\dots X_m$ are linearly independent\\
$\iff F = [X_1 \dots X_m]$ injective\\
$\iff \sum a_iX_i = 0 \iff a_i = 0 \forall i$\\
$\iff \mathbf{0}_V \notin \{X_i\}$ and none are linear combinations of some of the others.\\
$\iff X_1 \neq \mathbf{0}_V$ and $X_j$ is not a linear combination of any of $X_i$'s for $i < j$. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item The singleton list is linearly independent if its entry is not the null element
		\item Sublists of a linearly independent list are linearly independent
		\item List operations cannot create/destroy linearly independence. 
		\item If a linear map $L : X \rightarrow W$ injective, then $X_1\dots X_m$ linearly independent $\iff L(X_1)\dots L(X_m)$ linearly independent.  
	\end{enumerate}
\end{prop}
\subsubsection{Span}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Spans are subspaces. $\xpan(X_1\dots X_m), X_j \in V$ is a subspace of $V$.
		\item $\xpan(X_1\dots X_j) \subseteq \xpan(X_1\dots X_k)$ if $j\leq k$.
		\item Adding elements to a list that spans $V$ produces a list that spans $V$.
		\item The following list operations do not change the span of $V$: removing the null/linearly dependent element, inserting a linearly dependent element, scaling element(s), adding (multiples) of an element to another element.
		\item It is possible to reduce a list that spans to a list that spans AND have linearly independent elements.
		\item Consider $A : V \rightarrow W$. If $X_i$ span $W$ then $A(X_i)$ span $\Im(A)$. $i=1,2,\dots,m$.
		\item If $A : V \rightarrow W$ invertible, then $X_i$ span $V \iff A(X_i)$ span $W$, $i=1,2,\dots,m$. 
	\end{enumerate}
\end{prop}
\subsubsection{Bases}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A list is a basis of $V$ if the elements are linearly independent and they span $V$.
		\item A singleton linear space has no basis.
		\item Re-ordering the elements of a basis gives another basis.
		\item $\{X_1\dots X_m\}$ is a basis of $V$ if $(X_1\dots X_m)$ is injective AND $\Im(X_1\dots X_m) = V$, i.e. $(X_1\dots X_m)$ invertible.
		\item If $\{X_i\}$ is a basis of $V$ and $\{Y_i \}$ is a list of elements in $W$, then there exists a unique linear function $L : V \rightarrow W$ satisfying 
		\begin{align*}
		L[X_i] = Y_i
		\end{align*} 
		\item If $A: V \rightarrow W$ bijective, then $\{ V_i \}$ forms a basis of $V$ and $\{A(X_i) \}$ forms a basis of $W$.
		\item Elementary operations on bases give bases.  
	\end{enumerate}
\end{prop}
\subsection{Linear Bijections and Isomorphisms}
\begin{defn}
	Let linear spaces $V, W$ be given. $V$ is \textbf{isomorphic} to $W$ if $\exists F : V \rightarrow W$ bijective. We say $V \sim W$. 
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A non-zero scalar multiple of an isomorphism is an isomorphism.
		\item A composition of isomorphism is an isomorphism.
		\item ``Isomorphism'' behaves like an equivalence relation:
		\begin{enumerate}
			\item Reflexivity: $V \sim V$.
			\item Symmetry: if $V\sim W$ then $W\sim V$.
			\item Transitivity: if $V\sim W$ and $W \sim Z$ then $V\sim Z$.
		\end{enumerate}
		\item Consider $F : V \rightarrow W$ an isomorphism.
			\begin{enumerate}
				\item Isomorphisms preserve linear independence. $V_i$'s are linearly independent in $V$ $\iff$ $F(V_i)$'s are linearly independent in $W$.
				\item isomorphisms preserve spanning. $V_i$'s span $V$ $\iff$ $F(V_i)$'s span $W$.
				\item Isomorphisms preserve bases. $\{V_i\}$ is a basis of $V$ $\iff$ $F\{(V_i)\}$ is a basis of $W$.
			\end{enumerate}
		\item If $V\sim W$ then $\dim(V) = \dim(W)$ (finite or infinite).
		\item If a linear map $A : V \rightarrow W$ is given and $A(X_i)$'s are linearly independent, then $A_i$'s are linearly independent.  
		\item If a linear map $A : V \rightarrow W$ is injective and $\{ X_i\}$ is a basis of $V$ then $\{ A(X_i)\}$ is a basis of $\Im(A)$
	\end{enumerate}
\end{prop}
\subsection{Finite-Dimensional Linear Spaces}
\subsubsection{Dimension}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item $\R^m \sim R^n \iff m=n$.
		\item Isomorphisms $F : \R^n \rightarrow V$ are bijective atrices.
		\item Isomorphisms $G : W \rightarrow \R^m$ are inverses of bijective atrices.
		\item Consider a non-singleton linear space $V$
		\begin{enumerate}
			\item $V$ has a basis with $n$ elements.
			\item $V \sim \R^n$.
			\item $V \sim W$, where $W$ is any linear space with a basis of $n$ elements.
		\end{enumerate}
		\item If $V$ is a linear space with a basis with $n$ elements, then any basis of $V$ has $n$ elements.
		\item Linear space $V \sim W$ where $W$ is $n$-dimensional if $V$ is $n$-dimensional.
		\item For a non-singleton linear space $V$, $V\sim \R^n \iff \dim(V) = n$.
		\item $V \sim W \iff \dim(V) = \dim(W)$.
		\item If $W$ is a subspace of $V$, then $\dim(W)\leq \dim(V)$. Equality holds when $W = V$.
	\end{enumerate}
\end{prop}
\subsubsection{Rank-Nullity Theorem}
Let finite-dimensional linear space $V$ and linear map $F : V \rightarrow W$ be given. Then $\Im(F)$ is finite-dimensional and 
\begin{align*}
\dim(\Im(F)) + \dim(\ker(F)) = \dim(V)
\end{align*}

A stronger statement: If a linear map $F : V\rightarrow W$ has finite rank and finite nullity $\iff$ $V$ is finite-dimensional, then
\begin{align*}
\Im(F) + \ker(F) = \dim(V).
\end{align*}

\subsection{Infinite-Dimensional Spaces}
Consider a non-singleton linear space $V$. The following statements are equivalent:
\begin{enumerate}
	\item $V$ is infinite-dimensional.
	\item Every linearly independent list in $V$ can be enlarged to a strictly longer linearly independent set in $V$.
	\item Every linearly independent list in $V$ can be enlarged to an arbitrarily long (finite) linearly independent set in $V$.
	\item There are arbitrarily long (finite) linearly independent lists in $V$.
	\item There are linearly independent lists in $V$ of any (finite) length.
	\item No list of finitely many elements of $V$ spans $V$.
\end{enumerate}
\newpage
\section{Sums of Subspaces \& Products of vector spaces}
\subsection{Direct Sums}
\begin{defn}
	Let $U_j$, $j=1,2,\dots m$ are subspaces of $V$. $\sum_1^m U_j$ is a \textit{direct sum} if each $u \in \sum U_j$ can be written in only one way as $u = \sum_1^m u_j$. The direct sum $\sum^m_i U_j$ is denoted as $U_1 \oplus\dots\oplus U_m$.
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Condition for direct sum: If all $U_j$ are subspaces of $V$, then $\sum_1^m U_j$ is a direct sum $\iff$ the only way to write 0 as $\sum_1^m u_j$, where $u_j \in U_j$ is to take $u_j = 0$ for all $j$.  
		\item If $U, W$ are subspaces of $V$ and $U \bigcap W = \{ 0\}$ then $U+W$ is a direct sum. 
	\end{enumerate}
\end{prop}


\subsection{Products of Vector Spaces}
\begin{defn}
	Product of vectors spaces
	\begin{align*}
	V_1 \times\dots\times V_m = \{ (v_1,\dots,v_m): v_j \in V_j, j=1,2,\dots,m\}.
	\end{align*}
\end{defn}
\begin{defn}
	Addition on $V_1 \times\dots\times V_m$:
	\begin{align*}
	(u_1,\dots,u_m) + (v_1,\dots,v_m) = (u_1+v_1,\dots,v_m+u_m).
	\end{align*}
\end{defn}
\begin{defn}
	Scalar multiplication on $V_1 \times\dots\times V_m$:
	\begin{align*}
	\lambda(v_1,\dots,v_m) = (\lambda v_1,\dots,\lambda v_m).
	\end{align*}
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Product of vectors spaces is a vector space.
		\begin{align*}
		V_j \text{ are vectors spaces over } \F \implies V_1\times\dots\times V_m \text{ is a vector space over } \F.
		\end{align*}
		\item Dimension of a product is the sum of dimensions: 
		\begin{align*}
		\dim(V_1\times\dots\times V_m) = \sum_1^m\dim(V_j)
		\end{align*}
		\item Vector space products are NOT commutative:
		\begin{align*}
		W\times V \neq V\times W.
		\end{align*}
		However, 
		\begin{align*}
		V\times W \sim W \times V.
		\end{align*}
		\item Vector space products are NOT associative:
		\begin{align*}
		V\times(W\times Z) \neq (V\times W)\times Z
		\end{align*}
	\end{enumerate}
\end{prop}
\subsection{Products \& Direct Sums}
\begin{prop}
	$\,$
	\begin{enumerate}
	\item Let $U_1,\dots,U_m$ be subspaces of $V$. Define a linear map $\Gamma : U_1\times\dots\times U_m \rightarrow U_1 + \dots + U_m$ by:
	\begin{align*}
	\Gamma(u_1,\dots,u_m) = \sum_1^m u_j.
	\end{align*}
	$U_1 + \dots + U_m$ is a direct sum $\iff \Gamma$ is injective. 
	\item Let $U_j$ be finite-dimensional and are subspaces of $V$. 
	\begin{align*}
	U_1 \oplus \dots\oplus U_m \iff \dim(U_1+\dots+U_m) = \sum_1^m\dim(U_j)
	\end{align*}
	\end{enumerate}
\end{prop}

\subsection{Rank-Nullity Theorem}
Suppose $Z_1$ and $Z_2$ are subspaces of a finite-dimensional vector space $W$. Consider $z_1\in Z_1$, $z_2\in Z_2$, and a function $\phi : Z_1 \times Z_2 \to Z_1 + Z_2 \prec W$ defined by
\begin{align*}
\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
=
z_1 + z_2.
\end{align*}
First, $\phi$ is a linear function, as it satisfies the linearity condition:
\begin{align*}
\phi\left(\alpha
\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
+
\beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}\right) = \alpha\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix} + \beta \phi \beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}.
\end{align*}
By rank-nullity theorem,
\begin{align*}
\dim(Z_1\times Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi)). 
\end{align*}
But this is equivalent to
\begin{align*}
\dim(Z_1) + \dim(Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi))
\end{align*}
The kernel of $\phi$ is:
\begin{align*}
\ker(\phi) = 
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1, z\in Z_2
\right\}
=
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1 \cap Z_2
\right\}
\end{align*}
We can readily verify that $Z_1 \cap Z_2$ is a subspace of $W$. With this, $\dim(\ker(\phi)) = \dim(Z_1\cap Z_2)$. So we end up with
\begin{align*}
\dim(Z_1 + Z_2) = \dim(Z_1) + \dim(Z_2) - \dim(Z_1\cap Z_2).
\end{align*}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item When $Z_1\cap Z_2$ is trivial, then $Z_1 + Z_2$ is direct. 
		\item When $\dim(\ker(\phi)) = 0$, $\phi$ is injective. But $\phi$ is also surjective by definition, this implies $\phi$ is a bijection, in which case
		\begin{align*}
		Z_1 \oplus Z_2 \sim Z_1 + Z_2.
		\end{align*} 
	\end{enumerate}
\end{prop}


\subsection{Nullspaces \& Ranges of Operator Powers}

\begin{enumerate}
	\item Sequence of increasing null spaces: Suppose $T \in \lag(V)$, i.e., $T$ is some linear function mapping $V\to V$, then
	\begin{align*}
	\{ 0 \} = \ker(T^0) \subset \ker(T^1) \subset \ker(T^2) \subset \dots \subset \ker(T^k) \subset \ker(T^{k+1}) \subset\dots.
	\end{align*} 
	
	\begin{proof}[Proof Outline]
		Let $k$ be a nonnegative integer and $v\in \ker(T^k)$. Then $T^kv = 0$, so $T^{k+1}v = T(T^kv) = T(0) = 0$, so $v\in \ker T^{k+1}$. So $\ker(T^k) \subset \ker(T^{k+1})$. 
	\end{proof}
	\item Equality in the sequence of null spaces: Suppose $m$ is a nonnegative integer such that $\ker(T^m) = \ker(T^{m+1})$, then
	\begin{align*}
	\ker(T^m) = \ker(T^{m+1}) = \ker(T^{m+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		We want to show
		\begin{align*}
		\ker(T^{m+k}) = \null(T^{m+k+1}).
		\end{align*}
		We know that $\ker T^{m+k} \subset \ker T^{m+k+1}$. Suppose $v\in \ker T^{m+k+1}$, then
		\begin{align*}
		T^{m+1}(T^kv) = T^{m+k+1}v = 0.
		\end{align*}
		So
		\begin{align*}
		T^kv \in \ker T^{m+1} = \ker T^m.
		\end{align*}
		So
		\begin{align*}
		0 = T^m(T^kv) = T^{m+k}v,
		\end{align*}
		i.e., $v\in \ker T^{m+k}$. So $\ker T^{m+k+1} \subset \ker T^{m+k}$. This completes the proof. 
	\end{proof}
	\item Null spaces stop growing: If $n = \dim(V)$, then
	\begin{align*}
	\ker(T^n) = \ker(T^{n+1}) = \ker(T^{n+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n = \ker T^{n+1}.
		\end{align*}
		Suppose this is not true. Then the dimension of the kernel has to increase by at least 1 every step until $n+1$. Thus $\dim \ker T^{n+1} \geq n+1 > n = \dim(V)$. This is a contradiction. 
	\end{proof}
	\item $V$ is the direct sum of $\ker(T^{\dim(V)})$ and $\Im(T^{\dim(V)})$: If $n = \dim(V)$, then
	\begin{align*}
	V = \ker(T^n) \oplus \Im(T^n).
	\end{align*}
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n \cap \Im T^n = \{0 \}. 
		\end{align*}
		Suppose $v\in \ker T^n \cap \Im T^n$. Then $T^n v = 0$ and $\exists u\in V$ such that $v = T^n u$. So
		\begin{align*}
		T^n v = T^{2n}u = 0.
		\end{align*}
		So
		\begin{align*}
		T^n u = 0.
		\end{align*}
		But this means $v = 0$. 
	\end{proof}
	\item IT IS NOT TRUE THAT $V = \ker(T) \oplus \Im(T)$ in general. 
\end{enumerate}

\subsection{Generalized Eigenvectors and Eigenspaces}
\begin{defn}
	Suppose $T\in \lag(V)$ and $\Lambda$ is an eigenvalue of $T$. A vector $v\in V$ is called a \textbf{generalized eigenvector} of $T$ corresponding to $\lambda$ if $v\neq 0$ and
	\begin{align*}
	(T-\lambda I)^j v = 0
	\end{align*}
	for some positive integer $j$. 
\end{defn}

\begin{defn}
	\textbf{Generalized Eigenspace}: Suppose $T \in \lag(V)$ and $\lambda \in \mathbf{F}$. The \textbf{generalized eigenspace} of $T$ corresponding to $\lambda$, denoted $G(\lambda,T)$, is defined to be the set of all generalized eigenvectors of $T$ corresponding to $\lambda$, along with the 0 vector.
\end{defn}

\begin{prop}
	\begin{enumerate}
		\item Suppose $T\in\lag(V)$ and $\lambda \in \mathbf{F}$. Then 
		\begin{align*}
		G(\lambda,T) = \ker(T-\lambda I)^{\dim(V)}.
		\end{align*}
		\begin{proof}[Proof Outline]
			Suppose $v\in \ker (T_\lambda I)^{\dim(V)}$. Then $v\in G(\lambda,T)$. So, $\ker(T-\lambda I)^{\dim V} \subset G(\lambda,T)$. Next, suppose $v\in G(\lambda,T)$. Then these is a positive integer $j$ such that
			\begin{align*}
			v\ in \ker (T-\lambda I)^j.
			\end{align*}
			But if this is true, then 
			\begin{align*}
			v\ in \ker (T-\lambda I)^{\dim V},
			\end{align*}
			since $\ker(T-\lambda I)^{\dim V}$ is the largest possible kernel, in a sense. 
		\end{proof}
		\item Linearly independent generalized eigenvectors: Let $T\in\lag(V)$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $v_1,\dots,v_m$ are corresponding generalized eigenvectors. Then $v_1,\dots,v_m$ is linearly independent.
	\end{enumerate}
\end{prop}

\subsection{Nilpotent Operators}
\begin{defn}
	An operator is called \textbf{nilpotent} if some power of it equals 0.
\end{defn}
\begin{prop}
	\item Nilpotent operator raised to dimension of domain is 0: Suppose $N\in \lag(V)$ is nilpotent. Then \begin{align*}
	N^{\dim(V)} = 0.
	\end{align*}
	\item Matrix of a nilpotent operator: Suppose $N$ is a nilpotent operator on $V$. Then there is a basis of $V$ with espect to which the matrix of $N$ has the form
	\begin{align*}
	\begin{pmatrix}
	0 & & *\\
	&\ddots&\\
	0&&0
	\end{pmatrix};
	\end{align*}
	here all entries on and below the diagonal are 0's. 
\end{prop}

\subsection{Weyr Characteristic}

\newpage
\section{Idempotents \& Resolutions of Identity}




\newpage
\section{Block-representations of operators}
\subsection{Direct sums of operators}
\subsection{Coordinatization \& matricial representation of linear functions}
Consider a finite-dimensional linear space $V$ with basis $\{V_i \}$, $i=1,2,\dots,m$. An element $\tilde{V}$ in $V$ can be expressed in exactly one way:
\begin{align*}
\tilde{V} = \sum_{i=1}^m a_iV_i,
\end{align*}
where $\{ a_i\}$ is unique. We call $\{ a_i \}$ the coordinate tuple of $\tilde{V}$ and $a_i$'s the coordinates. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Inverse of a bijective atrix outputs the coordinates. Suppose $A = [V_i]$. Then
		\begin{align*}
		\tilde{V} = \sum_{i=1}^ma_iV_i \iff A^{-1}(Z) = \begin{pmatrix}
		a_1&\dots&a_m
		\end{pmatrix}^\top
		\end{align*}
	\end{enumerate}
\end{prop}
\subsection{Equality of rank and trace for idempotents; Resolution of Identity Revisited}



\newpage
\section{Invariant subspaces}
\subsection{Reducing subspaces}
\newpage 
\section{Polynomials applied to operators}
\subsection{Minimal polynomials of block-$\Delta^r$ operators}
\subsection{Minimal polynomials at a vector}
\newpage 
\section{Eigentheory}
\subsection{Spectral Mapping Theorem}
\newpage 
\section{Triangularization}
\subsection{Compression to invariant subspaces}
\subsection{Simultaneously $\Delta$-ity of commuting families}
\newpage 
\section{Diagonalization}
\subsection{Spectral resolutions}
\subsection{Compressions to reducing subspaces}
\subsection{Simultaneous diagonalizability for commuting families}
\newpage
\section{Primary decomposition over $\mathbb{C}$ and generalized eigenspaces}
\newpage 
\section{Cyclic decomposition and Jordan form}
\subsection{Square roots of operators}
\subsection{Similarity of a matrix and its transpose}
\subsection{Similarity of a matrix and its conjugate}
\subsection{Jordan forms of $\mathcal{AB}$ and $\mathcal{BA}$}
\subsection{Power-convergent operators}
\subsection{Power-bounded operators}
\subsection{Row-stochastic matrices}
\newpage 
\section{Determinant \& Trace}
\subsection{Classical adjoints}
\subsection{Cayley-Hamilton theorem}

\newpage

\section{Inner products and norms} 
\newpage 
\subsection{Riesz representation theorem}
\subsection{Adjoints}
\subsection{Grammians}
\subsection{Orthogonal complements and orthogonal decompositions}
\subsection{Ortho-projections}
\subsection{Closest point solutions}
\subsection{Gram-Schmidt and orthonormal bases}

\newpage

\section{Isometries and unitary operators}
\newpage 

\section{Ortho-triangularization}
\newpage 

\section{Spectral resolutions}
\newpage 

\section{Ortho-diagonalization; self-adjoint and normal operators; spectral theorems}
\newpage 

\section{Positive (semi-)definite operators}
\subsection{Classification of inner products}
\subsection{Positive square roots}
\newpage 

\section{Polar decomposition}
\newpage 

\section{Single value decomposition}
\subsection{Spectral/operator norm}
\subsection{Singular values and approximation}
\subsection{Singular values and eigenvalues}







































\newpage
\section{Problems and Solutions}
\subsection{Problem set 1 (under corrections)}
\begin{prob*} \textbf{1. Equivalent formulations of directness of a subspace sum:} Suppose that $\V_1, \V_2\,\dots,\V_{315}$ are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum $\V_1 + \V_2 + \dots + \V_{315}$ is direct. 
		\item If $x_i \in \V_i$ and $x_1 + x_2 + \dots + x_{315} = \mathbf{0}_\W$, then $x_i = \mathbf{0}_\W$, for every $i$.
		\item If $x_i, y_i \in \V_i$ and $x_1 + x_2 + x_3 + \dots + x_{315} = y_1 + y_2 + y_3 + \dots + y_{315}$, then $x_i = y_i$, for every $i$.
		\item For any $i$, no non-null element of $\V_i$ can be express as a sum of the elements of the other $V_j$'s.
		\item For any $i$, no non-null element of $\V_i$ can be expressed as a sum of the elements of the preceding $V_j$'s. 
	\end{enumerate}

\end{prob*}


\newpage

\begin{prob*} \textbf{2.} \textbf{Sub-sums of direct sums are direct: }
	Suppose that $\V_1, \V_2,\dots, \V_{315}$ are subspaces of a vector space $\W$, and the subspace sum $\V_1+\V_2+\dots+\V_{315}$ is direct.
	\begin{enumerate}
		\item Suppose that for each $i$, $\Z_i$ is a subspace of $\V_i$. Argue that the subspace sum $\Z_1 + \Z_2 + \dots \Z_{315}$ is direct. 
		\item Argue that the sum $\V_2 + \V_5 + \V_7 + \V_{12}$ is also direct. 
	\end{enumerate}

\end{prob*}


\newpage


\begin{prob*} \textbf{3.} \textbf{Associativity of directness of subspace sums:} Suppose that 
	\begin{align*}
	\Y, \V_1, \V_2, \dots, \V_5, \U_1, \U_2, \dots, \U_{12},\Z_1,\Z_2,\dots,\Z_4, \X_1, \X_2, \dots,\X_{53}
	\end{align*}
	are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum 
		\begin{align*}
		\Y+ \V_1+ \V_2+\dots+ \V_5+ \U_1+ \U_2+ \dots+ \U_{12}+\Z_1+\Z_2+\dots+\Z_4+ \X_1+ \X_2+ \dots+\X_{53}
		\end{align*}
		is direct. 
		\item The subspace sums
		\begin{align*}
		&\left[\V :=  \right] \V_1 +\V_2 + \dots + \V_5\\
		&\left[\U :=  \right] \U_1 +\U_2 + \dots + \U_{12}\\
		&\left[\Z :=  \right] \Z_1 +\Z_2 + \dots + \Z_4\\
		&\left[\X :=  \right] \X_1 +\X_2 + \dots + \X_{53}\\
		&\Y + \V + \U + \X + \Z
		\end{align*}
		are all direct. 
	\end{enumerate}

\end{prob*}


\newpage

\begin{prob*} \textbf{4.} \textbf{Direct sums preserve linear independence:} Suppose that $\U,\V,\W$ are subspaces of a vector space $\Z$, and the sum $\U,\V,\W$ is direct. 
	\begin{enumerate}
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a linearly independent list in $\U$, $V_1,V_2,\dots,V_{6}$ is a linearly independent list in $\V$, and $W_1,W_2,\dots,W_{134}$ is a linearly independent list in $\W$. Argue that the concatenated list 
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is linearly independent. 
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a basis of $\U$, $V_1,V_2,\dots,V_{6}$ is a basis of $\V$, and $W_1,W_2,\dots,W_{134}$ is a basis of $\W$. Argue that the concatenated list
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is a basis of $\U \oplus \V \oplus \W$.
	\end{enumerate}

\end{prob*}


\newpage




\begin{prob*} \textbf{5.} Suppose that $x_1, x_2,\dots,x_{14}$ are non-null elements of a linear space $\W$. 
	\begin{enumerate}
		\item Argue that the following claims are equivalent. 
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$ are linearly independent. 
			\item The subspace sum 
			\begin{align*}
			\xpan(x_1) + \xpan(x_2) + \dots + \xpan(x_{14})
			\end{align*}
			is direct. 
		\end{enumerate}
		\item Argue that the following claims are equivalent.
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$  is a basis of $\W$.
			\item $\W = \xpan(x_1) \oplus \xpan(x_2) \oplus \dots \oplus \xpan(x_{14})$
		\end{enumerate}
	\end{enumerate}


\end{prob*}


\newpage




















\subsection{Problem set 2}
\begin{prob*}\textbf{1.}
	\textbf{Finite unions of subspaces are rarely a subspace}\\
	Suppose that $\W_1,\W_2,\dots,\W_n$ are subspaces of a vector space $\V$. Prove that the following are equivalent:
	\begin{enumerate}
		\item $\W_1 \cup \W_2\cup\dots\cup\W_n$ is a subspace of $\V$.
		\item One of the $\W_i$'s contains all the others. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{1.}
		$\,$
		\begin{itemize}
			\item $[1. \implies 2.]:$ Suppose $2.$ is false and define 
			\begin{align*}
			\mathbf{S} = \bigcup_{i=1}^{n_0}\W_i
			\end{align*}
			a subspace of $\V$, where none of the $\W_i$'s $\prec \V$ contains all the others, and $n_0$ is minimal. If $n_0=1$, then the implication $1. \implies 2.$ is true because $\W_1$ contains itself. Therefore, in order for this implication to fail, $n_0 \geq 2$.\\
			
			Because $n_0$ is minimal, $\mathbf{S}$ cannot be obtained from a union of less than $n_0$ of the $\W_i$'s. Therefore, each $\W_i$ contains an element $w_i$ that does not belong to any other $\W_j$'s.\\
			
			Take $w_2\in\W_2\setminus \W_1$ and $w_1 \in \W_1$ such that $w_1 \notin \bigcup_{j\neq 1}^{n_0}\W_j$. It follows that $w_1, w_2 \in \mathbf{S}$ because $\W_1, \W_2 \subseteq \mathbf{S}$. And since $\mathbf{S}$ is a subspace, $mw_1 + w_2 \in \mathbf{S}$ for any $m\in\mathbb{C}$ by closure under addition. \\
			
			Consider a collection of $n_0$ linear combinations of $w_1$ and $w_2$, defined as
			\begin{align*}
			T = \{ w_1+w_2,2w_1+w_2,\dots,n_0 w_1 + w_2\} \subseteq \mathbf{S}.
			\end{align*}
			Consider a typical element of $T$: $n w_1 + w_2 \in \mathbf{S}$, $n\in\{1,2,\dots,n_0\}$. Suppose $n w_1 + w_2 \in \W_1$. By closure under addition, $$(nw_1 + w_2) - nw_1 = w_2\in \W_1.$$ But this contradicts the choice of $w_2$. Therefore, $nw_1 + w_2 \notin \W_1$, for any $n\in\{1,2,\dots,n_0\}$. It follows that $T \subseteq \bigcup_{i=2}^{n_0} \W_i$. \\
			
			By construction, $T$ has $n_0$ elements, while $\bigcup_{i=2}^{n_0} \W_i$ has $(n_0 - 1)$ $\W_i$'s. By the pigeonhole principle, a $\W_k$ of $\W_2, \dots, \W_{n_0}$ must contain $nw_1 + w_2$ for two distinct values of $n$. Let these values be $n_a$ and $n_b$. By closure under addition, we have
			\begin{align*}
			(n_a w_1 + w_2) - (n_b w_1 + w_2) = (n_a - n_b)w_1 \in \W_k.
			\end{align*}
			But since $n_a \neq n_b$, $w_1 \in \W_k$. This contradicts our initial choice of $w_1$ and thus rules out the possibility that the implication $1. \implies 2.$ fails to hold. Therefore, the implication $1. \implies 2.$ must be true.
			
			
			
			
			\item $[2. \implies 1.]:$ Without loss of generality, assume that $\W_1$ contains all the other $\W_j$'s. It follows that 
			\begin{align*}
			\bigcup_{i=1}^{n_0}\W_i = \W_1.
			\end{align*}
			Since $\W_1$ is a subspace of $\V$, $\bigcup_{i=1}^{n_0}\W_i$ is also a subspace of $\V$. Therefore, $2. \implies 1.$ must hold.
		\end{itemize}
	\end{sln*}
	
\end{prob*}











\newpage
\begin{prob*}\textbf{2.}
	\textbf{Images of pre-images and pre-images of images:} Is there a $3\times 3$ matrix $\mathcal{A}$ such that
	\begin{align*}
	\W := \left\{ \begin{pmatrix}
	x\\y\\0
	\end{pmatrix}\bigg\vert \,x,y\in \mathbb{C} \right\}
	\end{align*}
	is an invariant subspace for $\mathcal{A}$, and
	\begin{align*}
	\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \subsetneq \W \subsetneq \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]?
	\end{align*}
	Justify your answer.
	
	
	\begin{sln*}\textbf{2.}
		$\,$\\
		
		\noindent First, we observe that $\dim(\W) = 2$, because a basis set of $\W$,
		\begin{align*}
		\left\{ \begin{pmatrix}
		1\\0\\0
		\end{pmatrix},\begin{pmatrix}
		0\\1\\0
		\end{pmatrix}\right\},
		\end{align*}
		has two elements. Next, consider the vector space $\mathbb{C}^3$ over the complex numbers. Since $\W$ is a subspace and any element of $\W$ is contained in $\mathbb{C}^3$ by construction, $\W \prec \mathbb{C}^3$. In fact, $\W \subsetneq \mathbb{C}^3$ because $\mathbb{C}^3 \ni (0\,\,\,0\,\,\,1)^\top \notin\W$\\
		
		\noindent By the condition $\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \subsetneq \W \subsetneq \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]$, it must be true that
		\begin{align*}
		\dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right]   \right) < \dim(\W) < \dim\left(\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]\right).
		\end{align*}
		Now, since $\dim(\W) = 2$ and $\mathcal{A}$ is a $3\times 3$ matrix (i.e., neither $\dim(\ker(\mathcal{A}))$ nor $\dim(\Im(A))$ can exceed 3$^{(\dagger)}$), it is required that $\dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right]   \right) \leq 1$ and $\dim\left(\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]\right) = 3$. \\
		
		\noindent Because $\W$ is invariant under $\mathcal{A}$, $\mathcal{A}[\W] \subseteq \W$. It follows that $\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right] \subseteq \mathcal{A}^{-1}[\W]$. Therefore,
		\begin{align*}
		3 = \dim\left( \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]  \right) \leq \dim\left( \mathcal{A}^{-1}[\W]  \right).
		\end{align*}
		It follows, by fact $(\dagger)$, that
		\begin{align*}
		3 \leq \dim\left( \mathcal{A}^{-1}[\W]  \right) \leq 3,
		\end{align*}
		which implies $\dim\left( \mathcal{A}^{-1}[\W]  \right) = 3 = \dim(\mathbb{C}^3)$. Consider $v\in \mathcal{A}^{-1}[\W] $. $v$ can have the form $(x\,\,y\,\,z)^\top$, $x,y,z\in \mathbb{C}$, so $v\in\mathbb{C}^3$. This implies $\mathcal{A}^{-1}[\W] \subseteq \mathbb{C}^3$. But since their dimensions are both 3, $\mathcal{A}^{-1}[\W] = \mathbb{C}^3$. It follows that
		\begin{align*}
		\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] = \mathcal{A}[\mathbb{C}^3] \supseteq \mathcal{A}[\W],
		\end{align*}
		where the second relation comes from the fact that $\W$ is a subspace of $\mathbb{C}^3$. Hence,
		\begin{align*}
		1 \geq \dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \right) \geq \dim\left( \mathcal{A}[\W] \right),
		\end{align*}
		which implies $\dim(\mathcal{A}[\W]) = 0$ or $1$.\\
		
		\noindent So, such a $3\times 3$ matrix $\mathcal{A}$ that satisfies the given condition is
		\begin{align*}
		\mathcal{A} = \begin{pmatrix}
		1&0&0\\
		0&0&0\\
		0&0&0
		\end{pmatrix}.
		\end{align*}
		First, $\W$ is invariant under $\mathcal{A}$ because take any $(x\,\,\,y\,\,\,0)^\top \in \W$, $\mathcal{A}\left((x\,\,\,y\,\,\,0)^\top\right) = (x\,\,\,0\,\,\,0)^\top \in \W$. This implies $\mathcal{A}[\W] \subseteq [\W]$ $(a)$. \\
		
		\noindent Second, it is easy to see that the pre-image of $\W$ under $\mathcal{A}$ is $\mathbb{C}^3$, since take any $(x\,\,\,y\,\,\,z)^\top \in \mathbb{C}^3$, we have $\mathcal{A}\left((x\,\,\,y\,\,\,z)^\top\right) = (x\,\,\,0\,\,\,0)^\top \in \W$. But observe that $\Im(\mathcal{A})$ is a proper subset of $\W$, because any $(x\,\,\,y\,\,\,0)^\top \in \W$ with $y\neq 0$ is not contained in $\Im(\mathcal{A})$. This implies $\mathcal{A}\left[\mathcal{A}^{-1}[\W]\right] \subsetneq \W$  $(b)$.\\ 
		
		\noindent Lastly, as we have pointed out, because    $\dim\left(A^{-1}\left[\mathcal{A}[\W]\right]\right) = 3$ and $A^{-1}\left[\mathcal{A}[\W]\right] \prec \mathbb{C}^3$, $A^{-1}\left[\mathcal{A}[\W]\right] = \mathbb{C}^3 \supsetneq \W$ $(c)$.\\
		
		\noindent From $(a),(b),(c)$, we see that this $3\times 3$ matrix $\mathcal{A}$ works as desired.   
	\end{sln*}
	
	
\end{prob*}









\newpage

\begin{prob*}\textbf{3.}
	\textbf{Invariant subspaces form a ``lattice'':}
	\begin{enumerate}
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under intersection; i.e., that the intersection of any two invariant subspaces for $\lag$ is also an invariant subspace for $\lag$.
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under subspace sums; i.e., that a subspace sum of two invariant subspaces for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that an image under $\lag$ of an invariant subspace for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that a pre-image under $\lag$ of an invariant subspace for $\lag$ is again an invariant subspace for $\lag$. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{3.}
		$\,$
		\begin{enumerate}
			\item Let subspaces $\V, \W \in \mathfrak{Lat}(\lag)$ be given. $\mathfrak{Lat}(\lag)$ is closed under intersection if $\V \cap \W \in \mathfrak{Lat}(\lag)$. Consider $u\in \V\cap\W$, then $u\in\V$ and $u\in\W$. It follows that $\lag(u)\in \lag[\V]$ and $\lag(u)\in \lag[\W]$.\\
			
			Now, since $\lag[\V]\subseteq \V$ and $\lag[\W]\subseteq \W$ because $\W,\V \in \mathfrak{Lat}(\lag)$, we have $\lag(u)\in\V$ and $\lag(u)\in \W$, which implies $\lag(u)\in \V\cap \W$. Since this implication holds for any $u\in \V\cap\W$, $\lag[\V\cap\W]\subseteq \V\cap\W$. Therefore, $\V\cap\W$ is invariant under $\lag$; i.e., $\W\cap\V \in \mathfrak{Lat}(\lag)$. This completes the argument. \\
			
			\item Let subspaces $\V,\W \in \mathfrak{Lat}(\lag)$ be given. $\mathfrak{Lat}(\lag)$ is closed under subspace sums if $\V + \W \in \mathfrak{Lat}(\lag)$. Consider $v\in\V, w\in\W$. Then $v+w \in \V + \W$. Next, consider $l\in \lag[\U+\W]$ given by
			$$l = \lag(v+w) = \lag(v) + \lag(w) = v' + w',$$
			where $v'\in \V$ and $w'\in\W$ because $\V,\W$ are invariant under $\lag$. So $l \in \V+\W$. This implies $ \lag[\V+\W] \subseteq \V+\W $; i.e., $\mathfrak{Lat}(\lag)$ is closed under subspace sums.    \\
			
			\item Let $\V \in \mathfrak{Lat}(\lag)$ be given. By definition, $\lag[\V] \subseteq \V$. It follows that $\lag\left[ \lag[\V] \right] \subseteq \lag[\V]$. So, $\lag[\V]$ is invariant under $\lag$.\\
			
			\item Let $\W \in \mathfrak{Lat}(\lag)$ be given. Claim: $\lag\left[\lag^{-1}[\W] \right] \subseteq \lag^{-1}[\W] $.\\
			
			It follows from the definition of the pre-image of $\W$ under $\lag : \V \to \W$ that $\lag^{-1}[\W] = \left\{ v \in \V \vert \lag(v) \in \W \right\}$. This implies $\lag\left[\lag^{-1}[\W]\right] \subseteq \W$ (i).\\
			
			Next, consider $w\in\W \subseteq \V$. By definition, $\lag^{-1}\left[\lag[\W] \right] = \{ v\in \V \vert \lag(v) \in \lag[\W]\},$ which implies $w \in \lag^{-1}[
			\lag[\W]]$, because $\lag(w) \in \lag[\W]$. Therefore, $\W \subseteq \lag^{-1}\left[\lag[\W]\right]$ (ii).\\   
			
			Moreover, because $\W \in \mathfrak{Lat}(\lag)$, $\lag[\W] \subseteq \W$ (iii).\\ 
			
			From (i), (ii), and (iii) we have
			\begin{align*}
			\lag\left[ \lag^{-1}[\W] \right] \stackrel{(i)}\subseteq \W \stackrel{(ii)}\subseteq \lag^{-1}\left[ \lag[\W] \right] \stackrel{(iii)}\subseteq \lag^{-1}[\W].
			\end{align*} 
			Therefore, $\lag\left[ \lag^{-1}[\W] \right] \subseteq \lag^{-1}[\W] $, verifying the claim.
			
			
		\end{enumerate}
	\end{sln*}
	
\end{prob*}














\newpage

\begin{prob*}\textbf{4. Cyclic invariant subspaces}
	For a given $\lag\in\mathfrak{L}(\V)$ and a fixed $v_0 \in \V$, define
	\begin{align*}
	\mathbf{P}(\lag,v_0) := \left\{ \left(a_0 \lag^0 + a_1\lag^1 + a_2\lag^2 + \dots + a_k \lag^k\right)(v_0)\bigg\vert k \geq 0, a_i \in \mathbb{C} \right\}.
	\end{align*}
	\begin{enumerate}
		\item Argue that $\mathbf{P}(\lag,v_0)$ is a subspace of $\V$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is invariant under $\lag$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is the smallest invariant subspace for $\lag$ that contains $v_0$. We refer to $\mathbf{P}(\lag,v_0)$ as \textbf{the cyclic invariant subspace for $\lag$ generated by $v_0$}.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is 1-dimensional exactly when $v_0$ is an eigenvector of $\lag$.
		\item Argue that a subspace $\W$ of $\V$ is invariant under $\lag$ exactly when it is a union of cyclic invariant subspaces for $\lag$. 
	\end{enumerate}
	
	\begin{sln*}\textbf{4.}
		$\,$
		\begin{enumerate}
			\item Consider a typical element $P(\lag, v_0) \in \mathbf{P}(\lag,v_0) $. Since $\lag \in \mathfrak{L}(\V)$, $\lag^i(v_0) \in \V$ for all $i$ and $v_0 \in \V$. By closure under scalar multiplication and addition, for any $a_i\in \mathbb{C}$, $v_0\in \V$, and $k\geq 0$, $$ P(\lag, v_0) = \sum_{i=0}^k a_i \lag^i(v_0) \in \V.$$ Therefore $\mathbf{P}(\lag,v_0) \subseteq \V$ (i).\\
			
			With $a_i = 0$ for all $i$, $ \mathbf{P}(\lag,v_0) \ni P(\lag, v_0) = \sum^k_{i=1}0\cdot\lag^i(v_0) = \mathbf{0}_\mathbf{P}$. So $\mathbf{P}(\lag,v_0)$ contains the null element (ii).\\
			
			Consider $P_1(\lag, v_0) = \sum^k_{i=0}b_i\lag^i(v_0)$ and $P_2(\lag,v_0) = \sum^l_{j=0}c_j\lag^j(v_0)$, with $b_i, c_j \in \mathbb{C}$ for all $i,j$. Without loss of generality, assume that $k\geq l \geq 0$. It is clear that $\mathbf{P}(\lag,v_0)$ is closed under addition because, 
			\begin{align*}
			P_1(\lag,v_0) + P_2(\lag,v_0) &= \sum^k_{i=0}b_i\lag^i(v_0) + \sum^l_{j=0}c_j\lag^j(v_0)\\
			&= \sum^k_{i=0}b_i\lag^i(v_0) + \sum^k_{i=0}c_i\lag^i(v_0)\,\,\,\, \text{with }c_i = 0 \text{ for all } i > l\\
			&= \sum^k_{i=0}(b_i + c_i)\lag^i(v_0)\\ 
			&= \sum^k_{i=0}d_i\lag^i(v_0) \in \mathbf{P}(\lag,v_0),
			\end{align*}
			where $d_i = b_i + c_i \in \mathbb{C}$. (iii).\\ 
			
			It is also clear that $\mathbf{P}(\lag,v_0)$ is closed under scalar multiplication because given $\mu\in \mathbb{C}$, $$\mu P_1(\lag,v_0) = \mu\sum_{i=0}^kb_i\lag^i(v_0) = \sum_{i=0}^k\mu b_i\lag^i(v_0) = \sum_{i=0}^ke_i\lag^i(v_0) \in \mathbf{P}(\lag,v_0),$$ where $e_i = \mu b_i \in \mathbb{C}$ (iv).\\
			
			By (i), (ii), (iii), and (iv), $\mathbf{P}(\lag,v_0)$ is a subspace of $\V$. \\
			
			
			\item Let $P(\lag,v_0)$ be given. It suffices to show that $\lag(P(\lag, v_0)) \in \mathbf{P}(\lag,v_0)$, for all $P(\lag,v_0) \in \mathbf{P}(\lag,v_0)$. $$ \lag(P(\lag,v_0)) = \lag\left( \sum_{i=0}^k a_i\lag^i(v_0) \right) = \sum_{i=0}^k a_i \lag^{i+1}(v_0),$$ where $a_i\in\mathbb{C}$ and $k\geq 0$. Define new coefficients $b_{j} = a_{i}$ where $j=i+1$ and $b_0 = 0$, then $$ \lag(P(\lag,v_0)) = \sum_{j=1}^{k+1} b_j\lag^j(v_0) = \sum_{j=0}^{k+1} b_j\lag^j(v_0) \in \mathbf{P}(\lag,v_0). $$
			Therefore, $\lag\left[\mathbf{P}(\lag,v_0) \right] \subseteq \mathbf{P}(\lag,v_0)$; i.e., $\mathbf{P}(\lag,v_0)$ is invariant under $\lag$. \\
			
			
			
			\item  Let $\mathbf{P}(\lag,v_0)$ be given. $\mathbf{P}(\lag,v_0)$ is an invariant subspace under $\lag$ that contains $v_0$ by definition. Suppose $\mathbf{Q}(\lag,v_0)$ is some invariant subspace under $\lag$ that also contains $v_0$. It suffices to show $\mathbf{P}(\lag,v_0) \subseteq \mathbf{Q}(\lag,v_0)$ for any such $\mathbf{Q}(\lag,v_0)$. \\
			
			Consider $\lag^j(v_0)$ for some $j\geq 0$. $\lag^j{(v_0)}\in \mathbf{P}(\lag,v_0)$ by definition. We claim that $\lag^j(v_0) = q\in \mathbf{Q}(\lag,v_0)$ for any non-negative $j$. Assume that this is true. The base case where $j=0$ is true since $v_0 \in \mathbf{Q}(\lag,v_0)$. The inductive case is also true because
			\begin{align*}
			\lag^{(j+1)}(v_0) = \lag(\lag^j(v_0)) = \lag(q) \in \mathbf{Q}(\lag,v_0) 
			\end{align*}
			where the last relation comes from the fact that $\mathbf{Q}(\lag,v_0)$ is invariant under $\lag$. Therefore, by the principle of induction, $\lag^j(v_0) \in \mathbf{Q}(\lag,v_0)$ for any non-negative $j$. This implies $\mathbf{P}(\lag,v_0) \subseteq \mathbf{Q}(\lag,v_0)$ for any such $\mathbf{Q}(\lag,v_0)$. Thus $\mathbf{P}(\lag,v_0)$ must be the smallest invariant subspace under $\lag$ that contains $v_0$. 
			
			
			
			\item 
			\begin{enumerate}

			\item $(\implies):$ Let a $\mathbf{P}(\lag,v_0)$ be given such that $\dim(\mathbf{P}(\lag,v_0)) = 1$. Consider the subspace $\xpan(v_0)$. We know that $\dim(\xpan(v_0)) = 1$. Consider $v \in \xpan(v_0)$. Then $v$ can be expressed as $v = av_0$ where $a\in \mathbb{C}$. It follows immediately that $v\in \mathbf{P}(\lag,v_0)$. Therefore, $\xpan(v_0) \subseteq \mathbf{P}(\lag,v_0)$. But because $\dim(\mathbf{P}(\lag,v_0)) = 1 = \dim(\xpan(v_0))$, $\mathbf{P}(\lag,v_0) = \xpan(v_0)$; i.e., all elements of $\mathbf{P}(\lag,v_0)$ are some scalar multiples of $v_0$.\\
			
			Now, it suffices to show $\lag(v_0) = \lambda v_0$, where $\lambda \in \mathbb{C}$. Consider $P(\lag, v_0) = \lag^0(v_0) + \lag(v_0) \in \mathbf{P}(\lag,v_0) = \xpan(v_0)$, $v_0 \neq \mathbf{0}_\V$. By closure under addition, $$(\lag^0(v_0) + \lag(v_0)) - v_0 = (\lag^0(v_0) + \lag(v_0)) - \lag^0(v_0) = \lag(v_0) \in \xpan(v_0).$$ This implies $\lag(v_0) = \lambda v_0$ for some $\lambda \in \mathbb{C}$; i.e., $v_0$ is an eigenvector of $\lag$.\\
				
				
			\item $(\impliedby):$ If $v_0$ is an eigenvector of $\lag$, then $\lag^i(v_0) = \lambda^i v_0$, where $\lambda^i \in \mathbb{C}$ is the $\lag$'s $v_0$-eigenvalue raised to the $i^{\text{th}}$ power. It follows that a typical element of $\mathbf{P}(\lag,v_0)$ is $$P(\lambda,v_0) = \sum_{i=0}^k a_i \lag^i (v_0) =  \left(\sum_{i=0}^k a_i\lambda^i\right)  v_0 = \mu v_0,$$
			where $\mu = \sum_{i=0}^k a_i \lambda^i \in \mathbb{C}$. Therefore, $\mathbf{P}(\lag,v_0) = \xpan(v_0)$; i.e., $\dim\left(\mathbf{P}(\lag,v_0)\right) = 1$. This completes the argument. \\
			
			\end{enumerate}
		
		\item
		\begin{enumerate}
			
			\item $(\implies):$ Let $\W \in \mathfrak{Lat}(\lag)$ be given. Then $\lag[\W] \subseteq \W$. Let $w_1 \in \W$ be given. We first show that $\lag^k(w_1) \in \W$ for all non-negative $k$ by induction. Assume that $\lag^k(w_1) = v_1 \in \W$ for all non-negative $k$ holds. The base case where $k=0$ is true since $\lag^0(w_1) = w_1\in\W$. The inductive case is also true because $$\lag^{(k+1)}(w_1) = \lag(\lag^k(w_1)) = \lag(v_1) \in \W.$$ By the principle of induction, $\lag^k(w_1) \in \W$ for all $w_1\in\W$ and $k\geq 0$ ($\dagger$). Now, consider $P \in \mathbf{P}(\lag,w_1)$. By the definition of $\mathbf{P}(\lag,w_1)$, $P$ is a linear combination of $\lag^0(w_1),\dots,\lag^k(w_1)$ for some $k\geq 0$. So, by $(\dagger)$, $P \in \W$. This implies $\mathbf{P}(\lag,w_1) \subseteq \W$. \\
			
			We can repeat the argument above, starting with $w_2,w_3,\dots \in \W$, and conclude that $\mathbf{P}(\lag,w_2) \subseteq \W$, $\mathbf{P}(\lag,w_3) \subseteq \W$, and so on. This means $\W$ contains a union of $\mathbf{P}(\lag,w_1), \mathbf{P}(\lag,w_2), \mathbf{P}(\lag,w_3), \dots$. Moreover, observe the fact that any element $w_n \in \W$ can be used to generate a $\mathbf{P}(\lag,w_n)$ that is contained in the union of itself and the other $\mathbf{P}(\lag,w_m)$'s, which is ultimately contained in $\W$. Therefore, $\W$ \textit{is} itself a union of cyclic invariant subspaces for $\lag$.\\  
			
			
			\item $(\impliedby):$ Let $\W = \mathbf{P_1} \cup \mathbf{P_2} \cup \dots \cup \mathbf{P_n}$ be a subspace of $\V$, where $\mathbf{P_i}$ denotes $\mathbf{P}(\lag,v_i)$ and $v_i$ is an arbitrary element of $\V$. By problem 1, because $\mathbf{P_1} \cup \mathbf{P_2} \cup \dots \cup \mathbf{P_n}$ is a subspace of $\V$, a $\mathbf{P_i}$, $1\leq i \leq n$, must contain all the other $\mathbf{P_j}$'s. Without loss of generality, assume that such a $\mathbf{P_i}$ is $\mathbf{P_1}$. It follows that $\W = \mathbf{P_1}$. By part 2. of this problem, we know that $\mathbf{P_1}$ is invariant under $\lag$. Therefore, $\W$ is invariant under $\lag$.
			
			 
			\end{enumerate} 
			
		\end{enumerate}
	\end{sln*}
\end{prob*}














\newpage


\begin{prob*} \textbf{5. Invariant subspaces of commuting operators:} Suppose that linear functions $\lag, \mathcal{M} \in \mathfrak{L}(\V)$ commute; i.e.,
	\begin{align*}
	\lag \circ \mathcal{M} = \mathcal{M}\circ \lag.
	\end{align*}
	\begin{enumerate}
		\item Argue that for any non-negative integer $k$, $\Im(\mathcal{M}^k)$ and $\ker(\mathcal{M}^k)$ are invariant subspace for $\lag$.
		\item Argue that every eigenspace of $\mathcal{M}$ is an invariant subspace for $\lag$.
		\item By giving a general example (with justification, of course!) show that for each $n>1$ there are commuting matrices $\mathcal{A}$ and $\mathcal{B}$ in $\mathbb{M}_n$ such that
		\begin{align*}
		\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B}).
		\end{align*}
	\end{enumerate}
	\begin{sln*}\textbf{5.}
		$\,$
		\begin{enumerate}
			\item Because $\lag \circ \M = \M \circ \lag$, $\lag \circ \M^k = \M^k \circ \lag$ for any non-negative $k$. We can verify this by a short proof by induction. Assume that $\lag \circ \M^k = \M^k \circ \lag$ holds for any non-negative $k$. The base case where $k=0$ is  true, for $\lag\circ \M^0 = \lag \circ \mathcal{I} = \mathcal{I} \circ \lag = \M^0 \circ \lag$, where $\mathcal{I}$ is the identity operator. The inductive case is also true because $$ \lag\circ \M^{k+1} = \lag \circ \M^k \circ \M = \M^k\circ\lag\circ\M = \M^k\circ \M \circ \lag = \M^{k+1}\circ \lag.$$ Therefore, by the principle of induction, $\lag \circ \M^k = \M^k \circ \lag$ is indeed true for all $k\geq 0$.
			
			\begin{enumerate}
				\item \underline{To show}: $\lag\left[ \Im(\M^k)\right] \subseteq \Im(\M^k)$.\\
				
				 Since $\lag\circ \M^k = \M^k \circ \lag$,  $\lag\circ \M^k[\V] = \M^k\circ\lag[\V]$; i.e., $\lag[\Im(\M^k)] = \M^k[\Im(\lag)]$. But since $\Im(\lag) \subseteq \V$ (because $\lag \in \mathfrak{L}(\V)$), $$\M^k[\Im(\lag)] \subseteq \M^k[\V] = \Im(\M^k).$$ It follows that $\lag[\Im(\M^k)] \subseteq \Im(\M^k)$. So, $\Im(\M^k)$ is an invariant subspace for $\lag$.\\
				
				\item \underline{To show}: $\lag[\ker(\M^k)] \subseteq \ker(\M^k)$. \\
				
				Since $\lag\circ \M^k = \M^k \circ \lag$, $$\lag\circ \M^k[\ker(\M^k)] = \{\mathbf{0}_\V\} = \M^k \circ \lag[\ker(\M^k)].$$ It follows that $\lag[\ker(\M^k)] \subseteq \ker(\M^k)$. This completes the argument.\\
			\end{enumerate}
			
			
			\item Let $\mathbf{E}_\lambda$, the eigenspace of $\M$ associated with eigenvalue $\lambda$, be given. Consider $e \in \mathbf{E}_\lambda$, we have $$\M\circ \lag(e) = \lag \circ \M(e) = \lag(\lambda e) = \lambda \lag(e).$$ This implies $\lag(e) \in \mathbf{E}_\lambda$. Since this relation holds for any $e\in \mathbf{E}_\lambda$, $\lag[\mathbf{E}_\lambda] \subseteq \mathbf{E}_\lambda$. Therefore, $\mathbf{E}_\lambda$ is an invariant subspace for $\lag$ for any eigenvalue $\lambda$. This proves the claim that \textit{every} eigenspace of $\M$ is an invariant subspace for $\lag$.\\
			
			
			\item It suffices to find commuting matrices $\mathcal{A}$ and $\mathcal{B}$ such that there exists an invariant subspace under $\mathcal{A}$, $\J \in \mathfrak{Lat}(\mathcal{A})$, but $\J \notin \mathfrak{Lat}(\mathcal{B})$. \\
			
			For $n=2$, consider
			\begin{align*}
			\mathcal{A} = \begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix},\,\,\,\,\,
			\mathcal{B} = \begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}.
			\end{align*} 
			$\mathcal{A}$ and $\mathcal{B}$ commute:
			\begin{align*}
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}\begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}=
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}
			=
			\begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}
			\end{align*}
			
			Consider $\J = \xpan\{(1\,\,\,1)^\top\}$. $\J \in \mathfrak{Lat}(\mathcal{A})$ since $\mathcal{A}[\J] = \{ \mathbf{0}\} \subseteq \J$. However, consider $j = (1\,\,\,1)^\top \in \J$, we have
			\begin{align*}
			\mathcal{B}(j) = \begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}\begin{pmatrix}
			1\\1
			\end{pmatrix}=
			\begin{pmatrix}
			1\\0
			\end{pmatrix} \notin \xpan\{ (1\,\,\, 1)^\top \},
			\end{align*} 
			which implies $\J \notin \mathfrak{Lat}(\mathcal{B})$. Therefore, $\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B})$.\\
			
			Observe that we can give a more general example for any $n>1$ based on the $2\times 2$ example above. Let $\mathcal{A}$ be an $n\times n$ zero matrix, and $\mathcal{B}$ be an $n\times n$ matrix of the form
			\begin{align*}
			\mathcal{B} = \begin{pmatrix}
			1&0&\dots&0\\
			0&0\\
			\vdots&&\ddots\\
			0&&&0
			\end{pmatrix}.
			\end{align*}
			It is clear that $\mathcal{A}$ and $\mathcal{B}$ commute, since $\mathcal{A}$ is the zero matrix: $$\mathcal{AB} = [0]_{n\times n} = \mathcal{BA}.$$\\
		
			Consider $j = (1\,\,1\,\,\dots\,\,1)^\top \in \J = \xpan\{ (1\,\,1\,\,\dots\,\,1)^\top \} \subseteq \mathbb{C}^n$. It is clear that $\mathcal{B}(j) = (1\,\,0\,\,\dots\,\,0)^\top \notin \J$ but $\mathcal{A}(j') = \mathbf{0} \in \J$ for any $j'\in\J$. So, $\J \in \mathfrak{Lat}(\mathcal{A})$ but $\J \notin \mathfrak{Lat}(\mathcal{B})$; i.e., $$\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B}).$$ 
		\end{enumerate}
	\end{sln*}
\end{prob*}








\newpage

\begin{prob*}\textbf{6. Invariant subspace chains:} Suppose that $\lag \in \mathfrak{L}(\V)$ and $\W$ is an invariant subspace of $\lag$. 
	\begin{enumerate}
		\item Argue that the following inclusions hold:
		\begin{align*}
		\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq \lag^{-3}[\W]\subseteq\dots
		\end{align*}
		Note that by Problem 3, all of the subspaces listed in the chain are invariant under $\lag$.
		\item Argue that the following implications hold:
		\begin{align*}
		\lag^{k+1}[\W] = \lag^{k}[\W] \implies \lag^m[\W] = \lag^k[\W]\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and 
		\begin{align*}
		\lag^{-k}[\W] = \lag^{-(k+1)}[\W] \implies \lag^{-k}[\W] = \lag^{-m}[\W]\,\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and then explain why the chain in part 1 stabilizes in both directions, and has at most $\dim(\V)$ proper inclusions. 
		\item Argue that any subspace $\mathbf{M}$ that falls between two consecutive subspaces in the chain shown in part 1. is also invariant under $\lag$. 
		
		
	\end{enumerate}
	\begin{sln*}\textbf{6.}
		$\,$
		\begin{enumerate}
			\item 
			\begin{enumerate}
				\item \underline{To show}: $\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W$.\\
				
				Assume that $ \lag^{k+1}[\W] \subseteq \lag^k[\W] $ for all $k\geq 0$. The base case $k=0$ is true because $\W$ is an invariant subspace under $\lag$. The inductive case is also true because $$ \lag^{k+2}[\W] = \lag[\lag^{k+1}[\W]] \subseteq \lag[\lag^k[\W]] = \lag^{k+1}[\W],$$ where the second relation follows from assumption. By principle of induction, $\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W$.\\
				
				\item \underline{To show}: $\W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq \lag^{-3}[\W]\subseteq\dots$.\\
				
				Assume that $\lag^{-k}[\W] \subseteq \lag^{-k-1}[\W] $ for all $k\geq 0$. The base case where $k=0$ is true: Given $w\in\W\subseteq \V$, $\lag(w) \in \W$ because $\W$ is invariant under $\lag$. But since $\lag^{-1}[\W] = \{ v\in\V \vert \lag(v)\in \W\}$ and $\lag[\W]\subseteq \W$, $w \in \lag^{-1}[\W]$. Therefore, $\W \subseteq \lag^{-1}[\W]$ (i).\\
				
				The inductive case is also true: $$ \lag^{-k-1}[\W] = \lag^{-1}[\lag^{-k}[\W]] \subseteq \lag^{-1}[\lag^{-k-1}[\W]] \subseteq \lag^{-k-2}[\W].  $$ So by the principle of induction, $\W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq\dots$ (ii). \\
			\end{enumerate}
				
			From (i) and (ii), $$ \dots  \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq\dots $$
				
				
				
			\item 
			\begin{enumerate}
				\item Assume the hypothesis that $\lag^{k+1}[\W] = \lag^k[\W]$ for some $k\geq 0$. We can prove by induction that $\lag^{k+j}[\W] = \lag^k[\W]$ for all non-negative $j$. The base case where $j=0$ is true. The inductive case is also true since: $$ \lag^{k+j+1}[\W] =  \lag[\lag^{k+j}[\W]] = \lag[\lag^k[\W]] = \lag^{k+1}[\W] = \lag^k[\W]. $$ By the principle of induction, $\lag^{m}[\W] = \lag^k[\W]$ must hold for any $m\geq k\geq 0$ if $\lag^{k+1}[\W] = \lag^k[\W]$.\\
				
				\item Assume that hypothesis that $\lag^{-k}[\W] = \lag^{-(k+1)}[\W]$ for all $k \geq 0$. Also assume that $\lag^{-k}[\W] = \lag^{-(k+j)}[\W]$ for all non-negative $j$. The base case where $j=0$ is true. In inductive case is also true since: 
				 \begin{align*}
				 				 \lag^{-(k+j+1)}[\W] &= \lag^{-1}[\lag^{-(k+j)}[\W]\\ &=  \lag^{-1}[\lag^{-k}[\W]]\\ &= \lag^{-(k+1)}[\W]\\ &= \lag^{-k}[\W].
				 \end{align*}
 				By the principle of induction, $\lag^{-k}[\W] = \lag^{-m}[\W]$ must hold for $m\geq k$ if $\lag^{-k}[\W] = \lag^{-(k+1)}[\W]$.\\
				
				

				\item It follows from part 1. that $$ \dots \leq \dim(\lag[\W]) \leq \dim(\W) \leq \dim(\lag^{-1}[\W]) \leq \dots  $$
				Because $\dim(\lag^{-k}[\W]) \leq \dim(\V)$ for any $k\geq 0$, there exists an $\lag^{-h}[\W]$, $h\geq 0$, that is an invariant subspace in the chain with the highest dimension where $h$ is minimal. It follows that $\dim(\lag^{-(h+1)}[\W]) = \dim(\lag^{-(h)}[\W])$ is maximal. But because $\lag^{-h}[\W] \subseteq \lag^{-(h+1)}[\W]$, $\lag^{-h}[\W] = \lag^{-(h+1)}[\W]$. As a result, $\lag^{-h}[\W] = \lag^{-j}[\W]$ for any $j\geq h$, which follows from (b). Therefore, the chain in part 1. stabilizes in the ``inclusion'' direction.  \\
				
				A similar argument can be given to show that the chain in part 1. also stabilizes in the ``inclusion by'' direction. Since $0 \leq \dim(\lag^{l}[\W])$ for any $l\geq 0$, there exists an $\lag^l[\W]$ in the chain with the lowest dimension but $l$ is minimal. It follows that $\dim(\lag^{(l+1)}[\W]) = \dim(\lag^{l}[\W])$ is minimal. But because $\lag^{(l+1)}[\W] \subseteq \lag^{l}[\W]$, $\lag^{(l+1)}[\W] = \lag^{l}[\W]$. As a result, $\lag^{k}[\W] = \lag^{l}[\W]$ for any $k\geq l$, which follows from (a). Therefore, the chain in part 1. stabilizes in the ``inclusion by'' direction as well.\\
				
				We claim that for every \textit{proper} inclusion, there is a dimension loss. Revisiting $\lag^{-h}[\W]$, we know that $\dim(\lag^{-h}[\W]) \leq \dim(\V)$. Consider the first proper inclusion: $$\dots \lag^{-(h-j)}[\W] \subsetneq \lag^{-(h-j+1)}[\W] \subseteq \dots \subseteq \lag^{-h}[\W]\subseteq \dots$$ 
				
				This implies $\dim(\lag^{-(h-j)}[\W]) < \dim(\lag^{-h}[\W]) \leq \dim(\V)$. Suppose $\dim(\lag^{-h}[\W]) = \dim(\V)$, i.e., maximal, then $$\dim(\lag^{-(h-j)}[\W]) \leq \dim(\V) - 1.$$ By assuming equality is satisfied and consider the next proper inclusion in the chain, a similar argument shows that the invariant subspace of interest has at most $\dim(\V) - 2$ dimensions. It follows that, at the $\dim(\V)^{\text{th}}$ proper inclusion, the considered invariant subspace has at most $\dim(\V) - \dim(\V) = 0$ dimensions; i.e. the subspace is $\{\mathbf{0}_\V\}$. At this point, no further proper inclusion is possible. Therefore, the chain in part 1. has at most $\dim(\V)$ proper inclusion.\\
			\end{enumerate}
				
				
				
				
				
			\item	
			Let $\lag^{(k+1)}[\W] \subseteq \mathbf{M} \subseteq \lag^{k}[\W]$ be given for some nonnegative integer $k$. $\mathbf{M}$ is a subspace. We have that
			$$\lag[\mathbf{M}] \subseteq \lag[\lag^{k}[\W]] = \lag^{(k+1)}[\W] \subseteq \mathbf{M}^{(i)}. $$
			
			Let $\lag^{-h}[\mathbf{W}] \subseteq \mathbf{M} \subseteq \lag^{-(h+1)}[\W]$ be given for some nonnegative integer $h$. We have that
			$$ \lag[\mathbf{M}] \subseteq \lag[\lag^{-(h+1)}[\W]] = \lag[\lag^{-1}[\lag^{-h}[\W]]] \subseteq \lag[\lag^{-1}[\mathbf{M}]].  $$
			But because $\lag^{-1}[\mathbf{M}] = \{ v\in \V \vert \lag(v)\in \mathbf{M} \}$, $\lag[\lag^{-1}[\mathbf{M}]] \subseteq \mathbf{M}$. Therefore, we have $\lag[\mathbf{M}] \subseteq \mathbf{M}^{(ii)}$.\\ 
			
			From (i) and (ii), $\mathbf{M}$ is invariant under $\lag$ if $\mathbf{M}$ is a subspace that falls between two consecutive subspaces in the chain shown in part 1.. 
			
		\end{enumerate}
	\end{sln*}
\end{prob*}






































\newpage
\subsection{Problem set 3}

\begin{prob*}\textbf{1.} Suppose that $\V_1, \V_2, \V_3$ are non-trivial vector spaces, and for each $i,j \in \{ 1,2,3\}$,
	\begin{align*}
	\lag_{ij} : \V_j \overset{\text{linear}}{\longrightarrow} \V_i.
	\end{align*}
	\begin{enumerate}
		\item Argue that 
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times
		\end{align*}
		is a linear function.\\
		
		
		
		
		
		
		
		
		
		

		
		\item Argue that every linear function $T : \V_1 \times \V_2 \times \V_3 \to \V_1\times\V_2\times \V_3$ can be expressed as 
		\begin{align*} 
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33} 
		\end{bmatrix}_\times,
		\end{align*} 
		with $\lag_{ij} : \V_j \overset{\text{linear}}{\longrightarrow} \V_i$, in exactly one way. \\
		
		\item If the function
		\begin{align*}
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times : \V_1 \times \V_2 \times \V_3 \to \V_1\times\V_2\times \V_3
		\end{align*}
		is defined similarly, find (with proof) the block-matrix form of the functions
		\begin{align*}
		2&\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times 
		+ 
		3\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times 
		\end{align*}
		and
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times
		\circ 
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times.
		\end{align*}
	\end{enumerate}
	
	\newpage
	
	\begin{sln*}\textbf{1. }
		\begin{enumerate}
			\item It suffices to show that $\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times$ satisfies the linearity condition.\\
			
			Consider 
			\begin{align*}
			\begin{pmatrix}
			a_1\\a_2\\a_3
			\end{pmatrix}, \begin{pmatrix}
			b_1 \\ b_2 \\ b_3
			\end{pmatrix} \in \V_1 \times \V_2 \times \V_3
			\end{align*} 
			where $a_j, b_j \in \V_j$ for $j \in \{1,2,3 \}$. By definition, 
			\begin{align*}
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			a_1 \\ a_2 \\ a_3
			\end{pmatrix} 
			+ 
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			b_1 \\ b_2 \\ b_3
			\end{pmatrix} \\
			=
			&\begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3)
			\end{pmatrix} 
			+
			\begin{pmatrix}
			\lag_{11}(b_1) + \lag_{12}(b_2) + \lag_{13}(b_3)\\
			\lag_{21}(b_1) + \lag_{22}(b_2) + \lag_{23}(b_3)\\
			\lag_{31}(b_1) + \lag_{32}(b_2) + \lag_{33}(b_3)
			\end{pmatrix}\\
			=
			& \begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3) + \lag_{11}(b_1) + \lag_{12}(b_2) + \lag_{13}(b_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3) + \lag_{21}(b_1) + \lag_{22}(b_2) + \lag_{23}(b_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3) + \lag_{31}(b_1) + \lag_{32}(b_2) + \lag_{33}(b_3)
			\end{pmatrix}\\
			=
			& \begin{pmatrix}
			[\lag_{11}(a_1) + \lag_{11}(b_1)] + [\lag_{12}(a_2) + \lag_{12}(b_2)] + [\lag_{13}(a_3) + \lag_{13}(b_3)]\\
			[\lag_{21}(a_1) + \lag_{21}(b_1)] + [\lag_{22}(a_2) + \lag_{22}(b_2)] + [\lag_{23}(a_3) + \lag_{23}(b_3)]\\
			[\lag_{31}(a_1) + \lag_{31}(b_1)] + [\lag_{32}(a_2) + \lag_{32}(b_2)] + [\lag_{33}(a_3) + \lag_{33}(b_3)]
			\end{pmatrix}, \hspace{0.5cm} \text{by the linearity of $\lag_{ij}$}\\
			=
			& \begin{pmatrix}
			\lag_{11}(a_1 + b_1) + \lag_{12}(a_2 + b_2) + \lag_{13}(a_2 + b_2)\\
			\lag_{21}(a_1 + b_1) + \lag_{22}(a_2 + b_2) + \lag_{23}(a_2 + b_2)\\
			\lag_{31}(a_1 + b_1) + \lag_{32}(a_2 + b_2) + \lag_{33}(a_2 + b_2)
			\end{pmatrix}\\
			=
			& \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			a_1 + b_1\\a_2 + b_2\\a_3 + b_3
			\end{pmatrix}\\
			=
			& \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times
			\left( 
			\begin{pmatrix}
			a_1\\a_2\\a_3
			\end{pmatrix}
			+
			\begin{pmatrix}
			b_1\\b_2\\b_3
			\end{pmatrix}
			\right).
			\end{align*}  
			
			Therefore, the additivity condition is satisfied $^\dagger$. Next, we consider the scalar multiplication condition. Let $c \in \mathbb{C}$ be given,
			
			
			\begin{align*}
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\left(c\begin{pmatrix}
			a_1 \\ a_2 \\ a_3
			\end{pmatrix}\right) \\
			= 
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			ca_1 \\ ca_2 \\ ca_3
			\end{pmatrix}\\
			=
			&\begin{pmatrix}
			\lag_{11}(ca_1) + \lag_{12}(ca_2) + \lag_{13}(ca_3)\\
			\lag_{21}(ca_1) + \lag_{22}(ca_2) + \lag_{23}(ca_3)\\
			\lag_{31}(ca_1) + \lag_{32}(ca_2) + \lag_{33}(ca_3)
			\end{pmatrix} \\
			=
			& \begin{pmatrix}
			c\lag_{11}(a_1) + c\lag_{12}(a_2) + c\lag_{13}(a_3)\\
			c\lag_{21}(a_1) + c\lag_{22}(a_2) + c\lag_{23}(a_3)\\
			c\lag_{31}(a_1) + c\lag_{32}(a_2) + c\lag_{33}(a_3)
			\end{pmatrix}\hspace{0.5cm}\text{by the linearity of $\lag_{ij}$}\\
			=
			& c\begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3)
			\end{pmatrix}.
			\end{align*}
			Therefore, the scalar multiplication condition is also satisfied $^{\dagger\dagger}$.\\
			
			From $(\dagger)$ and $(\dagger\dagger)$, $
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times
			$
			is a linear function.\\
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			\item
			\begin{enumerate}
			\item \underline{Existence:} Let $\Pi_i$ be a coordinate projection on $\W_1 \times \W_2 \times \W_3$ onto $\V_i$ defined in Definition 0.2. Let $\gamma_i$ be a coordinate injection of $\W_1 \lin \W_1 \times \W_2$ into $\W_3$ also defined in Definition 0.2. \\
			
			Consider the block-matrix $\lag$
			\begin{align*}
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times.
			\end{align*}
			Let the linear function $\T : \V \lin \V$ be given. Let each $\lag_{ij} : \W_j \lin \W_i$ be defined as
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j}.
			\end{align*}
			Then we have
			\begin{align*}
			\lag &= \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times\\
			&=
			\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_1 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\W_3}\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_2 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\W_3}\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_3\bigg\vert_{\W_3} 
			\end{bmatrix}_\times.
			\end{align*}
			
			
			
			Consider $\begin{pmatrix}
			w_1 & \mathbf{0}_\V & \mathbf{0}_\V
			\end{pmatrix}^\top \in \W_1\times\W_2\times\W_3$ with $w_1\in \W_1$.
			\begin{align*}
			&\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_1 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\W_3}\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_2 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\W_3}\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\W_1} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\W_2} & \Pi_3 \circ \T \circ \gamma_3\bigg\vert_{\W_3} 
			\end{bmatrix}_\times
			\begin{pmatrix}
			w_1 \\ \mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}\\
			=
			&\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\W_1}(w_1)
			\end{bmatrix}\\
			=
			& \begin{bmatrix}
			\Pi_1 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_2 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_3 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top
			\end{bmatrix}.
			\end{align*}
			By the definition of $\Pi_i$ we can write this as
			\begin{align*}
			\begin{bmatrix}
			\Pi_1 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_2 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_3 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top
			\end{bmatrix} 
			&= \T\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}.
			\end{align*}
			Therefore,
			\begin{align*}
			\lag\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} = \T\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}.
			\end{align*}
			In general, the equality above also holds for any $v = w_1 + w_2 + w_3 \in \V$ with $w_i \in \V_i$, since $v$ can be expressed as
			\begin{align*}
			v = \begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\ w_2\\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\\mathbf{0}_\V \\ w_3
			\end{pmatrix},
			\end{align*}
			and the linearity of $\lag$ and $\T$ ensures that $\lag(v) = \T(v)$ for any $v\in \V$. Hence, $\T$ can be expressed as the matrix $\lag$ whose elements $\lag_{ij}$'s are defined as
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j}.
			\end{align*}.

			\item \underline{Uniqueness: } We observe that all of the steps in the ``Existence part'' are reversible; i.e., if each $\lag_{ij}$ is constructed as
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j}
			\end{align*}
			then the matrix $\lag = [\lag_{ij}]$ expresses the linear map $\T$. Conversely, given the linear map $\T$, the elements of the matrix $\lag$ of $\T$ must have the form
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j}.
			\end{align*}
			This demonstrates the uniqueness of $\lag$. 
			
			\end{enumerate}
			From parts (a) and (b), the linear function $\T$ can be expressed as the matrix $\lag$ in exactly one way.
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			
			
			
			
			
			\item
			
			Let 
			\begin{align*}
			\lag := \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times : \V \lin \V
			\end{align*}
			and
			\begin{align*}
			\M := \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times : \V \lin \V
			\end{align*}
			be given. 
			\begin{enumerate}
			\item Let $\lag$ be the matrix of the linear function $\T$ above and $\M$ be the matrix of a linear function $\K : \V \lin \V$. Consider the matrix element $2\lag_{ij} + 3\M_{ij}$. Following our construction of each $\lag_{ij}$ and $\M_{ij}$ from part 1, 
			\begin{align*}
			2\lag_{ij} + 3\M_{ij} &= 2\Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j} + 
			3\Pi_i \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ 2\T \circ \gamma_j\bigg\vert_{\W_j} + 
			\Pi_i \circ 3\K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \left( 2\T \circ \gamma_j\bigg\vert_{\W_j} + 3\K \circ \gamma_j\bigg\vert_{\W_j}  \right)\\
			&= \Pi_i \circ \left( 2\T + 3\K \right) \circ \gamma_j\bigg\vert_{\W_j}\\
			&= (2\T + 3\K)_{ij}
			\end{align*}
			This shows that each element of the matrix of the linear function $(2\lag + 3\K)$ is given by
			\begin{align*}
			(2\T + 3\K)_{ij} = 2\lag_{ij} + 3\M_{ij},
			\end{align*}
			i.e.,
			\begin{align*}
			&2\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			+ 
			3\begin{bmatrix}
			\M_{11} & \M_{12} & \M_{13}\\
			\M_{21} & \M_{22} & \M_{23}\\
			\M_{31} & \M_{32} & \M_{33}
			\end{bmatrix}_\times\\
			=
			& \begin{bmatrix}
			2\lag_{11} + 3\M_{11} & 2\lag_{12} + 3\M_{12} & 2\lag_{13} + 3\M_{13}\\
			2\lag_{21} + 3\M_{21} & 2\lag_{22} + 3\M_{22} & 2\lag_{23} + 3\M_{23}\\
			2\lag_{31} + 3\M_{31} & 2\lag_{32} + 3\M_{32} & 2\lag_{33} + 3\M_{33}
			\end{bmatrix}_\times.
			\end{align*}
			
			
			\newpage
			
			\item Let $\M$ and $\K$ be defined similarly: $\lag$ is the matrix of the linear function $\T$ and $\M$ is the matrix of the linear function $\K$. Consider the matrix element $\sum^3_{k=1}\lag_{ik}\circ \M_{kj}$. Following our construction of each $\lag_{ij}$ and $\M_{ij}$ from part 1,
			\begin{align*}
			\sum^3_{k=1} \lag_{ik} \circ \M_{kj} 
			&= \sum^3_{k=1}\Pi_i \circ \T \circ \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \sum^3_{k=1} \Pi_i \circ \T \circ \left(\gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right) \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \T \circ \left(\sum_{k=1}^3 \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right) \circ \K \circ \gamma_j\bigg\vert_{\W_j}.
			\end{align*}
			Consider $v = \begin{pmatrix}w_1 & w_2 & w_2\end{pmatrix}^\top \in \V$ such that $w_i \in \W_i$ for $i\in \{1,2,3\}$. We can show that the summation in the expression above is simply the identity function:
			\begin{align*}
			\left(\sum_{k=1}^3 \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right)(v) 
			&=
			\gamma_1\bigg\vert_{\W_1} \circ \Pi_1(v) + \gamma_2\bigg\vert_{\W_2} \circ \Pi_2(v) + \gamma_3\bigg\vert_{\W_3} \circ \Pi_3(v)\\
			&= \gamma_1(w_1) + \gamma_2(w_2) + \gamma_3(w_3)\\
			&=\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\ w_2\\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V \\ \mathbf{0}_\V \\ w_3
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
			w_1\\w_2\\w_3
			\end{pmatrix}\\
			&= v.
			\end{align*}
			Therefore, we have
			\begin{align*}
			\sum^3_{k=1} \lag_{ik} \circ \M_{kj} 
			&= \Pi_i \circ \T \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \left(\T \circ \K\right) \circ \gamma_j\bigg\vert_{\W_j}\\
			&= (\T \circ \K)_{ij}.
			\end{align*}
			This shows that each element of the matrix of the linear function $(\T\circ \K)$ is given by
			\begin{align*}
			(\T\circ\K)_{ij} = \sum^3_{k=1} \lag_{ik} \circ \M_{kj}.
			\end{align*}
			
			
			\end{enumerate}
			
			
			
			
			
			\newpage
			
		\end{enumerate}
			 
			
	\end{sln*}
	
	
\end{prob*}
























\newpage


\begin{prob*}\textbf{2.} Suppose that $\V = \W_1\oplus \W_2\oplus \W_3$ and the $\W_i$'s are non-trivial subspaces of $\V$. Suppose that for each $i,j \in \{1,2,3 \}$,
	\begin{align*}
	\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i.
	\end{align*}
	\begin{enumerate}
		\item Verify for formula 
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_{\oplus} 
		\begin{pmatrix}
		x_1\\x_2\\x_3
		\end{pmatrix}_{\maltese} 
		=
		\left( \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times \begin{pmatrix}
		x_1\\x_2\\x_3
		\end{pmatrix}   \right)_{\maltese},
		\end{align*}
		where we have used as few brackets as possible without losing clarity.
		
		
		
		\item Argue that every $T : \V \overset{\text{linear}}{\longrightarrow} \V$ can be expressed as $\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus$, with $\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i$, in exactly one way.
		
		
		
		\item If the function 
		\begin{align*}
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus : \V \longrightarrow \V
		\end{align*}
		is defined similarly, find (with proof) the corresponding black-matrix form of the functions 
		\begin{align*}
		2&\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus
		+ 
		3\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus 
		\end{align*}
		and
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus
		\circ 
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus.
		\end{align*}

		
		
		\item Suppose that $\mathcal{E}_1 + \mathcal{E}_2 + \mathcal{E}_3 = \mathcal{I}_\V$ is a resolution of the identity, where $\ima(\mathcal{E}_i) = \W_i$. If
		\begin{align*}
		\lag = \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus,
		\end{align*}
		express (with proof, of course) $\lag_{ij}$ in terms of $\lag, \mathcal{E}_k$'s and $\W_k$'s.
		
		
		
		\item Continuing with the set-up of part 4, find $\M_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i$ such that 
		\begin{align*}
		\mathcal{E}_1 = \begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus.
		\end{align*}
		Then do the same for $\mathcal{E}_2$ and $\mathcal{E}_3$.
	\end{enumerate}



	\newpage
	
	\begin{sln*}\textbf{2.}
		\begin{enumerate}
			\item  By definition,
			\begin{align*}
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_{\oplus}
			\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}_{\maltese} 
			&=
			\left[\maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\circ 
			\maltese^{-1}\right]\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}_{\maltese} \\
			&= \left[\maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\circ 
			\maltese^{-1}\circ\maltese\right]\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}\\
			&= \maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times \begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}\\
			&= 
			\left( \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times \begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}   \right)_{\maltese}.
			\end{align*}
						
						
			
			\item 
			\begin{enumerate}
				\item \underline{Existence}: Let the linear functions $\E_i := \Pi_i \circ \maltese^{-1} : \V \lin \W$ and $\K_i := \maltese\circ \gamma_i : \W_i \lin \V$ be given, where the $\Pi_i$'s and $\gamma_i$ are defined the same way as in Problem 1. \\
				
				Consider the block matrix $\lag$
				\begin{align*}
				\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus.
				\end{align*}
				Let the linear function $\T : \V \lin \V$ be given. Let each $\lag_{ij} : \W_j \lin \W_i$ be defined as:
				\begin{align*}
				\lag_{ij} = \E_i \circ \T \circ \K_j \bigg\vert_{\W_j}.
				\end{align*}
				By definition,
				\begin{align*}
				\lag_{ij} &= \E_i \circ \T \circ \K_j \bigg\vert_{\W_j}\\
				&= \Pi_i \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_j\bigg\vert_{\W_j}.
				\end{align*}
				Consider $\begin{pmatrix}
				w_1&\mathbf{0}_\V&\mathbf{0}_\V
				\end{pmatrix}^\top$ and $w_1\in \W_1$, by letting $\lag$ act on $\begin{pmatrix}
				w_1&\mathbf{0}_\V&\mathbf{0}_\V
				\end{pmatrix}^\top_\maltese$, we get
				\begin{align*}
				\lag\begin{pmatrix}
				w_1\\\mathbf{0}_\V\\\mathbf{0}_\V
				\end{pmatrix}_\maltese 
				&=
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
				\Pi_2 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
				\Pi_3 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)
				\end{bmatrix}_\maltese\\
				&=
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top\\
				\Pi_2 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top\\
				\Pi_3 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top
				\end{bmatrix}_\maltese\\
				&= 
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_2 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_3 \circ \maltese^{-1}\circ \T (w_1) 
				\end{bmatrix}_\maltese\\
				&= 
				\maltese\circ\maltese^{-1}\circ \T(w_1)\\
				&= \T(w_1)
				\end{align*} 
				By the definition of $\maltese$, we can write this as
				\begin{align*}
				\lag(w_1 + \mathbf{0}_\V + \mathbf{0}_\V) = \lag(w_1) = \T(w_1).
				\end{align*}
				In general, the equality above also holds if we start with any $v = w_1 + w_2 + w_3 \in \V$ with $w_i \in \W_i$ for $i=\{1,2,3\}$, because as we have shown in Part 2(a) of Problem 1, $v$ can be written as
				\begin{align*}
				v = \begin{pmatrix}
				w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
				\end{pmatrix}_\maltese + 
				\begin{pmatrix}
				\mathbf{0}_\V\\w_2 \\ \mathbf{0}_\V
				\end{pmatrix}_\maltese + 
				\begin{pmatrix}
				\mathbf{0}_\V\\\mathbf{0}_\V \\ w_3
				\end{pmatrix}_\maltese,
				\end{align*}
				and the linearity of $\lag$ and $\T$ ensures $\lag(v) = \T(v)$. Hence, $\T$ can expressed as the matrix $\lag$ whose elements are defined as
				\begin{align*}
				\lag_{ij} = \E_i \circ \T \circ \K_j\bigg\vert_{\W_j}.
				\end{align*}
				
				
				\item \underline{Uniqueness}: Once again, we observe that all of the steps in the ``Existence'' part are reversible, i.e., if each $\lag_{ij}$ is defined as above, then the matrix $\lag = [\lag_{ij}]$ expressed the linear function $\T$. Conversely, given $\T$, the elements of the matrix $\lag$ of $\T$ must be defined the same way. This ensures the uniqueness of $\lag$.\\ 
			\end{enumerate}
		From parts (a) and (b), the linear function $\T$ can be expressed as the matrix $\lag$ defined in the problem statement in exactly one way. 
			
						
						
						
						
						
						
		
			\newpage
			
			
			

			
			
			\item 
			\begin{enumerate}
				\item Let the notation $\lag_\oplus$ denote the matrix $\lag : \W_1\oplus\W_2\oplus\W_3\lin\W_1\oplus\W_2\oplus\W_3$, and $\M_\oplus$ is defined similarly for $\M$. By part 1, we can re-express
				\begin{align*}
				[(2\lag_\oplus + 3\M_\oplus)_{ij}] = 2\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus
				+ 
				3\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\oplus 
				\end{align*}
				as 
				\begin{align*}
				&2\,\,\maltese \circ \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				+ 
				3\,\,\maltese\circ\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times \circ \maltese^{-1} \\
				=\,\,
				&\maltese \circ \left(
				2\,\, \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				+
				3\,\, \begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				\right)\\
				=\,\,
				&\maltese \circ \left(
				2\,\, \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times
				+
				3\,\, \begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times
				\right) \circ \maltese^{-1}\\
				=\,\, &\maltese\circ \left( 2[{\lag_\times}_{ij} ] + 3[{\M_\times}_{ij} ]  \right) \circ \maltese^{-1},
				\end{align*}
				where the notation $[A_{ij}]$ denotes the block-matrix of elements $A_{ij}$ where $A$ is a linear function, and the last relation:
				\begin{align*}
				[(2\lag_\times + 3\M_\times)_{ij}] = 2[{\lag_\times}_{ij}] + 3[{\M_\times}_{ij}].
				\end{align*}
				comes from Problem 1, where we have use the notation $\lag_\oplus$ to indicate the matrix $\lag : \W_1\times\W_2\times\W_3 \lin \W_1\times\W_2\times\W_3$. It follows that
				\begin{align*}
				[(2\lag_\oplus + 3\M_\oplus)_{ij}]
				&= \maltese\circ \left( 2[{\lag_\times}_{ij} ] + 3[{\M_\times}_{ij} ]  \right) \circ \maltese^{-1}\\
				&= \maltese\circ [2{\lag_\times}_{ij}]\circ \maltese^{-1} + \maltese\circ [3{\M_\times}_{ij}]\circ \maltese^{-1}\\
				&= 2[{\lag_\oplus}_{ij}] + 3[{\M_\oplus}_{ij}].
				\end{align*}
				So, in the block-matrix form:
				\begin{align*}
				&2\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus
				+ 
				3\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\oplus\\
				=\,\, 
				&\begin{bmatrix}
				2\lag_{11} + 3\M_{11} & 2\lag_{12} + 3\M_{12} & 2\lag_{13} + 3\M_{13}\\
				2\lag_{21} + 3\M_{21} & 2\lag_{22} + 3\M_{22} & 2\lag_{23} + 3\M_{23}\\
				2\lag_{31} + 3\M_{31} & 2\lag_{32} + 3\M_{32} & 2\lag_{33} + 3\M_{33}
				\end{bmatrix}_\oplus.
				\end{align*}
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				\item Let $\lag_\oplus$ and $\M_\oplus$ denote the same matrices in part (a), and let $\lag_\oplus$ represent the linear function $\T$, $\M_\oplus$ represent the linear function $\N$. Let the matrix $[\lag_\oplus\circ\M_\oplus] $ express the composition $\T\circ\N$. Hence, the elements of $[\lag_\oplus\circ\M_\oplus] $ are given by
				\begin{align*}
				(\lag_\oplus\circ\M_\oplus)_{ij} = \E_i \circ (\lag_\oplus \circ \M_\oplus) \circ \K_j\bigg\vert_{\W_j} = \E_i \circ (\T \circ \N) \circ \K_j\bigg\vert_{\W_j} ,
				\end{align*}
				as we have shown in part 1. Consider the matrix element $\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}$. By part 1, we have
				\begin{align*}
				\lag_{\oplus_{ik}} = \E_i \circ \T \circ \K_k\bigg\vert_{\W_k}\text{ and }\M_{\oplus_{kj}} = \E_k \circ \N \circ \K_j\bigg\vert_{\W_j}.
				\end{align*} 
				This gives
				\begin{align*}
				\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}
				&= \sum^3_{k=1} \E_i \circ \T \circ \left(\K_k\bigg\vert_{\W_k}\circ \E_k \right)\circ \N \circ \K_j\bigg\vert_{\W_j}\\
				&= \E_i \circ \T \circ \left(\sum^3_{k=1}\K_k\bigg\vert_{\W_k}\circ \E_k\right)  \circ \N \circ \K_j\bigg\vert_{\W_j}
				\end{align*}
				By definition, we have
				\begin{align*}
				\K_k\bigg\vert_{\W_k} = \maltese\circ \gamma_k\bigg\vert_{\W_k}\text{ and }\E_k= \Pi_k\circ\maltese^{-1},
				\end{align*}
				which implies
				\begin{align*}
				\sum^3_{k=1}\K_k\bigg\vert_{\W_k}\circ \E_k &= \sum^3_{k=1} \maltese\circ \gamma_k\bigg\vert_{\W_k}\circ \Pi_k\circ\maltese^{-1}\\
				&= \maltese \circ \left(\sum^3_{k=1} \gamma_k\bigg\vert_{\W_k}\circ \Pi_k \right)\maltese^{-1}\\
				&= \maltese\circ\maltese^{-1},\hspace{0.2cm}\text{by Part 3(b) of Problem 1}\\
				&= Id.
				\end{align*}
				It follows that,
				\begin{align*}
					\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}} &= \E_i \circ \T\circ\N\circ\K_j\bigg\vert_{\W_j}\\
					&= \E_i \circ (\lag_\oplus\circ \M_\oplus)\circ\K_j\bigg\vert_{\W_j}\\
					&= (\lag_\oplus\circ\M_\oplus)_{ij}.
				\end{align*}
				Therefore, the matrix elements of $[\lag_\oplus\circ \M_\oplus]$ is given by
				\begin{align*}
				(\lag_\oplus\circ\M_\oplus)_{ij} = \sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}.
				\end{align*}
				
			\end{enumerate}
			
			
			
			
			
			
			
			
			\newpage
			
			
			
			
			
			\item Let $\E_1 + \E_2 + \E_3 = \mathcal{I}_\V$ be a resolution of identity, with $\ima(\E_i) = \W_i$, be given. Furthermore, let the linear map $\lag : \V \lin \V$ be given where
			\begin{align*}
			\lag = \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\oplus.
			\end{align*} 
			\underline{To show}: 
			\begin{align*}
			\boxed{\lag_{ij} =  \E_{i} \circ \lag\bigg\vert_{\W_j} : \W_j \lin \W_i}
			\end{align*}
			Consider the restricted map $\lag\big\vert_{\W_j} : \W_j \lin \V$. Without loss of generality, let $\W_j = \W_1$ and let
			$\begin{pmatrix}
			w_1 & \mathbf{0}_{\W_2} & \mathbf{0}_{\W_3}
			\end{pmatrix}_\maltese^\top \in \V$ be given. We have 
			\begin{align*}
			\lag \begin{pmatrix}
			w_1\\ \mathbf{0}_{\W_2} \\ \mathbf{0}_{\W_3}
			\end{pmatrix}_\maltese
			= 
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\oplus \begin{pmatrix}
			w_1\\ \mathbf{0}_{\W_2} \\ \mathbf{0}_{\W_3}
			\end{pmatrix}_\maltese = 
			\begin{pmatrix}
			\lag_{11}(w_1)\\
			\lag_{21}(w_1)\\
			\lag_{31}(w_1)
			\end{pmatrix}_\maltese.
			\end{align*}
			Consider $v = w_1 + w_2 + w_3\in \V$ with $w_i \in \W_i$. Since $\bigoplus^3_{i=1}\W_i = \V$, there is exactly one way to express $v$ as a sum of these $w_i$'s. Now, consider
			\begin{align*}
			w_1 + w_2 +w_3 &= \mathcal{I}_\V(v)\\
			&= (\E_1 + \E_2 + \E_3)(v)\\
			&= (\E_1 + \E_2 + \E_3)(w_1 + w_2 + w_3)\\
			&= \E_1(w_1 + w_2 + w_3) + \E_2(w_1 + w_2 + w_3) + \E_3(w_1 + w_2 + w_3).
			\end{align*} 
			Since $\ima(\E_i) = \W_i$, it is required that $\E_i(w_1 + w_2 + w_3) = w'_i \in \W_i$. It follows that
			\begin{align*}
			v = w_1 + w_2 +w_3 = w'_1 + w'_2 +w'_3.
			\end{align*}
			But because $v$ can be expressed in only one way as a sum of $w_i$'s $\in \W_i$'s, $w_i = w'_i$ for any $i\in \{1,2,3\}$. Hence,
			\begin{align*}
			\E_i(w_1 + w_2 +w_3) = w_i, \hspace{0.5cm} i \in \{1,2,3\}.
			\end{align*}
			In ``bad'' notation, 
			\begin{align*}
			\E_i \begin{pmatrix}
			w_1\\w_2\\w_3
			\end{pmatrix}_\maltese = w_i, \hspace{0.5cm} i \in \{1,2,3\}.
			\end{align*}
			Applying $\E_2$ to $\lag(w_1 + \mathbf{0}_{\W_2} + \mathbf{0}_{\W_3})$, we have
			\begin{align*}
			\E_2\circ \lag \begin{pmatrix}
			w_1\\ \mathbf{0}_{\W_2} \\ \mathbf{0}_{\W_3}
			\end{pmatrix}_\maltese = \E_2\begin{pmatrix}
			\lag_{11}(w_1)\\
			\lag_{21}(w_1)\\
			\lag_{31}(w_1)
			\end{pmatrix}_\maltese =  \lag_{21}(w_1).
			\end{align*}
			Hence, if we only consider the restricted map $\lag\big\vert_{\W_j}$, then
			\begin{align*}
			\E_2 \circ \lag\bigg\vert_{\W_1} = \lag_{21} : \W_1 \lin \W_1.
			\end{align*}
			In general, the exact same procedure can be repeated to find the other $\lag_{ij}$. We notice that the index $j$ in $\lag_{ij}$ indicates that we use $\E_j$, while the index $i$ indicates that we use the restricted map $\lag\big\vert_{\W_i}$. A general expression for this procedure is
			\begin{align*}
			\lag_{ij} = \E_j \circ \lag\bigg\vert_{\W_i}.
			\end{align*}
			
			\newpage
			
			
			
			
			
			
			
			\item Let
			\begin{align*}
			\mathcal{E}_1 = \begin{bmatrix}
			\M_{11} & \M_{12} & \M_{13}\\
			\M_{21} & \M_{22} & \M_{23}\\
			\M_{31} & \M_{32} & \M_{33}
			\end{bmatrix}_\oplus.
			\end{align*}
			be given, where $\ima(\E_i) = \W_i$ and $\E_1 + \E_2 + \E_3 = \mathcal{I}_\V$ is a resolution of identity. Continuing with the set-up of part 4, we have
			\begin{align*}
			\M_{ij} &= \E_j \circ \E_1\bigg\vert_{\W_i}.
			\end{align*}
			From part 4, we have shown $\E_k(w_1 + w_2 + w_3) = \E_k\sum_{i=1}^3w_i = w_k$, for $i\in \{1,2,3 \}$ and $w_i \in \W_i$. Since the $\W_i$'s direct sum to $\V$, any intersection between two $\W_i$'s is trivial. So, for $v\in \V$ that also belongs to $\W_i$, the one and only way express $v = \sum^3_{i=3}w_i$ is as an element in $\W_i$:
			\begin{align*}
			v = w_1 + w_2 + w_3 =  \delta^j_iw_j = w_i, \hspace{0.5cm} i,j,\in \{1,2,3\}, w_j\in \W_j
			\end{align*}
			where the Kronecker delta
			\begin{align*}
			\delta^j_i = \begin{cases}
			1,\hspace{0.5cm}i=j\\
			0,\hspace{0.5cm}i\neq j
			\end{cases}
			\end{align*}
			is introduced to simplify our notation. It follows that, for $\E_1$
			\begin{align*}
			\E_k(v) = \E_k(w_i) = \delta^i_kw_i, \hspace{0.5cm}\text{for any } w_i\in\W_i.
			\end{align*}
			This simply says that the general expression$^\dagger$
			\begin{align*}
			\E_j\circ\E_k\bigg\vert_{\W_i} =
			\begin{cases}
			\mathcal{O},\hspace{1.1 cm} i\neq k\\
			\E_j\big\vert_{\W_1},\hspace{0.5cm} i=k,
			\end{cases}
			\end{align*}
			where 
			\begin{align*}
			\E_j\bigg\vert_{\W_k} =
			\begin{cases}
			\mathcal{O},\hspace{1.2 cm} j\neq k\\
			Id_{\W_k},\hspace{0.6cm} j=k.
			\end{cases}
			\end{align*}
			where $Id_{\W_k}$ denotes the identity function on $\W_k$. Therefore, for $\E_1$, we simply set $k=1$ to get
			\begin{align*}
			\text{For }\E_1:\hspace{0.5cm}\M_{ij} = 
			\begin{cases}
			Id_{\W_1},\hspace{0.6cm}   i,j=1\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 1
			\end{cases}
			\end{align*}
			To get the $M_{ij}$'s for $\E_2$ and $\E_3$, we simply set $k=2,3$ accordingly. So, for 
			\begin{align*}
			&\text{For }\E_2:\hspace{0.5cm}\M_{ij} = 
			\begin{cases}
			Id_{\W_2},\hspace{0.6cm}   i,j=2\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 3
			\end{cases}\\
			&\text{For }\E_3:\hspace{0.5cm}\M_{ij} = 
			\begin{cases}
			Id_{\W_3},\hspace{0.6cm}   i,j=3\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 3
			\end{cases}
			\end{align*}
			
		\end{enumerate} 
		
	\end{sln*}
	
\end{prob*}






\newpage



\begin{prob*}\textbf{3.}
	\begin{enumerate}
		\item Using the set-up of Definition 0.7, verify the identity
		\begin{align*}
		[\mathcal{T(X)}]_\Omega = [\mathcal{T}]_{\Omega \leftarrow \Gamma}[\mathcal{X}]_\Gamma.
		\end{align*}
		
		
		
		\item Use part 1 to prove that 
		\begin{align*}
		[\mathcal{T}]_{\Omega\leftarrow\Gamma} = \begin{bmatrix}
		[\mathcal{T}(v_1)]_\Omega & [\mathcal{T}(v_2)]_\Omega & \dots & [\mathcal{T}(v_N)]_\Omega  
		\end{bmatrix}.
		\end{align*}
		
		
		
		\item If $\U$ is a finite-dimensional vector space with a coordinate system $\Delta$, and $\mathcal{S} : \Z \overset{\text{linear}}{\longrightarrow} \U$, argue that 
		\begin{align*}
		[\mathcal{S}\circ \mathcal{T}]_{\Delta \leftarrow \Gamma} = [\mathcal{S}]_{\Delta \leftarrow \Omega}[\mathcal{T}]_{\Omega\leftarrow\Gamma}.
		\end{align*}
		
		
		\item Verify that in the case $\mathcal{T}$ is invertible (in which case $m=n$), so is $[\mathcal{T}]_{\Omega\leftarrow \Gamma}$, and 
		\begin{align*}
		\begin{bmatrix}
		\mathcal{T}^{-1}
		\end{bmatrix}_{\Gamma\leftarrow\Omega} = [\mathcal{T}]^{-1}_{\Omega\leftarrow \Gamma}.
		\end{align*}
	\end{enumerate}

	
	
	
	\begin{sln*}\textbf{3.}
		\begin{enumerate}
			
			
			\item By the set-up in Definition 0.3, we have $[\T]_{\Omega\leftarrow \Gamma} = [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma$, and $[\mathcal{X}]_\Gamma = [\,\,\,]_\Gamma(\mathcal{X})$. Hence,
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma}[\mathcal{X}]_\Gamma 
			&= [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma \circ [\mathcal{X}]_\Gamma \\
			&= [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma \circ [\,\,\,]_\Gamma(\mathcal{X})
			&= [\,\,\,]_\Omega\circ\T\circ \left(\mathcal{A}_\Gamma \circ [\,\,\,]_\Gamma\right)(\mathcal{X})\\
			&= [\,\,\,]_\Omega\circ\T(\mathcal{X})\\
			&= [\,\,\,]_\Omega\left(\T(\mathcal{X})\right)\\
			&= [\mathcal{T}(\mathcal{X})]_\Omega.
			\end{align*}
			
			
			
			
			\item \underline{Claim}:
			\begin{align*}
			[\T]_{\Omega\leftarrow\Gamma} = \begin{bmatrix}
			[\T(v_1)]_\Omega & [\T(v_2)]_\Omega & \dots & [\T(v_n)]_\Omega
			\end{bmatrix}.
			\end{align*}
			 To find the columns of the matrix $[\T]_{\Omega\leftarrow\Gamma}$ we let $\mathcal{X}$ be $v_i \in \{v_1,v_2,\dots,v_n\}$ the set of basis vectors of $\Gamma$. This works because 
			\begin{align*}
			[\mathcal{X}]_\Gamma = [v_i]_\Gamma = [\,\,\,]_\Gamma(v_i)=A^{-1}_\Gamma(v_i) = \begin{bmatrix}
			v_1&v_2&\dots&v_n
			\end{bmatrix}^{-1}(v_i) =  \vec{e}_i \in \mathbb{C}^n,
			\end{align*}
			where $\vec{e}_i$ is a standard unit vector in $\mathbb{C}^n$ with a 1 on the $i^{th}$ row and 0 everywhere else and $A_\Gamma$ is a bijective atrix responsible for the changes of basis $\mathbb{C}^n\to \V$.\\
			 
			From part 1, the $i^{th}$ column of $[\T]_{\Omega\leftarrow \Gamma}$ is given by 
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma}[v_i]_\Gamma = [\T(v_i)]_\Omega.
			\end{align*}
			The matrix $[\T]_{\Omega\leftarrow\Gamma}$ can be constructed by putting the columns together in order, $v_1$ to $v_n$:
			\begin{align*}
			[\T]_{\Omega\leftarrow\Gamma} = \begin{bmatrix}
			[\T(v_1)]_\Omega & [\T(v_2)]_\Omega & \dots & [\T(v_n)]_\Omega
			\end{bmatrix},
			\end{align*}
 			which is what we claimed $[\T]_{\Gamma\leftarrow \Gamma}$ to be.\\
 			
 			
 			
 			
 			\item Let the finite-dimensional vector space $\U$ with coordinate system $\Delta$ be given. Let $\mathcal{S}:\Z \lin \U$ be given. \underline{Claim}:
			\begin{align*}
 			[\mathcal{S} \circ \T]_{\Delta\leftarrow\Gamma} = [\mathcal{S}]_{\Delta\leftarrow\Omega}[\T]_{\Omega\leftarrow\Gamma}.
 			\end{align*}
 			
 			Let $\Delta = (w_1,w_2,\dots,w_l)$ be a coordinate system for $\U$. We also define the bijective atrix
 			\begin{align*}
 			A_\Delta := \begin{bmatrix}
 			w_1&w_2&\dots&w_l
 			\end{bmatrix} : \mathbb{C}^l \rightarrow \U.
 			\end{align*}
 			And since $A_\Delta$ s invertible, we can write $[\,\,\,]_\Delta$ for $A^{-1}_\Delta$. \\
 			
			By the set-up in definition 0.3, we have the following
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma} &= [\,\,\,]_\Omega\circ \T\circ \mathcal{A}_\Gamma\\
			[\mathcal{S}]_{\Delta\leftarrow\Omega} &= [\,\,\,]_\Delta \circ \mathcal{S} \circ \mathcal{A}_\Omega.			
			\end{align*}
 			It follows that
 			\begin{align*}
 			[\mathcal{S}]_{\Delta \leftarrow \Omega}[\mathcal{T}]_{\Omega\leftarrow\Gamma}
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \mathcal{A}_\Omega \circ [\,\,\,]_\Omega\circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \left( \mathcal{A}_\Omega \circ [\,\,\,]_\Omega \right) \circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \left(\mathcal{S} \circ \T \right) \circ \mathcal{A}_\Gamma\\
			&= [\mathcal{S}\circ \T]_{\Delta \leftarrow \Gamma},
 			\end{align*}
			where we have used the fact that $A_\Omega [\,\,\,]_\Omega = A_\Omega A_\Omega^{-1}$ is the identity function. \\
			
			
			
			\item Let $\T$ be invertible (in which case $m=n$). \underline{Claim}:
			\begin{align*}
			\begin{bmatrix}
			\mathcal{T}^{-1}
			\end{bmatrix}_{\Gamma\leftarrow\Omega} = [\mathcal{T}]^{-1}_{\Omega\leftarrow \Gamma}.
			\end{align*}
			
			Since $\T$, $A_{\Omega}$, $A_\Gamma$ are all invertible, their inverses are defined. By the set-up in definition 0.3, the matrix $[\T]_{\Omega\leftarrow\Gamma}$ can be written as the composition $[\,\,\,]_\Omega \circ \T \circ A_{\Gamma}$ for the linear function $T : \V \lin \Z$. Now, since $\T^{-1} : \Z \lin \V$ is also defined, we can express its matrix $[\T^{-1}]_{\Gamma\leftarrow\Omega}$ as the composition $[\,\,\,]_\Gamma\circ \T^{-1} \circ A_\Omega$, i.e.,
			\begin{align*}
			[\T^{-1}]_{\Gamma\leftarrow\Omega} = [\,\,\,]_\Gamma\circ \T^{-1} \circ A_\Omega^\dagger.
			\end{align*}
			Next, we consider the inverse of the matrix $[\T]_{\Omega\leftarrow\Gamma}$:
			\begin{align*}
			[\T]^{-1}_{\Omega\leftarrow\Gamma} &= \left([\,\,\,]_\Omega\circ \T \circ A_\Gamma \right)^{-1}\\
			&= A^{-1}_\Gamma \circ \left( [\,\,\,]_\Gamma \circ \T \right)^{-1}\\
			&= A^{-1}_\Gamma \circ \T^{-1} \circ [\,\,\,]_\Omega^{-1}\\
			&= [\,\,\,]_\Gamma \circ \T^{-1} \circ A_\Omega^{\dagger\dagger}.
			\end{align*}
			From ($\dagger$) and $(\dagger\dagger)$, we have
			\begin{align*}
				[\T^{-1}]_{\Gamma\leftarrow\Omega} = [\T]^{-1}_{\Omega\leftarrow\Gamma}
			\end{align*}
			as claimed.
			
		\end{enumerate}
	\end{sln*}




\end{prob*}

\newpage




\begin{prob*}\textbf{4.} Let us write $\Gamma_0$ for the standard coordinate system $(1,x,x^2,x^3)$ of the vector space $\mathbb{P}_3$ of all polynomials of degree at most 3. 
	\begin{enumerate}
		\item Use \textit{Mathematica} and the fact that $\begin{bmatrix}\,&\,\end{bmatrix}_{\Gamma_0}$ is an isomorphism (and isomorphisms map bases to bases) to verify that 
		\begin{align*}
		\Delta := (1 + x + x^2 &+ x^3, 1 + 2x+4x^2 + 8x^3,\\
		&1 + 3x+9x^2 + 27x^3, 1+ 4x +16x^2 + 64x^3)
		\end{align*}
		and 
		\begin{align*}
		\Omega := (x+x^2, x-6x^3, 1 + 4x^2, 1+ 8x^3) 
		\end{align*}
		are also coordinate systems of $\mathbb{P}_3$.
	


	\item Verify that general identity
	\begin{align*}
	[\mathcal{T}]_{\Delta \leftarrow \Omega} &= 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Delta \leftarrow \Gamma_0} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}\\
	&= \left(\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\gamma_0 \leftarrow \Omega},
	\end{align*}
	for $\mathcal{T} : \mathbb{P}_3 \overset{\text{linear}}{\longrightarrow} \mathbb{P}_3$.
	
	
	\item Consider $\mathcal{T} : \mathbb{P}_3 \rightarrow \mathbb{P}_3$ defined by 
	\begin{align*}
	\mathcal{T}(p(x)) := xp'(2x) + p(x).
	\end{align*}
	Prove that $\mathcal{T}$ is a linear function and use part 2 to find $[\mathcal{T}]_{\Delta\leftarrow \Omega}$.
	
	
	\end{enumerate}



	\begin{sln*}\textbf{4.}
		$\,$
		\begin{enumerate}
			\item 
			\begin{enumerate}
				\item Let $\Delta := (1 + x + x^2 + x^3, 1 + 2x+4x^2 + 8x^3, 1 + 3x+9x^2 + 27x^3, 1+ 4x +16x^2 + 64x^3) $ be given. To show that $\Delta$ is a coordinate system of $\V$, it suffices to show there exists an isomorphism $[\mathcal{I}]_{{\Gamma_0}\leftarrow\Delta} : \mathbb{C}^4_{\Delta_0}\to\mathbb{C}^4_{\Gamma_0}$. We can construct $[\mathcal{I}]_{{\Gamma_0}\leftarrow\Delta}$ column by column by letting $[\,\,\,]_{\Gamma_0}$ act on the elements of $\Delta$:
				
				\begin{align*}
				[\mathcal{I}]_{\Gamma_0\leftarrow\Delta}
				&= \begin{bmatrix}
				[(1+x+x^2+x^3)]_{\Gamma_0}\\
				[(1+2x+4x^2+8x^3)]_{\Gamma_0} \\ 
				[(1+3x+9x^2+27x^3)]_{\Gamma_0} \\ 
				[(1+4x+16x^2+64x^3)]_{\Gamma_0}
				\end{bmatrix}^\top
				= \begin{pmatrix}
				1&1&1&1\\
				1&2&3&4\\
				1&4&9&16\\
				1&8&27&64
				\end{pmatrix}.
				\end{align*} 
				To show $[\mathcal{I}]_{\Gamma_0\leftarrow\Delta}$ is an isomorphism, it suffices to check if its determinant is nonzero, which can be done in \textit{Mathematica}:
				\begin{align*}
				\det\left([\mathcal{I}]_{\Gamma_0\leftarrow\Delta}\right) = 12 \neq 0.
				\end{align*}
				Therefore, $\Delta$ is a coordinate system of $\mathbb{P}_3$.\\
				
				\textit{Mathematica} code:
		\begin{lstlisting}
In[4]:= Det[{{1, 1, 1, 1}, {1, 2, 3, 4}, {1, 4, 9, 16}, 
						{1, 8, 27, 64}}]
				
Out[4]= 12
		\end{lstlisting}
		
		
		
		
		
		
				\item For $\Omega := (x+x^2,x-6x^3,1+4x^2,1+8x^3)$, we repeat the procedure laid out above to determine whether $\Omega$ is a coordinate system for $\mathbb{P}_3$. 
				\begin{align*}
				[\mathcal{I}]_{\Gamma_0\leftarrow\Omega}
				= \begin{bmatrix}
				[(x+x^2)]_{\Gamma_0} \\ [(x-6x^3)]_{\Gamma_0} \\ [(1+4x^2)]_{\Gamma_0} \\ [(1+8x^3)]_{\Gamma_0}
				\end{bmatrix}^\top
				= \begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}. 
				\end{align*} 
				We can calculate the determinant of $[\mathcal{I}]_{\Gamma_0\leftarrow\Omega}$ in \textit{Mathematica}:
				\begin{align*}
				\det\left( [\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}  \right) = -32 \neq 0.
				\end{align*}
				Therefore, $\Omega$ is a coordinate system of $\mathbb{P}_3$.\\
				
				\textit{Mathematica} code:
				\begin{lstlisting}
In[6]:= Det[{{0, 0, 1, 1}, {1, 1, 0, 0},
						 {1, 0, 4, 0}, {0, -6, 0, 8}}]
				
Out[6]= -32
				\end{lstlisting}
				
			\end{enumerate}
		
		
		
			\item Let $\T : \mathbb{P}_3 \lin \mathbb{P}_3$ be given. By the set-up in Definition 0.3,
			\begin{enumerate}
				\item Verify the first identity:
				\begin{align*}
				[\T]_{\Delta\leftarrow \Omega} &= [\,\,\,]_{\Delta}\circ \T\circ A_\Omega\\
				&= \left([\,\,\,]_{\Delta\leftarrow \Gamma_0} \circ [\,\,\,]_{\Gamma_0}\right) \circ \T \circ \left( [\,\,\,]_{\Gamma_0}\circ[\,\,\,]_{\Gamma_0\leftarrow\Omega}\right)\\
				&= [\mathcal{I}_{\mathbb{P}_3}]_{\Delta\leftarrow \Gamma_0} \circ [\,\,\,]_{\Gamma_0} \circ \T \circ  [\,\,\,]_{\Gamma_0}\circ[\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}\\
				&= [\mathcal{I}_{\mathbb{P}_3}]_{\Delta\leftarrow \Gamma_0} \left( \circ [\,\,\,]_{\Gamma_0} \circ \T \circ  [\,\,\,]_{\Gamma_0} \right) \circ[\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}\\
				&= [\mathcal{I}_{\mathbb{P}_3}]_{\Delta\leftarrow \Gamma_0} \circ [\T]_{\Gamma_0 \leftarrow \Gamma_0}\circ [\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}.
				\end{align*}
				
				\item Verify the second identity: The second identity follows from the first identity, by the fact that
				\begin{align*}
				[\mathcal{I}_{\mathbb{P}_3}]_{\Delta\leftarrow \Gamma_0} = \left(\begin{bmatrix}\mathcal{I}_{\mathbb{P}_3}\end{bmatrix}_{\Gamma_0\leftarrow\Delta}\right)^{-1}.
				\end{align*}
				So we have
				\begin{align*}
				[\T]_{\Delta\leftarrow \Omega} &= [\mathcal{I}_{\mathbb{P}_3}]_{\Delta\leftarrow \Gamma_0} \circ [\T]_{\Gamma_0 \leftarrow \Gamma_0}\circ [\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}\\
				&=  \left(\begin{bmatrix}\mathcal{I}_{\mathbb{P}_3}\end{bmatrix}_{\Gamma_0\leftarrow\Delta}\right)^{-1} \circ [\T]_{\Gamma_0 \leftarrow \Gamma_0}\circ [\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Omega}.
				\end{align*}
			\end{enumerate}
		
			
			
			
			\item Let $\T : \mathbb{P}_3 \to \mathbb{P}_3$ be given. $T$ is defined as
			\begin{align*}
			\T(p(x)) := xp'(2x) + p(x).
			\end{align*}
			\begin{enumerate}
				\item \underline{Claim}: $\T$ is a linear function.\\
				$\T$ is a linear function if it satisfies the linearity conditions. Consider $p(x), q(x) \in \mathbb{P}_3$. Then we have
				\begin{align*}
				\T(p(x) + q(x)) &= x(p(2x) + q(2x))' + (p(x) + q(x))\\
				&= (xp'(2x) + p(x))  + (xq'(2x) + q(x))\\ 
				&= \T(p(x)) + \T(q(x))^\dagger.
				\end{align*}
				Consider $c\in \mathbb{C}$ and $p(x)\in \mathbb{P}_3$,
				\begin{align*}
				\T(cp(x)) &= x(cp(2x))' + cp(x)\\
				&= cxp'(2x) + cp(x)\\ &= c(xp'(2x) + p(x))\\ &= c\T(p(x))^{\dagger\dagger}.
				\end{align*}
				From ($\dagger$) and ($\dagger$), $\T$ is a linear function. \\
				
				
				
				
				\item \underline{Find} $[\T]_{\Delta\leftarrow\Omega}$.\\
				
				By part 2 of Problem 3
				\begin{align*}
				[\mathcal{T}]_{\Delta \leftarrow \Omega} &= 
				\left(\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\gamma_0 \leftarrow \Omega}.
				\end{align*}
				In part 1, we have found the matrices $\left(\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1}$ and $\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}$ to be
				\begin{align*}
				\left(\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} &= \begin{pmatrix}
				1&1&1&1\\
				1&2&3&4\\
				1&4&9&16\\
				1&8&27&64
				\end{pmatrix}^{-1} = 
				\begin{pmatrix}
				4&-13/3&3/2&-1/6\\
				-6&19/2&-4&1/2\\
				4&-7&7/2&-1/2\\
				-1&11/6&-1&1/6
				\end{pmatrix}\\
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}
				&= \begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}.
				\end{align*}
				To find $[\T]_{\Delta\leftarrow\Omega}$ we only need to find $[\T]_{\Gamma_0\leftarrow\Gamma_0}$. To do this, we use part 2 of Problem 3, which says
				\begin{align*}
				[\T]_{\Gamma_0\leftarrow\Gamma_0} &= \begin{bmatrix}
				[\T(1)]_{\Gamma_0} & [\T(x)]_{\Gamma_0} & [\T(x^2)]_{\Gamma_0} & [\T(x^3)]_{\Gamma_0}
				\end{bmatrix}.
				\end{align*}
				We also have
				\begin{align*}
				\begin{cases}
				\T(1) = x\frac{d}{dx}2 + 1 = 1\\
				\T(x) = x\frac{d}{dx}(2x) + x = 3x\\
				\T(x^2) = x\frac{d}{dx}(2x)^2 + x^2 = 9x^2\\
				\T(x^3) = x\frac{d}{dx}(2x)^3 + x^3 = 25x^3.
				\end{cases}
				\end{align*}
				Plugging these into the formula above, we get
				\begin{align*}
				[\T]_{\Gamma_0\leftarrow\Gamma_0} &= \begin{bmatrix}
				[T(1)]_{\Gamma_0} & [T(x)]_{\Gamma_0} & [T(x^2)]_{\Gamma_0} & [T(x^3)]_{\Gamma_0}
				\end{bmatrix}\\
				&= \begin{pmatrix}
				1&0&0&0\\
				0&3&0&0\\
				0&0&9&0\\
				0&0&0&25
				\end{pmatrix}.
				\end{align*}
				
				Finally, we can compute $[\T]_{\Delta\leftarrow\Omega}$:
				\begin{align*}
				[\T]_{\Delta\leftarrow\Omega} &=     
				\begin{pmatrix}
				4&-13/3&3/2&-1/6\\
				-6&19/2&-4&1/2\\
				4&-7&7/2&-1/2\\
				-1&11/6&-1&1/6
				\end{pmatrix}
				\begin{pmatrix}
				1&0&0&0\\
				0&3&0&0\\
				0&0&9&0\\
				0&0&0&25
				\end{pmatrix}
				\begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}\\
				&= \begin{pmatrix}
				0&0&0&0\\
				0&57/2&0&0\\
				0&0&126&0\\
				0&0&0&100/3
				\end{pmatrix}.
				\end{align*}
				
				\textit{Mathematica code:}
				\begin{lstlisting}
A = 
Inverse[{{1, 1, 1, 1}, {1, 2, 3, 4}, {1, 4, 9, 16}, {1, 8, 27, 64}}]
				
{{4, -(13/3), 3/2, -(1/6)}, {-6, 19/2, -4, 1/2}, {4, -7, 7/
			2, -(1/2)}, {-1, 11/6, -1, 1/6}}
				
B = {{1, 0, 0, 0}, {0, 3, 0, 0}, {0, 0, 9, 0}, {0, 0, 0, 25}}
				
				
CC = {{0, 0, 1, 1}, {1, 1, 0, 0}, {1, 0, 4, 0}, {0, -6, 0, 8}}
						
In[7]:= A*B*CC
				
Out[7]= {{0, 0, 0, 0}, {0, 57/2, 0, 0}, {0, 0, 126, 0},
 			{0, 0, 0, 100/3}}
				\end{lstlisting}
			\end{enumerate}
		\end{enumerate}
		
	\end{sln*}






\end{prob*}

\newpage



\begin{prob*}\textbf{5.} Suppose that $\V = \W_1 \oplus \W_2 \oplus \W_3$ and $\Gamma_i$ is a coordinate system of $\W_i$. Suppose that for each $i,j \in \{1,2,3 \}$, 
	\begin{align*}
	\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i.
	\end{align*}
	Let 
	\begin{align*}
	\lag := \begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\oplus.
	\end{align*}
	Let $\Delta$ be the concatenation $\Gamma_1 \vert\vert \Gamma_2\vert\vert \Gamma_3$ of the coordinate systems $\Gamma_i$. As we know, $\Delta$ is a coordinate system of $\V$. \\
	
	Prove that $[\lag]_{\Delta \leftarrow \Delta}$ equals the partitioned matrix
	\begin{align*}
	\begin{bmatrix}
	[\lag_{11}]_{\Gamma_1 \leftarrow \Gamma_1} & [\lag_{12}]_{\Gamma_1 \leftarrow \Gamma_2} & [\lag_{13}]_{\Gamma_1 \leftarrow \Gamma_3}\\
	[\lag_{21}]_{\Gamma_2 \leftarrow \Gamma_1} & [\lag_{22}]_{\Gamma_2 \leftarrow \Gamma_2} & [\lag_{23}]_{\Gamma_2 \leftarrow \Gamma_3}\\
	[\lag_{31}]_{\Gamma_3 \leftarrow \Gamma_1} & [\lag_{32}]_{\Gamma_3 \leftarrow \Gamma_2} & [\lag_{33}]_{\Gamma_3 \leftarrow \Gamma_3}
	\end{bmatrix}.
	\end{align*}
	 
	
	\begin{sln*}\textbf{5.} Consider a $3\times 3$ block-matrix, which consists of the linear function $\lag_{ij} : \W_j \lin \W_i$ at the $i^{th}$ row and $j^{th}$ column and the zero function everywhere else. Let $[\lag_{ij}]_\oplus : \V \lin \V$ denote this matrix. By this construction, we have
		\begin{align*}
		\sum^3_{i=1}\sum^3_{k=1}[\lag_{ij}]_\oplus = \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus.
		\end{align*}
		
	Consider the coordinate system $\Gamma_k$ where $k\in \{1,2,3\}$. Let $\Gamma_k$ be formed by a (finite) collection of basis elements, called $g_{lk}$'s. Consider the mapping $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ and the matrix $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$. \\
	
	We observe that $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ is $\mathbf{0}_\V$ if $k\neq j$ and $\maltese\left([\lag_{ij}]_\oplus(g_{lj}) + \mathbf{0}_\V + \mathbf{0}_\V \right)$ if $j=k$, where $\maltese$ is defined in Definition 0.4. What this says is that $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ gives a column vector with $[\lag_{ij}]_\oplus(g_{lj})$ on the $i^{th}$ row and $\mathbf{0}_\V$ everywhere else. This means that if we consider the coordinatization $\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{lj})_\oplus
	\end{bmatrix}_\Delta$, we will end up with a column vector with $0$'s everywhere except $\begin{bmatrix}[\lag_{ij}]_\oplus(g_{lj})\end{bmatrix}_{\Gamma_i}$ on the $i^{th}$ row$^\dagger$. \\
	
	Now, we can construct $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$ column-by-column as
	\begin{align*}
	\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}
	&=
	\begin{bmatrix}
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{11})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{a1})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	&
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{12})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{b2})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	&
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{11})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{a1})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	\end{bmatrix}
	\end{align*}
	where $a,b,c$ are the number of elements in $\Gamma_1$, $\Gamma_2$, $\Gamma_3$, respectively. From $(\dagger)$, we know that two of three matrix-blocks in the expression above will be the zero-block because $j$ is either 1,2, or 3. Consider a general non-zero block (located at the $j^{th}$ column)
	\begin{align*}
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{11})_\oplus
	\end{bmatrix}_\Delta
	&\dots&
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{a1})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix},
	\end{align*}
	we immediately recognize that this is exactly how the matrix
	\begin{align*}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus
	\end{bmatrix}_{\Gamma_i \leftarrow \Gamma_j}
	\end{align*}
	is constructed column-by-column. Therefore, we observe that the matrix $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$ can be written as a 
	
	
	\end{sln*}
\end{prob*}








































\newpage


\end{document}
