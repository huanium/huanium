\documentclass{article}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{authblk}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage{bbold}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{amssymb}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Properties}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem*{prob*}{Problem}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{tensor}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\p}{\partial}

\newcommand{\V}{\mathbf{V}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\X}{\mathbf{X}}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}

\newcommand{\xpan}{\text{span}}

\newcommand{\lag}{\mathcal{L}}

\newcommand{\J}{\mathbf{J}}

\newcommand{\M}{\mathcal{M}}

\newcommand{\K}{\mathcal{K}}

\newcommand{\N}{\mathcal{N}}

\newcommand{\E}{\mathcal{E}}

\newcommand{\ima}{\text{Im}}
\newcommand{\lin}{\overset{\text{linear}}{\longrightarrow}}
\newcommand{\T}{\mathcal{T}}
\newcommand{\poly}{\mathbb{P}}
\newcommand{\s}{\mathcal{S}}

\newcommand{\jor}{\mathcal{J}}
\newcommand{\FF}{\mathfrak{F}}
\newcommand{\LL}{\mathfrak{L}}
\newcommand{\lat}{\mathfrak{Lat}}

\newcommand{\gives}{\rotatebox[origin=c]{180}{$\Rsh$}	}


\newcommand{\bigzero}{\mbox{\normalfont\Large\bfseries 0}}
\newcommand{\rvline}{\hspace*{-\arraycolsep}\vline\hspace*{-\arraycolsep}}



\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
	\begin{titlepage}\centering
		\clearpage
		\title{\textsc{\bf{MATRIX ANALYSIS}}\\\smallskip A Quick Guide\\}
		\author{\bigskip Huan Q. Bui}
		 \affil{Colby College\\$\,$\\ PHYSICS \& MATHEMATICS\\ Statistics \\$\,$\\Class of 2021\\}
		\date{\today}
		\maketitle
		\thispagestyle{empty}
	\end{titlepage}

\newpage

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

\textit{Matrix Analysis: A Quick Guide to} is compiled based on my MA353: Matrix Analysis notes with professor Leo Livshits. The sections are based on a number of resources: \textit{Linear Algebra Done Right} by Axler, \textit{A Second Course in Linear Algebra} by Horn and Garcia, \textit{Matrices and Linear Transformations} by Cullen, \textit{Matrices: Methods and Applications} by Barnett, \textit{Problems and Theorems in Linear Algebra} by Prasolov, \textit{Matrix Operations} by Richard Bronson, and professor Leo Livshits' own textbook (in the making). Prerequisites: some prior exposure to a first course in linear algebra.\\

The development of this text will come in layers. The first layer, one that I am working on during the course of S'19 MA353, will be an overview of the key topics listed in the table of contents. As the semester progresses, I will be constantly updating the existing notes, as well as adding prof. Livshits' problems and my solutions to the problems. The second layer will come after the course is over, when concepts will have hopefully ``come together.''\\ 

I will decide how much narrative I should put into the text as text is developed over the semester. I'm thinking that I will only add detailed explanations wherever I find fit or necessary for my own studies. I will most likely keep the text as condensed as I can.\\

Enjoy!


\newpage
\tableofcontents
\newpage

\section{List of Special Matrices \& Their Properties}
\begin{enumerate}
	\item \textbf{Hermitian/Self-adjoint}: $H = H^\dagger$. A Hermitian matrix is matrix that is equal to its own conjugate transpose:
	\begin{align*}
	H \text{ is Hermitian } \iff H_{ij} = \bar{H}_{ji}
	\end{align*} 
	\begin{prop}
	$\,$
	\begin{enumerate}
		\item $H$ is Hermitian $ \iff \langle w, Hv\rangle = \langle Hw, v \rangle$, where $\langle, \rangle$ denotes the inner product.
		\item $H$ is Hermitian $ \iff \langle v, Hv\rangle \in \R$.
		\item $H$ is Hermitian $\iff$ it is \textit{unitarily diagonalizable} with \textit{real eigenvalues}.
	\end{enumerate}
	\item \textbf{Unitary}: $U^*U = UU^* = I = U^\dagger U = UU^\dagger$. The real analogue of a unitary matrix is an orthogonal matrix. The following list contain the properties of $U$:
	\begin{enumerate}
		\item $U$ preserves the inner product:
		\begin{align*}
		\langle Ux, Uy \rangle = \langle x,y\rangle.
		\end{align*}
		
		\item $U$ is normal: it commutes with $U^* = U^\dagger$.
		
		\item $U$ is diagonalizable:
		\begin{align*}
		U = VDV^*,
		\end{align*}
		where $D$ is diagonal and unitary, and $V$ is unitary.
		
		\item $\vert \det(U) \vert = 1 $ (hence the real analogue to $U$ is an orthogonal matrix)
		
		\item Its eigenspaces are orthogonal.
		
		\item $U$ can be written as
		\begin{align*}
		U = e^{iH},
		\end{align*}
		where $H$ is a Hermitian matrix. 
		
		\item Any square matrix with unit Euclidean norm is the average of two unitary matrices.
	\end{enumerate}
	\end{prop}
	\item \textbf{Idempotent:} $M$ idempotent $\iff M^2 = M$.
	\begin{enumerate}
		\item Singularity: its number of independent rows (and columns) is less than its number of rows (and columns). 
		\item When an idempotent matrix is subtracted from the identity matrix, the result is also idempotent. 
		\begin{proof}[``Proof'']
			\begin{align*}
			[I-M][I-M] = I - M - M + M^2 = I-M - M + M = I - M.
			\end{align*}
		\end{proof}
		\item $M$ is idempotent $\iff \forall n \in \mathbb{N}$, $A^n = A$. 
		\item Eigenvalues: an idempotent matrix is always diagonalizable and its eigenvalues are either 0 or 1. (think ``projection")
		\item Trace: the trace of an idempotent matrix equals the rank of the matrix and thus is always an integer. So
		\begin{align*}
		\tr(A) = \dim(\Im A).
		\end{align*}
	\end{enumerate}
	\item \textbf{Nilpotent:} a nilpotent matrix is a square matrix $N$ such that
	\begin{align*}
	N^k = 0
	\end{align*}
	for some positive integer $k$. The smallest such $k$ is sometimes called the \textbf{index} of $N$.\\
	
	The following statements are equivalent:
	\begin{enumerate} 
		\item $N$ is nilpotent.
		\item The minimal polynomial for $N$ is $x^k$ for some positive integer $k\leq n$.
		\item The characteristic polynomial for $N$ is $x^n$.
		\item The only complex eigenvalue for $N$ is 0.
		\item $\tr N^k = 0$ for all $k > 0$.
	\end{enumerate}
	\begin{prop}
		$\,$
		\begin{enumerate}
			\item The degree of an $n\times n$ nilpotent matrix is always less than or equal to $n$. 
			\item $\det N = \tr(N) = 0$.
			\item Nilpotent matrices are not invertible.
			\item The only nilpotent diagonalizable matrix is the zero matrix. 
		\end{enumerate}
	\end{prop}
\end{enumerate}

\newpage
\section{List of Operations}
\begin{enumerate}
	\item \textbf{Conjugate transpose} is what its name suggests.
	\item \textbf{Classical adjoint/Adjugate/adjunct} of a square matrix is the transpose of its cofactor matrix. 
\end{enumerate}
\newpage

\section{List of Algorithms}
\newpage

\section{Complex Numbers}
\subsection{A different point of view}
We often think of complex numbers as
\begin{align*}
a + ib
\end{align*}
where $a,b \in \R$ and $i = \sqrt{-1}$. While there is nothing ``bad'' about this way of thinking - in fact thinking of complex numbers as $a+ib$ allows us to very quickly and intuitively do arithmetics operations on them - a ``matrix representation'' of complex numbers can give us some insights on ``what we actually do'' when we perform complex arithmetics.\\

Let us think of 
\begin{align*}
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\end{align*}
as a different representation of the same object - the same complex number ``$a+ib$.'' Note that it does not make sense to say the matrix representation \textbf{equals} the complex number itself. But we shall see that a lot of the properties of complex numbers are carried into this matrix representation under interesting matricial properties.
\begin{align*}
\boxed{\begin{pmatrix}
	a & -b\\
	b & a
	\end{pmatrix}
\sim a + ib}
\end{align*}

First, let us break the matrix down:
\begin{align*}
a+ib = a\times 1 + i \times b \sim \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix} 
= 
a\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
+
b\begin{pmatrix}
0 & -1\\
1 & 0
\end{pmatrix}
= aI + b\mathcal{I}.
\end{align*}
Right away, we can make some ``mental connections'' between the representations:
\begin{align*}
I &\sim 1\\
\mathcal{I} \sim i.
\end{align*}

Now, we know that complex number multiplications commute:
\begin{align*}
(a+ib)(c+id) = (c+id)(a+ib).
\end{align*}
Matrix multiplications are not commutative. So, we might wonder whether commutativity holds under the this new representation of complex numbers. Well, the answer is yes. We can readily verify that
\begin{align*}
(aI+b\mathcal{I})(cI+b\mathcal{I}) = (cI+b\mathcal{I})(aI+b\mathcal{I}).
\end{align*}
How about additions? Let's check:
\begin{align*}
(a + ib) + (c+ id) = (a+c) + i(b+d) \sim \begin{pmatrix}
a+c & -(b+d)\\
(b+d) & a+c 
\end{pmatrix}=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}+
\begin{pmatrix}
c & -d\\
d & c
\end{pmatrix}.
\end{align*}
Ah! Additions work. So, the new representation of complex numbers seems to be working flawlessly. However, we have yet to gain any interesting insights into the connections between the representations. To do that, we have to look into changing the form of the matrix. First, let's see what conjugation does:
\begin{align*}
(a+ib)^* = a-ib \sim \begin{pmatrix}
a & b \\
-b & a
\end{pmatrix}
=
\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}^{\top}
\end{align*}
Ah, so conjugation to a complex number in the traditional representation is the same as transposition in the matrix representations. What about the amplitude square? Let us call 
\begin{align*}
M = \begin{pmatrix}
a & -b \\
b & a
\end{pmatrix}.
\end{align*}
We have
\begin{align*}
(a+ib)(a-ib) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
a & b \\
-b & a
\end{pmatrix} =
MM^\top= (a^2+b^2)I = \det(M)I
\end{align*}
Interesting. But observe that if $\det(M) \neq 0$
\begin{align*}
\frac{1}{\det(M)}MM^\top = I.
\end{align*}
This tells us that
\begin{align*}
M^\top = M^{-1},
\end{align*}
where $M^{-1}$ is the inverse of $M$, and, not surprisingly, it corresponds to the reciprocal to the complex number $a+ib$. We can readily show that
\begin{align*}
M^{-1} \sim (a+ib)^{-1} = \frac{1}{a^2+b^2}(a-ib).
\end{align*} 
Remember that we can also think of a complex number as a column vector:
\begin{align*}
c + id \sim \begin{pmatrix}
c\\d
\end{pmatrix}.
\end{align*}
Let us look back at complex number multiplication under matrix representation:
\begin{align*}
(a+ib)(c+id) = (ac-bd) + i(bc + ad) \sim \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}
\begin{pmatrix}
c\\d
\end{pmatrix}=
\begin{pmatrix}
ac-bd\\
bc +ad
\end{pmatrix}.
\end{align*}
Multiplication actually works in this ``mixed'' way of representing complex numbers as well. Now, observe that what we just did was performing a linear transformation on a vector in $\R^2$. It is always interesting to look at the geometrical interpretation of this transformation. To do this, let us call $N$ the ``normalized'' version of $M$:
\begin{align*}
N = \frac{1}{\sqrt{a^2 + b^2}}\begin{pmatrix}
a & -b\\
b & a
\end{pmatrix}.
\end{align*}
We immediately recognize that $N$ is an orthogonal matrix. This means $N$ is an orthogonal transformation (length preserving). Now, it is reasonable to define
\begin{align*}
\cos\theta &= \frac{a}{\sqrt{a^2 + b^2}}\\
\sin\theta &= \frac{b}{\sqrt{a^2+b^2}}.
\end{align*}
We can write $N$ as
\begin{align*}
N = \begin{pmatrix}
\cos\theta & -\sin\theta
\sin\theta & \cos\theta
\end{pmatrix},
\end{align*}
which is a physicists' favorite matrix: the rotation by $\theta$. So, let us write $M$ in terms of $N$:
\begin{align*}
M = \begin{pmatrix}
a & -b\\
b & a
\end{pmatrix} = \sqrt{a^2 + b^2}N = \sqrt{a^2 + b^2}\begin{pmatrix}
\cos\theta & -\sin\theta\\
\sin\theta & \cos\theta
\end{pmatrix}.
\end{align*}
We can interpret $M$ as a rotation by $\theta$, followed by a scaling by $\sqrt(a^2 + b^2)$. But what $\sqrt{a^2 + b^2}$ exactly is just the ``length'' or the ``amplitude'' of the complex number $a+ib$, if we think of it as an arrow in a plane. 
\subsection{Relevant properties and definitions}
\begin{enumerate}
	\item The \textit{modulus} of $z = a+ib$ is the ``amplitude'' of $z$, denoted by $\vert z \vert = \sqrt{a^2 + b^2} = z\bar{z}$. 
	\item The modulus is \textit{multiplicative}, i.e. 
	\begin{align*}
	\vert wz \vert = \vert w \vert \vert z \vert.
	\end{align*}
	\item Triangle inequality:
	\begin{align*}
	\vert z + w \vert \leq \vert z \vert + \vert w \vert.
	\end{align*}
	We can readily show this geometrically, or algebraically. 
	\item The \textit{argument} of $z = a+ib$ is $\theta$, where
	\begin{align*}
	\theta = 
	\begin{cases}
	\tan^{-1}\left( \frac{b}{a}\right), \text{ if } a > 0\\
	\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b > 0\\
	-\frac{\pi}{2} + k2\pi, k\in\R \text{ if } a = 0, b < 0\\
	\text{Undefined if } a=b=0.
	\end{cases}
	\end{align*} 
	\item The \textit{conjugate} of $a+ib$ is $a-ib$. Conjugation is \textit{additive} and \textit{multiplicative}, i.e.
	\begin{align*}
	\bar{z+w} &= \bar{z} + \bar{w}\\
	\bar{wz} &= \bar{w}\bar{z}.
	\end{align*}
	Note that we can also show the multiplicative property with the matrix representation as well:
	\begin{align*}
	\bar{wz} \sim (WZ)^\top = Z^\top W^\top \sim \bar{z}\bar{w} = \bar{w}\bar{z}.
	\end{align*}
	\item Euler's identity, generalized to de Moivre's formula:
	\begin{align*}
	z^n = r^ne^{in\theta}.
	\end{align*}
\end{enumerate}

\newpage
\section{Vector Spaces \& Linear Functions}
\subsection{Review of Linear Spaces and Subspaces}
\begin{prop} of linear spaces:
\begin{enumerate}
	\item Commutativity and associativity of addition
	\item Existence of an additively neutral element (null element). Zero multiples of elements give the null element: $0\cdot V = \mathbf{0}$\\
	\item Every element has an (unique) additively antipodal element
	\item Scalar multiplication distributes over addition
	\item Multiplicative identity: 
	\item $ab\cdot V = a\cdot(bV)$
	\item $(a+b)V = aV + bV$
\end{enumerate}
\end{prop}
$W$ is a subspace of $V$ if
\begin{enumerate}
	\item $S \subseteq V$ 
	\item $S$ is non-empty
	\item $S$ is closed under addition and scalar multiplication
\end{enumerate}
\begin{prop} that are interesting/important/maybe-not-so-obvious:
\begin{enumerate}
	\item If $S$ is a subspace of $V$ and $S \neq V$ then $S$ is a proper subspace of $V$.
	\item If $X$ in a subspace of $Y$ and $Y$ is a subspace of $Z$, then $X$ is a subspace of $W$.
	\item Non-trivial linear (non-singleton) spaces are infinite. 
\end{enumerate}
\end{prop}
\subsection{Review of Linear Maps}
Consider linear spaces $V$ and $W$ and elements $v \in V$ and $w \in W$ and scalars $\alpha, \beta \in \R$, a function $F : V \rightarrow W$ is a linear map if
\begin{align*}
F[\alpha v + \beta w] = \alpha F[v] + \beta F[w].
\end{align*}
\begin{prop}
	$\,$
\begin{enumerate}
	\item $F[\mathbf{0}_V] = \mathbf{0}_W$
	\item $G[w] = (\alpha \cdot F)[w] = \alpha\cdot F[w]$
	\item Given $F : V \rightarrow W$ and $G : V \rightarrow W$, $H[v] = F[v] + G[v] = (F+G)[v]$ is call the sum of the functions $F$ and $G$.
	\item Linear combinations of linear maps are linear.
	\item Compositions of linear maps are linear. 
	\item Compositions distributes over linear combinations of linear maps.
	\item Inverses of linear functions (if they exist) are linear.
	\item Inverse of a bijective linear function is a bijective linear function
\end{enumerate}
\end{prop}
\subsection{Review of Kernels and Images}
\begin{defn}
	Let $F : V \rightarrow W$ be given. The kernel of $F$ is defined as
	\begin{align*}
	\ker(F) = \{v\in V \vert F[v] = \mathbf{0}_W\}.
	\end{align*}
\end{defn}
\begin{prop}
	Let $F : V \rightarrow W$ a linear map be given. Also, consider a linear map $G$ such that $F\circ G$ is defined 
	\begin{enumerate}
		\item $F$ is null $\iff$ $\ker(F) = V \iff \Im(F) = \mathbf{0}_W $
		\item $\ker(F)$ is a subspace of $V$
		\item $\Im(F)$ is a subspace of $W$
		\item $F$ is injective $\iff \ker(F) = \mathbf{0}_V$
		\item $\ker(F) \subseteq \ker(F\circ G)$.
		\item $F$ injective $\implies \ker(F) = \ker(F\circ G)$
	\end{enumerate}
\end{prop}


\begin{defn}
	Let $F : V \rightarrow W$ be given. The image of $F$ is defined as
	\begin{align*}
	\Im(F) = \{w\in W \vert \exists v\in V, F[v] = w \}. 
	\end{align*}
\end{defn}
\subsection{Atrices}
\begin{defn}
	\textbf{Atrix functions:} Let $V_1,\dots,V_m \in \mathbf{V}$ be given. Consider $f : \R^m \rightarrow \mathbf{V}$ be defined by
	\begin{align*}
	f\begin{pmatrix}
	a_1\\
	a_2\\
	\vdots\\
	a_m
	\end{pmatrix}
	=
	\sum_{i=1}^m a_i V_i.
	\end{align*} 
	We denote $f$ by
	\begin{align*}
	\begin{pmatrix}
	V_1 & V_2 &\dots& V_m
	\end{pmatrix}.
	\end{align*}
	We refer to the $V_i$'s as the \textbf{columns} of $f$, even though there doesn't have to be any columns. Basically, $f$ is simply a function that takes in an ordered list of coefficients and returns a linear combination of $V_i$ with the respective coefficients. A matrix is a special atrix. Not every atrix is a matrix. 
\end{defn}
\begin{prop} of atrices
	$\,$
	\begin{enumerate}
	\item The $V_i$'s - the columns of an atrix - are the images of the standard basis tuples. 
	\item $\mathbf{e}_j \in \ker(V_1\dots V_m) \iff V_j = \mathbf{0}_V$, where $\mathbf{e}_j$ denotes a standard basis tuple with a 1 at the j$^\text{th}$ position. To put in words, a kernel of an atrix contains a standard basis if and only if one of its columns in a null element. 
	\item $\Im(f) \equiv \Im(V_1\dots V_m) = \xpan(V_1\dots V_m)$
	\item $B$ is a null atrix $\iff \ker(B) = \R^m \iff \Im(B) = \mathbf{0}_V \iff V_j = \mathbf{0}_V \forall j=1,2,\dots,m$.
	\item  $f$ is a linear function $\R^m \rightarrow V \iff f$ is an atrix function $\R^m \rightarrow V$.
	\item An atrix $A$ is bijective/invertible, then its inverse $A^{-1}$ is a linear function, but is an atrix only if $A$ is a matrix. 
	\item Linear combinations of atrices are atrices:
	\begin{align*}
	\alpha \cdot\begin{pmatrix}
	V_1 & \cdots & V_m
	\end{pmatrix}
	+ 
	\beta \cdot \begin{pmatrix}
	W_1 & \dots & W_m
	\end{pmatrix}
	\\= \begin{pmatrix}
	\alpha V_1 + \beta W_1 &\dots& \alpha V_m + \beta W_m
	\end{pmatrix}
	\end{align*}  
	\item Compositions of two atrices are NOT defined unless the atrix going first is a matrix. Consider $F : \R^m \rightarrow W$ and $G : \R^n \rightarrow T$. $F\circ G$ is only defined if $T = \R^m$. This make $G$ an $n\times m$ matrix. It follows that the atrix $F\circ G$ has the form
	\begin{align*}
	\begin{pmatrix}
	f(g_1)&f(g_2)&\dots&f(g_m)
	\end{pmatrix}.
	\end{align*}
	\item Consider $F : \R^m \rightarrow V$ and $G : \R^k \rightarrow V$. $\Im(F) \subseteq \Im(G) \iff F = G \circ C$, with $C \in \mathbb{M}_{k\times m}$, i.e. $C$ is an $k \times m$ matrix. 
	\item Consider an atrix $A : \R^m \rightarrow V$. $A = (V_1\dots V_m)$. $A$ is NOT injective.\\
	$\iff \exists$ a non-trivial linear combination of the columns of $A$ that gives $\mathbf{0}_V$\\
	$\iff$ $A = [\mathbf{0}_v]$ or $\exists j \vert V_j$ is linear combination of other columns of $A$\\
	$\iff$ The first column of $A$ is $\mathbf{0}_V$ or $\exists j \vert V_j$ is a linear combination some of $V_i, i < j$.
	\item If atrix $A: \R \rightarrow V$ has a single column then it is injective if the column is not $\mathbf{0}_V$.
	\end{enumerate}
\end{prop}
\begin{prop}
	of elementary column operations for atrices. Elementary operations on the columns of $F$ can be expressed as a composition of $F$ and an appropriate elementary matrix $E$, $F\circ E$. 
	\begin{enumerate}
		\item Swapping $i^{th}$ and $j^{th}$ columns: $F\circ E^{[i]\leftrightarrow[j]}$.
		\item Scaling the $j^{th}$ column by $\alpha$: $F\circ E^{\alpha\cdot[j]}$.
		\item Adjust the $j^{th}$ column by adding to it $\alpha\times i^{th}$ column: $F\circ E^{[i]\stackrel{+}\leftarrow \alpha\cdot[j]}$.
		\item Elementary column operations do not change the 'jectivity nor image of $F$.
		\item If a column of $F$ is a linear combination of some of the other columns then elementary column operations can turn it into $\mathbf{0}_V$.
		\item Removing/Inserting null columns or columns that are linear combinations of other columns does not change the image of $F$.
		\item Given atrix $A$, it is possible to eliminate (or not) columns of $A$ to end up with an atrix $B$ with $\Im(B) = \Im(A)$.
		\item If $B$ is obtained from insertion of columns into atrix $A$, then $\Im(B) = \Im(A) \iff$ the insert columns $\in \Im(A)$.
		\item Inserting columns to a surjective $A$ does not destroy surjectivity of $A$. ($A$ is already having extra or just enough columns) 
		\item $A$ surjective $\iff$ $A$ is obtained by inserting columns (or not) into an invertible atrix $\iff$ deleting some columns of $A$ (or not) gives an invertible matrix.
		\item If $A$ is injective, then column deletion does not destroy injectivity. ($A$ is already ``lacking'' or having just enough columns)
		\item The new atrix obtained from inserting columns from $\Im(A)$ into $A$ is injective $\iff$ $A$ is injective.
	\end{enumerate}
\end{prop}
\subsection{Linear Independence, Span, and Bases}
\subsubsection{Linear Independence}
$\,\,\,\,\,\,\,\,\,\,\,\,\,\,X_1\dots X_m$ are linearly independent\\
$\iff F = [X_1 \dots X_m]$ injective\\
$\iff \sum a_iX_i = 0 \iff a_i = 0 \forall i$\\
$\iff \mathbf{0}_V \notin \{X_i\}$ and none are linear combinations of some of the others.\\
$\iff X_1 \neq \mathbf{0}_V$ and $X_j$ is not a linear combination of any of $X_i$'s for $i < j$. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item The singleton list is linearly independent if its entry is not the null element
		\item Sublists of a linearly independent list are linearly independent
		\item List operations cannot create/destroy linearly independence. 
		\item If a linear map $L : X \rightarrow W$ injective, then $X_1\dots X_m$ linearly independent $\iff L(X_1)\dots L(X_m)$ linearly independent.  
	\end{enumerate}
\end{prop}
\subsubsection{Span}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Spans are subspaces. $\xpan(X_1\dots X_m), X_j \in V$ is a subspace of $V$.
		\item $\xpan(X_1\dots X_j) \subseteq \xpan(X_1\dots X_k)$ if $j\leq k$.
		\item Adding elements to a list that spans $V$ produces a list that spans $V$.
		\item The following list operations do not change the span of $V$: removing the null/linearly dependent element, inserting a linearly dependent element, scaling element(s), adding (multiples) of an element to another element.
		\item It is possible to reduce a list that spans to a list that spans AND have linearly independent elements.
		\item Consider $A : V \rightarrow W$. If $X_i$ span $W$ then $A(X_i)$ span $\Im(A)$. $i=1,2,\dots,m$.
		\item If $A : V \rightarrow W$ invertible, then $X_i$ span $V \iff A(X_i)$ span $W$, $i=1,2,\dots,m$. 
	\end{enumerate}
\end{prop}
\subsubsection{Bases}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A list is a basis of $V$ if the elements are linearly independent and they span $V$.
		\item A singleton linear space has no basis.
		\item Re-ordering the elements of a basis gives another basis.
		\item $\{X_1\dots X_m\}$ is a basis of $V$ if $(X_1\dots X_m)$ is injective AND $\Im(X_1\dots X_m) = V$, i.e. $(X_1\dots X_m)$ invertible.
		\item If $\{X_i\}$ is a basis of $V$ and $\{Y_i \}$ is a list of elements in $W$, then there exists a unique linear function $L : V \rightarrow W$ satisfying 
		\begin{align*}
		L[X_i] = Y_i
		\end{align*} 
		\item If $A: V \rightarrow W$ bijective, then $\{ V_i \}$ forms a basis of $V$ and $\{A(X_i) \}$ forms a basis of $W$.
		\item Elementary operations on bases give bases.  
	\end{enumerate}
\end{prop}
\subsection{Linear Bijections and Isomorphisms}
\begin{defn}
	Let linear spaces $V, W$ be given. $V$ is \textbf{isomorphic} to $W$ if $\exists F : V \rightarrow W$ bijective. We say $V \sim W$. 
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item A non-zero scalar multiple of an isomorphism is an isomorphism.
		\item A composition of isomorphism is an isomorphism.
		\item ``Isomorphism'' behaves like an equivalence relation:
		\begin{enumerate}
			\item Reflexivity: $V \sim V$.
			\item Symmetry: if $V\sim W$ then $W\sim V$.
			\item Transitivity: if $V\sim W$ and $W \sim Z$ then $V\sim Z$.
		\end{enumerate}
		\item Consider $F : V \rightarrow W$ an isomorphism.
			\begin{enumerate}
				\item Isomorphisms preserve linear independence. $V_i$'s are linearly independent in $V$ $\iff$ $F(V_i)$'s are linearly independent in $W$.
				\item isomorphisms preserve spanning. $V_i$'s span $V$ $\iff$ $F(V_i)$'s span $W$.
				\item Isomorphisms preserve bases. $\{V_i\}$ is a basis of $V$ $\iff$ $F\{(V_i)\}$ is a basis of $W$.
			\end{enumerate}
		\item If $V\sim W$ then $\dim(V) = \dim(W)$ (finite or infinite).
		\item If a linear map $A : V \rightarrow W$ is given and $A(X_i)$'s are linearly independent, then $A_i$'s are linearly independent.  
		\item If a linear map $A : V \rightarrow W$ is injective and $\{ X_i\}$ is a basis of $V$ then $\{ A(X_i)\}$ is a basis of $\Im(A)$
	\end{enumerate}
\end{prop}
\subsection{Finite-Dimensional Linear Spaces}
\subsubsection{Dimension}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item $\R^m \sim R^n \iff m=n$.
		\item Isomorphisms $F : \R^n \rightarrow V$ are bijective atrices.
		\item Isomorphisms $G : W \rightarrow \R^m$ are inverses of bijective atrices.
		\item Consider a non-singleton linear space $V$
		\begin{enumerate}
			\item $V$ has a basis with $n$ elements.
			\item $V \sim \R^n$.
			\item $V \sim W$, where $W$ is any linear space with a basis of $n$ elements.
		\end{enumerate}
		\item If $V$ is a linear space with a basis with $n$ elements, then any basis of $V$ has $n$ elements.
		\item Linear space $V \sim W$ where $W$ is $n$-dimensional if $V$ is $n$-dimensional.
		\item For a non-singleton linear space $V$, $V\sim \R^n \iff \dim(V) = n$.
		\item $V \sim W \iff \dim(V) = \dim(W)$.
		\item If $W$ is a subspace of $V$, then $\dim(W)\leq \dim(V)$. Equality holds when $W = V$.
	\end{enumerate}
\end{prop}
\subsubsection{Rank-Nullity Theorem}
Let finite-dimensional linear space $V$ and linear map $F : V \rightarrow W$ be given. Then $\Im(F)$ is finite-dimensional and 
\begin{align*}
\dim(\Im(F)) + \dim(\ker(F)) = \dim(V)
\end{align*}

A stronger statement: If a linear map $F : V\rightarrow W$ has finite rank and finite nullity $\iff$ $V$ is finite-dimensional, then
\begin{align*}
\Im(F) + \ker(F) = \dim(V).
\end{align*}

\subsection{Infinite-Dimensional Spaces}
Consider a non-singleton linear space $V$. The following statements are equivalent:
\begin{enumerate}
	\item $V$ is infinite-dimensional.
	\item Every linearly independent list in $V$ can be enlarged to a strictly longer linearly independent set in $V$.
	\item Every linearly independent list in $V$ can be enlarged to an arbitrarily long (finite) linearly independent set in $V$.
	\item There are arbitrarily long (finite) linearly independent lists in $V$.
	\item There are linearly independent lists in $V$ of any (finite) length.
	\item No list of finitely many elements of $V$ spans $V$.
\end{enumerate}
\newpage
\section{Sums of Subspaces \& Products of vector spaces}
\subsection{Direct Sums}
\begin{defn}
	Let $U_j$, $j=1,2,\dots m$ are subspaces of $V$. $\sum_1^m U_j$ is a \textit{direct sum} if each $u \in \sum U_j$ can be written in only one way as $u = \sum_1^m u_j$. The direct sum $\sum^m_i U_j$ is denoted as $U_1 \oplus\dots\oplus U_m$.
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Condition for direct sum: If all $U_j$ are subspaces of $V$, then $\sum_1^m U_j$ is a direct sum $\iff$ the only way to write 0 as $\sum_1^m u_j$, where $u_j \in U_j$ is to take $u_j = 0$ for all $j$.  
		\item If $U, W$ are subspaces of $V$ and $U \bigcap W = \{ 0\}$ then $U+W$ is a direct sum. 
		
		\item Let $U_j$ be finite-dimensional and are subspaces of $V$. 
		\begin{align*}
		U_1 \oplus \dots\oplus U_m \iff \dim(U_1+\dots+U_m) = \sum_1^m\dim(U_j)
		\end{align*}
		
		\item Let $U_1,\dots,U_m$ be subspaces of $V$. Define a linear map $\Gamma : U_1\times\dots\times U_m \rightarrow U_1 + \dots + U_m$ by:
		\begin{align*}
		\Gamma(u_1,\dots,u_m) = \sum_1^m u_j.
		\end{align*}
		$U_1 + \dots + U_m$ is a direct sum $\iff \Gamma$ is injective. 
	\end{enumerate}


		\item Subspace addition is associative.
		
		\item Subspace addition is commutative.
\end{prop}



\subsection{Products and Direct Sums}


Here is a connection between direct sums and products of vector spaces. Suppose $\Z_1$ and $\Z_2$ are subspaces of $\W$ and $z_1 \in \Z_1$, $z_2 \in \Z_2$. Consider the function $\maltese : \Z_1 \times \Z_2 \lin \Z_1 \oplus \Z_2 \prec \W$ defined by
\begin{align}
\maltese \begin{pmatrix}
z_1\\z_2
\end{pmatrix} \stackrel{\Delta}{=} z_1 + z_2.
\end{align}

We can show that
\begin{enumerate}
	\item $\maltese$ is a linear function. 
	
	\item $\maltese$ is an isomorphism, i.e., $\Z_1 \times \Z_2 \sim \Z_1 \oplus \Z_2$.
\end{enumerate}






\subsection{Products of Vector Spaces}




\begin{defn}
	Product of vectors spaces
	\begin{align*}
	V_1 \times\dots\times V_m = \{ (v_1,\dots,v_m): v_j \in V_j, j=1,2,\dots,m\}.
	\end{align*}
\end{defn}
\begin{defn}
	Addition on $V_1 \times\dots\times V_m$:
	\begin{align*}
	(u_1,\dots,u_m) + (v_1,\dots,v_m) = (u_1+v_1,\dots,v_m+u_m).
	\end{align*}
\end{defn}
\begin{defn}
	Scalar multiplication on $V_1 \times\dots\times V_m$:
	\begin{align*}
	\lambda(v_1,\dots,v_m) = (\lambda v_1,\dots,\lambda v_m).
	\end{align*}
\end{defn}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Product of vectors spaces is a vector space.
		\begin{align*}
		V_j \text{ are vectors spaces over } \F \implies V_1\times\dots\times V_m \text{ is a vector space over } \F.
		\end{align*}
		\item Dimension of a product is the sum of dimensions: 
		\begin{align*}
		\dim(V_1\times\dots\times V_m) = \sum_1^m\dim(V_j)
		\end{align*}
		\item Vector space products are NOT commutative:
		\begin{align*}
		W\times V \neq V\times W.
		\end{align*}
		However, 
		\begin{align*}
		V\times W \sim W \times V.
		\end{align*}
		\item Vector space products are NOT associative:
		\begin{align*}
		V\times(W\times Z) \neq (V\times W)\times Z
		\end{align*}
	\end{enumerate}
\end{prop}



\subsection{Rank-Nullity Theorem}
Suppose $Z_1$ and $Z_2$ are subspaces of a finite-dimensional vector space $W$. Consider $z_1\in Z_1$, $z_2\in Z_2$, and a function $\phi : Z_1 \times Z_2 \to Z_1 + Z_2 \prec W$ defined by
\begin{align*}
\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
=
z_1 + z_2.
\end{align*}
First, $\phi$ is a linear function, as it satisfies the linearity condition:
\begin{align*}
\phi\left(\alpha
\begin{pmatrix}
z_1\\z_2
\end{pmatrix}
+
\beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}\right) = \alpha\phi\begin{pmatrix}
z_1\\z_2
\end{pmatrix} + \beta \phi \beta\begin{pmatrix}
z'_1\\
z'_2
\end{pmatrix}.
\end{align*}
By rank-nullity theorem,
\begin{align*}
\dim(Z_1\times Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi)). 
\end{align*}
But this is equivalent to
\begin{align*}
\dim(Z_1) + \dim(Z_2) = \dim(Z_1 + Z_2) +\dim(\ker(\phi))
\end{align*}
The kernel of $\phi$ is:
\begin{align*}
\ker(\phi) = 
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1, z\in Z_2
\right\}
=
\left\{
\begin{pmatrix}
v\\-v
\end{pmatrix}
\bigg\vert v\in z\in Z_1 \cap Z_2
\right\}
\end{align*}
We can readily verify that $Z_1 \cap Z_2$ is a subspace of $W$. With this, $\dim(\ker(\phi)) = \dim(Z_1\cap Z_2)$. So we end up with
\begin{align*}
\dim(Z_1 + Z_2) = \dim(Z_1) + \dim(Z_2) - \dim(Z_1\cap Z_2).
\end{align*}
\begin{prop}
	$\,$
	\begin{enumerate}
		\item When $Z_1\cap Z_2$ is trivial, then $Z_1 + Z_2$ is direct. 
		\item When $\dim(\ker(\phi)) = 0$, $\phi$ is injective. But $\phi$ is also surjective by definition, this implies $\phi$ is a bijection, in which case
		\begin{align*}
		Z_1 \oplus Z_2 \sim Z_1 + Z_2.
		\end{align*} 
	\end{enumerate}
\end{prop}


\subsection{Nullspaces \& Ranges of Operator Powers}

\begin{enumerate}
	\item Sequence of increasing null spaces: Suppose $T \in \lag(V)$, i.e., $T$ is some linear function mapping $V\to V$, then
	\begin{align*}
	\{ 0 \} = \ker(T^0) \subset \ker(T^1) \subset \ker(T^2) \subset \dots \subset \ker(T^k) \subset \ker(T^{k+1}) \subset\dots.
	\end{align*} 
	
	\begin{proof}[Proof Outline]
		Let $k$ be a nonnegative integer and $v\in \ker(T^k)$. Then $T^kv = 0$, so $T^{k+1}v = T(T^kv) = T(0) = 0$, so $v\in \ker T^{k+1}$. So $\ker(T^k) \subset \ker(T^{k+1})$. 
	\end{proof}
	\item Equality in the sequence of null spaces: Suppose $m$ is a nonnegative integer such that $\ker(T^m) = \ker(T^{m+1})$, then
	\begin{align*}
	\ker(T^m) = \ker(T^{m+1}) = \ker(T^{m+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		We want to show
		\begin{align*}
		\ker(T^{m+k}) = \null(T^{m+k+1}).
		\end{align*}
		We know that $\ker T^{m+k} \subset \ker T^{m+k+1}$. Suppose $v\in \ker T^{m+k+1}$, then
		\begin{align*}
		T^{m+1}(T^kv) = T^{m+k+1}v = 0.
		\end{align*}
		So
		\begin{align*}
		T^kv \in \ker T^{m+1} = \ker T^m.
		\end{align*}
		So
		\begin{align*}
		0 = T^m(T^kv) = T^{m+k}v,
		\end{align*}
		i.e., $v\in \ker T^{m+k}$. So $\ker T^{m+k+1} \subset \ker T^{m+k}$. This completes the proof. 
	\end{proof}
	\item Null spaces stop growing: If $n = \dim(V)$, then
	\begin{align*}
	\ker(T^n) = \ker(T^{n+1}) = \ker(T^{n+2}) = \dots.
	\end{align*}
	
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n = \ker T^{n+1}.
		\end{align*}
		Suppose this is not true. Then the dimension of the kernel has to increase by at least 1 every step until $n+1$. Thus $\dim \ker T^{n+1} \geq n+1 > n = \dim(V)$. This is a contradiction. 
	\end{proof}
	\item $V$ is the direct sum of $\ker(T^{\dim(V)})$ and $\Im(T^{\dim(V)})$: If $n = \dim(V)$, then
	\begin{align*}
	V = \ker(T^n) \oplus \Im(T^n).
	\end{align*}
	\begin{proof}[Proof Outline]
		To show:
		\begin{align*}
		\ker T^n \cap \Im T^n = \{0 \}. 
		\end{align*}
		Suppose $v\in \ker T^n \cap \Im T^n$. Then $T^n v = 0$ and $\exists u\in V$ such that $v = T^n u$. So
		\begin{align*}
		T^n v = T^{2n}u = 0.
		\end{align*}
		So
		\begin{align*}
		T^n u = 0.
		\end{align*}
		But this means $v = 0$. 
	\end{proof}
	\item IT IS NOT TRUE THAT $V = \ker(T) \oplus \Im(T)$ in general. 
\end{enumerate}

\subsection{Generalized Eigenvectors and Eigenspaces}
\begin{defn}
	Suppose $T\in \lag(V)$ and $\Lambda$ is an eigenvalue of $T$. A vector $v\in V$ is called a \textbf{generalized eigenvector} of $T$ corresponding to $\lambda$ if $v\neq 0$ and
	\begin{align*}
	(T-\lambda I)^j v = 0
	\end{align*}
	for some positive integer $j$. 
\end{defn}

\begin{defn}
	\textbf{Generalized Eigenspace}: Suppose $T \in \lag(V)$ and $\lambda \in \mathbf{F}$. The \textbf{generalized eigenspace} of $T$ corresponding to $\lambda$, denoted $G(\lambda,T)$, is defined to be the set of all generalized eigenvectors of $T$ corresponding to $\lambda$, along with the 0 vector.
\end{defn}

\begin{prop}
	\begin{enumerate}
		\item Suppose $T\in\lag(V)$ and $\lambda \in \mathbf{F}$. Then 
		\begin{align*}
		G(\lambda,T) = \ker(T-\lambda I)^{\dim(V)}.
		\end{align*}
		\begin{proof}[Proof Outline]
			Suppose $v\in \ker (T_\lambda I)^{\dim(V)}$. Then $v\in G(\lambda,T)$. So, $\ker(T-\lambda I)^{\dim V} \subset G(\lambda,T)$. Next, suppose $v\in G(\lambda,T)$. Then these is a positive integer $j$ such that
			\begin{align*}
			v\ in \ker (T-\lambda I)^j.
			\end{align*}
			But if this is true, then 
			\begin{align*}
			v\ in \ker (T-\lambda I)^{\dim V},
			\end{align*}
			since $\ker(T-\lambda I)^{\dim V}$ is the largest possible kernel, in a sense. 
		\end{proof}
		\item Linearly independent generalized eigenvectors: Let $T\in\lag(V)$. Suppose $\lambda_1,\dots,\lambda_m$ are distinct eigenvalues of $T$ and $v_1,\dots,v_m$ are corresponding generalized eigenvectors. Then $v_1,\dots,v_m$ is linearly independent.
	\end{enumerate}
\end{prop}

\subsection{Nilpotent Operators}
\begin{defn}
	An operator is called \textbf{nilpotent} if some power of it equals 0.
\end{defn}
\begin{prop}
	\item Nilpotent operator raised to dimension of domain is 0: Suppose $N\in \lag(V)$ is nilpotent. Then \begin{align*}
	N^{\dim(V)} = 0.
	\end{align*}
	\item Matrix of a nilpotent operator: Suppose $N$ is a nilpotent operator on $V$. Then there is a basis of $V$ with espect to which the matrix of $N$ has the form
	\begin{align*}
	\begin{pmatrix}
	0 & & *\\
	&\ddots&\\
	0&&0
	\end{pmatrix};
	\end{align*}
	here all entries on and below the diagonal are 0's. 
\end{prop}

\subsection{Weyr Characteristic}

\newpage
\section{Idempotents \& Resolutions of Identity}
\begin{defn}
	An \textbf{Operator} is a linear function from a vector space to itself:
	\begin{align*}
	\E : \V \lin \V.
	\end{align*}
\end{defn}


\begin{defn}
	\textbf{Idempotents:} are operators with the property $\E^2 = \E$, i.e., 
	\begin{align*}
	\E \circ \E = \E.
	\end{align*}
	
	Recall that if $V = \W \oplus \Z$, then there exists an idempotent $\E \in \lag(\V)$ such that
	\begin{align*}
	&\W = \ima(\E)\\
	&\Z = \ker(\E).
	\end{align*}
\end{defn}
In fact, if $\V = \W \oplus \Z$ then there exists at least \textbf{two} idempotents $\E$, $\W = \ima(\E), \Z = \ker(\E)$ and $\F$, $\W = \ker(\F)$, $\Z = \ima(\F)$.\\

A natural question to ask now is whether every idempotent $\E \in \lag(\V)$ can be generated this way. The answer is \textbf{Yes}, but to answer to this, we need to show \textbf{existence} and \textbf{uniqueness}. \\

\begin{thm}
	If $\E \circ \E = \E^2 = \E$, then $\V = \ima(\E)\oplus \ker(\E)$.
	
	\begin{proof}
		$\,$
		\begin{enumerate}
			\item Show that $\ima(\E) + \ker(\E) = \V$.\\
			
			Let $v\in \V$ be given, then $v = \E(v) + v - \E(v)$. Now, observe that
			\begin{align*}
			\E(v - \E(v)) = \E(v) - \E^2(v) = \E(v) - \E(v) = 0.
			\end{align*}
			Therefore, $\E(v)\in \ima(\E)$ and $ (v - \E(v)) \in \ker(\E)$. So, $\ima(\E) + \ker(\E) = \V$.
			
			
			\item Show that $\ima(\E)\oplus \ker(\E) = \V$.\\
			
			We want to show that $\ima(\E) \bigcap \ker(\E) = \{\mathbf{0}\}$. So, let $v\in \ima(\E) \bigcap \ker(\E) $ be given. Then $v\in \ima(\E)$, which implies $\E(\E(x)) = \E(x) = v = \mathbf{0}$ for some $x$. Hence, the intersection is the trivial subspace.
	
			
			
		\end{enumerate}
	\end{proof} 
\qed
\end{thm} 

\begin{prop}
	A by product of this previous item is this fact. For any $x\in \V$
	\begin{align}
	\E(\E(x)) = \E(x) \iff \E = \E^2,
	\end{align}
	i.e., $\E$ is an idempotent exactly when it acts as an identity function on its own image.\\
\end{prop}


\begin{thm}
	If $\E^2 = \E$ and $\mathcal{G}^2 = \mathcal{G} $ in $\lag(\V)$ and 
	\begin{align*}
	&\ima(\mathcal{G}) = \ima(\E)\\
	&\ker(\mathcal{G}) = \ker(\E)
	\end{align*}
	then
	\begin{align*}
	\E = \mathcal{G}.
	\end{align*}
	
	\begin{proof}
		Let $\W = \ima(\E) = \ima(\mathcal{G})$ and $\Z = \ker(\E) = \ker(\mathcal{G})$. Note that $\W \oplus \Z = \V$. Consider $v\in \V$. Then
		\begin{align*}
		\E(v) = \E(w+z) = \E(w) + \E(z) = \E(w) + \mathbb{0} = \E(w) = w,
		\end{align*}
		where $v = z+w$ is unique by directness. We can do the same thing with $G$, and get $G = E$.
	\end{proof}
\qed
\end{thm}


\begin{thm}
	For each decomposition $V = \W \oplus \Z$, there exists a unique idempotent whose image is $\W$ and kernel is $\Z$.\qed
\end{thm}

Now, notice that for $v = w + z$, we have $v = w + z = \E(w) + \F(z) = \E(v) + \F(v) = Id(v)$. So we have a little theorem:

\begin{thm}
	If $\E$ and $\F$ are idempotents such that $\ima(\E) = \ker(\F)$ and $\ker(\E) = \ima(\F)$ then
	\begin{align*}
	\E + \F = Id. 
	\end{align*}\qed
\end{thm} 

As a result we get another two results:

	\begin{enumerate}
		\item If $\E^2 = \E \in \lag(\V)$ then $(Id - \E)^2 = (Id - \E)$ also an idempotent.
		
		\item 
		\begin{align*}
		&\ima(\E) = \ker(Id - \E)\\
		&\ker(\E) = \ima(Id - \E)
		\end{align*}
	\end{enumerate}

We say that in this case idempotents come in pairs. So, to informally summarize, if $V = \W \oplus \Z$, then there is a pair of idempotents. More concretely, if $V = \W \oplus \Z$, then there exist unique idempotents $\E$ and $\F$ such that $\E + \F = Id$ and $\W = \ima(\E) = \ker(\F)$ and $\Z = \ker(\E) = \ima(\F)$. The converse is also true. If there exist idempotents $\E$ and $\F$ such that if $\E + \F = Id$ and $\W = \ima(\E) = \ker(\F)$ and $\Z = \ker(\E) = \ima(\F)$ then $\V = \W \oplus \Z$.\\

All is good, but what if the vector space $\V$ is decomposed into three pieces as $\V = \W \oplus \Z \oplus \U$? Well, we simply repeat what we have done before to $\V = \W \oplus (\Z \oplus \U)$. We know that there exist a unique $\E_\W$ such that $\E_\W^2 = \E_\W$ and that $\ima(\E_\W) = \W$ and $\ker(\E_\W) = \U\oplus \Z$. Similarly,  there exist a unique $\E_\U$ such that $\E_\U^2 = \E_\U$ and that $\ima(\E_\U) = \V$ and $\ker(\E_\U) = \W\oplus \Z$; and there exist a unique $\E_\W$ such that $\E_\W^2 = \E_\W$ and that $\ima(\E_\W) = \W$ and $\ker(\E_\W) = \U\oplus \Z$.\\

There are a few observations we can make:
\begin{enumerate}
	\item Let $v\in \V$ be given, then $(\E_\W + \E_\U + \E_\Z)(v) = (\E_\W + \E_\U + \E_\Z)(w + u + z) = w + u + z = v$. So $\E_\W + \E_\U + \E_\Z = Id$.
	
	\item $\E_\W\circ \E_\Z(v) = \E_\W(z) = \mathbf{0}_\V$. So, in general, $\E_i \circ \E_j = \mathcal{O}$ for $i \neq j$ and $\E_i$ if $i=j$.  
\end{enumerate}


What about the converse? Given the linear functions such that $\E_j^2 = \E$ such that $\ima(\E_i) = \ker(\E_j + \E_k)$, then does $V = \Z \oplus \W + \U$ hold? The answer is yes! We can restate this for rigorously:

\begin{thm}
	Suppose $\E_1, \E_2, \E_3$ are idempotents such that $\E_1 + \E_2 + \E_3 = Id$ and $\E_i \circ \E_j = \delta^j_i \E_i$ then 
	\begin{align*}
	\ima(\E_1) \oplus \ima(\E_2) \oplus \ima(\E_3) = \V
	\end{align*}
	and
	\begin{align*}
	\ker(\E_1) = \ima(\E_2) \oplus \ima(\E_3), 
	\end{align*}
	etc.\qed
\end{thm}



The following two statements are equivalent:
\begin{enumerate}
	\item $\W \oplus \Z \oplus \U = \V$.
	
	\item There are idempotents $\E_1, \E_2, \E_3 \in \lag(\V)$ such that 
	\begin{align*}
	\ima(\E_1) = \W, \ima(\E_2) = \Z, \ima(\E_3) = \U
	\end{align*} 
	and
	\begin{align*}
	&\E_1 + \E_2 + \E_3 = Id\\
	&\E_j \circ \E_i = \delta^j_i \E_i
	\end{align*}
	
	\begin{proof}
		\begin{itemize}
			\item The forward direction has been shown.
			
			\item We will show how (2) implies (1). We first show that $\ima(\E_1) + \ima(\E_2) + \ima(\E_3) = \V$, then
			\begin{align*}
			\V \ni v = Id(v) = (\E_1 + \E_2 + \E_3)(v) = \E_1(v) + \E_2(v) + \E_3(v) \in \ima(\E_1) + \ima(\E_2) + \ima(\E_3).
			\end{align*}
			It is straightforward to show that $V = \ima(\E_1) + \ima(\E_2) + \ima(\E_3)$. To show directness, suppose that $x_1 + x_2 + x_3 = \mathbf{0}$, $x_i \in \ima(\E_i)$. We want to show that $x_i = \mathbf{0}$ for any $i \in \{1,2,3\}$. By construction, we have that
			\begin{align*}
			\E_1(x_1) + \E_2(x_2) + \E_3(x_3) = \mathbf{0},
			\end{align*}
			so
			\begin{align*}
			\E_1\left( \E_1(x_1) + \E_2(x_2) + \E_3(x_3) \right) = \E_1(\mathbf{0}) = \mathbf{0}.
			\end{align*}
			But this implies that 
			\begin{align*}
			\E_1(\E_1(x_1)) = \E_1(x_1) = x_1 = \mathbf{0}.
			\end{align*}
			Similarly, we get $x_j = \mathbf{0}$ for all $j$. By one of the equivalent statements about directness, we get that $\ima(\E_1) \oplus \ima(\E_2) \oplus \ima(\E_3) = \V$. 
		\end{itemize}
	\end{proof}
\end{enumerate}



We can also (of course) look at the kernel. Observe that $\E_1(\E_2(v)) = \mathbf{0}$. So $\ima(\E_2) \subseteq \ker(\E_1)$. Similarly, $\ima(\E_3) \subseteq \ker(\E_1)$. So, $\ima(\E_3) + \ima(\E_2) \subseteq \ker(\E_1)$. But we also know that the images of $\E_1$ and $E_2$ intersect trivially, so this sum is direct. So we have $\ima(\E_3) \oplus \ima(\E_2) \subseteq \ker(\E_1).$ But we also know that $V = \ima(\E_1) \oplus \ker(\E_1) = \ima(\E_1) \oplus \ima(\E_2) \oplus \ima(\E_3) $, so
\begin{align*}
\ima(\E_2) \oplus \ima(\E_3) = \ker(\E_1)
\end{align*}
as desired. \\

So, just a recap of what we have done so far, the following statements are equivalent for $\mathcal{A} \in \lag(\V)$:
\begin{enumerate}
	\item $\A^2 = \A$.
	\item $\A(x) = x$ for $x\in \ima(\A)$.
	\item $(Id - \A)^2 = (Id - \A)$.
	\item $\ima(\A) = \ker(Id - \A)$.
	\item $\ima(Id - \A) = \ker(\A)$.
\end{enumerate}



\newpage
\section{Block-representations of operators}

\subsection{Coordinatization}
Consider a finite-dimensional linear space $V$ with basis $\{V_i \}$, $i=1,2,\dots,m$. An element $\tilde{V}$ in $V$ can be expressed in exactly one way:
\begin{align*}
\tilde{V} = \sum_{i=1}^m a_iV_i,
\end{align*}
where $\{ a_i\}$ is unique. We call $\{ a_i \}$ the coordinate tuple of $\tilde{V}$ and $a_i$'s the coordinates. 
\begin{prop}
	$\,$
	\begin{enumerate}
		\item Inverse of a bijective atrix outputs the coordinates. Suppose $A = [V_i]$. Then
		\begin{align*}
		\tilde{V} = \sum_{i=1}^ma_iV_i \iff A^{-1}(Z) = \begin{pmatrix}
		a_1&\dots&a_m
		\end{pmatrix}^\top
		\end{align*}
	\end{enumerate}
\end{prop}

\subsection{Matricial representation of linear functions}

\subsubsection{In Cartesian products}

Let us start with a Cartesian product of two vector spaces and a linear function $\lag$ mapping this Cartesian product to itself $\lag : \V \times \W \lin \V \times \W$, where a typical element in $\V\times \W$ is given by
\begin{align*}
\begin{pmatrix}
v\\w
\end{pmatrix} = \begin{pmatrix}
v\\ \mathbf{0}
\end{pmatrix} + \begin{pmatrix}
\mathbf{0} \\ w
\end{pmatrix}.
\end{align*}
Formally, we can represent the linear function $\lag$ as a block-matrix function
\begin{align*}
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix}\begin{pmatrix}
v\\w
\end{pmatrix} \stackrel{\Delta}{=} \begin{pmatrix}
F_{11}(v) + F_{12}(w)\\
F_{21}(v) + F_{22}(w)
\end{pmatrix}.
\end{align*}
So we have 
\begin{align*}
F_{11} : \V \lin \V\\
F_{12} : \W \lin \V\\
F_{21} : \V \lin \W\\
F_{22} : \W \lin \W.
\end{align*}
It can be illuminating if we show what the linear functions $F_{ij}$ does in a block-matrix form:
\begin{align*}
\begin{tabular}{c|c|c|}
	& $\V$ & $\W$\\
	\hline
$\V$& $\gives$& $\gives$\\	
$\W$& $\gives$ & $\gives$
\end{tabular}.
\end{align*}
Now, since $F_{ij}$ are all linear, the block-matrix function 
\begin{align*}
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix} : \V \times \W \to \V \times \W
\end{align*}
is linear as well. \\

A natural question to ask would be: ``Can every linear function be represented this way?'' The answer is Yes, and we shall see how this is done. \\

Notice that 
\begin{align}
\lag\begin{pmatrix}
v\\\mathbf{0}
\end{pmatrix}
=
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix}\begin{pmatrix}
v\\\mathbf{0}
\end{pmatrix} = \begin{pmatrix}
F_{11}(v) \\ F_{21}(v)
\end{pmatrix}.
\end{align}
In particular, 
\begin{align}
\Pi_\V \circ \lag \begin{pmatrix}
v \\ \mathbf{0}
\end{pmatrix} = F_{11}(v),
\end{align}
where $\Pi_\V : \V\times \W \to \V$ is a coordinate projection from $\V\times \W$ onto $\V$. In fact, we can also defined the linear function $\gamma_\V$ that is a coordinate injection from $\V$ into $\V\times\W$. So, given $\lag : \V\times \W \lin \V\times \W$, we let:
\begin{align}
F_{11} := \Pi_\V \circ \lag \circ \gamma_\V\\
F_{12} := \Pi_\V \circ \lag \circ \gamma_\W\\
F_{21} := \Pi_\W \circ \lag \circ \gamma_\V\\
F_{22} := \Pi_\W \circ \lag \circ \gamma_\W.
\end{align}
Now, we can easily check that $\lag$ is indeed
\begin{align}
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix}.
\end{align}
We do this by considering $\begin{pmatrix}
v & w
\end{pmatrix}^\top \in \V \times \W$. We know that
\begin{align}
&F_{11}(v) + F_{12}(w) = \Pi_1 \circ \lag\begin{pmatrix}
v \\ \mathbf{0}
\end{pmatrix}
+
\Pi_1 \circ \lag \begin{pmatrix}
\mathbf{0} \\ w
\end{pmatrix}
=
\Pi_1 \circ \lag\begin{pmatrix}
v\\w
\end{pmatrix}\\
&F_{21}(v) + F_{22}(w) = \Pi_2 \circ \lag\begin{pmatrix}
v \\ \mathbf{0}
\end{pmatrix}
+
\Pi_2 \circ \lag \begin{pmatrix}
\mathbf{0} \\ w
\end{pmatrix}
=
\Pi_2 \circ \lag\begin{pmatrix}
v\\w
\end{pmatrix}.
\end{align}
So,
\begin{align*}
\begin{pmatrix}
F_{11}(v) + F_{12}(w)\\F_{21}(v) + F_{22}(w)
\end{pmatrix}
= \begin{pmatrix}
\Pi_1 \circ \lag\begin{pmatrix}
v\\w
\end{pmatrix}\\
\Pi_2 \circ \lag\begin{pmatrix}
v\\w
\end{pmatrix}
\end{pmatrix}
=
\lag\begin{pmatrix}
v\\w
\end{pmatrix}.
\end{align*}
So, if we start with any $\lag$, we can represent $\lag$ as a matrix of the linear functions $F_{ij}$.



\subsubsection{In direct sums}
We don't often work with Cartesian products. So we want to use the idea developed above to break a vector space into direct sums. Suppose that we are given $V = \Z \oplus \W$. Let us use the bad notation
\begin{align*}
\begin{pmatrix}
w\\z
\end{pmatrix}_+ = w + z,
\end{align*}
so that we can mimic our previous idea. Recall the idea 
\begin{align*}
\begin{tabular}{c|c|c|}
& $\V$ & $\W$\\
\hline
$\V$& $\gives$& $\gives$\\	
$\W$& $\gives$ & $\gives$
\end{tabular},
\end{align*}
given 
\begin{align*}
&F_{11} : \W \to \W\\
&F_{12} : \Z \to \W\\
&F_{21} : \W \to \Z\\
&F_{22} : \Z \to \Z,
\end{align*}
we can define a linear function
\begin{align*}
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix} : \V \to \V
\end{align*}
by 
\begin{align*}
\begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix}\begin{pmatrix}
w\\z
\end{pmatrix}_+ = \begin{pmatrix}
F_{11}(w) + F_{12}(z)\\
F_{21}(w) + F_{22}(z)
\end{pmatrix}_+.
\end{align*}
Now, we want to find something that is similar to $\Pi_i$ defined before. Since we are dealing with direct sums rather than Cartesian products, \textbf{idempotents} are perfect candidates. In fact, if 
\begin{align*}
\lag = \begin{pmatrix}
\begin{matrix}
F_{11}
\end{matrix}
& \rvline & F_{12} \\
\hline
F_{21} & \rvline &
F_{22}
\end{pmatrix},
\end{align*}
then 
\begin{align*}
\lag\begin{pmatrix}
w\\ \mathbf{0}
\end{pmatrix}_+ = \begin{pmatrix}
F_{11}(w)\\
F_{21}(w)
\end{pmatrix}_+
\end{align*}
So it is easy to see that
\begin{align*}
&\E \circ \lag \begin{pmatrix}
w & \mathbf{0}
\end{pmatrix}^\top = F_{11}(w) \in \W\\
&(Id - \E) \circ \lag \begin{pmatrix}
w & \mathbf{0}
\end{pmatrix}^\top = F_{21}(w) \in \Z.
\end{align*}
The same argument applies to find $F_{12}$ and $F_{22}$. So we can once again check that 
\begin{align*}
\lag 
=
\begin{pmatrix}
\E \circ \lag\bigg\vert_\W & \rvline & \E\circ\lag\bigg\vert_\Z\\
\hline
(Id - \E) \circ \lag\bigg\vert_\W & \rvline & (Id - \E)\circ \lag \bigg\vert_\Z
\end{pmatrix}.
\end{align*}
So, if we have a linear function $\lag : \V \to \V$ and with respect to the decomposition $|V = \W \oplus \Z$ and that $\lag$ has the block form $\begin{pmatrix}
A & \rvline & B\\
\hline
C & \rvline & D
\end{pmatrix}$ then it means
\begin{align*}
A := \E \circ \lag \bigg\vert_\W : \W \lin \W,
\end{align*}
and so on, where $\E$ is an idempotent with $\ima(\E) = \W$ and $\ker(\E) = \Z$. \\

There is a little caveat: direct sum is commutative, BUT here the order of $\W$ and $\Z$ matters in the definition of the block-matrix representation of $\lag$. 

\begin{exmp}
	Consider the linear function $G : \V \to \V$ an idempotent, such that $\V = \ima(G) \oplus \ker(G)$. So,
	\begin{align*}
	G = \begin{pmatrix}
	Id \bigg\vert_{\ima(G)} & \rvline & \mathcal{O}\\
	\hline
	\mathcal{O} & \rvline & \mathcal{O}
	\end{pmatrix}.
	\end{align*}\qed
\end{exmp}







\subsection{Properties of Block-matrix representation}
We will show in the homework the following properties:


\begin{enumerate}
	\item \textbf{Addition:} If 
	\begin{align*}
	L = \begin{pmatrix}
	A & \rvline & B\\
	\hline
	C & \rvline & D
	\end{pmatrix}
	\end{align*}
	and
	\begin{align*}
	S = \begin{pmatrix}
	P & \rvline & Q\\
	\hline
	R & \rvline & T
	\end{pmatrix}
	\end{align*}
	then
	\begin{align*}
	L + S = \begin{pmatrix}
	A + P & \rvline & B+Q\\
	\hline
	C + R & \rvline & D + T
	\end{pmatrix}.
	\end{align*}
	It is not too hard to show why this is true:
	\begin{align*}
	[L+S]_{11}(w) = \E [L+S](w) = \E \circ L(w) + \E\circ S(w) = A(w) + P(w) = [A+P](w).
	\end{align*}
	
	
	
	\item \textbf{Scaling:}
	\begin{align*}
	\alpha \begin{pmatrix}
	A & \rvline & B\\
	\hline
	C & \rvline & D
	\end{pmatrix} = \begin{pmatrix}
	\alpha A & \rvline & \alpha B\\
	\hline
	\alpha C & \rvline & \alpha D
	\end{pmatrix}.
	\end{align*}
	
	
	\item \textbf{Composition} also works nicely:
	\begin{align}
	\begin{pmatrix}
	A & \rvline & B\\
	\hline
	C & \rvline & D
	\end{pmatrix} \circ \begin{pmatrix}
	P & \rvline & Q\\
	\hline
	R & \rvline & T
	\end{pmatrix} 
	=
	\begin{pmatrix}
	A\circ P + B \circ R & \rvline & A \circ Q + B \circ T \\
	\hline
	C \circ P + D \circ R & \rvline & C \circ Q + D \circ T
	\end{pmatrix} .
	\end{align}
	It is also quite straightforward to show why this is true. Consider $[L\circ S]_{11}$:
	\begin{align*}
	[L\circ S]_{11}(w) &= \E[L\circ S](w)\\
	&= \E \circ L[\E + (Id. - \E)]\circ S(w)\\
	&= \E \circ L \circ \E \circ S(w) + \E \circ L (Id. - \E) \circ S(w)\\
	&= \E \circ L \circ P(w) + \E \circ L \circ R(w)\\
	&= A\circ P(w)  +  B \circ R(w)\\
	&= [A\circ P + B \circ R](w).
	\end{align*}
	
	
	\item Suppose 
	\begin{align*}
	L = \begin{pmatrix}
	A & \rvline & B\\
	\hline
	C & \rvline & D
	\end{pmatrix}.
	\end{align*}
	If we pick a basis $\beta$ for $\W$ and $\gamma$ for $\Z$ then we can write the atrix
	\begin{align*}
	[L]_{\beta||\gamma \leftarrow \beta||\gamma} = [c_i \dots c_k].
	\end{align*}
	where $\beta||\gamma$ is a concatenation of the bases and the columns:
	\begin{align*}
	c_i &= [L(b_i)]_{\beta||\gamma}\\
	&= \begin{bmatrix}
	\begin{pmatrix}
	A & \rvline & B\\
	\hline
	C & \rvline & D
	\end{pmatrix} \begin{pmatrix}
	b_i \\ \mathbf{0}
	\end{pmatrix}_+
	\end{bmatrix}_{\beta||\gamma} \\
	&= \begin{bmatrix}
	\begin{pmatrix}
	A(b_i)\\
	C(b_i)
	\end{pmatrix}_+
	\end{bmatrix}_{\beta||\gamma}\\
	&= \begin{bmatrix}
	[A(b_i)]_\beta\\
	[C(b_i)]_\gamma
	\end{bmatrix}
	\end{align*}
	where $b_i$ is the $i^{th}$ element of the basis set $\beta$. If we repeat this procedure for all $b_i$'s and $g_i$'s, we will get:
	\begin{align*}
	[L]_{\beta||\gamma \leftarrow \beta||\gamma} = 
	\begin{pmatrix}
	[A]_{\beta\leftarrow \beta} & \rvline & [B]_{\gamma\leftarrow \beta}\\
	\hline
	[C]_{\beta\leftarrow \gamma} & \rvline & [D]_{\gamma\leftarrow \gamma}
	\end{pmatrix}
	\end{align*}
	where a block, say $[C]_{\beta\leftarrow\gamma}$ is generated by
	\begin{align*}
	[C]_{\gamma\leftarrow\beta} = \begin{bmatrix}
	[C(b_1)]_\gamma & [C(b_1)]_\gamma & \dots [C(b_n)]_\gamma
	\end{bmatrix}.
	\end{align*}
	
\end{enumerate}



































\subsection{Equality of rank and trace for idempotents; Resolution of Identity Revisited}



Consider an illuminating example with an idempotent matrix: $\E = \E^2 \in \mathbb{M}_{n\times n}$. We know that $\ima(\E) \oplus \ker(\E) = \mathbb{C}^n$. So, $\E : \ima(\E) \oplus \ker(\E) \lin \ima(\E) \oplus \ker(\E)$ can be represented as
\begin{align*}
\E =
\begin{pmatrix}
\E_{11} & \rvline & \E_{12}\\
\hline
\E_{21} & \rvline & \E_{22}
\end{pmatrix}
\end{align*} 
where 
\begin{align*}
&\E_{11} = \E \circ \E\bigg\vert_{\ima(\E)} = Id.\\
&\E_{12} = \E \circ \E\bigg\vert_{\ker(\E)} = \mathcal{O}\\
&\E_{21} = (Id. - \E) \circ \E \bigg\vert_{\ima(\E)} = \mathcal{O}\\
&\E_{22} = (Id. - \E) \circ \E \bigg\vert_{\ker(\E)} = \mathcal{O},
\end{align*}
i.e.,
\begin{align*}
\E \sim \begin{pmatrix}
Id.\bigg\vert_{\ima(\E)} & \rvline & \mathcal{O}\\
\hline
\mathcal{O} & \rvline & \mathcal{O}
\end{pmatrix}.
\end{align*}
The idea we have just explored in the beginning is that we can pick a basis for $\ima(\E)$ and $\ker(\E)$ and concatenate to get a basis for the whole space so that we can write
\begin{align*}
[\E]_{\gamma_1||\gamma_2 \leftarrow \gamma_1||\gamma_2} =
\begin{bmatrix}
[Id.]_{\gamma_1\leftarrow \gamma_1} & \rvline & [\mathcal{O}]_{\gamma_2 \leftarrow \gamma_1}\\
\hline
[\mathcal{O}]_{\gamma_1 \leftarrow \gamma_2} & \rvline & [\mathcal{O}]_{\gamma_2 \leftarrow \gamma_2}
\end{bmatrix}.
\end{align*}
Now since
\begin{align*}
&[Id.]_{\gamma_1\leftarrow \gamma_1} = I\\
&[\mathcal{O}]_{\gamma_i \leftarrow \gamma_j} = 0,
\end{align*}
we can write
\begin{align*}
\E = 
\begin{pmatrix}
I & \rvline & 0\\
\hline
0 & \rvline & 0
\end{pmatrix}.
\end{align*}

We have a theorem:

\begin{thm}
	Every idempotent $\E \in \mathbb{M}_n$ is similar to  matrix of the form
	\begin{align*}
	\begin{bmatrix}
	1 & & \dots & &0 \\
	& 1 & & &\\
	\vdots & & \ddots & &\vdots\\
	& & & 0 &\\
	0 & & & &0\\
	\end{bmatrix}
	\end{align*}
	where the $k$ 1's along the diagonal is $\rank(\E)$, which also happens to be the trace of $\E$, i.e.,
	\begin{align*}
	\dim(\ima(\E)) = \rank(\E) = \Tr(\E).
	\end{align*}
	\qed
\end{thm}

Consequently, we have:
\begin{prop}
	\begin{enumerate}
	\item Since $\rank(\E) = \Tr(\E)$, if $\Tr(M)$ is not an integer or negative, then $M$ is not an idempotent.
	
	\item Suppose we have a resolution of identity of $m$ idempotents: 
	\begin{align*}
	\E_1 + \E_2 + \dots + \E_m = I_{\mathbb{C}^n}
	\end{align*}
	then 
	\begin{align*}
	\Tr(\E_1 + \E_2 + \dots + \E_m) = \Tr(I_{\mathbb{C}^n}) = n,
	\end{align*}
	i.e.,
	\begin{align*}
	\sum_{i=1}^m \Tr(\E_i) = n,
	\end{align*}
	i.e.,
	\begin{align*}
	\sum_{i=1}^m \rank(\E_i) = n.
	\end{align*}
	
	
	
	\item Consider the same resolution of identity. Let us consider 
	\begin{align*}
	\ima(\E_1 + \E_2 + \dots + \E_m)
	\end{align*}
	and
	\begin{align*}
	\ima(\E_1) + \ima(\E_2) + \dots + \ima(\E_m).
	\end{align*}
	Next, consider
	\begin{align*}
	(\E_1 + \E_2 + \dots + \E_m)(x) = \E_1(x) + \E_2(x) + \dots + \E_m(x) \in \sum^m_{i=1}\ima(\E_i)
	\end{align*}
	Therefore,
	\begin{align*}
	\mathbb{C}^n = \ima(I_{\mathbb{C}^n}) = \ima\left( \sum^m_{i=1}\E_m \right) \subseteq \sum^m_{i=1}\ima(\E_i) \subseteq \mathbb{C}^n.
	\end{align*}
	So,
	\begin{align*}
	\sum^m_{i=1}\ima(\E_i) = \mathbb{C}^n.
	\end{align*}
	But we also know that
	\begin{align*}
	\sum^m_{i=1}\dim(\ima(\E_i)) = \sum^m_{i=1}\rank(\E_i) = n.
	\end{align*}
	So, we have a direct sum:
	\begin{align*}
	\boxed{\bigoplus^m_{i=1} \ima(\E_i) = \mathbb{C}^n}
	\end{align*}
	In particular,
	\begin{align*}
	\dim\left( \bigoplus^m_{i=2} \ima(\E_i)  \right) = n - \dim(\ima(\E_1)) = \dim(\ker(\E_1)).
	\end{align*}
	But we also know that 
	\begin{align*}
	\bigoplus^m_{i=2} \ima(\E_i) \supseteq \ima \left( \sum^m_{i=2}\E_i \right) = \ima(I - \E_1) = \ker(\E_1).
	\end{align*}
	But since
	\begin{align*}
	\dim(\ker(\E_1)) = \dim\left( \bigoplus^m_{i=2} \ima(\E_i)  \right), 
	\end{align*}
	it must be true that
	\begin{align*}
	\boxed{\ker(\E_1) = \bigoplus^m_{i=2} \ima(\E_i)}
	\end{align*}
	This implies
	\begin{align*}
	\ima(\E_2) \subseteq \ker(\E_1),
	\end{align*}
	which means
	\begin{align*}
	\E_1 \circ \E_2 = \mathcal{O}.
	\end{align*}
	In general, for $i \neq j$
	\begin{align*}
	\boxed{\E_i \circ \E_j = \mathcal{O}_{n\times n}}
	\end{align*}
	This leads us to the next item:
	\item If $\E_1, \E_2, \dots, \E_m$ are idempotents and $\sum^m_{i=1}\E_i = I$ then 
	\begin{align*}
	\begin{cases}
	&\E_i \circ \E_j = \mathcal{O}\\
	&\bigoplus^m_{i=1}\ima(\E_i) = \mathbb{C}^n.
	\end{cases}
	\end{align*}
	\end{enumerate}
\end{prop}


































\subsection{Direct sums of operators}


In the last subsection, we have worked with idempotents and resolution of identity. Now, suppose we have any linear function $\lag : \V \lin \V$, where $\V$ is finite dimensional. Suppose we have a non-trivial $\lag$-invariant proper subspace $\W$ of $\V$. Then we can write
\begin{align*}
\V = \W \oplus \square.
\end{align*}
Let $v_1,v_2,\dots,v_m \in \W$ a basis of $\W$, and $v_{m+1}, v_{m+2},\dots, v_{n}$ be a basis of $\square$. We immediately know that 
\begin{align*}
\xpan(v_1,v_2,\dots,v_m) \oplus \xpan(v_{m+1}, v_{m+2},\dots, v_{n}) = \V.
\end{align*}
Now, with respect to the decomposition $\V = \W \oplus \square$, we can represent $\lag$ as
\begin{align*}
\begin{tabular}{c|c|c|}
& $\W$ & $\square$\\
\hline
$\W$& $\lag_{11}$& $\lag_{12}$\\	
$\square$& $\lag_{21}$ & $\lag_{22}$
\end{tabular}
\end{align*}
where, like what we have done before
\begin{align*}
&\lag_{11} = \E \circ \lag\bigg\vert_{\W}\\
&\lag_{12} = \E \circ \lag\bigg\vert_{\square}\\
&\lag_{21} = (Id.-\E) \circ \lag\bigg\vert_{\W}\\
&\lag_{22} = (Id.-\E) \circ \lag\bigg\vert_{\square}.
\end{align*}
But notice that since $\W \in \mathfrak{Lat}(\lag)$
\begin{align*}
\lag(w) \in \W = \ima(\E) = \ker(Id.- \E),
\end{align*}
i.e.,
\begin{align*}
(Id. - \E)\circ \lag \bigg\vert_{\W} = \mathcal{O}.
\end{align*}
As a little aside, the converse is also true:
\begin{align*}
&(Id. -\E)\circ \lag\bigg\vert_{\W} = \mathcal{O} \\
\implies &\ima\left(\lag\bigg\vert_\W\right) \subseteq \ker(Id. - \E) = \ima(\E) = \W \\
\implies &\W \in \mathfrak{Lat}(\lag).
\end{align*}
So, we have established that 
\begin{align*}
&\lag_{12} = \mathcal{O} \iff \begin{tabular}{c|c|c|}
& $\W$ & $\square$\\
\hline
$\W$& & \\	
$\square$& $\mathcal{O}$ & 
\end{tabular} \iff \W \in \mathfrak{Lat}(\lag)\\
&\lag_{21} = \mathcal{O} \iff \begin{tabular}{c|c|c|}
& $\W$ & $\square$\\
\hline
$\W$&  & $\mathcal{O}$ \\	
$\square$&  & 
\end{tabular} \iff \square \in \mathfrak{Lat}(\lag).
\end{align*}
So, if $\W, \square \in \mathfrak{Lat}(\lag)$ and $\W \oplus \square = \V$ then $\lag$ can be represented by
\begin{align*}
\begin{tabular}{c|c|c|}
& $\W$ & $\square$\\
\hline
$\W$& $\mathcal{A}$ & $\mathcal{O}$ \\	
$\square$& $\mathcal{O}$  & $\mathcal{B}$ 
\end{tabular}.
\end{align*}
But of course the question is whether we know there is an invariant subspace of $\lag$ to start. While we don't really have a clue as to how to \textit{find} one for any linear function $\lag$, we can surely \textit{construct} one. For $v_0 \neq \mathbf{0} \in \V$. We want to construct $\W \in \mathfrak{Lat}(\lag)$ such with $v_0 \in \W$, i.e.,
\begin{align*}
v_0 \in \W, \lag(v_0) \in \W, \dots, \lag^n(v_0) \in \W.
\end{align*}
Say $\lag^k(v_0)$ is that first one that is a linear combination of previous ones, i.e.,
\begin{align*}
\lag^k(v_0) = \sum^{k-1}_{0} a_i \lag^i (v_0)
\end{align*}
then, say for $k = 3$, we have
\begin{align*}
\left(\lag^3 - a_0 \lag^0 - a_1 \lag^1 - a_2 \lag^2 \right)(v_0) = \mathbf{0}.
\end{align*}
By the fundamental theorem of algebra, we can factor out the ``polynomial'' above to get
\begin{align*}
(\lag - r_1 I)(\lag - r_2 I)(\lag - r_3 I)(v_0) = \mathbf{0}.
\end{align*}
But since $v_0 \neq \mathbf{0}$, at least one of $\lag - r_i I$ is NOT injective, i.e., for some $\hat{v} \neq \mathbf{0}$, 
\begin{align*}
\lag(\hat{v}) = r_i \hat{v}.
\end{align*} 
We have just shown that $r_i$ is an eigenvalue for $\lag$ and $\hat{v}$ is an eigenvector for $\lag$. 

\begin{thm}
	Every linear function on $\mathbb{C}$ on finite dimensional vector space has an eigenvector.\qed
\end{thm}

It follows immediately that
\begin{thm}
	Every linear function on $\mathbb{C}$ on finite dimensional vector space has a one-dimensional invariant subspace. \qed
\end{thm}








\newpage
\section{Invariant subspaces}

\subsection{Triangularization}

Consider $\V = \W_1 \oplus \Z$, with $\W \in \mathfrak{Lat}(\lag)$ and is one-dimensional. Then
\begin{align*}
\lag = \begin{tabular}{c|c|c|}
& $\W_1$ & $\Z$\\
\hline
$\W_1$& $\mathcal{A}$ & $\mathcal{B}$ \\	
$\Z$& $\mathcal{O}$  & $\mathcal{D}$ 
\end{tabular}=
\begin{pmatrix}
``\lambda_1'' & \rvline & \mathcal{B}\\
\hline
\mathcal{O} & \rvline & \mathcal{D}
\end{pmatrix}.
\end{align*}
Now, for $\mathcal{D} : \Z \lin \Z$ we can do the same thing we did for $\lag$ if $\Z$ is not one-dimensional. Let's say that $\Z = \W_2 \oplus \U$, with $\W_2 \in \mathfrak{Lat}(\lag)$, is one-dimensional, then
\begin{align*}
\mathcal{D} = \begin{pmatrix}
``\lambda_2'' & \rvline & \mathcal{B'}\\
\hline
\mathcal{O} & \rvline & \mathcal{D'}
\end{pmatrix}
\end{align*}
and so on. So, 
\begin{align*}
\lag = \begin{pmatrix}
"\lambda_2" & \rvline & \mathcal{B}_1 & \rvline & \mathcal{B}_2\\
\hline
\mathcal{O} & \rvline & "\lambda_2" & \rvline &\square\\
\hline
\mathcal{O} & \rvline & \mathcal{O}& \rvline &\hat{\mathcal{D}}\\
\end{pmatrix},
\end{align*}
and so on until we get an upper triangular block-matrix. We have a theorem:

\begin{thm}
	\textbf{Schur's Theorem:} Every $n\times n$ (complex-valued) matrix is similar to an upper triangular matrix.
	
	\begin{proof}
		We prove by induction. The base case of $n=1$ is trivial. Suppose that $n_0$ is the smallest size for which there is a counter example $\mathcal{A}$ ($n_0 \geq 2$). Now, $\mathcal{A}$ is a linear function, and hence has an eigenvector called $w_0$. We know that $\xpan(w_0) \in \mathfrak{Lat}(\mathcal{A})$. It follows that we can write
		\begin{align*}
		\mathbb{C}^n = \xpan(w_0) \oplus \Z.
		\end{align*} 
		With respect to this decomposition, $\mathcal{A}$ has the form
		\begin{align*}
		\mathcal{A} = \begin{pmatrix}
		\lambda & \rvline & \mathcal{B}\\
		\hline
		\mathcal{O} & \rvline & \mathcal{D}
		\end{pmatrix}.
		\end{align*}
		Now, pick a basis of $\Z$, called $\Gamma_2 : (w_{1},\dots,w_{n_0 - 1})$. Let the basis of $\xpan(w_0)$ be $\Gamma_1$. We have $\Gamma = \Gamma_1 || \Gamma_2$. We know that
		\begin{align*}
		[\mathcal{A}]_\Gamma = \begin{bmatrix}
		[\mathcal{A}(w_0)]_\Gamma & [\mathcal{A}(w_1)]_\Gamma & \dots & [\mathcal{A}(w_{n_0 - 1 })]_\Gamma
		\end{bmatrix}
		=
		\begin{pmatrix}
		\lambda & \rvline & \square\\
		\hline
		\mathcal{O} & \rvline & [T]_{\Gamma_2 \leftarrow \Gamma_2}
		\end{pmatrix}.
		\end{align*}
		We know that $\mathcal{A} \sim [\mathcal{A}]_\Gamma$. We also know (or assume) that $\mathcal{A}$ is the smallest counterexample. Then $\mathcal{D}$ cannot be a counter example since it is smaller than $\mathcal{A}$, i.e., $\mathcal{A} = S^{-1} \circ T \circ S$ for some upper triangular $T$. Hence
		\begin{align*}
		\mathcal{A} = \begin{pmatrix}
		\lambda & \rvline & \square\\
		\hline
		\mathcal{O} & \rvline & S^{-1}\circ T \circ S
		\end{pmatrix}
		=
		\begin{pmatrix}
		1 & \rvline & \mathcal{O}\\
		\hline
		\mathcal{O} & \rvline & S^{-1}
		\end{pmatrix}
		\begin{pmatrix}
		\lambda & \rvline & \triangle\\
		\hline
		\mathcal{O} & \rvline & T
		\end{pmatrix}
		\begin{pmatrix}
		1 & \rvline & \mathcal{O}\\
		\hline
		\mathcal{O} & \rvline & S
		\end{pmatrix}
		\end{align*}
		But notice that the left and right block-matrices are inverses of each other, so
		\begin{align*}
		\mathcal{A} \sim \begin{pmatrix}
		\lambda & \rvline & \triangle\\
		\hline
		\mathcal{O} & \rvline & T
		\end{pmatrix},
		\end{align*}
		which is an upper triangular matrix, since $T$ is upper triangular. Hence, $\mathcal{A}$ is similar to an upper triangular matrix. But this is a contradiction, i.e., there is no such $\mathcal{A}$. 
	\end{proof}
\end{thm}



\subsection{Reducing subspaces}







\newpage 









\section{Polynomials applied to operators}

In this section we will explore a class of functions $\rho_\lag : \mathbb{P} \lin \mathfrak{L}(\V)$.  We first know from the problems that $\rho_\lag$ is 

\begin{prop}
	$\,$
	\begin{enumerate}
		\item Linear
		\item Multiplicative.
	\end{enumerate}
\end{prop}


\subsection{Minimal polynomials at a vector}

We have shown that for each $\mathbf{0} \neq v_0 \in \V$ there is some $\mathbb{0} \neq P \in \mathbb{P}$ such that $(P(\lag))(v_0) = \mathbf{0}$. Now, let us consider
\begin{align*}
\left\{ 
P \in \mathbb{P} 
\bigg\vert
(P(\lag))(v_0) = \mathbf{0}  
\right\} 
\subseteq \mathbb{P}.
\end{align*}
It is easy to show that this is a subspace. But also notice that it also has an absorption property, i.e., if $(P(\lag))(v_0) = \mathbf{0} $ then $(q(\lag) \cdot P(\lag))(v_0) = (q\cdot P)(\lag)(v_0) = \mathbf{0}  $ for any $q\in \mathbb{P}$. We call subspaces like these (with an additional absorption property) an \textbf{ideal}. 

\begin{prop}
	$\,$
	\begin{enumerate}
	\item There is the smallest unique monic polynomial.
	\item There exists a unique and smallest $P(\lag)$ such that $P(\lag)(v_0) = \mathbf{0}$. 
	\item There exists a $P(\lag)$ such that $P(\lag)(v) = \mathbf{0}$ for all $v\in \V$.
	\item There exists a unique and smallest $P(\mathcal{A})$ such that $P(\mathcal{A}) = \mathcal{O}$ for any $\mathcal{A} \in \mathbb{M}_{n\times n}$ 
\end{enumerate}
\end{prop}
	
	








\subsection{Existence of eigenvalues}
\subsection{Global minimal polynomials}

	If we think about it a little, it is true that
\begin{align*}
[P(\lag)]_{\Gamma\leftarrow\Gamma} = P\left([\lag]_{\Gamma\leftarrow\Gamma}\right).
\end{align*}
Let us call 
\begin{align*}
\mathcal{A} = [\lag]_{\Gamma\leftarrow\Gamma}.
\end{align*}
It is true that $\mathcal{A}$ is nothing but an element in the space of matrices, and so for some $k$
\begin{align*}
P(\A) = \A^k - a_0 \A^{0} - \dots - a_{k-1}\A^{k-1} = \mathcal{O}.
\end{align*}
So, we look at the ideal:
\begin{align*}
\left\{ P \in \mathbb{P} \bigg\vert P(\A) = \mathcal{O} \right\}.
\end{align*}
There exists a singe minimal monic element that sends $\A$ to $\mathcal{O}$ (from the problems). But if $P(\A) = \mathcal{O}$, then $(P(\A))(v_0) = \mathbf{0}$ for any $v\in \V$. Therefore,
\begin{align*}
P(\A) \in 
\left\{ 
P \in \mathbb{P} 
\bigg\vert
(P(\lag))(v_0) = \mathbf{0}  
\right\} .
\end{align*}
In particular, all elements in this ideal are divisors of $P$ where $P(\A) = \mathcal{O}$. (We also showed why this is true in the homework - has to do with the \textbf{generator} of the ideal.) It turns out that $P(\A)$ is also the smallest degree monic polynomial to annihilates $\A$. We call $P(\A)$ the \textbf{global minimum polynomial of $\A$}, denoted $\mu_\A$.

What about \textbf{local minimum polynomials}? These are the minimal polynomials at the vectors $v\in \V$ constructed from looking at the first $\A^k(v_0)$ that can be expressed as a linear combination of the previous ones. We denote the local minimal polynomial of $\A$ at $v_0$ $\mu_{\A,v_0}$. As we have shown before,
\begin{align*}
\boxed{\mu_{\A,v_0} \text{ divides } \mu_\A}
\end{align*}  
since $\mu_\A$ is necessarily a multiple of $\mu_{\A,v_0}$, as shown in the problems. \\

Another point to notice is that a polynomial has at most finitely man monic divisors. So, suppose that
\begin{align*}
\mu_\A(z) = (z- r_1)^{\gamma_1}\dots (z - r_k)^{\gamma_k}.
\end{align*}
Consider a bunch of local minimal polynomials $\mu_{\A,v_1}(z), \mu_{\A,v_2}(z), \dots \mu_{\A,v_135}(z) $. Then consider a subspace
\begin{align*}
\left\{
v \bigg\vert (\mu_{\A,v_7})(v) = \mathbf{0}
\right\} = \ker\left( \mu_{\A,v_7} \right).
\end{align*}
The union of these 135 subspaces is the whole space, $\V$. And so, by problem set 2, one of these subspaces must equal to whole union, i.e., equals the whole space. So, $\mu_{\A,v_i} = \mathbf{0}$ for all $v\in \V$ for at least one $i$, i.e., $\mu_{\A, v_i}$ annihilates $\A$. But because $\mu_{\A,v_i}$ divides $\mu_{\A} = q\cdot \mu_{\A,v_i}$, we have
\begin{align*}
\mu_{\A,v_i} = \mu_{\A}
\end{align*} 
for at least one $i$. 





\subsection{Existence of eigenvalues}

In this section we will see why we are interested in these minimal polynomials. Suppose we have $\mu_\A(z) = (z- r_1)^{\gamma_1}\dots (z - r_k)^{\gamma_k}$. Then
\begin{align*}
\mu_\A(\A) = (\A- r_1 I)^{\gamma_1}\circ \dots \circ (\A - r_k I)^{\gamma_k} = \mathcal{O}.
\end{align*}
Now, we can show (by induction) that none of these factors can be invertible (multiplying an inverse of one factor to both sides and show that we get a smaller annihilating polynomial). Therefore, we are guaranteed that $(\A - r_i I)(v) = \mathbf{0}$ for some non-zero vector $v$ for any $r_i$ roots of $\mu_{\A}$. So, we have that \textbf{all roots of $\mu_\A$ are eigenvalues of $\A$}. \\

But are there other eigenvalues that are NOT roots of $\mu_\A$? The answer, fortunately, is no. Consider a vector $(z - \lambda)$, where $\lambda$ is an eigenvalue of $\A$ but is not a root of $\mu_\A$. It follows that $\mu_\A(z)$ and $(z- \lambda)$ are relatively prime (since they share no common roots). Hence, by the problems, there exist $q,p\in \mathbb{P}$ such that 
\begin{align*}
p(z)\mu_\A(z) + q(z)(z-\lambda) = \mathbb{1}, 
\end{align*}
i.e.,
\begin{align*}
p(\A)\mu_\A(\A) + q(\A)(\A-\lambda I) = I.
\end{align*}
But since $\mu_{\A}(\A) = \mathcal{O}$, we have
\begin{align*}
q(\A)(\A-\lambda I) = I.
\end{align*}
This implies $(\A-\lambda I)$ is invertible, hence $\ker(\A - \lambda I) = \{ \mathbf{0} \}$, thus $\lambda$ is not an eigenvalue of $\A$, a contradiction. So no such $\lambda$ exists. So, we have a theorem:

\begin{thm}
	All roots of $\mu_\A$ are \textbf{exactly} the eigenvalues of $\A$. \qed
\end{thm}

We notice that this is a much easier way to find eigenvalues than through using characteristic polynomials since the degrees of minimal polynomials are often much smaller. This leads us to an interesting point. Notice that $\mu_\A(z)$ has degree at most $n^2$ for an $n\times n$ matrix $\A$ (recall how we constructed $\mu_\A$ by going through $I,\A,\A^2,\dots$ and that $\dim(\mathbb{M})_{n\times n} = n^2$). However, the characteristic polynomial of $\A$ has degree at most $n$. By Cayley-Hamilton theorem, $\deg(\mu_\A) \leq \deg(\text{char} (\A))$, so

\begin{thm}
	For $\A \in \M_{n\times n}$, $\deg(\mu_\A) \leq n$. \qed 
\end{thm}


\begin{thm}
	Similar matrices have the same minimal polynomial.
	
	\begin{proof}
		Recall that 
		\begin{align*}
		[P(\lag)]_{\Gamma\leftarrow\Gamma} = P\left([\lag]_{\Gamma\leftarrow\Gamma}\right).
		\end{align*}
		This simple says if $P(z)$ happens to be annihilating and is smallest, then $P(\lag) = \mathcal{O}$. It is helpful to think of polynomials as belong to the linear function, not to matrices, as they are nothing but representations of the linear function, not that linear function itself. Now, of course the converse is not true (consider two identity matrices of different sizes).
	\end{proof}
\end{thm}

\begin{thm}
	\begin{align*}
	\boxed{\mu_{\A^\top} = \mu_\A}
	\end{align*}
	
	\begin{proof}
		If $\A\in \M_{n\times n}$ and $a_0 I + a_1 \A + \dots + a_k \A^k = \mathcal{O}$. Apply the transformation to this,
		\begin{align*}
		\sum^k_j a_j (\A^j)^\top = \sum^k_j a_j (\A^\top)^j = \mathcal{O}.
		\end{align*}
		So, if $P(\A) = \mathcal{O}$, then $P(\A^\top) = \mathcal{O}$. But if $P(\A^\top) = \mathcal{O}$, then $P(\A) = \mathcal{O}$. Therefore, $\mu_\A = \mu_{\A^\top}$.  
	\end{proof}
\end{thm}


\subsection{Minimal polynomials of block-diagonal operators}

We observe that if
\begin{align*}
T = \begin{pmatrix}
\A & \mathcal{O}\\
\mathcal{O} & \M
\end{pmatrix}
\end{align*}
then
\begin{align*}
T^0 = \begin{pmatrix}
\A^0 & \mathcal{O}\\
\mathcal{O} & \M^0
\end{pmatrix}, 
T^2 = \begin{pmatrix}
\A^2 & \mathcal{O}\\
\mathcal{O} & \M^2
\end{pmatrix}, 
\dots,
T^n = \begin{pmatrix}
\A^n & \mathcal{O}\\
\mathcal{O} & \M^n
\end{pmatrix}.
\end{align*}
Thus we can convince ourselves that
\begin{align*}
P(T) = \begin{pmatrix}
P(\A) & \mathbb{0}\\
\mathbb{0} & P(\M)
\end{pmatrix}.
\end{align*}
We can in fact think about of $\mu_T$ relates to $\mu_\A$ and $\mu_\M$. A polynomial, say $P$, annihilates $T \iff$ $P$ annihilates both $\A$ and $\M \iff$ $\mu_\A$ and $\mu_\M$ both divide $P \iff$ $P$ is a common multiple of $\mu_\A, \mu_\M$. Now, since $\mu_T$ is the minimal polynomial, whose multiples are such $P$'s, we get
\begin{align*}
\boxed{\mu_T = \text{LCM}(\mu_\A, \mu_\M)}
\end{align*}  
In particular, for a diagonal matrix, say
\begin{align*}
T = \begin{pmatrix}
\alpha_1 & & \\
& \ddots & \\
& & \alpha_k
\end{pmatrix},
\end{align*}
the minimal polynomial 
\begin{align*}
\mu_T = \text{LCM}(z-\alpha_1, z-\alpha_2, \dots, z-\alpha_k) = (z- \alpha_1)\dots(z-\alpha_j),
\end{align*}
for distinct $\alpha_i$, $i\neq j \leq k$.

\begin{exmp}
	Let 
	\begin{align*}
	T = \begin{pmatrix}
	3 &&&\\
	&4&&\\
	&&1&\\
	&&&3
	\end{pmatrix}.
	\end{align*}
	Then it is clear that 
	\begin{align*}
	\mu_T(z) = (z-3)(z-4)(z-1).
	\end{align*}
	And thus the ``spectrum'' of $T$ is 
	\begin{align*}
	\sigma_{\mathbb{C}}(T) = \{1,3,4\}.
	\end{align*}
	\qed
\end{exmp}








\subsection{(Minimal) polynomials of block-$\Delta^r$ operators}


Consider 
\begin{align*}
T = \begin{pmatrix}
\A & \K \\
\mathcal{O} & \M
\end{pmatrix}.
\end{align*}
Then, like before
\begin{align*}
T^0 = \begin{pmatrix}
I & \mathcal{O}\\
\mathcal{O} & I
\end{pmatrix}, 
T^2 = \begin{pmatrix}
\A^2 & \triangle\\
\mathcal{O} & \M^2
\end{pmatrix}, 
\dots,
T^n = \begin{pmatrix}
\A^n & \triangle'\\
\mathcal{O} & \M^n
\end{pmatrix}.
\end{align*}
So, 
\begin{align*}
P(T) = \begin{pmatrix}
P(\A) & \square\\
\mathcal{O} & P(\M)
\end{pmatrix}.
\end{align*}
If $P(T) = \mathcal{O}$, then $P(\A) = \mathcal{O}$ and $P(\M) = \mathcal{O}$. Thus, $P$ is a common multiple (not necessarily least) of $\mu_\A$ and $\mu_\M$. (Notice that we no longer have $\iff$ like with block-diagonal matrices.) 

Thus we have 
\begin{align*}
\text{LCM}(\mu_\A,\mu_\M) \big\vert P(T).
\end{align*}
But we also know that 
\begin{align*}
(\mu_\A \cdot \mu_\M)(T) &= \mu_\A (T) \cdot \mu_\M(T)\\
&= \begin{pmatrix}
\mathcal{O} & \mathcal{O}\\
\mathcal{O} & \mu_\A(\M)
\end{pmatrix}
\begin{pmatrix}
\mu_\M(\A) & \mathcal{O}\\
\mathcal{O} & \mathcal{O}
\end{pmatrix}\\
&=
\begin{pmatrix}
\mathcal{O} & \mathcal{O}\\
\mathcal{O} & \mathcal{O}
\end{pmatrix}\\
&= [0].
\end{align*}
Therefore,
\begin{align*}
\boxed{\text{LCM}(\mu_\A, \mu_\M) \big\vert \mu_T \big\vert \mu_\A \mu_\M}
\end{align*}
and hence
\begin{align*}
\boxed{\sigma_{\mathbb{C}}(T) =  \sigma_{\mathbb{C}}(\A) \cup \sigma_{\mathbb{C}}(\M)  }
\end{align*}
i.e., linear factors of $T$ are exactly those that appear in $\mu_\A$ or $\mu_\M$ (or both). \\

Of course ``equality'' occurs when $\mu_\A$ and $\mu_\M$ are relatively prime. 

\begin{exmp}
	Consider 
	\begin{align*}
	T = 
	\begin{pmatrix}
	\begin{matrix}
	3 & \rvline & 1\\
	\hline
	& \rvline & 4
	\end{matrix} 
	& \rvline &
	\begin{matrix}
	2 & 8 \\
	5 & 1
	\end{matrix}\\
	\hline
	&\rvline & 
	\begin{matrix}
	2 & \rvline & 7\\
	\hline
	& \rvline & 3
	\end{matrix}
	\end{pmatrix}.
	\end{align*}
	Then
	\begin{align*}
	\sigma_{\mathbb{C}}(T) 
	&= \sigma_{\mathbb{C}}
	\begin{pmatrix}
	3 & \rvline & 1\\
	\hline
	& \rvline & 4
	\end{pmatrix}  
	\cup
	\sigma_{\mathbb{C}}  
	\begin{pmatrix}
	2 & \rvline & 7\\
	\hline
	& \rvline & 3
	\end{pmatrix} \\
	&= \sigma_{\mathbb{C}}[3] \cup \sigma_{\mathbb{C}}[4] \cup \sigma_{\mathbb{C}}[2] \cup \sigma_{\mathbb{C}}[3]\\
	&= \{ 2,3,4 \}.    
	\end{align*}
	We could have guessed this result - these are just the values on the diagonal of $T$. What, then, is $\mu_T(z)$? Unfortunately, we don't know:
	\begin{align*}
	\mu_T(z) = (z-2)(z-4)(z-3)^{1\text{ or }2}.
	\end{align*}\qed
\end{exmp}

\begin{exmp}
	If $T = \begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}$ then $p(z) = (z-a)(z-d) - bc$ annihilates $T$. 
	
	\begin{proof}
		\begin{align*}
		p(T) &= (T-aI)(T_dI) - bcI\\
		&= \begin{pmatrix}
		0 & b\\c & d-a
		\end{pmatrix}
		\begin{pmatrix}
		a-d&b\\c& 0
		\end{pmatrix}
		-
		\begin{pmatrix}
		bc & 0\\0 & bc
		\end{pmatrix}\\
		&=
		\begin{pmatrix}
		bc & 0\\0 & bc
		\end{pmatrix}
		-
		\begin{pmatrix}
		bc & 0\\0 & bc
		\end{pmatrix}\\
		&= [0].
		\end{align*}
	\end{proof}
\end{exmp}

We have a theorem:

\begin{thm}
	If $T = \begin{pmatrix}
	a&b\\c&d
	\end{pmatrix}$ is NOT a multiple of identity, then $\mu_T(z) = (z-a)(z-d) - bc$.\qed
\end{thm}


\newpage 
\section{Eigentheory}
\subsection{Minimal polynomials and the spectrum}
\subsection{Spectral Mapping Theorem}
\newpage 

\section{Diagonalization}
\subsection{Spectral resolutions}
\subsection{Compressions to invariant subspaces}
\newpage
\section{Simultaneous triangularization and  diagonalization for commuting families}
\newpage
\section{Primary decomposition over $\mathbb{C}$ and generalized eigenspaces}

Before getting into primary decomposition, we first establish a few stepping stones to get to and understand the theorems to come. Suppose we have relatively prime polynomials $p_1$ and $p_2$. Then
\begin{align*}
\ker(p_1(\lag)) \cap \ker(p_2(\lag)) = \{ \mathcal{O} \}.
\end{align*}

\begin{proof}[Sketch]
	We know (from the problem sets) that given relatively prime $p_1$ and $p_2$, there exist $q_1, q_2$ such that 
	\begin{align*}
	q_1(z)p_1(z) + q_2(z)p_2(z) = \mathbb{1}.
	\end{align*}
	Thus, for $\lag$ as argument,
	\begin{align*}
	q_1(\lag)p_1(\lag) + q_2(\lag)p_2(\lag) = I,
	\end{align*}
	which we can write as
	\begin{align*}
	\left(q_1(\lag)p_1(\lag) + q_2(\lag)p_2(\lag)\right)(v) = v,
	\end{align*}
	i.e.,
	\begin{align*}
	q_1(\lag)[(p_1(\lag)(v))] + q_2(\lag)[(p_2(\lag)(v))] = v.
	\end{align*}
	Now, if $v\in \ker(p_1(\lag)) \cap \ker(p_2(\lag))$, then
	\begin{align*}
	q_1(\lag)(\mathbf{0}) + q_2(\lag)(\mathbf{0}) = v \implies v = \mathbf{0}.
	\end{align*} 
	This shows that
	\begin{align*}
	\ker(p_1(\lag)) \cap \ker(p_2(\lag)) = \{ \mathbf{0} \}.
	\end{align*}
\end{proof}

Now, let's say we're given 
\begin{align*}
\mu_\A(z) = (z-\lambda_1)^{k_1} \dots (z - \lambda_n)^{k_n}.
\end{align*}
Let us call
\begin{align*}
&p_1 = (z-\lambda_1)^{k_1} \dots (z - \lambda_{n-1})^{k_{n-1}}\\
&p_2 = (z - \lambda_n)^{k_n}.
\end{align*}
It is clear that $p_1$, $p_2$ are relatively prime. So, $\ker(p_1(\lag)) \cap \ker(p_2(\lag)) = \{ \mathbf{0} \}$. Also,
\begin{align*}
p_1(\A)p_2(\A) = \mu_\A(\A) = \mathcal{O} = p_2(\A)p_1(\A),
\end{align*}
so,
\begin{align*}\label{im}
\begin{cases}
\ima(p_1(\A)) \subseteq \ker(p_2(\A))\\
\ima(p_2(\A)) \subseteq \ker(p_1(\A))
\end{cases}.
\end{align*}
Yet also, for some $q_1, q_2$
\begin{align*}
q_1(z)p_1(z) + q_2(z)p_2(z) = 1.
\end{align*}
Thus,
\begin{align*}
p_1(\A)q_1(\A) + p_2(\A)q_2(\A) = I.
\end{align*}
Hence, for all $y\in \V$
\begin{align*}
p_1(\A)[q_1(\A)(y)] + p_2[q_2(\A)(y)] = y.
\end{align*}
But from the subset conditions, it is necessary that
\begin{align*}
\ker(p_2(\A)) + \ker(p_1(\A)) = \V.
\end{align*}
But since these kernels intersect trivially, 
\begin{align*}
\boxed{\V = \ker(p_1(\A)) \oplus \ker(p_2(\A))}
\end{align*}
Now, we observe that $p(\A)$ and $\A$ commute, so from the problems, we get two invariant subspaces
\begin{align*}
\begin{cases}
\ker(p_1(\A)) \in \mathfrak{Lat}(\A)\\
\ker(p_2(\A)) \in \mathfrak{Lat}(\A)
\end{cases}.
\end{align*}
With respect to this decomposition, $\A$ can be expressed as
\begin{align*}
\A = \begin{tabular}{c|c|c|}
& $\ker(p_1(\A))$ & $\ker(p_2(\A))$\\
\hline
$\ker(p_1(\A))$& $\mathcal{C}$ & $\mathcal{O}$ \\	
$\ker(p_2(\A))$& $\mathcal{O}$ & $\mathcal{D}$ 
\end{tabular}.
\end{align*}
What are $\mu_\mathcal{C}$ and $\mu_\mathcal{D}$? It is clear that $\mu_\mathcal{C} = p_1$ and $\mathcal{D} = p_2$. Consider
\begin{align*}
q_1(\A)\begin{pmatrix}
x\\y
\end{pmatrix}_+ = q_1(\mathcal{A})(x) + q_1(\mathcal{A})(y) = q_1(\mathcal{C})(x) + q_1(\mathcal{D})(y).
\end{align*}
We know that $x\in \ker(p_1(\A))$, so this reduces to
\begin{align*}
q_1(\mathcal{A})(y) = q_1(\mathcal{C})(x) + q_1(\mathcal{D})(y),
\end{align*}
which holds for all $x$. So, for $y = \mathbf{0}$, we must have $q_1(\mathcal{C}) = \mathcal{O}$, i.e, $p_1$ annihilates $\mathcal{C}$. So we can keep going and break the blocks $\mathcal{C}$ and $\mathcal{D}$ down into smaller block-diagonal pieces. \\

So, just a recap, if $q_1, q_2$ are relatively prime and $q_1\cdot q_2$ annihilates $\A : \V \lin \V$, then $\V = \ker(p_1(\A)) \oplus \ker(p_2(\A))$ and with respect to this decomposition $\A$ has the form
\begin{align*}
A = \begin{pmatrix}
\mathcal{C} & \rvline & \mathcal{O}\\
\hline
\mathcal{O} & \rvline & \mathcal{D}
\end{pmatrix}
\end{align*}
where $q_1$ annihilates $\mathcal{C}$ and $q_2$ annihilates $\mathcal{D}$. So, we have a little theorem:

\begin{thm}
	Under the hypotheses of the summary, if $q_1, q_2$ are monic and $\mu_\A= q_1 \cdot q_2$, then 
	\begin{align*}
	\begin{cases}
	q_1 = \mu_\mathcal{C}\\
	q_2 = \mu_\mathcal{D}
	\end{cases}.
	\end{align*}
	
	\begin{proof}[Sketch]
		We know that \begin{align*}
		\mu_\A = \mu_{\begin{pmatrix}
			\mathcal{C} & \mathcal{O}\\
			\mathcal{O} & \mathcal{D}
			\end{pmatrix}} = \text{LCM}(\mu_\mathcal{C} , \mu_\mathcal{D}).
		\end{align*}
		Now, $\mu_\mathcal{C}$ divides $q_1$ because $q_1$ annihilates $\mathcal{C}$. Similarly, $\mu_\mathcal{D}$ divides $q_2$ because $q_2$ annihilates $\mathcal{D}$. But since $q_1, q_2$ are relatively prime, $\mu_\mathcal{C}, \mu_\mathcal{D}$ are also relatively prime. Hence,
		\begin{align*}
		\text{LCM}(\mu_\mathcal{C}, \mu_\mathcal{D}) = \mu_\mathcal{C} \cdot \mu_\mathcal{D}.
		\end{align*}
		Therefore,
		\begin{align*}
		\mu_\A = \mu_\mathcal{C} \cdot \mu_\mathcal{D}.
		\end{align*}
		But we also know that $\mu_\A = q_1 \cdot q_2 = \mu_\mathcal{C} \cdot \mu_\mathcal{D}$ and $\mu_\mathcal{C}\big\vert p_1$ and $\mu_\mathcal{D}\big\vert p_2$, we must have
		\begin{align*}
		\begin{cases}
		q_1 = \mu_\mathcal{C}\\
		q_2 = \mu_\mathcal{D}
		\end{cases}.
		\end{align*}
	\end{proof}
\end{thm}


\begin{thm}
	\textbf{Primary Decomposition Theorem:}\\ Suppose $\A : \V \lin \V$ and $\mu_\A(z) = (z-\lambda_1)^{p_1}\dots (z-\lambda_k)^{p_k}$, then 
	\begin{align*}
	\V = \ker(\A - \lambda_1 I )^{p_1} \oplus \dots \oplus \ker(\A - \lambda_k I)^{p_k}
	\end{align*}
	and with respect to this decomposition, $\A$ has the form
	\begin{align*}
	\A = \begin{pmatrix}
	\A_1 & & \\
	& \ddots &\\
	& & \A_k 
	\end{pmatrix}
	\end{align*}
	with $\mu_{\A_i}(z) = (z - \lambda_i)^{p_i}$.
	
	
	\begin{proof}
		We induct on $k$. The base case where $k=1$ is trivially true. Consider the inductive hypothesis. Suppose that $k=1,2,\dots,m$ hold. We want to show that the case where $k = m+1$ also holds, i.e., 
		\begin{align*}
		\mu_\A = (z- \lambda_1)^{q_1}\dots (z - \lambda_m)^{q_m}(z - \lambda_{m+1})^{q_{m+1}}.
		\end{align*}
		Let $p_1 = (z- \lambda_1)^{q_1}\dots (z - \lambda_m)^{q_m}$ and $p_2 = (z - \lambda_{m+1})^{q_{m+1}}$, i.e., by the lemma we just proved, $\A = \begin{pmatrix}
		\mathcal{B} & \mathcal{O}\\
		\mathcal{O} & \mathcal{C}
		\end{pmatrix}$ with respect to the decomposition $\V = \ker(p_1(\A)) \oplus \ker(p_2(\A))$ and $\mu_\mathcal{B} = p_1$ and $\mathcal{C} = p_2$. \\
		
		By the inductive hypothesis, $\ker(p_1(\A)) = \ker(\mathcal{B} - \lambda_1 I){q_1} \oplus \dots \oplus \ker(\mathcal{B} - \lambda_m)^{q_m}$, and with respect to this decomposition, $\mathcal{B} = \begin{pmatrix}
		\mathcal{B}_1 & & \\
		&\ddots&\\
		&&\mathcal{B_m}
		\end{pmatrix}$ and $\mu_{\mathcal{B}_i}(z) = (z - \lambda_i)^{q_i}$. Hence, by the problem sets, $\A$ can be represented as
		\begin{align*}
		\A = 
		\begin{pmatrix}
		\begin{matrix}
		\mathcal{B}_1 & &\\
		& \ddots &\\
		& & \mathcal{B}_m
		\end{matrix}& \rvline & \\
		\hline
		& \rvline & \mathcal{C}
		\end{pmatrix}
		\end{align*}
		with respect to the decomposition $\V = \ker(\mathcal{B} - \lambda_1 I){q_1} \oplus \dots \oplus \ker(\mathcal{B} - \lambda_m)^{q_m} \oplus \ker(\A - \lambda_{m+1} I )^{q_{m+1}}$.
		
		So, the next step is to show $\ker(\mathcal{B} - \lambda_i I)^{q_i} = \ker(\A - \lambda_i I)^{q_i}$ for all $i$. First, we know that
		\begin{align*}
		\ker(\A - \lambda_i I)^{q_i} \subseteq \ker\left( (\A - \lambda_1)^{q_1}\dots (\A - \lambda_m I)^{q_m} \right)
		\end{align*},
		since the factors can commute among themselves. Hence, we just rewrite this as
		\begin{align*}
		\ker(\A - \lambda_i I)^{q_i} \subseteq \ker(q_1(\A)).
		\end{align*}
		Similarly, we can show 
		\begin{align*}
		\ker(\mathcal{B} - \lambda_i I)^{q_i} \subseteq \ker(q_1(\A)).
		\end{align*}
		Now, recall that 
		\begin{align*}
		\A = \begin{pmatrix}
		\mathcal{B} & \mathcal{O}\\
		\mathcal{O} & \mathcal{C}
		\end{pmatrix},
		\end{align*}
		thus,
		\begin{align*}
		(\A - \lambda_i I)^{q_i} = \begin{pmatrix}
		(\mathcal{B} - \lambda_i)^{q_i} & \mathcal{O}\\
		\mathcal{O} & \square
		\end{pmatrix}.
		\end{align*}
		Consider $x \in \ker\left((\A - \lambda_i I)^{q_i}\right)$, i.e., 
		\begin{align*}
		&(\A - \lambda_i I)^{q_i}(x) = \mathbf{0}\\
		\iff &\begin{pmatrix}
		\mathcal{B} & \mathcal{O}\\
		\mathcal{O} & \mathcal{C}
		\end{pmatrix}\begin{pmatrix}
		x\\\mathbf{0}
		\end{pmatrix}_+\\
		\iff &(\mathcal{B} - \lambda_i I)^{q_i}(x) = \mathbf{0}\\
		\iff &x \in \ker\left( (\mathcal{B} - \lambda_i I)^{q_i} \right).
		\end{align*}
		So we have just shown that 
		\begin{align*}
		\ker\left( (\A - \lambda_i I)^{q_i} \right) = \ker\left( (\mathcal{B} - \lambda_i I)^{q_i}\right)
		\end{align*}
		for all $i$. This completes our proof. 
	\end{proof}
\end{thm}





\newpage 
\section{Cyclic decomposition and Jordan form}

We can immediately follow the primary decomposition theorem with another theorem:

\begin{thm}
	Ever $\lag : \V \lin \V$, where $\V$ is finite dimensional, can be expressed as a direct sum of operators:
	\begin{align*}
	\lag = \lag_1 \oplus \lag_2 \oplus \dots \oplus \lag_m,
	\end{align*}
	where 
	\begin{align*}
	\lag_i = \lambda_i I + \mathcal{N}_i,
	\end{align*}
	where $\mathcal{N}_i$ is nilpotent. 
\end{thm}

It is worthwhile to study some properties of nilpotents.

\subsection{Nilpotents}

First, we want to look at the structure of nilpotents. Given a nilpotent $\mathcal{N} : \V \lin \V$, such that, say $\mathcal{N}^8 = \mathcal{O}$ (and hence $\mu_\mathcal{N}(z) = z^8$) and $v_0 \in \V$, we write
\begin{align*}
\langle v_0 \rangle = \xpan \left\{ v_0 ,\mathcal{N}(v_0),\dots, \mathcal{N}^7(v_0) \right\} = \{ P(\mathcal{N})(v_0) \big\vert \deg(P) \leq 7 \} \in \mathfrak{Lat}(\mathcal{N}),
\end{align*}
a cyclic invariant subspace. 

\begin{thm}
	For any nilpotent $\mathcal{N}$ as above, there exist some $v_1, v_2,\dots v_k \in \V$ such that 
	\begin{align*}
	\V = \langle v_0 \rangle \oplus \langle v_1 \rangle \oplus \dots \oplus \langle v_k \rangle
	\end{align*} \qed
\end{thm}

We will look at the proof later, but with respect to this decomposition, $\mathcal{N}$ is block-diagonal:
\begin{align*}
\mathcal{N}= \begin{pmatrix}
\mathcal{N}_1 & \mathcal{O} & \mathcal{O}\\
\mathcal{O}& \ddots & \mathcal{O}\\
\mathcal{O} & \mathcal{O} & \mathcal{N}_k
\end{pmatrix}.
\end{align*}
Let us suppose that the $v_0$ up to $\mathcal{N}^5(v_0)$ are linear independent, then 
\begin{align*}
\mathcal{N}^6(v_0) = \sum^5_{i=0}a_i \mathcal{N}^i (v_0),
\end{align*}
i.e.,
\begin{align*}
(\mathcal{N}^6 - a_5\mathcal{N}^5 - \dots a_0 I)(v_0) =  q(\mathcal{N})(v_0)  = \mathbf{0}.
\end{align*}
But recall that the global minimal polynomial for $\mathcal{N}$ is $\mu_\mathcal{N}(z) = z^8$, and that the local minimal polynomial divides th global. This means that $q(\mathcal{N}) = \mathcal{N}^k$ for some $k \leq 8$. So, the local minimal polynomial has to have degree 6. This implies $\mathcal{N}^6(v_0) = \mathbf{0}$. Therefore, $v_0 ,\mathcal{N}(v_0),\dots , \mathcal{N}^5(v_0)$ is a basis of $\langle v_0 \rangle$. So, the first block along the diagonal of $\mathcal{N}$ can be written as 
\begin{align*}
\mathcal{N}_1 &= 
\begin{tabular}{c|c|c|c|c|}
& $\xpan(v_0)$ & $\xpan(\mathcal{N}(v_0))$ & \dots & $\xpan(\mathcal{N}^5(v_0))$\\
\hline
$\xpan(v_0)$& $0$ & $0$  & \dots & 0\\	
$\xpan(\mathcal{N}(v_0))$& $1$ & $0$ &\dots& $0$\\
$\vdots$ &\vdots&1&&\vdots \\
$\vdots$ &\vdots&\vdots&& \vdots\\
$\xpan(\mathcal{N}^5(v_0))$ &0 &0 & \dots& 0
\end{tabular}\\
&= \begin{pmatrix}
0 & 0 &  && 0\\
1 & 0 &  && \\
 & 1 & \ddots && \\
 &  & \ddots &\ddots&\\
0 & 0 &  &1& 0
\end{pmatrix}
\end{align*}

Of course, conventionally, we would prefer upper triangular matrices, so we simply reverse the order of the basis elements to get 
\begin{align*}
\mathcal{N}_1 = \begin{pmatrix}
0 & 1 &  && 0\\
  & 0 & 1 && \\
 & & \ddots& \ddots & \\
&  & &\ddots&1\\
0 &  &  && 0
\end{pmatrix}
\end{align*}

\begin{thm} \textbf{Cyclic Decomposition for Nilpotents:} \\
	If $\mathcal{N} : \V \lin \V$, $\V$ is finite dimensional, is nilpotent then $\V = \bigoplus^k_{1} \langle v_i \rangle_\mathcal{N}$ for some non-zero $v_i$.\\
	
	In fact, we can choose any $v_1$ to start with. And with respect to this decomposition, $\mathcal{N}$ has the form $\mathcal{N}_1 \oplus \dots \oplus \mathcal{N}_k$ \qed
\end{thm}


\begin{thm}
	If $\mathcal{N}: \V \lin \V$ nilpotent, $\V$ is finite dimensional, then there is a basis $\psi$ (which can be constructed by concatenation) for $\V$ such that 
	\begin{align*}
	[\mathcal{N}]_{\psi \leftarrow \psi} = \begin{pmatrix}
	[\mathcal{N}_1] & & \\
	& \ddots &\\
	& & [\mathcal{N}_k]
	\end{pmatrix} ,
	\end{align*}
	where each block $[\mathcal{N}_i]$ has the form
	\begin{align*}
	[\mathcal{N}_i] = \begin{pmatrix}
	0 & 1 &  && 0\\
	& 0 & 1 && \\
	& & \ddots& \ddots & \\
	&  & &\ddots&1\\
	0 &  &  && 0
	\end{pmatrix}
	\end{align*}\qed
\end{thm}


\subsection{Jordan canonical form theorem}

\begin{thm}
	\begin{align*}
	\boxed{\A, \mathcal{B} \text{ similar } \iff \A, \mathcal{B} \text{ have the same JCF}}
	\end{align*}\qed
\end{thm}

We can ``improve'' the previous idea by considering $\A - \lambda I = \mathcal{N}$, i.e., $\A = \lambda I + \mathcal{N}$, and so
\begin{align*}
[\lambda I + \mathcal{N}]_{\psi \leftarrow \psi} = [\lambda I]_{\psi \leftarrow \psi} + [\mathcal{N}]_{\psi\leftarrow\psi}  = \begin{pmatrix}
\A_1 & & \\
& \ddots & \\
& & \A_k
\end{pmatrix}
\end{align*}
where each $\A_i$ is called a \textbf{Jordan block}, denoted $J_{\lambda, size}$. For example,
\begin{align*}
J_{\lambda, 2} = \begin{pmatrix}
\lambda & 1\\0& \lambda 
\end{pmatrix}.
\end{align*}

\begin{thm}\textbf{Jordan canonical form theorem:}\\
	Every matrix is similar to a \textit{unique (up to changing order of appearance)} direct sum of Jordan blocks. \qed
	
	\begin{proof}
		There are two parts to this proof. First, we want to show that this holds for nilpotents $\mathcal{N}$. Then, we extend it and show it also holds for $\A = \lambda I + \mathcal{N}$.
	\end{proof}
\end{thm}

\textbf{Step 1:} Let us revisit the idea of Weyr characteristic. Consider 
\begin{align*}
\mathcal{N} = \begin{pmatrix}
J_{0,9} & & \\
& \ddots & \\
& & J_{0,1}
\end{pmatrix}.
\end{align*}
Let's say that from$\mathcal{N}$ we find that
\begin{align*}
\{ \mathbf{0} \} \stackrel{+8}{\subset} \ker(\mathcal{N}) \stackrel{+7}{\subset} \ker(\mathcal{N}^2) \stackrel{+7}{\subset} \ker(\mathcal{N}^3) \subset \dots \subset \ker(\mathcal{N}^9) = \V
\end{align*}
where the numbers indicate the increase in the dimension. Then, if we were given only the Weyr characteristics, i.e., the inclusion chain above, is it possible for us to generate $\mathcal{N}$? It turns out that the answer is YES. Basically, each number, going from left-to-right, gives us some extra information about $\mathcal{N}$. For instance, the first dimension increase gives us the number of Jordan blocks in $\mathcal{N}$. The difference between the second and the first number gives us the number of $J_{0,1}$ blocks. The different between the third and the second number gives us the number of $J_{0,2}$ blocks, and so on. (The most straightforward way to see how this works is to generate an example and go through the algorithm.)\\

So, we can see that no matter how the decomposition might differ, this Jordan construction is determined by just $\mathcal{N}$ (since we can reconstruct the Jordan form of $\mathcal{N}$ by just looking at the Weyr characteristics). So, if we have two similar idempotents, then they have the same Weyr characteristics, i.e., they must also have the same Jordan canonical form. But since we also know that if two nilpotents have the same Jordan-block decomposition then they are similar. Therefore, the \textbf{Weyr characteristics of $\mathcal{N}$ completely determines the Jordan block sizes and their multiplicities, i.e., completely determines the Jordan form of $\mathcal{N}$ (of course, up to order of appearance)}. \\

Similar matrices have the same Weyr characteristics, similar nilpotents have the same Jordan form. Conversely, if two nilpotents have the same Jordan form, hen they are similar to it, and so to each other. Therefore, we say that \textbf{Jordan form is a ``complete invariant'' for similarity of nilpotents.}\\


\textbf{Step 2:} But what about $\A + \lambda I + \mathcal{N}$? We observe that if $A = \lambda I + \mathcal{N}$ then the Weyr characteristics of $\A - \lambda I$ determines the Jordan form of $\A$. But what if we don't know $\lambda$? What if all we know is that $\A = \square \cdot I + \mathcal{N}$? It turns out that we can simply use $\Tr(\A)$ to figure out what $\lambda$ is because $\A$ in matrix form has only $\lambda$'s along the diagonal:
\begin{align*}
\lambda = \frac{\Tr(\A)}{\text{Size of }\A}. 
\end{align*}
(recall that $\Tr(\A)$ does not change in different representations, i.e., it is ``similarity-invariant'').\\

Now, similar matrices have the same Jordan forms. 

  

\subsection{Square roots of operators}
\subsection{Similarity of a matrix and its transpose}
\subsection{Similarity of a matrix and its conjugate}
\subsection{Jordan forms of $\mathcal{AB}$ and $\mathcal{BA}$}
\subsection{Power-convergent operators}
\subsection{Power-bounded operators}
\subsection{Row-stochastic matrices}
\newpage 
\section{Determinant \& Trace}
\subsection{Classical adjoints}
\subsection{Cayley-Hamilton theorem}
\subsection{General Laplace expansions}
\subsection{Cauchy-Binet formula}

\newpage

\section{Inner products and norms} 
\newpage 
\subsection{Riesz representation theorem}
\subsection{Adjoints}
\subsection{Grammians}
\subsection{Orthogonal complements and orthogonal decompositions}
\subsection{Ortho-projections}
\subsection{Closest point solutions}
\subsection{Gram-Schmidt and orthonormal bases}

\newpage

\section{Isometries and unitary operators}
\newpage 

\section{Ortho-triangularization}
\newpage 

\section{Spectral resolutions}
\newpage 

\section{Ortho-diagonalization; self-adjoint and normal operators; spectral theorems}
\newpage 

\section{Positive (semi-)definite operators}
\subsection{Classification of inner products}
\subsection{Positive square roots}
\newpage 

\section{Polar decomposition}
\newpage 

\section{Single value decomposition}
\subsection{Spectral/operator norm}
\subsection{Singular values and approximation}
\subsection{Singular values and eigenvalues}







































\newpage
\section{Problems and Solutions}
\subsection{Problem set 1 (under corrections)}
\begin{prob*} \textbf{1. Equivalent formulations of directness of a subspace sum:} Suppose that $\V_1, \V_2\,\dots,\V_{315}$ are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum $\V_1 + \V_2 + \dots + \V_{315}$ is direct. 
		\item If $x_i \in \V_i$ and $x_1 + x_2 + \dots + x_{315} = \mathbf{0}_\W$, then $x_i = \mathbf{0}_\W$, for every $i$.
		\item If $x_i, y_i \in \V_i$ and $x_1 + x_2 + x_3 + \dots + x_{315} = y_1 + y_2 + y_3 + \dots + y_{315}$, then $x_i = y_i$, for every $i$.
		\item For any $i$, no non-null element of $\V_i$ can be express as a sum of the elements of the other $V_j$'s.
		\item For any $i$, no non-null element of $\V_i$ can be expressed as a sum of the elements of the preceding $V_j$'s. 
	\end{enumerate}

\end{prob*}


\newpage

\begin{prob*} \textbf{2.} \textbf{Sub-sums of direct sums are direct: }
	Suppose that $\V_1, \V_2,\dots, \V_{315}$ are subspaces of a vector space $\W$, and the subspace sum $\V_1+\V_2+\dots+\V_{315}$ is direct.
	\begin{enumerate}
		\item Suppose that for each $i$, $\Z_i$ is a subspace of $\V_i$. Argue that the subspace sum $\Z_1 + \Z_2 + \dots \Z_{315}$ is direct. 
		\item Argue that the sum $\V_2 + \V_5 + \V_7 + \V_{12}$ is also direct. 
	\end{enumerate}

\end{prob*}


\newpage


\begin{prob*} \textbf{3.} \textbf{Associativity of directness of subspace sums:} Suppose that 
	\begin{align*}
	\Y, \V_1, \V_2, \dots, \V_5, \U_1, \U_2, \dots, \U_{12},\Z_1,\Z_2,\dots,\Z_4, \X_1, \X_2, \dots,\X_{53}
	\end{align*}
	are subspaces of a vector space $\W$. Argue that the following claims are equivalent. 
	\begin{enumerate}
		\item The subspace sum 
		\begin{align*}
		\Y+ \V_1+ \V_2+\dots+ \V_5+ \U_1+ \U_2+ \dots+ \U_{12}+\Z_1+\Z_2+\dots+\Z_4+ \X_1+ \X_2+ \dots+\X_{53}
		\end{align*}
		is direct. 
		\item The subspace sums
		\begin{align*}
		&\left[\V :=  \right] \V_1 +\V_2 + \dots + \V_5\\
		&\left[\U :=  \right] \U_1 +\U_2 + \dots + \U_{12}\\
		&\left[\Z :=  \right] \Z_1 +\Z_2 + \dots + \Z_4\\
		&\left[\X :=  \right] \X_1 +\X_2 + \dots + \X_{53}\\
		&\Y + \V + \U + \X + \Z
		\end{align*}
		are all direct. 
	\end{enumerate}

\end{prob*}


\newpage

\begin{prob*} \textbf{4.} \textbf{Direct sums preserve linear independence:} Suppose that $\U,\V,\W$ are subspaces of a vector space $\Z$, and the sum $\U,\V,\W$ is direct. 
	\begin{enumerate}
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a linearly independent list in $\U$, $V_1,V_2,\dots,V_{6}$ is a linearly independent list in $\V$, and $W_1,W_2,\dots,W_{134}$ is a linearly independent list in $\W$. Argue that the concatenated list 
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is linearly independent. 
		\item Suppose that $U_1,U_2,\dots,U_{13}$ is a basis of $\U$, $V_1,V_2,\dots,V_{6}$ is a basis of $\V$, and $W_1,W_2,\dots,W_{134}$ is a basis of $\W$. Argue that the concatenated list
		\begin{align*}
		U_1,U_2,\dots,U_{13},V_1,V_2,\dots,V_{6},W_1,W_2,\dots,W_{134}
		\end{align*}
		is a basis of $\U \oplus \V \oplus \W$.
	\end{enumerate}

\end{prob*}


\newpage




\begin{prob*} \textbf{5.} Suppose that $x_1, x_2,\dots,x_{14}$ are non-null elements of a linear space $\W$. 
	\begin{enumerate}
		\item Argue that the following claims are equivalent. 
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$ are linearly independent. 
			\item The subspace sum 
			\begin{align*}
			\xpan(x_1) + \xpan(x_2) + \dots + \xpan(x_{14})
			\end{align*}
			is direct. 
		\end{enumerate}
		\item Argue that the following claims are equivalent.
		\begin{enumerate}
			\item $x_1, x_2,\dots,x_{14}$  is a basis of $\W$.
			\item $\W = \xpan(x_1) \oplus \xpan(x_2) \oplus \dots \oplus \xpan(x_{14})$
		\end{enumerate}
	\end{enumerate}


\end{prob*}


\newpage




















\subsection{Problem set 2}
\begin{prob*}\textbf{1.}
	\textbf{Finite unions of subspaces are rarely a subspace}\\
	Suppose that $\W_1,\W_2,\dots,\W_n$ are subspaces of a vector space $\V$. Prove that the following are equivalent:
	\begin{enumerate}
		\item $\W_1 \cup \W_2\cup\dots\cup\W_n$ is a subspace of $\V$.
		\item One of the $\W_i$'s contains all the others. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{1.}
		$\,$
		\begin{itemize}
			\item $[1. \implies 2.]:$ Suppose $2.$ is false and define 
			\begin{align*}
			\mathbf{S} = \bigcup_{i=1}^{n_0}\W_i
			\end{align*}
			a subspace of $\V$, where none of the $\W_i$'s $\prec \V$ contains all the others, and $n_0$ is minimal. If $n_0=1$, then the implication $1. \implies 2.$ is true because $\W_1$ contains itself. Therefore, in order for this implication to fail, $n_0 \geq 2$.\\
			
			Because $n_0$ is minimal, $\mathbf{S}$ cannot be obtained from a union of less than $n_0$ of the $\W_i$'s. Therefore, each $\W_i$ contains an element $w_i$ that does not belong to any other $\W_j$'s.\\
			
			Take $w_2\in\W_2\setminus \W_1$ and $w_1 \in \W_1$ such that $w_1 \notin \bigcup_{j\neq 1}^{n_0}\W_j$. It follows that $w_1, w_2 \in \mathbf{S}$ because $\W_1, \W_2 \subseteq \mathbf{S}$. And since $\mathbf{S}$ is a subspace, $mw_1 + w_2 \in \mathbf{S}$ for any $m\in\mathbb{C}$ by closure under addition. \\
			
			Consider a collection of $n_0$ linear combinations of $w_1$ and $w_2$, defined as
			\begin{align*}
			T = \{ w_1+w_2,2w_1+w_2,\dots,n_0 w_1 + w_2\} \subseteq \mathbf{S}.
			\end{align*}
			Consider a typical element of $T$: $n w_1 + w_2 \in \mathbf{S}$, $n\in\{1,2,\dots,n_0\}$. Suppose $n w_1 + w_2 \in \W_1$. By closure under addition, $$(nw_1 + w_2) - nw_1 = w_2\in \W_1.$$ But this contradicts the choice of $w_2$. Therefore, $nw_1 + w_2 \notin \W_1$, for any $n\in\{1,2,\dots,n_0\}$. It follows that $T \subseteq \bigcup_{i=2}^{n_0} \W_i$. \\
			
			By construction, $T$ has $n_0$ elements, while $\bigcup_{i=2}^{n_0} \W_i$ has $(n_0 - 1)$ $\W_i$'s. By the pigeonhole principle, a $\W_k$ of $\W_2, \dots, \W_{n_0}$ must contain $nw_1 + w_2$ for two distinct values of $n$. Let these values be $n_a$ and $n_b$. By closure under addition, we have
			\begin{align*}
			(n_a w_1 + w_2) - (n_b w_1 + w_2) = (n_a - n_b)w_1 \in \W_k.
			\end{align*}
			But since $n_a \neq n_b$, $w_1 \in \W_k$. This contradicts our initial choice of $w_1$ and thus rules out the possibility that the implication $1. \implies 2.$ fails to hold. Therefore, the implication $1. \implies 2.$ must be true.
			
			
			
			
			\item $[2. \implies 1.]:$ Without loss of generality, assume that $\W_1$ contains all the other $\W_j$'s. It follows that 
			\begin{align*}
			\bigcup_{i=1}^{n_0}\W_i = \W_1.
			\end{align*}
			Since $\W_1$ is a subspace of $\V$, $\bigcup_{i=1}^{n_0}\W_i$ is also a subspace of $\V$. Therefore, $2. \implies 1.$ must hold.
		\end{itemize}
	\end{sln*}
	
\end{prob*}











\newpage
\begin{prob*}\textbf{2.}
	\textbf{Images of pre-images and pre-images of images:} Is there a $3\times 3$ matrix $\mathcal{A}$ such that
	\begin{align*}
	\W := \left\{ \begin{pmatrix}
	x\\y\\0
	\end{pmatrix}\bigg\vert \,x,y\in \mathbb{C} \right\}
	\end{align*}
	is an invariant subspace for $\mathcal{A}$, and
	\begin{align*}
	\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \subsetneq \W \subsetneq \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]?
	\end{align*}
	Justify your answer.
	
	
	\begin{sln*}\textbf{2.}
		$\,$\\
		
		\noindent First, we observe that $\dim(\W) = 2$, because a basis set of $\W$,
		\begin{align*}
		\left\{ \begin{pmatrix}
		1\\0\\0
		\end{pmatrix},\begin{pmatrix}
		0\\1\\0
		\end{pmatrix}\right\},
		\end{align*}
		has two elements. Next, consider the vector space $\mathbb{C}^3$ over the complex numbers. Since $\W$ is a subspace and any element of $\W$ is contained in $\mathbb{C}^3$ by construction, $\W \prec \mathbb{C}^3$. In fact, $\W \subsetneq \mathbb{C}^3$ because $\mathbb{C}^3 \ni (0\,\,\,0\,\,\,1)^\top \notin\W$\\
		
		\noindent By the condition $\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \subsetneq \W \subsetneq \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]$, it must be true that
		\begin{align*}
		\dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right]   \right) < \dim(\W) < \dim\left(\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]\right).
		\end{align*}
		Now, since $\dim(\W) = 2$ and $\mathcal{A}$ is a $3\times 3$ matrix (i.e., neither $\dim(\ker(\mathcal{A}))$ nor $\dim(\Im(A))$ can exceed 3$^{(\dagger)}$), it is required that $\dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right]   \right) \leq 1$ and $\dim\left(\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]\right) = 3$. \\
		
		\noindent Because $\W$ is invariant under $\mathcal{A}$, $\mathcal{A}[\W] \subseteq \W$. It follows that $\mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right] \subseteq \mathcal{A}^{-1}[\W]$. Therefore,
		\begin{align*}
		3 = \dim\left( \mathcal{A}^{-1}\left[ \mathcal{A}[\W] \right]  \right) \leq \dim\left( \mathcal{A}^{-1}[\W]  \right).
		\end{align*}
		It follows, by fact $(\dagger)$, that
		\begin{align*}
		3 \leq \dim\left( \mathcal{A}^{-1}[\W]  \right) \leq 3,
		\end{align*}
		which implies $\dim\left( \mathcal{A}^{-1}[\W]  \right) = 3 = \dim(\mathbb{C}^3)$. Consider $v\in \mathcal{A}^{-1}[\W] $. $v$ can have the form $(x\,\,y\,\,z)^\top$, $x,y,z\in \mathbb{C}$, so $v\in\mathbb{C}^3$. This implies $\mathcal{A}^{-1}[\W] \subseteq \mathbb{C}^3$. But since their dimensions are both 3, $\mathcal{A}^{-1}[\W] = \mathbb{C}^3$. It follows that
		\begin{align*}
		\mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] = \mathcal{A}[\mathbb{C}^3] \supseteq \mathcal{A}[\W],
		\end{align*}
		where the second relation comes from the fact that $\W$ is a subspace of $\mathbb{C}^3$. Hence,
		\begin{align*}
		1 \geq \dim\left( \mathcal{A}\left[ \mathcal{A}^{-1}[\W] \right] \right) \geq \dim\left( \mathcal{A}[\W] \right),
		\end{align*}
		which implies $\dim(\mathcal{A}[\W]) = 0$ or $1$.\\
		
		\noindent So, such a $3\times 3$ matrix $\mathcal{A}$ that satisfies the given condition is
		\begin{align*}
		\mathcal{A} = \begin{pmatrix}
		1&0&0\\
		0&0&0\\
		0&0&0
		\end{pmatrix}.
		\end{align*}
		First, $\W$ is invariant under $\mathcal{A}$ because take any $(x\,\,\,y\,\,\,0)^\top \in \W$, $\mathcal{A}\left((x\,\,\,y\,\,\,0)^\top\right) = (x\,\,\,0\,\,\,0)^\top \in \W$. This implies $\mathcal{A}[\W] \subseteq [\W]$ $(a)$. \\
		
		\noindent Second, it is easy to see that the pre-image of $\W$ under $\mathcal{A}$ is $\mathbb{C}^3$, since take any $(x\,\,\,y\,\,\,z)^\top \in \mathbb{C}^3$, we have $\mathcal{A}\left((x\,\,\,y\,\,\,z)^\top\right) = (x\,\,\,0\,\,\,0)^\top \in \W$. But observe that $\Im(\mathcal{A})$ is a proper subset of $\W$, because any $(x\,\,\,y\,\,\,0)^\top \in \W$ with $y\neq 0$ is not contained in $\Im(\mathcal{A})$. This implies $\mathcal{A}\left[\mathcal{A}^{-1}[\W]\right] \subsetneq \W$  $(b)$.\\ 
		
		\noindent Lastly, as we have pointed out, because    $\dim\left(A^{-1}\left[\mathcal{A}[\W]\right]\right) = 3$ and $A^{-1}\left[\mathcal{A}[\W]\right] \prec \mathbb{C}^3$, $A^{-1}\left[\mathcal{A}[\W]\right] = \mathbb{C}^3 \supsetneq \W$ $(c)$.\\
		
		\noindent From $(a),(b),(c)$, we see that this $3\times 3$ matrix $\mathcal{A}$ works as desired.   
	\end{sln*}
	
	
\end{prob*}









\newpage

\begin{prob*}\textbf{3.}
	\textbf{Invariant subspaces form a ``lattice'':}
	\begin{enumerate}
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under intersection; i.e., that the intersection of any two invariant subspaces for $\lag$ is also an invariant subspace for $\lag$.
		\item Argue that $\mathfrak{Lat}(\lag)$ is closed under subspace sums; i.e., that a subspace sum of two invariant subspaces for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that an image under $\lag$ of an invariant subspace for $\lag$ is again an invariant subspace for $\lag$.
		\item Show that a pre-image under $\lag$ of an invariant subspace for $\lag$ is again an invariant subspace for $\lag$. 
	\end{enumerate}
	
	
	\begin{sln*}\textbf{3.}
		$\,$
		\begin{enumerate}
			\item Let subspaces $\V, \W \in \mathfrak{Lat}(\lag)$ be given. $\mathfrak{Lat}(\lag)$ is closed under intersection if $\V \cap \W \in \mathfrak{Lat}(\lag)$. Consider $u\in \V\cap\W$, then $u\in\V$ and $u\in\W$. It follows that $\lag(u)\in \lag[\V]$ and $\lag(u)\in \lag[\W]$.\\
			
			Now, since $\lag[\V]\subseteq \V$ and $\lag[\W]\subseteq \W$ because $\W,\V \in \mathfrak{Lat}(\lag)$, we have $\lag(u)\in\V$ and $\lag(u)\in \W$, which implies $\lag(u)\in \V\cap \W$. Since this implication holds for any $u\in \V\cap\W$, $\lag[\V\cap\W]\subseteq \V\cap\W$. Therefore, $\V\cap\W$ is invariant under $\lag$; i.e., $\W\cap\V \in \mathfrak{Lat}(\lag)$. This completes the argument. \\
			
			\item Let subspaces $\V,\W \in \mathfrak{Lat}(\lag)$ be given. $\mathfrak{Lat}(\lag)$ is closed under subspace sums if $\V + \W \in \mathfrak{Lat}(\lag)$. Consider $v\in\V, w\in\W$. Then $v+w \in \V + \W$. Next, consider $l\in \lag[\U+\W]$ given by
			$$l = \lag(v+w) = \lag(v) + \lag(w) = v' + w',$$
			where $v'\in \V$ and $w'\in\W$ because $\V,\W$ are invariant under $\lag$. So $l \in \V+\W$. This implies $ \lag[\V+\W] \subseteq \V+\W $; i.e., $\mathfrak{Lat}(\lag)$ is closed under subspace sums.    \\
			
			\item Let $\V \in \mathfrak{Lat}(\lag)$ be given. By definition, $\lag[\V] \subseteq \V$. It follows that $\lag\left[ \lag[\V] \right] \subseteq \lag[\V]$. So, $\lag[\V]$ is invariant under $\lag$.\\
			
			\item Let $\W \in \mathfrak{Lat}(\lag)$ be given. Claim: $\lag\left[\lag^{-1}[\W] \right] \subseteq \lag^{-1}[\W] $.\\
			
			It follows from the definition of the pre-image of $\W$ under $\lag : \V \to \W$ that $\lag^{-1}[\W] = \left\{ v \in \V \vert \lag(v) \in \W \right\}$. This implies $\lag\left[\lag^{-1}[\W]\right] \subseteq \W$ (i).\\
			
			Next, consider $w\in\W \subseteq \V$. By definition, $\lag^{-1}\left[\lag[\W] \right] = \{ v\in \V \vert \lag(v) \in \lag[\W]\},$ which implies $w \in \lag^{-1}[
			\lag[\W]]$, because $\lag(w) \in \lag[\W]$. Therefore, $\W \subseteq \lag^{-1}\left[\lag[\W]\right]$ (ii).\\   
			
			Moreover, because $\W \in \mathfrak{Lat}(\lag)$, $\lag[\W] \subseteq \W$ (iii).\\ 
			
			From (i), (ii), and (iii) we have
			\begin{align*}
			\lag\left[ \lag^{-1}[\W] \right] \stackrel{(i)}\subseteq \W \stackrel{(ii)}\subseteq \lag^{-1}\left[ \lag[\W] \right] \stackrel{(iii)}\subseteq \lag^{-1}[\W].
			\end{align*} 
			Therefore, $\lag\left[ \lag^{-1}[\W] \right] \subseteq \lag^{-1}[\W] $, verifying the claim.
			
			
		\end{enumerate}
	\end{sln*}
	
\end{prob*}














\newpage

\begin{prob*}\textbf{4. Cyclic invariant subspaces}
	For a given $\lag\in\mathfrak{L}(\V)$ and a fixed $v_0 \in \V$, define
	\begin{align*}
	\mathbf{P}(\lag,v_0) := \left\{ \left(a_0 \lag^0 + a_1\lag^1 + a_2\lag^2 + \dots + a_k \lag^k\right)(v_0)\bigg\vert k \geq 0, a_i \in \mathbb{C} \right\}.
	\end{align*}
	\begin{enumerate}
		\item Argue that $\mathbf{P}(\lag,v_0)$ is a subspace of $\V$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is invariant under $\lag$.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is the smallest invariant subspace for $\lag$ that contains $v_0$. We refer to $\mathbf{P}(\lag,v_0)$ as \textbf{the cyclic invariant subspace for $\lag$ generated by $v_0$}.
		\item Argue that $\mathbf{P}(\lag,v_0)$ is 1-dimensional exactly when $v_0$ is an eigenvector of $\lag$.
		\item Argue that a subspace $\W$ of $\V$ is invariant under $\lag$ exactly when it is a union of cyclic invariant subspaces for $\lag$. 
	\end{enumerate}
	
	\begin{sln*}\textbf{4.}
		$\,$
		\begin{enumerate}
			\item Consider a typical element $P(\lag, v_0) \in \mathbf{P}(\lag,v_0) $. Since $\lag \in \mathfrak{L}(\V)$, $\lag^i(v_0) \in \V$ for all $i$ and $v_0 \in \V$. By closure under scalar multiplication and addition, for any $a_i\in \mathbb{C}$, $v_0\in \V$, and $k\geq 0$, $$ P(\lag, v_0) = \sum_{i=0}^k a_i \lag^i(v_0) \in \V.$$ Therefore $\mathbf{P}(\lag,v_0) \subseteq \V$ (i).\\
			
			With $a_i = 0$ for all $i$, $ \mathbf{P}(\lag,v_0) \ni P(\lag, v_0) = \sum^k_{i=1}0\cdot\lag^i(v_0) = \mathbf{0}_\mathbf{P}$. So $\mathbf{P}(\lag,v_0)$ contains the null element (ii).\\
			
			Consider $P_1(\lag, v_0) = \sum^k_{i=0}b_i\lag^i(v_0)$ and $P_2(\lag,v_0) = \sum^l_{j=0}c_j\lag^j(v_0)$, with $b_i, c_j \in \mathbb{C}$ for all $i,j$. Without loss of generality, assume that $k\geq l \geq 0$. It is clear that $\mathbf{P}(\lag,v_0)$ is closed under addition because, 
			\begin{align*}
			P_1(\lag,v_0) + P_2(\lag,v_0) &= \sum^k_{i=0}b_i\lag^i(v_0) + \sum^l_{j=0}c_j\lag^j(v_0)\\
			&= \sum^k_{i=0}b_i\lag^i(v_0) + \sum^k_{i=0}c_i\lag^i(v_0)\,\,\,\, \text{with }c_i = 0 \text{ for all } i > l\\
			&= \sum^k_{i=0}(b_i + c_i)\lag^i(v_0)\\ 
			&= \sum^k_{i=0}d_i\lag^i(v_0) \in \mathbf{P}(\lag,v_0),
			\end{align*}
			where $d_i = b_i + c_i \in \mathbb{C}$. (iii).\\ 
			
			It is also clear that $\mathbf{P}(\lag,v_0)$ is closed under scalar multiplication because given $\mu\in \mathbb{C}$, $$\mu P_1(\lag,v_0) = \mu\sum_{i=0}^kb_i\lag^i(v_0) = \sum_{i=0}^k\mu b_i\lag^i(v_0) = \sum_{i=0}^ke_i\lag^i(v_0) \in \mathbf{P}(\lag,v_0),$$ where $e_i = \mu b_i \in \mathbb{C}$ (iv).\\
			
			By (i), (ii), (iii), and (iv), $\mathbf{P}(\lag,v_0)$ is a subspace of $\V$. \\
			
			
			\item Let $P(\lag,v_0)$ be given. It suffices to show that $\lag(P(\lag, v_0)) \in \mathbf{P}(\lag,v_0)$, for all $P(\lag,v_0) \in \mathbf{P}(\lag,v_0)$. $$ \lag(P(\lag,v_0)) = \lag\left( \sum_{i=0}^k a_i\lag^i(v_0) \right) = \sum_{i=0}^k a_i \lag^{i+1}(v_0),$$ where $a_i\in\mathbb{C}$ and $k\geq 0$. Define new coefficients $b_{j} = a_{i}$ where $j=i+1$ and $b_0 = 0$, then $$ \lag(P(\lag,v_0)) = \sum_{j=1}^{k+1} b_j\lag^j(v_0) = \sum_{j=0}^{k+1} b_j\lag^j(v_0) \in \mathbf{P}(\lag,v_0). $$
			Therefore, $\lag\left[\mathbf{P}(\lag,v_0) \right] \subseteq \mathbf{P}(\lag,v_0)$; i.e., $\mathbf{P}(\lag,v_0)$ is invariant under $\lag$. \\
			
			
			
			\item  Let $\mathbf{P}(\lag,v_0)$ be given. $\mathbf{P}(\lag,v_0)$ is an invariant subspace under $\lag$ that contains $v_0$ by definition. Suppose $\mathbf{Q}(\lag,v_0)$ is some invariant subspace under $\lag$ that also contains $v_0$. It suffices to show $\mathbf{P}(\lag,v_0) \subseteq \mathbf{Q}(\lag,v_0)$ for any such $\mathbf{Q}(\lag,v_0)$. \\
			
			Consider $\lag^j(v_0)$ for some $j\geq 0$. $\lag^j{(v_0)}\in \mathbf{P}(\lag,v_0)$ by definition. We claim that $\lag^j(v_0) = q\in \mathbf{Q}(\lag,v_0)$ for any non-negative $j$. Assume that this is true. The base case where $j=0$ is true since $v_0 \in \mathbf{Q}(\lag,v_0)$. The inductive case is also true because
			\begin{align*}
			\lag^{(j+1)}(v_0) = \lag(\lag^j(v_0)) = \lag(q) \in \mathbf{Q}(\lag,v_0) 
			\end{align*}
			where the last relation comes from the fact that $\mathbf{Q}(\lag,v_0)$ is invariant under $\lag$. Therefore, by the principle of induction, $\lag^j(v_0) \in \mathbf{Q}(\lag,v_0)$ for any non-negative $j$. This implies $\mathbf{P}(\lag,v_0) \subseteq \mathbf{Q}(\lag,v_0)$ for any such $\mathbf{Q}(\lag,v_0)$. Thus $\mathbf{P}(\lag,v_0)$ must be the smallest invariant subspace under $\lag$ that contains $v_0$. 
			
			
			
			\item 
			\begin{enumerate}

			\item $(\implies):$ Let a $\mathbf{P}(\lag,v_0)$ be given such that $\dim(\mathbf{P}(\lag,v_0)) = 1$. Consider the subspace $\xpan(v_0)$. We know that $\dim(\xpan(v_0)) = 1$. Consider $v \in \xpan(v_0)$. Then $v$ can be expressed as $v = av_0$ where $a\in \mathbb{C}$. It follows immediately that $v\in \mathbf{P}(\lag,v_0)$. Therefore, $\xpan(v_0) \subseteq \mathbf{P}(\lag,v_0)$. But because $\dim(\mathbf{P}(\lag,v_0)) = 1 = \dim(\xpan(v_0))$, $\mathbf{P}(\lag,v_0) = \xpan(v_0)$; i.e., all elements of $\mathbf{P}(\lag,v_0)$ are some scalar multiples of $v_0$.\\
			
			Now, it suffices to show $\lag(v_0) = \lambda v_0$, where $\lambda \in \mathbb{C}$. Consider $P(\lag, v_0) = \lag^0(v_0) + \lag(v_0) \in \mathbf{P}(\lag,v_0) = \xpan(v_0)$, $v_0 \neq \mathbf{0}_\V$. By closure under addition, $$(\lag^0(v_0) + \lag(v_0)) - v_0 = (\lag^0(v_0) + \lag(v_0)) - \lag^0(v_0) = \lag(v_0) \in \xpan(v_0).$$ This implies $\lag(v_0) = \lambda v_0$ for some $\lambda \in \mathbb{C}$; i.e., $v_0$ is an eigenvector of $\lag$.\\
				
				
			\item $(\impliedby):$ If $v_0$ is an eigenvector of $\lag$, then $\lag^i(v_0) = \lambda^i v_0$, where $\lambda^i \in \mathbb{C}$ is the $\lag$'s $v_0$-eigenvalue raised to the $i^{\text{th}}$ power. It follows that a typical element of $\mathbf{P}(\lag,v_0)$ is $$P(\lambda,v_0) = \sum_{i=0}^k a_i \lag^i (v_0) =  \left(\sum_{i=0}^k a_i\lambda^i\right)  v_0 = \mu v_0,$$
			where $\mu = \sum_{i=0}^k a_i \lambda^i \in \mathbb{C}$. Therefore, $\mathbf{P}(\lag,v_0) = \xpan(v_0)$; i.e., $\dim\left(\mathbf{P}(\lag,v_0)\right) = 1$. This completes the argument. \\
			
			\end{enumerate}
		
		\item
		\begin{enumerate}
			
			\item $(\implies):$ Let $\W \in \mathfrak{Lat}(\lag)$ be given. Then $\lag[\W] \subseteq \W$. Let $w_1 \in \W$ be given. We first show that $\lag^k(w_1) \in \W$ for all non-negative $k$ by induction. Assume that $\lag^k(w_1) = v_1 \in \W$ for all non-negative $k$ holds. The base case where $k=0$ is true since $\lag^0(w_1) = w_1\in\W$. The inductive case is also true because $$\lag^{(k+1)}(w_1) = \lag(\lag^k(w_1)) = \lag(v_1) \in \W.$$ By the principle of induction, $\lag^k(w_1) \in \W$ for all $w_1\in\W$ and $k\geq 0$ ($\dagger$). Now, consider $P \in \mathbf{P}(\lag,w_1)$. By the definition of $\mathbf{P}(\lag,w_1)$, $P$ is a linear combination of $\lag^0(w_1),\dots,\lag^k(w_1)$ for some $k\geq 0$. So, by $(\dagger)$, $P \in \W$. This implies $\mathbf{P}(\lag,w_1) \subseteq \W$. \\
			
			We can repeat the argument above, starting with $w_2,w_3,\dots \in \W$, and conclude that $\mathbf{P}(\lag,w_2) \subseteq \W$, $\mathbf{P}(\lag,w_3) \subseteq \W$, and so on. This means $\W$ contains a union of $\mathbf{P}(\lag,w_1), \mathbf{P}(\lag,w_2), \mathbf{P}(\lag,w_3), \dots$. Moreover, observe the fact that any element $w_n \in \W$ can be used to generate a $\mathbf{P}(\lag,w_n)$ that is contained in the union of itself and the other $\mathbf{P}(\lag,w_m)$'s, which is ultimately contained in $\W$. Therefore, $\W$ \textit{is} itself a union of cyclic invariant subspaces for $\lag$.\\  
			
			
			\item $(\impliedby):$ Let $\W = \mathbf{P_1} \cup \mathbf{P_2} \cup \dots \cup \mathbf{P_n}$ be a subspace of $\V$, where $\mathbf{P_i}$ denotes $\mathbf{P}(\lag,v_i)$ and $v_i$ is an arbitrary element of $\V$. By problem 1, because $\mathbf{P_1} \cup \mathbf{P_2} \cup \dots \cup \mathbf{P_n}$ is a subspace of $\V$, a $\mathbf{P_i}$, $1\leq i \leq n$, must contain all the other $\mathbf{P_j}$'s. Without loss of generality, assume that such a $\mathbf{P_i}$ is $\mathbf{P_1}$. It follows that $\W = \mathbf{P_1}$. By part 2. of this problem, we know that $\mathbf{P_1}$ is invariant under $\lag$. Therefore, $\W$ is invariant under $\lag$.
			
			 
			\end{enumerate} 
			
		\end{enumerate}
	\end{sln*}
\end{prob*}














\newpage


\begin{prob*} \textbf{5. Invariant subspaces of commuting operators:} Suppose that linear functions $\lag, \mathcal{M} \in \mathfrak{L}(\V)$ commute; i.e.,
	\begin{align*}
	\lag \circ \mathcal{M} = \mathcal{M}\circ \lag.
	\end{align*}
	\begin{enumerate}
		\item Argue that for any non-negative integer $k$, $\Im(\mathcal{M}^k)$ and $\ker(\mathcal{M}^k)$ are invariant subspace for $\lag$.
		\item Argue that every eigenspace of $\mathcal{M}$ is an invariant subspace for $\lag$.
		\item By giving a general example (with justification, of course!) show that for each $n>1$ there are commuting matrices $\mathcal{A}$ and $\mathcal{B}$ in $\mathbb{M}_n$ such that
		\begin{align*}
		\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B}).
		\end{align*}
	\end{enumerate}
	\begin{sln*}\textbf{5.}
		$\,$
		\begin{enumerate}
			\item Because $\lag \circ \M = \M \circ \lag$, $\lag \circ \M^k = \M^k \circ \lag$ for any non-negative $k$. We can verify this by a short proof by induction. Assume that $\lag \circ \M^k = \M^k \circ \lag$ holds for any non-negative $k$. The base case where $k=0$ is  true, for $\lag\circ \M^0 = \lag \circ \mathcal{I} = \mathcal{I} \circ \lag = \M^0 \circ \lag$, where $\mathcal{I}$ is the identity operator. The inductive case is also true because $$ \lag\circ \M^{k+1} = \lag \circ \M^k \circ \M = \M^k\circ\lag\circ\M = \M^k\circ \M \circ \lag = \M^{k+1}\circ \lag.$$ Therefore, by the principle of induction, $\lag \circ \M^k = \M^k \circ \lag$ is indeed true for all $k\geq 0$.
			
			\begin{enumerate}
				\item \underline{To show}: $\lag\left[ \Im(\M^k)\right] \subseteq \Im(\M^k)$.\\
				
				 Since $\lag\circ \M^k = \M^k \circ \lag$,  $\lag\circ \M^k[\V] = \M^k\circ\lag[\V]$; i.e., $\lag[\Im(\M^k)] = \M^k[\Im(\lag)]$. But since $\Im(\lag) \subseteq \V$ (because $\lag \in \mathfrak{L}(\V)$), $$\M^k[\Im(\lag)] \subseteq \M^k[\V] = \Im(\M^k).$$ It follows that $\lag[\Im(\M^k)] \subseteq \Im(\M^k)$. So, $\Im(\M^k)$ is an invariant subspace for $\lag$.\\
				
				\item \underline{To show}: $\lag[\ker(\M^k)] \subseteq \ker(\M^k)$. \\
				
				Since $\lag\circ \M^k = \M^k \circ \lag$, $$\lag\circ \M^k[\ker(\M^k)] = \{\mathbf{0}_\V\} = \M^k \circ \lag[\ker(\M^k)].$$ It follows that $\lag[\ker(\M^k)] \subseteq \ker(\M^k)$. This completes the argument.\\
			\end{enumerate}
			
			
			\item Let $\mathbf{E}_\lambda$, the eigenspace of $\M$ associated with eigenvalue $\lambda$, be given. Consider $e \in \mathbf{E}_\lambda$, we have $$\M\circ \lag(e) = \lag \circ \M(e) = \lag(\lambda e) = \lambda \lag(e).$$ This implies $\lag(e) \in \mathbf{E}_\lambda$. Since this relation holds for any $e\in \mathbf{E}_\lambda$, $\lag[\mathbf{E}_\lambda] \subseteq \mathbf{E}_\lambda$. Therefore, $\mathbf{E}_\lambda$ is an invariant subspace for $\lag$ for any eigenvalue $\lambda$. This proves the claim that \textit{every} eigenspace of $\M$ is an invariant subspace for $\lag$.\\
			
			
			\item It suffices to find commuting matrices $\mathcal{A}$ and $\mathcal{B}$ such that there exists an invariant subspace under $\mathcal{A}$, $\J \in \mathfrak{Lat}(\mathcal{A})$, but $\J \notin \mathfrak{Lat}(\mathcal{B})$. \\
			
			For $n=2$, consider
			\begin{align*}
			\mathcal{A} = \begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix},\,\,\,\,\,
			\mathcal{B} = \begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}.
			\end{align*} 
			$\mathcal{A}$ and $\mathcal{B}$ commute:
			\begin{align*}
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}\begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}=
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}
			=
			\begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}
			\begin{pmatrix}
			0&0\\
			0&0
			\end{pmatrix}
			\end{align*}
			
			Consider $\J = \xpan\{(1\,\,\,1)^\top\}$. $\J \in \mathfrak{Lat}(\mathcal{A})$ since $\mathcal{A}[\J] = \{ \mathbf{0}\} \subseteq \J$. However, consider $j = (1\,\,\,1)^\top \in \J$, we have
			\begin{align*}
			\mathcal{B}(j) = \begin{pmatrix}
			1&0\\
			0&0
			\end{pmatrix}\begin{pmatrix}
			1\\1
			\end{pmatrix}=
			\begin{pmatrix}
			1\\0
			\end{pmatrix} \notin \xpan\{ (1\,\,\, 1)^\top \},
			\end{align*} 
			which implies $\J \notin \mathfrak{Lat}(\mathcal{B})$. Therefore, $\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B})$.\\
			
			Observe that we can give a more general example for any $n>1$ based on the $2\times 2$ example above. Let $\mathcal{A}$ be an $n\times n$ zero matrix, and $\mathcal{B}$ be an $n\times n$ matrix of the form
			\begin{align*}
			\mathcal{B} = \begin{pmatrix}
			1&0&\dots&0\\
			0&0\\
			\vdots&&\ddots\\
			0&&&0
			\end{pmatrix}.
			\end{align*}
			It is clear that $\mathcal{A}$ and $\mathcal{B}$ commute, since $\mathcal{A}$ is the zero matrix: $$\mathcal{AB} = [0]_{n\times n} = \mathcal{BA}.$$\\
		
			Consider $j = (1\,\,1\,\,\dots\,\,1)^\top \in \J = \xpan\{ (1\,\,1\,\,\dots\,\,1)^\top \} \subseteq \mathbb{C}^n$. It is clear that $\mathcal{B}(j) = (1\,\,0\,\,\dots\,\,0)^\top \notin \J$ but $\mathcal{A}(j') = \mathbf{0} \in \J$ for any $j'\in\J$. So, $\J \in \mathfrak{Lat}(\mathcal{A})$ but $\J \notin \mathfrak{Lat}(\mathcal{B})$; i.e., $$\mathfrak{Lat}(\mathcal{A}) \neq \mathfrak{Lat}(\mathcal{B}).$$ 
		\end{enumerate}
	\end{sln*}
\end{prob*}








\newpage

\begin{prob*}\textbf{6. Invariant subspace chains:} Suppose that $\lag \in \mathfrak{L}(\V)$ and $\W$ is an invariant subspace of $\lag$. 
	\begin{enumerate}
		\item Argue that the following inclusions hold:
		\begin{align*}
		\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq \lag^{-3}[\W]\subseteq\dots
		\end{align*}
		Note that by Problem 3, all of the subspaces listed in the chain are invariant under $\lag$.
		\item Argue that the following implications hold:
		\begin{align*}
		\lag^{k+1}[\W] = \lag^{k}[\W] \implies \lag^m[\W] = \lag^k[\W]\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and 
		\begin{align*}
		\lag^{-k}[\W] = \lag^{-(k+1)}[\W] \implies \lag^{-k}[\W] = \lag^{-m}[\W]\,\,\,\,\, \text{for any } m\geq k,
		\end{align*}
		and then explain why the chain in part 1 stabilizes in both directions, and has at most $\dim(\V)$ proper inclusions. 
		\item Argue that any subspace $\mathbf{M}$ that falls between two consecutive subspaces in the chain shown in part 1. is also invariant under $\lag$. 
		
		
	\end{enumerate}
	\begin{sln*}\textbf{6.}
		$\,$
		\begin{enumerate}
			\item 
			\begin{enumerate}
				\item \underline{To show}: $\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W$.\\
				
				Assume that $ \lag^{k+1}[\W] \subseteq \lag^k[\W] $ for all $k\geq 0$. The base case $k=0$ is true because $\W$ is an invariant subspace under $\lag$. The inductive case is also true because $$ \lag^{k+2}[\W] = \lag[\lag^{k+1}[\W]] \subseteq \lag[\lag^k[\W]] = \lag^{k+1}[\W],$$ where the second relation follows from assumption. By principle of induction, $\dots \subseteq \lag^3[\W] \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W$.\\
				
				\item \underline{To show}: $\W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq \lag^{-3}[\W]\subseteq\dots$.\\
				
				Assume that $\lag^{-k}[\W] \subseteq \lag^{-k-1}[\W] $ for all $k\geq 0$. The base case where $k=0$ is true: Given $w\in\W\subseteq \V$, $\lag(w) \in \W$ because $\W$ is invariant under $\lag$. But since $\lag^{-1}[\W] = \{ v\in\V \vert \lag(v)\in \W\}$ and $\lag[\W]\subseteq \W$, $w \in \lag^{-1}[\W]$. Therefore, $\W \subseteq \lag^{-1}[\W]$ (i).\\
				
				The inductive case is also true: $$ \lag^{-k-1}[\W] = \lag^{-1}[\lag^{-k}[\W]] \subseteq \lag^{-1}[\lag^{-k-1}[\W]] \subseteq \lag^{-k-2}[\W].  $$ So by the principle of induction, $\W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq\dots$ (ii). \\
			\end{enumerate}
				
			From (i) and (ii), $$ \dots  \subseteq \lag^2[\W]\subseteq \lag [\W] \subseteq \W \subseteq \lag^{-1}[\W] \subseteq \lag^{-2}[\W] \subseteq\dots $$
				
				
				
			\item 
			\begin{enumerate}
				\item Assume the hypothesis that $\lag^{k+1}[\W] = \lag^k[\W]$ for some $k\geq 0$. We can prove by induction that $\lag^{k+j}[\W] = \lag^k[\W]$ for all non-negative $j$. The base case where $j=0$ is true. The inductive case is also true since: $$ \lag^{k+j+1}[\W] =  \lag[\lag^{k+j}[\W]] = \lag[\lag^k[\W]] = \lag^{k+1}[\W] = \lag^k[\W]. $$ By the principle of induction, $\lag^{m}[\W] = \lag^k[\W]$ must hold for any $m\geq k\geq 0$ if $\lag^{k+1}[\W] = \lag^k[\W]$.\\
				
				\item Assume that hypothesis that $\lag^{-k}[\W] = \lag^{-(k+1)}[\W]$ for all $k \geq 0$. Also assume that $\lag^{-k}[\W] = \lag^{-(k+j)}[\W]$ for all non-negative $j$. The base case where $j=0$ is true. In inductive case is also true since: 
				 \begin{align*}
				 				 \lag^{-(k+j+1)}[\W] &= \lag^{-1}[\lag^{-(k+j)}[\W]\\ &=  \lag^{-1}[\lag^{-k}[\W]]\\ &= \lag^{-(k+1)}[\W]\\ &= \lag^{-k}[\W].
				 \end{align*}
 				By the principle of induction, $\lag^{-k}[\W] = \lag^{-m}[\W]$ must hold for $m\geq k$ if $\lag^{-k}[\W] = \lag^{-(k+1)}[\W]$.\\
				
				

				\item It follows from part 1. that $$ \dots \leq \dim(\lag[\W]) \leq \dim(\W) \leq \dim(\lag^{-1}[\W]) \leq \dots  $$
				Because $\dim(\lag^{-k}[\W]) \leq \dim(\V)$ for any $k\geq 0$, there exists an $\lag^{-h}[\W]$, $h\geq 0$, that is an invariant subspace in the chain with the highest dimension where $h$ is minimal. It follows that $\dim(\lag^{-(h+1)}[\W]) = \dim(\lag^{-(h)}[\W])$ is maximal. But because $\lag^{-h}[\W] \subseteq \lag^{-(h+1)}[\W]$, $\lag^{-h}[\W] = \lag^{-(h+1)}[\W]$. As a result, $\lag^{-h}[\W] = \lag^{-j}[\W]$ for any $j\geq h$, which follows from (b). Therefore, the chain in part 1. stabilizes in the ``inclusion'' direction.  \\
				
				A similar argument can be given to show that the chain in part 1. also stabilizes in the ``inclusion by'' direction. Since $0 \leq \dim(\lag^{l}[\W])$ for any $l\geq 0$, there exists an $\lag^l[\W]$ in the chain with the lowest dimension but $l$ is minimal. It follows that $\dim(\lag^{(l+1)}[\W]) = \dim(\lag^{l}[\W])$ is minimal. But because $\lag^{(l+1)}[\W] \subseteq \lag^{l}[\W]$, $\lag^{(l+1)}[\W] = \lag^{l}[\W]$. As a result, $\lag^{k}[\W] = \lag^{l}[\W]$ for any $k\geq l$, which follows from (a). Therefore, the chain in part 1. stabilizes in the ``inclusion by'' direction as well.\\
				
				We claim that for every \textit{proper} inclusion, there is a dimension loss. Revisiting $\lag^{-h}[\W]$, we know that $\dim(\lag^{-h}[\W]) \leq \dim(\V)$. Consider the first proper inclusion: $$\dots \lag^{-(h-j)}[\W] \subsetneq \lag^{-(h-j+1)}[\W] \subseteq \dots \subseteq \lag^{-h}[\W]\subseteq \dots$$ 
				
				This implies $\dim(\lag^{-(h-j)}[\W]) < \dim(\lag^{-h}[\W]) \leq \dim(\V)$. Suppose $\dim(\lag^{-h}[\W]) = \dim(\V)$, i.e., maximal, then $$\dim(\lag^{-(h-j)}[\W]) \leq \dim(\V) - 1.$$ By assuming equality is satisfied and consider the next proper inclusion in the chain, a similar argument shows that the invariant subspace of interest has at most $\dim(\V) - 2$ dimensions. It follows that, at the $\dim(\V)^{\text{th}}$ proper inclusion, the considered invariant subspace has at most $\dim(\V) - \dim(\V) = 0$ dimensions; i.e. the subspace is $\{\mathbf{0}_\V\}$. At this point, no further proper inclusion is possible. Therefore, the chain in part 1. has at most $\dim(\V)$ proper inclusion.\\
			\end{enumerate}
				
				
				
				
				
			\item	
			Let $\lag^{(k+1)}[\W] \subseteq \mathbf{M} \subseteq \lag^{k}[\W]$ be given for some nonnegative integer $k$. $\mathbf{M}$ is a subspace. We have that
			$$\lag[\mathbf{M}] \subseteq \lag[\lag^{k}[\W]] = \lag^{(k+1)}[\W] \subseteq \mathbf{M}^{(i)}. $$
			
			Let $\lag^{-h}[\mathbf{W}] \subseteq \mathbf{M} \subseteq \lag^{-(h+1)}[\W]$ be given for some nonnegative integer $h$. We have that
			$$ \lag[\mathbf{M}] \subseteq \lag[\lag^{-(h+1)}[\W]] = \lag[\lag^{-1}[\lag^{-h}[\W]]] \subseteq \lag[\lag^{-1}[\mathbf{M}]].  $$
			But because $\lag^{-1}[\mathbf{M}] = \{ v\in \V \vert \lag(v)\in \mathbf{M} \}$, $\lag[\lag^{-1}[\mathbf{M}]] \subseteq \mathbf{M}$. Therefore, we have $\lag[\mathbf{M}] \subseteq \mathbf{M}^{(ii)}$.\\ 
			
			From (i) and (ii), $\mathbf{M}$ is invariant under $\lag$ if $\mathbf{M}$ is a subspace that falls between two consecutive subspaces in the chain shown in part 1.. 
			
		\end{enumerate}
	\end{sln*}
\end{prob*}






































\newpage
\subsection{Problem set 3}

\begin{prob*}\textbf{1.} Suppose that $\V_1, \V_2, \V_3$ are non-trivial vector spaces, and for each $i,j \in \{ 1,2,3\}$,
	\begin{align*}
	\lag_{ij} : \V_j \overset{\text{linear}}{\longrightarrow} \V_i.
	\end{align*}
	\begin{enumerate}
		\item Argue that 
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times
		\end{align*}
		is a linear function.\\
		
		
		
		
		
		
		
		
		
		

		
		\item Argue that every linear function $\T : \V_1 \times \V_2 \times \V_3 \to \V_1\times\V_2\times \V_3$ can be expressed as 
		\begin{align*} 
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33} 
		\end{bmatrix}_\times,
		\end{align*} 
		with $\lag_{ij} : \V_j \overset{\text{linear}}{\longrightarrow} \V_i$, in exactly one way. \\
		
		\item If the function
		\begin{align*}
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times : \V_1 \times \V_2 \times \V_3 \to \V_1\times\V_2\times \V_3
		\end{align*}
		is defined similarly, find (with proof) the block-matrix form of the functions
		\begin{align*}
		2&\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times 
		+ 
		3\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times 
		\end{align*}
		and
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times
		\circ 
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\times.
		\end{align*}
	\end{enumerate}
	
	\newpage
	
	\begin{sln*}\textbf{1. }
		\begin{enumerate}
			\item 
			\begin{enumerate}
			\item It suffices to show that $\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times$ satisfies the linearity conditions.\\
			
			Consider 
			\begin{align*}
			\begin{pmatrix}
			a_1\\a_2\\a_3
			\end{pmatrix}, \begin{pmatrix}
			b_1 \\ b_2 \\ b_3
			\end{pmatrix} \in \V_1 \times \V_2 \times \V_3
			\end{align*} 
			where $a_j, b_j \in \V_j$ for $j \in \{1,2,3 \}$. By definition, 
			\begin{align*}
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			a_1 \\ a_2 \\ a_3
			\end{pmatrix} 
			+ 
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			b_1 \\ b_2 \\ b_3
			\end{pmatrix} \\
			=
			&\begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3)
			\end{pmatrix} 
			+
			\begin{pmatrix}
			\lag_{11}(b_1) + \lag_{12}(b_2) + \lag_{13}(b_3)\\
			\lag_{21}(b_1) + \lag_{22}(b_2) + \lag_{23}(b_3)\\
			\lag_{31}(b_1) + \lag_{32}(b_2) + \lag_{33}(b_3)
			\end{pmatrix}\\
			=
			& \begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3) + \lag_{11}(b_1) + \lag_{12}(b_2) + \lag_{13}(b_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3) + \lag_{21}(b_1) + \lag_{22}(b_2) + \lag_{23}(b_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3) + \lag_{31}(b_1) + \lag_{32}(b_2) + \lag_{33}(b_3)
			\end{pmatrix}\\
			=
			& \begin{pmatrix}
			[\lag_{11}(a_1) + \lag_{11}(b_1)] + [\lag_{12}(a_2) + \lag_{12}(b_2)] + [\lag_{13}(a_3) + \lag_{13}(b_3)]\\
			[\lag_{21}(a_1) + \lag_{21}(b_1)] + [\lag_{22}(a_2) + \lag_{22}(b_2)] + [\lag_{23}(a_3) + \lag_{23}(b_3)]\\
			[\lag_{31}(a_1) + \lag_{31}(b_1)] + [\lag_{32}(a_2) + \lag_{32}(b_2)] + [\lag_{33}(a_3) + \lag_{33}(b_3)]
			\end{pmatrix}, \hspace{0.2cm} \text{by the linearity of $\lag_{ij}$}\\
			=
			& \begin{pmatrix}
			\lag_{11}(a_1 + b_1) + \lag_{12}(a_2 + b_2) + \lag_{13}(a_2 + b_2)\\
			\lag_{21}(a_1 + b_1) + \lag_{22}(a_2 + b_2) + \lag_{23}(a_2 + b_2)\\
			\lag_{31}(a_1 + b_1) + \lag_{32}(a_2 + b_2) + \lag_{33}(a_2 + b_2)
			\end{pmatrix}\\
			=
			& \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			a_1 + b_1\\a_2 + b_2\\a_3 + b_3
			\end{pmatrix}\\
			=
			& \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times
			\left( 
			\begin{pmatrix}
			a_1\\a_2\\a_3
			\end{pmatrix}
			+
			\begin{pmatrix}
			b_1\\b_2\\b_3
			\end{pmatrix}
			\right).
			\end{align*}  
			
			Therefore, the additivity condition is satisfied. 
			
			\item Next, we consider the scaling condition. Let $c \in \mathbb{C}$ be given,
			
			
			\begin{align*}
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\left(c\begin{pmatrix}
			a_1 \\ a_2 \\ a_3
			\end{pmatrix}\right) \\
			= 
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\begin{pmatrix}
			ca_1 \\ ca_2 \\ ca_3
			\end{pmatrix}\\
			=
			&\begin{pmatrix}
			\lag_{11}(ca_1) + \lag_{12}(ca_2) + \lag_{13}(ca_3)\\
			\lag_{21}(ca_1) + \lag_{22}(ca_2) + \lag_{23}(ca_3)\\
			\lag_{31}(ca_1) + \lag_{32}(ca_2) + \lag_{33}(ca_3)
			\end{pmatrix} \\
			=
			& \begin{pmatrix}
			c\lag_{11}(a_1) + c\lag_{12}(a_2) + c\lag_{13}(a_3)\\
			c\lag_{21}(a_1) + c\lag_{22}(a_2) + c\lag_{23}(a_3)\\
			c\lag_{31}(a_1) + c\lag_{32}(a_2) + c\lag_{33}(a_3)
			\end{pmatrix},\hspace{0.2cm}\text{by the linearity of $\lag_{ij}$}\\
			=
			& c\begin{pmatrix}
			\lag_{11}(a_1) + \lag_{12}(a_2) + \lag_{13}(a_3)\\
			\lag_{21}(a_1) + \lag_{22}(a_2) + \lag_{23}(a_3)\\
			\lag_{31}(a_1) + \lag_{32}(a_2) + \lag_{33}(a_3)
			\end{pmatrix}.
			\end{align*}
			Therefore, the scalar multiplication condition is also satisfied.\\
			\end{enumerate}
			From (a) and (b), $
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times
			$
			is a linear function.\\
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			\item
			\begin{enumerate}
			\item \underline{Existence:} Let $\Pi_i$ be a coordinate projection on $\V_1 \times \V_2 \times \V_3$ onto $\V_i$ defined in Definition 0.2. Let $\gamma_i$ be a coordinate injection of $\V_i$ into $\V_1 \times \V_2 \times \V_3$ also defined in Definition 0.2. \\
			
			Consider the block-matrix $\lag_\times$,
			\begin{align*}
			\lag_\times = \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times.
			\end{align*}
			Let the linear function $\T : \V \lin \V$ be given. Let each $\lag_{ij} : \V_j \lin \V_i$ be defined as
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\V_j}.
			\end{align*}
			Then we have
			\begin{align*}
			\lag_\times &= \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times\\
			&=
			\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_1 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_1 \circ \T \circ \gamma_3\bigg\vert_{\V_3}\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_2 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_2 \circ \T \circ \gamma_3\bigg\vert_{\V_3}\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_3 \circ \T \circ \gamma_3\bigg\vert_{\V_3} 
			\end{bmatrix}_\times.
			\end{align*}
			
			
			
			Consider $\begin{pmatrix}
			w_1 & \mathbf{0}_\V & \mathbf{0}_\V
			\end{pmatrix}^\top \in \V_1\times\V_2\times\V_3$ with $w_1\in \V_1$.
			\begin{align*}
			&\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_1 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\V_3}\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_2 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\V_3}\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\V_1} & \Pi_3 \circ \T \circ \gamma_2\bigg\vert_{\V_2} & \Pi_3 \circ \T \circ \gamma_3\bigg\vert_{\V_3} 
			\end{bmatrix}_\times
			\begin{pmatrix}
			w_1 \\ \mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}\\
			=
			&\begin{bmatrix}
			\Pi_1 \circ \T \circ \gamma_1\bigg\vert_{\V_1}(w_1)\\
			\Pi_2 \circ \T \circ \gamma_1\bigg\vert_{\V_1}(w_1)\\
			\Pi_3 \circ \T \circ \gamma_1\bigg\vert_{\V_1}(w_1)
			\end{bmatrix}\\
			=
			& \begin{bmatrix}
			\Pi_1 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_2 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_3 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top
			\end{bmatrix}.
			\end{align*}
			
			Let $\T\left(\begin{pmatrix}
			w_1&\mathbf{0}_\V&\mathbf{0}_\V
			\end{pmatrix}^\top\right)$ be $\begin{pmatrix}
			a&b&c
			\end{pmatrix}^\top\in \V_1\times\V_2\times\V_3$. By definition,
			\begin{align*}
			&\begin{bmatrix}
			\Pi_1 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_2 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top\\
			\Pi_3 \circ \T\begin{pmatrix} w_1 & \mathbf{0}_\V & \mathbf{0}_\V \end{pmatrix}^\top
			\end{bmatrix}\\ =
			&\begin{bmatrix}
			\Pi_1 \left(\begin{pmatrix}
			a&b&c
			\end{pmatrix}^\top\right)\\
			\mathbf{0}_\V\\
			\mathbf{0}_\V
			\end{bmatrix} +
			 \begin{bmatrix}
			 \mathbf{0}_\V\\
			 \Pi_2 \left(\begin{pmatrix}
			 a&b&c
			 \end{pmatrix}^\top\right)\\
			 \mathbf{0}_\V
			 \end{bmatrix} + 
			 \begin{bmatrix}
			 \mathbf{0}_\V\\
			 \mathbf{0}_\V\\
			 \Pi_3 \left(\begin{pmatrix}
			 a&b&c
			 \end{pmatrix}^\top\right)
			 \end{bmatrix}\\
			 =
			 &\begin{bmatrix}
			 a\\
			 \mathbf{0}_\V\\
			 \mathbf{0}_\V
			 \end{bmatrix} 
			 +
			 \begin{bmatrix}
			 \mathbf{0}_\V\\
			 b\\
			 \mathbf{0}_\V
			 \end{bmatrix} 
			 + 
			 \begin{bmatrix}
			 \mathbf{0}_\V\\
			 \mathbf{0}_\V\\
			 c
			 \end{bmatrix} = \begin{bmatrix}
			 a\\b\\c
			 \end{bmatrix} = \T\begin{pmatrix}
			 w_1\\\mathbf{0}_\V\\\mathbf{0}_\V
			 \end{pmatrix}
			\end{align*}

			Therefore,
			\begin{align*}
			\lag_\times\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} = \T\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}.
			\end{align*}
			In general, the equality above also holds for any $v = w_1 + w_2 + w_3 \in \V$ with $w_i \in \V_i$, since $v$ can be expressed as
			\begin{align*}
			v = \begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\ w_2\\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\\mathbf{0}_\V \\ w_3
			\end{pmatrix},
			\end{align*}
			and the linearity of $\lag_\times$ and $\T$ ensures that $\lag_\times(v) = \T(v)$. Hence, $\T$ can be expressed as the matrix $\lag_\times$ whose elements $\lag_{ij}$'s are defined as
			\begin{align*}
			\lag_{ij} = \Pi_i \circ \T \circ \gamma_j\bigg\vert_{\V_j}.
			\end{align*}.

			\item \underline{Uniqueness: } We observe that all of the steps in the ``Existence part'' are reversible. This means if we were given the linear map $\T$, we could consider applying $\T$ to $\begin{pmatrix}
			w_1&\mathbf{0}_\V&\mathbf{0}_\V
			\end{pmatrix}^\top, \begin{pmatrix}
			\mathbf{0}_\V&w_2&\mathbf{0}_\V
			\end{pmatrix}^\top,$ and $\begin{pmatrix}
			\mathbf{0}_\V&\mathbf{0}_\V&w_3
			\end{pmatrix}^\top  \in \V$; and by reversing the steps in part (a), we know that $\T$ can be expressed as a $3\times 3$ block-matrix whose elements must have the form $\Pi_i \circ \T \circ \gamma_j\bigg\vert_{\V_j} = \lag_{ij}$. 
			
			\end{enumerate}
			From parts (a) and (b), the linear function $\T$ can be expressed as the matrix $\lag_\times$ in exactly one way.
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			
			
			
			
			
			\item
			
			Let 
			\begin{align*}
			\lag := \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times : \V \lin \V
			\end{align*}
			and
			\begin{align*}
			\M := \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times : \V \lin \V
			\end{align*}
			be given. 
			\begin{enumerate}
			\item Let $\lag$ be the matrix of the linear function $\T$ above and $\M$ be the matrix of a linear function $\K : \V \lin \V$. Consider the expression $2\lag_{ij} + 3\M_{ij}$. Let $\lag_{ij}$ be constructed as in Part 1 for the linear function $\T$. Let $\M$ be constructed similarly for $\K$. Then, we have
			\begin{align*}
			2\lag_{ij} + 3\M_{ij} &= 2\Pi_i \circ \T \circ \gamma_j\bigg\vert_{\W_j} + 
			3\Pi_i \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ 2\T \circ \gamma_j\bigg\vert_{\W_j} + 
			\Pi_i \circ 3\K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \left( 2\T \circ \gamma_j\bigg\vert_{\W_j} + 3\K \circ \gamma_j\bigg\vert_{\W_j}  \right)\\
			&= \Pi_i \circ \left( 2\T + 3\K \right) \circ \gamma_j\bigg\vert_{\W_j}\\
			&= (2\T + 3\K)_{ij},\hspace{0.2cm}\text{by definition.}
			\end{align*}
			This shows that each element of the matrix of the linear function $(2\lag + 3\K)$ is given by
			\begin{align*}
			\boxed{(2\T + 3\K)_{ij} = 2\lag_{ij} + 3\M_{ij}}
			\end{align*}
			i.e.,
			\begin{align*}
			&2\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			+ 
			3\begin{bmatrix}
			\M_{11} & \M_{12} & \M_{13}\\
			\M_{21} & \M_{22} & \M_{23}\\
			\M_{31} & \M_{32} & \M_{33}
			\end{bmatrix}_\times\\
			=
			& \begin{bmatrix}
			2\lag_{11} + 3\M_{11} & 2\lag_{12} + 3\M_{12} & 2\lag_{13} + 3\M_{13}\\
			2\lag_{21} + 3\M_{21} & 2\lag_{22} + 3\M_{22} & 2\lag_{23} + 3\M_{23}\\
			2\lag_{31} + 3\M_{31} & 2\lag_{32} + 3\M_{32} & 2\lag_{33} + 3\M_{33}
			\end{bmatrix}_\times.
			\end{align*}
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			
			\item Let $\T$, $\M_{ij}$, $\K$, and $\lag_{ij}$ be defined similarly: $\lag$ is the matrix of the linear function $\T$ and $\M$ is the matrix of the linear function $\K$. Consider the expression $\sum^3_{k=1}\lag_{ik}\circ \M_{kj}$, we have
			\begin{align*}
			\sum^3_{k=1} \lag_{ik} \circ \M_{kj} 
			&= \sum^3_{k=1}\Pi_i \circ \T \circ \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \sum^3_{k=1} \Pi_i \circ \T \circ \left(\gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right) \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \T \circ \left(\sum_{k=1}^3 \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right) \circ \K \circ \gamma_j\bigg\vert_{\W_j}.
			\end{align*}
			Consider $v = \begin{pmatrix}w_1 & w_2 & w_2\end{pmatrix}^\top \in \V$ such that $w_i \in \W_i$ for $i\in \{1,2,3\}$. We can show that the summation in the expression above is simply the identity function:
			\begin{align*}
			\left(\sum_{k=1}^3 \gamma_k\bigg\vert_{\W_k} \circ \Pi_k \right)(v) 
			&=
			\gamma_1\bigg\vert_{\W_1} \circ \Pi_1(v) + \gamma_2\bigg\vert_{\W_2} \circ \Pi_2(v) + \gamma_3\bigg\vert_{\W_3} \circ \Pi_3(v)\\
			&= \gamma_1(w_1) + \gamma_2(w_2) + \gamma_3(w_3)\\
			&=\begin{pmatrix}
			w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V\\ w_2\\ \mathbf{0}_\V
			\end{pmatrix} + \begin{pmatrix}
			\mathbf{0}_\V \\ \mathbf{0}_\V \\ w_3
			\end{pmatrix}\\
			&=
			\begin{pmatrix}
			w_1\\w_2\\w_3
			\end{pmatrix}\\
			&= v.
			\end{align*}
			Therefore,
			\begin{align*}
			\sum^3_{k=1} \lag_{ik} \circ \M_{kj} 
			&= \Pi_i \circ \T \circ \K \circ \gamma_j\bigg\vert_{\W_j}\\
			&= \Pi_i \circ \left(\T \circ \K\right) \circ \gamma_j\bigg\vert_{\W_j}\\
			&= (\T \circ \K)_{ij}.
			\end{align*}
			This shows that each element of the matrix of the linear function $(\T\circ \K)$ is given by
			\begin{align*}
			\boxed{(\T\circ\K)_{ij} = \sum^3_{k=1} \lag_{ik} \circ \M_{kj}}
			\end{align*}
			which gives the full block-matrix form:
			\begin{align*}
			&\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times
			\circ 
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33} 
			\end{bmatrix}_\times\\
			=
			&\begin{bmatrix}
			\sum^3_{k=1} \lag_{1k} \circ \M_{k1} &
			\sum^3_{k=1} \lag_{1k} \circ \M_{k2} &
			\sum^3_{k=1} \lag_{1k} \circ \M_{k3} \\
			\sum^3_{k=1} \lag_{2k} \circ \M_{k1} &
			\sum^3_{k=1} \lag_{2k} \circ \M_{k2} &
			\sum^3_{k=1} \lag_{2k} \circ \M_{k3} \\
			\sum^3_{k=1} \lag_{3k} \circ \M_{k1} &
			\sum^3_{k=1} \lag_{3k} \circ \M_{k2} &
			\sum^3_{k=1} \lag_{3k} \circ \M_{k3} 
			\end{bmatrix}_\times
			\end{align*}
			
			\end{enumerate}
			
			
			
			
			
			\newpage
			
		\end{enumerate}
			 
			
	\end{sln*}
	
	
\end{prob*}





























\newpage


\begin{prob*}\textbf{2.} Suppose that $\V = \W_1\oplus \W_2\oplus \W_3$ and the $\W_i$'s are non-trivial subspaces of $\V$. Suppose that for each $i,j \in \{1,2,3 \}$,
	\begin{align*}
	\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i.
	\end{align*}
	\begin{enumerate}
		\item Verify for formula 
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_{\oplus} 
		\begin{pmatrix}
		x_1\\x_2\\x_3
		\end{pmatrix}_{\maltese} 
		=
		\left( \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times \begin{pmatrix}
		x_1\\x_2\\x_3
		\end{pmatrix}   \right)_{\maltese},
		\end{align*}
		where we have used as few brackets as possible without losing clarity.
		
		
		
		\item Argue that every $T : \V \overset{\text{linear}}{\longrightarrow} \V$ can be expressed as $\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus$, with $\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i$, in exactly one way.
		
		
		
		\item If the function 
		\begin{align*}
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus : \V \longrightarrow \V
		\end{align*}
		is defined similarly, find (with proof) the corresponding black-matrix form of the functions 
		\begin{align*}
		2&\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus
		+ 
		3\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus 
		\end{align*}
		and
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus
		\circ 
		\begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus.
		\end{align*}

		
		
		\item Suppose that $\mathcal{E}_1 + \mathcal{E}_2 + \mathcal{E}_3 = \mathcal{I}_\V$ is a resolution of the identity, where $\ima(\mathcal{E}_i) = \W_i$. If
		\begin{align*}
		\lag = \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus,
		\end{align*}
		express (with proof, of course) $\lag_{ij}$ in terms of $\lag, \mathcal{E}_k$'s and $\W_k$'s.
		
		
		
		\item Continuing with the set-up of part 4, find $\M_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i$ such that 
		\begin{align*}
		\mathcal{E}_1 = \begin{bmatrix}
		\M_{11} & \M_{12} & \M_{13}\\
		\M_{21} & \M_{22} & \M_{23}\\
		\M_{31} & \M_{32} & \M_{33}
		\end{bmatrix}_\oplus.
		\end{align*}
		Then do the same for $\mathcal{E}_2$ and $\mathcal{E}_3$.
	\end{enumerate}











	\newpage
	
	\begin{sln*}\textbf{2.}
		\begin{enumerate}
			\item  By definition,
			\begin{align*}
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_{\oplus}
			\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}_{\maltese} 
			&=
			\left[\maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\circ 
			\maltese^{-1}\right]\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}_{\maltese} \\
			&= \left[\maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times 
			\circ 
			\maltese^{-1}\circ\maltese\right]\begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}\\
			&= \maltese
			\circ
			\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times \begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}\\
			&=
			\maltese\left(\begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times \begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}\right)\\
			&= 
			\left( \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\times \begin{pmatrix}
			x_1\\x_2\\x_3
			\end{pmatrix}   \right)_{\maltese}.
			\end{align*}
						
						
			
			\item 
			\begin{enumerate}
				\item \underline{Existence}: Let the linear functions $\E_i := \Pi_i \circ \maltese^{-1} : \V \lin \W_i$ and $\K_i := \maltese\circ \gamma_i : \W_i \lin \V$ be given, where the $\Pi_i$'s and $\gamma_i$'s are defined the same way as in Problem 1. \\
				
				Consider the block matrix $\lag_\oplus$
				\begin{align*}
				\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus.
				\end{align*}
				
				Let the linear function $\T : \V \lin \V$ be given. Let each $\lag_{ij} : \W_j \lin \W_i$ be defined as:
				\begin{align*}
				\lag_{ij} = \E_i \circ \T \circ \K_j \bigg\vert_{\W_j}.
				\end{align*}
				By definition,
				\begin{align*}
				\lag_{ij} &= \E_i \circ \T \circ \K_j \bigg\vert_{\W_j}\\
				&= \Pi_i \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_j\bigg\vert_{\W_j}.
				\end{align*}
				
				Applying $\lag_\oplus$ to $\begin{pmatrix}
				w_1&\mathbf{0}_\V&\mathbf{0}_\V
				\end{pmatrix}^\top_\maltese \in \V$, we get
				\begin{align*}
				\lag\begin{pmatrix}
				w_1\\\mathbf{0}_\V\\\mathbf{0}_\V
				\end{pmatrix}_\maltese 
				&=
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
				\Pi_2 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)\\
				\Pi_3 \circ \maltese^{-1} \circ \T \circ \maltese\circ \gamma_1\bigg\vert_{\W_1}(w_1)
				\end{bmatrix}_\maltese\\
				&=
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top\\
				\Pi_2 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top\\
				\Pi_3 \circ \maltese^{-1} \circ \T \circ \maltese
				\begin{pmatrix}
				w_1 & \mathbf{0}_\V & \mathbf{0}_\V
				\end{pmatrix}^\top
				\end{bmatrix}_\maltese\\
				&= 
				\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_2 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_3 \circ \maltese^{-1}\circ \T (w_1) 
				\end{bmatrix}_\maltese\\
				&=
				\maltese\left(\begin{bmatrix}
				\Pi_1 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_2 \circ \maltese^{-1}\circ \T (w_1) \\
				\Pi_3 \circ \maltese^{-1}\circ \T (w_1) 
				\end{bmatrix}\right)\\
				&= 
				\maltese\circ\maltese^{-1}\circ \T(w_1),\hspace{0.2cm}\text{by Part 2(a) of Problem 1}\\
				&= \T(w_1)
				\end{align*} 
				By the definition of $\maltese$, we can write this as
				\begin{align*}
				\lag(w_1 + \mathbf{0}_\V + \mathbf{0}_\V) = \lag(w_1) = \T(w_1).
				\end{align*}
				In general, the equality above also holds if we start with any $v = w_1 + w_2 + w_3 \in \V$ with $w_i \in \W_i$ for $i=\{1,2,3\}$, because as we have shown in Part 2(a) of Problem 1, $v$ can be written as
				\begin{align*}
				v = \begin{pmatrix}
				w_1\\\mathbf{0}_\V \\ \mathbf{0}_\V
				\end{pmatrix}_\maltese + 
				\begin{pmatrix}
				\mathbf{0}_\V\\w_2 \\ \mathbf{0}_\V
				\end{pmatrix}_\maltese + 
				\begin{pmatrix}
				\mathbf{0}_\V\\\mathbf{0}_\V \\ w_3
				\end{pmatrix}_\maltese,
				\end{align*}
				and the linearity of $\lag$ and $\T$ ensures $\lag(v) = \T(v)$. Hence, $\T$ can expressed as the matrix $\lag$ whose elements are defined as
				\begin{align*}
				\lag_{ij} = \E_i \circ \T \circ \K_j\bigg\vert_{\W_j}.
				\end{align*}
				
				
				\item \underline{Uniqueness}: Once again, we observe that all of the steps in the ``Existence'' part are reversible. We can consider applying $\T$ to $v$, expressed in Part 3(a), and by reversing the steps in Part (a) show that the $3\times 3$ block-matrix of $\T$ must consist elements defined by $\E_i \circ \T \circ \K_j\bigg\vert_{\W_j} = \lag_{\oplus_{ij}}$ for $i,j\in \{1,2,3\}$. \\ 
			\end{enumerate}
		From parts (a) and (b), the linear function $\T$ can be expressed as the matrix $\lag_\oplus$ defined in the problem statement in exactly one way. 
			
						
						
						
						
						
						
		
			\newpage
			
			
			

			
			
			\item 
			\begin{enumerate}
				\item Let $\lag_\oplus = [\lag_{\oplus_{ij}}]$ denote the matrix $\lag_\oplus : \W_1\oplus\W_2\oplus\W_3\lin\W_1\oplus\W_2\oplus\W_3$. Let $\M_\oplus = [\M_{\oplus_{ij}}]$ be defined similarly for $\M$. By part 1, we can re-express the matrix 
				\begin{align*}
				[(2\lag_\oplus + 3\M_\oplus)_{ij}] = 2\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus
				+ 
				3\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\oplus 
				\end{align*}
				as 
				\begin{align*}
				&2\,\,\maltese \circ \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				+ 
				3\,\,\maltese\circ\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times \circ \maltese^{-1} \\
				=\,\,
				&\,\,\maltese \circ 2\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				+ 
				\,\,\maltese\circ 3\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times \circ \maltese^{-1} \\
				=\,\,
				&\maltese \circ \left(
				2\,\, \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				+
				3\,\, \begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times \circ \maltese^{-1}
				\right)\\
				=\,\,
				&\maltese \circ \left(
				2\,\, \begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\times
				+
				3\,\, \begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\times
				\right) \circ \maltese^{-1}\\
				=\,\, &\maltese\circ \left( 2[{\lag_\times}_{ij} ] + 3[{\M_\times}_{ij} ]  \right) \circ \maltese^{-1},
				\end{align*}
				where the notation $[A_{ij}]$ denotes the block-matrix of elements $A_{ij}$ where $A$ is a linear function, and the last relation:
				\begin{align*}
				[(2\lag_\times + 3\M_\times)_{ij}] = 2[{\lag_\times}_{ij}] + 3[{\M_\times}_{ij}].
				\end{align*}
				follows from Problem 1. Then, we have
				\begin{align*}
				[(2\lag_\oplus + 3\M_\oplus)_{ij}]
				&= \maltese\circ \left( 2[{\lag_\times}_{ij} ] + 3[{\M_\times}_{ij} ]  \right) \circ \maltese^{-1}\\
				&= \maltese\circ [2{\lag_\times}_{ij}]\circ \maltese^{-1} + \maltese\circ [3{\M_\times}_{ij}]\circ \maltese^{-1}\\
				&= 2[{\lag_\oplus}_{ij}] + 3[{\M_\oplus}_{ij}].
				\end{align*}
				So, in the block-matrix form:
				\begin{align*}
				&2\begin{bmatrix}
				\lag_{11} & \lag_{12} & \lag_{13}\\
				\lag_{21} & \lag_{22} & \lag_{23}\\
				\lag_{31} & \lag_{32} & \lag_{33}
				\end{bmatrix}_\oplus
				+ 
				3\begin{bmatrix}
				\M_{11} & \M_{12} & \M_{13}\\
				\M_{21} & \M_{22} & \M_{23}\\
				\M_{31} & \M_{32} & \M_{33}
				\end{bmatrix}_\oplus\\
				=\,\, 
				&\begin{bmatrix}
				2\lag_{11} + 3\M_{11} & 2\lag_{12} + 3\M_{12} & 2\lag_{13} + 3\M_{13}\\
				2\lag_{21} + 3\M_{21} & 2\lag_{22} + 3\M_{22} & 2\lag_{23} + 3\M_{23}\\
				2\lag_{31} + 3\M_{31} & 2\lag_{32} + 3\M_{32} & 2\lag_{33} + 3\M_{33}
				\end{bmatrix}_\oplus.
				\end{align*}
				
				
				
				
				
				
				
				
				
				
				
				
				
				
				\item Let $\lag_\oplus$ and $\M_\oplus$ denote the same matrices in part (a), and let $\lag_\oplus$ represent the linear function $\T$, $\M_\oplus$ represent the linear function $\N$. Let the matrix $[(\lag_\oplus\circ\M_\oplus)_{ij}] $ express the composition $\T\circ\N$. Its the elements $[\lag_\oplus\circ\M_\oplus] $ can be constructed as
				\begin{align*}
				(\lag_\oplus\circ\M_\oplus)_{ij} = \E_i \circ (\lag_\oplus \circ \M_\oplus) \circ \K_j\bigg\vert_{\W_j} = \E_i \circ (\T \circ \N) \circ \K_j\bigg\vert_{\W_j} ,\hspace{0.1cm}(\dagger)
				\end{align*}
				as we have done in Part 1. Consider the expression $\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}$. By part 1, we have
				\begin{align*}
				\lag_{\oplus_{ik}} = \E_i \circ \T \circ \K_k\bigg\vert_{\W_k}\text{ and }\M_{\oplus_{kj}} = \E_k \circ \N \circ \K_j\bigg\vert_{\W_j}.
				\end{align*} 
				This gives
				\begin{align*}
				\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}
				&= \sum^3_{k=1} \E_i \circ \T \circ \left(\K_k\bigg\vert_{\W_k}\circ \E_k \right)\circ \N \circ \K_j\bigg\vert_{\W_j}\\
				&= \E_i \circ \T \circ \left(\sum^3_{k=1}\K_k\bigg\vert_{\W_k}\circ \E_k\right)  \circ \N \circ \K_j\bigg\vert_{\W_j}
				\end{align*}
				By definition, we have
				\begin{align*}
				\K_k\bigg\vert_{\W_k} = \maltese\circ \gamma_k\bigg\vert_{\W_k}\text{ and }\E_k= \Pi_k\circ\maltese^{-1},
				\end{align*}
				which imply
				\begin{align*}
				\sum^3_{k=1}\K_k\bigg\vert_{\W_k}\circ \E_k &= \sum^3_{k=1} \maltese\circ \gamma_k\bigg\vert_{\W_k}\circ \Pi_k\circ\maltese^{-1}\\
				&= \maltese \circ \left(\sum^3_{k=1} \gamma_k\bigg\vert_{\W_k}\circ \Pi_k \right)\maltese^{-1}\\
				&= \maltese\circ\maltese^{-1},\hspace{0.2cm}\text{by Part 3(b) of Problem 1}\\
				&= \text{Identity}.
				\end{align*}
				It follows that,
				\begin{align*}
					\sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}} &= \E_i \circ \T\circ\N\circ\K_j\bigg\vert_{\W_j}= (\lag_\oplus\circ\M_\oplus)_{ij},\hspace{0.2cm}\text{by $(\dagger)$}.
				\end{align*}
				Therefore, the matrix elements of $[\lag_\oplus\circ \M_\oplus]$ are given by
				\begin{align*}
				\boxed{(\lag_\oplus\circ\M_\oplus)_{ij} = \sum^3_{k=1}\lag_{\oplus_{ik}}\circ\M_{\oplus_{kj}}}
				\end{align*}
				Since the full block-matrix expression for $[\lag_\oplus\circ \M_\oplus]$ is very similar to what we have found with $\lag_\times \circ \M_\times$, we will not produce it here.
			\end{enumerate}
			
			
			
			
			
			
			
			
			\newpage
			
			
			
		
			
			
			
			
			
			\item Let $\E_1 + \E_2 + \E_3 = \mathcal{I}_\V$ be a resolution of identity, with $\ima(\E_i) = \W_i$, be given. Furthermore, let the linear map $\lag : \V \lin \V$ be given where
			\begin{align*}
			\lag = \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\oplus.
			\end{align*} 
			\underline{Claim}: 
			\begin{align*}
			\boxed{\lag_{ij} =  \E_{i} \circ \lag\bigg\vert_{\W_j} : \W_j \lin \W_i}
			\end{align*}
			
			Consider applying $\lag$ to $\begin{pmatrix}
			w_1 & \mathbf{0}_\V & \mathbf{0}_\V
			\end{pmatrix}^\top_\maltese \in \V$, we have
			\begin{align*}
			\lag\begin{pmatrix}
			w_1 \\ \mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}_\maltese = \begin{bmatrix}
			\lag_{11} & \lag_{12} & \lag_{13}\\
			\lag_{21} & \lag_{22} & \lag_{23}\\
			\lag_{31} & \lag_{32} & \lag_{33}
			\end{bmatrix}_\oplus
			\begin{pmatrix}
			w_1 \\ \mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}_\maltese
			=
			\begin{pmatrix}
			\lag_{11}(w_1)\\
			\lag_{21}(w_1)\\
			\lag_{31}(w_1)
			\end{pmatrix}_\maltese.
			\end{align*}
			To extract an element $\lag_{i1}$, we can apply $\E_i$ to this result:
			\begin{align*}
			\E_i \circ \lag \begin{pmatrix}
			w_1 \\ \mathbf{0}_\V \\ \mathbf{0}_\V
			\end{pmatrix}_\maltese = \E_i \begin{pmatrix}
			\lag_{11}(w_1)\\
			\lag_{21}(w_1)\\
			\lag_{31}(w_1)
			\end{pmatrix}_\maltese = \lag_{i1}(w_1).
			\end{align*}
			This shows that by considering the restricted map $\lag\bigg\vert_{\W_1}$, we are able to obtain the $\lag_{{i1}}$ element of $\lag$ by composing the idempotent $\E_i$ with $\lag\bigg\vert_{\W_1}$. So in general, if we consider the restricted map $\lag\bigg\vert_{\W_j}$, the same procedure will give us the element $\lag_{ij}$.  Hence, we have verified the claim that
			\begin{align*}
			\lag_{{ij}} = \E_i \circ \lag\bigg\vert_{\W_j}.
			\end{align*}
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			
			\newpage
			
			
			
			
			
			
			
			\item Let
			\begin{align*}
			\mathcal{E}_1 = \begin{bmatrix}
			\M_{11} & \M_{12} & \M_{13}\\
			\M_{21} & \M_{22} & \M_{23}\\
			\M_{31} & \M_{32} & \M_{33}
			\end{bmatrix}_\oplus.
			\end{align*}
			be given, where $\ima(\E_i) = \W_i$ and $\E_1 + \E_2 + \E_3 = \mathcal{I}_\V$ is a resolution of identity. Continuing with the set-up of part 4, we have
			\begin{align*}
			\M_{ij} &= \E_i \circ \E_1\bigg\vert_{\W_j}.
			\end{align*}
			Since $\E_1 + \E_2 + \E_3 = \mathcal{I}_\V$ is a resolution of identity, and $\ima(\E_i) = \W_i$ where $\W_1\oplus\W_2\oplus\W_3 = \V$, $\ker(\E_1) = \W_2 \oplus \W_3$. This implies the restricted map $\E_1\bigg\vert_{\W_j}$ gives $\mathbf{0}_\V$ for $j\neq 1$ and acts as an identity on $\W_j$ if $j = 1$. Therefore, for $j \neq 1$, $M_{ij}$ is a zero function.\\
			
			If $j = 1$, then 
			\begin{align*}
			\M_{ij} = \E_i \circ \E_1 \bigg\vert_{\W_1} = \E_i \bigg\vert_{\W_1},
			\end{align*} 
			since $\E_1$ acts as identity on $\W_1$. The same argument for the restricted map $\E_i\bigg\vert_{\W_1}$ shows that $\M_{ij}$ is the zero function if $i\neq 1$ and the identity on $\W_1$ if $i=1$. Therefore,
			\begin{align*}
			\text{For }\E_1:\hspace{0.5cm}
			\M_{ij} = \begin{cases}
			Id_{\W_1},\hspace{0.6cm}   i,j=1\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 1
			\end{cases}
			\end{align*}
			The exact same procedure can be used to find the elements $M_{ij}$ of $\E_2$ and $\E_3$, simply replacing all 1's with 2's and 3's. So,
			\begin{align*}
			&\text{For }\E_2:\hspace{0.5cm}\M_{ij} = 
			\begin{cases}
			Id_{\W_2},\hspace{0.6cm}   i,j=2\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 3
			\end{cases}\\
			&\text{For }\E_3:\hspace{0.5cm}\M_{ij} = 
			\begin{cases}
			Id_{\W_3},\hspace{0.6cm}   i,j=3\\
			\mathcal{O},\hspace{1.2cm}    i\text{ or }j\neq 3
			\end{cases}
			\end{align*}
			
			
			
			
			
			
			
			
			
			
		
			
		\end{enumerate} 
		
	\end{sln*}
	
\end{prob*}






\newpage



\begin{prob*}\textbf{3.}
	\begin{enumerate}
		\item Using the set-up of Definition 0.7, verify the identity
		\begin{align*}
		[\mathcal{T(X)}]_\Omega = [\mathcal{T}]_{\Omega \leftarrow \Gamma}[\mathcal{X}]_\Gamma.
		\end{align*}
		
		
		
		\item Use part 1 to prove that 
		\begin{align*}
		[\mathcal{T}]_{\Omega\leftarrow\Gamma} = \begin{bmatrix}
		[\mathcal{T}(v_1)]_\Omega & [\mathcal{T}(v_2)]_\Omega & \dots & [\mathcal{T}(v_n)]_\Omega  
		\end{bmatrix}.
		\end{align*}
		
		
		
		\item If $\U$ is a finite-dimensional vector space with a coordinate system $\Delta$, and $\mathcal{S} : \Z \overset{\text{linear}}{\longrightarrow} \U$, argue that 
		\begin{align*}
		[\mathcal{S}\circ \mathcal{T}]_{\Delta \leftarrow \Gamma} = [\mathcal{S}]_{\Delta \leftarrow \Omega}[\mathcal{T}]_{\Omega\leftarrow\Gamma}.
		\end{align*}
		
		
		\item Verify that in the case $\mathcal{T}$ is invertible (in which case $m=n$), so is $[\mathcal{T}]_{\Omega\leftarrow \Gamma}$, and 
		\begin{align*}
		\begin{bmatrix}
		\mathcal{T}^{-1}
		\end{bmatrix}_{\Gamma\leftarrow\Omega} = [\mathcal{T}]^{-1}_{\Omega\leftarrow \Gamma}.
		\end{align*}
	\end{enumerate}

	
	
	
	\begin{sln*}\textbf{3.}
		\begin{enumerate}
			
			
			\item By the set-up in Definition 0.3, we have $[\T]_{\Omega\leftarrow \Gamma} = [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma$, and $[\mathcal{X}]_\Gamma = [\,\,\,]_\Gamma(\mathcal{X})$. Hence,
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma}[\mathcal{X}]_\Gamma 
			&= [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma \circ [\mathcal{X}]_\Gamma \\
			&= [\,\,\,]_\Omega\circ\T\circ \mathcal{A}_\Gamma \circ [\,\,\,]_\Gamma(\mathcal{X})\\
			&= [\,\,\,]_\Omega\circ\T\circ \left(\mathcal{A}_\Gamma \circ [\,\,\,]_\Gamma\right)(\mathcal{X})\\
			&= [\,\,\,]_\Omega\circ\T(\mathcal{X})\\
			&= [\,\,\,]_\Omega\left(\T(\mathcal{X})\right)\\
			&= [\mathcal{T}(\mathcal{X})]_\Omega.
			\end{align*}
			
			
			
			
			\item \underline{Claim}:
			\begin{align*}
			[\T]_{\Omega\leftarrow\Gamma} = \begin{bmatrix}
			[\T(v_1)]_\Omega & [\T(v_2)]_\Omega & \dots & [\T(v_n)]_\Omega
			\end{bmatrix}.
			\end{align*}
			Let $\mathcal{X}$ be $v_i \in \{v_1,v_2,\dots,v_n\}$ a coordinate system of $\Gamma$. Then we can create a bijective atrix 
			\begin{align*}
			A_\Gamma = \begin{bmatrix}
			v_1 & v_2 & \dots & v_n
			\end{bmatrix}
			\end{align*}
			whose inverse is $[\,\,\,]_\Gamma$. To find the columns of the matrix $[\T]_{\Omega\leftarrow\Gamma}$, we want to apply $[\T]_{\Omega\leftarrow\Gamma}$ to the standard basis vectors. To generate these standard basis vectors, we simply coordinatize $\mathcal{X}$:
			\begin{align*}
			[\mathcal{X}]_\Gamma = [v_i]_\Gamma = [\,\,\,]_\Gamma(v_i)=A^{-1}_\Gamma(v_i) = \begin{bmatrix}
			v_1&v_2&\dots&v_n
			\end{bmatrix}^{-1}(v_i) =  \vec{e}_i \in \mathbb{C}^n.
			\end{align*} 
			 
			Thus, the $i^{th}$ column of $[\T]_{\Omega\leftarrow \Gamma}$ is given by 
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma}(\vec{e}_i) = [\T]_{\Omega\leftarrow \Gamma}[v_i]_\Gamma = [\T(v_i)]_\Gamma, \hspace{0.2cm}\text{by Part 1}. 
			\end{align*}
			This verifies the claim.\\






 			
 			
 			
 			
 			\item Let the finite-dimensional vector space $\U$ with coordinate system $\Delta$ be given. Let $\mathcal{S}:\Z \lin \U$ be given. \underline{Claim}:
			\begin{align*}
 			[\mathcal{S} \circ \T]_{\Delta\leftarrow\Gamma} = [\mathcal{S}]_{\Delta\leftarrow\Omega}[\T]_{\Omega\leftarrow\Gamma}.
 			\end{align*}
 			
 			Let $\Delta = (w_1,w_2,\dots,w_l)$ be a coordinate system for $\U$. We also define the bijective atrix
 			\begin{align*}
 			A_\Delta := \begin{bmatrix}
 			w_1&w_2&\dots&w_l
 			\end{bmatrix} : \mathbb{C}^l \rightarrow \U.
 			\end{align*}
 			Since $\mathcal{A}_\Delta$ s invertible, we can write $[\,\,\,]_\Delta$ for $\mathcal{A}^{-1}_\Delta$. \\
 			
			By the set-up in definition 0.7, we have the following
			\begin{align*}
			[\T]_{\Omega\leftarrow \Gamma} &= [\,\,\,]_\Omega\circ \T\circ \mathcal{A}_\Gamma\\
			[\mathcal{S}]_{\Delta\leftarrow\Omega} &= [\,\,\,]_\Delta \circ \mathcal{S} \circ \mathcal{A}_\Omega.			
			\end{align*}
 			It follows that
 			\begin{align*}
 			[\mathcal{S}]_{\Delta \leftarrow \Omega}[\mathcal{T}]_{\Omega\leftarrow\Gamma}
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \mathcal{A}_\Omega \circ [\,\,\,]_\Omega\circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \left( \mathcal{A}_\Omega \circ [\,\,\,]_\Omega \right) \circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \mathcal{S} \circ \T\circ \mathcal{A}_\Gamma\\
			&= [\,\,\,]_\Delta \circ \left(\mathcal{S} \circ \T \right) \circ \mathcal{A}_\Gamma\\
			&= [\mathcal{S}\circ \T]_{\Delta \leftarrow \Gamma},
 			\end{align*}
			where we have used the fact that $A_\Omega [\,\,\,]_\Omega = A_\Omega A_\Omega^{-1}$ is the identity function. This completes the argument. \\
			
			
			
			
			
			\item 
			\begin{enumerate}
			\item \underline{To show}: $[\T]_{\Omega\leftarrow\Gamma}$ is invertible when $\T$ is invertible.\\
				
			The matrix $[\T]_{\Omega\leftarrow\Gamma}$ can be expressed as 
			\begin{align*}
			[\T]_{\Omega\leftarrow\Gamma} = [\,\,\,]_{\Omega}\circ \T \circ \mathcal{A}_{\Gamma}.
			\end{align*}
			Since $T$, $[\,\,\,]_\Omega$, and $\mathcal{A}_\Gamma$ are invertible, the above composition is also invertible, i.e., $[\T]_{\Omega\leftarrow\Gamma}$ is invertible. \\
			
			\item Let $\T$ be invertible (in which case $m=n$). \underline{Claim}:
			\begin{align*}
			\begin{bmatrix}
			\mathcal{T}^{-1}
			\end{bmatrix}_{\Gamma\leftarrow\Omega} = [\mathcal{T}]^{-1}_{\Omega\leftarrow \Gamma}.
			\end{align*}
			
			Since $\T$, $\mathcal{A}_{\Omega}$, $\mathcal{A}_\Gamma$ are all invertible, their inverses are defined. By the set-up in definition 0.7, the matrix $[\T]_{\Omega\leftarrow\Gamma}$ can be written as the composition $[\,\,\,]_\Omega \circ \T \circ A_{\Gamma}$ for the linear function $T : \V \lin \Z$. Since $\T^{-1} : \Z \lin \V$ is defined, we can express its matrix $[\T^{-1}]_{\Gamma\leftarrow\Omega}$ as
			\begin{align*}
			[\T^{-1}]_{\Gamma\leftarrow\Omega} = [\,\,\,]_\Gamma\circ \T^{-1} \circ \mathcal{A}_\Omega.\hspace{0.2cm}(\dagger)
			\end{align*}
			Consider the inverse of the matrix $[\T]_{\Omega\leftarrow\Gamma}$:
			\begin{align*}
			[\T]^{-1}_{\Omega\leftarrow\Gamma} &= \left([\,\,\,]_\Omega\circ \T \circ \mathcal{A}_\Gamma \right)^{-1}\\
			&= \mathcal{A}^{-1}_\Gamma \circ \left( [\,\,\,]_\Gamma \circ \T \right)^{-1}\\
			&= \mathcal{A}^{-1}_\Gamma \circ \T^{-1} \circ [\,\,\,]_\Omega^{-1}\\
			&= [\,\,\,]_\Gamma \circ \T^{-1} \circ \mathcal{A}_\Omega.\hspace{0.2cm}(\dagger\dagger)
			\end{align*}
			From ($\dagger$) and $(\dagger\dagger)$, we have
			\begin{align*}
				[\T^{-1}]_{\Gamma\leftarrow\Omega} = [\T]^{-1}_{\Omega\leftarrow\Gamma}
			\end{align*}
			as claimed.
			
			\end{enumerate}
			
		\end{enumerate}
	\end{sln*}




\end{prob*}

\newpage














\begin{prob*}\textbf{4.} Let us write $\Gamma_0$ for the standard coordinate system $(1,x,x^2,x^3)$ of the vector space $\mathbb{P}_3$ of all polynomials of degree at most 3. 
	\begin{enumerate}
		\item Use \textit{Mathematica} and the fact that $\begin{bmatrix}\,&\,\end{bmatrix}_{\Gamma_0}$ is an isomorphism (and isomorphisms map bases to bases) to verify that 
		\begin{align*}
		\Delta := (1 + x + x^2 &+ x^3, 1 + 2x+4x^2 + 8x^3,\\
		&1 + 3x+9x^2 + 27x^3, 1+ 4x +16x^2 + 64x^3)
		\end{align*}
		and 
		\begin{align*}
		\Omega := (x+x^2, x-6x^3, 1 + 4x^2, 1+ 8x^3) 
		\end{align*}
		are also coordinate systems of $\mathbb{P}_3$.
	


	\item Verify that general identity
	\begin{align*}
	[\mathcal{T}]_{\Delta \leftarrow \Omega} &= 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Delta \leftarrow \Gamma_0} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}\\
	&= \left(\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
	\begin{bmatrix}
	\mathcal{I}_{\mathbb{P}_3}
	\end{bmatrix}_{\Gamma_0 \leftarrow \Omega},
	\end{align*}
	for $\mathcal{T} : \mathbb{P}_3 \overset{\text{linear}}{\longrightarrow} \mathbb{P}_3$.
	
	
	\item Consider $\mathcal{T} : \mathbb{P}_3 \rightarrow \mathbb{P}_3$ defined by 
	\begin{align*}
	\mathcal{T}(p(x)) := xp'(2x) + p(x).
	\end{align*}
	Prove that $\mathcal{T}$ is a linear function and use part 2 to find $[\mathcal{T}]_{\Delta\leftarrow \Omega}$.
	
	
	\end{enumerate}



	\begin{sln*}\textbf{4.}
		$\,$
		\begin{enumerate}
			\item 
			\begin{enumerate}
				\item Let 
				\begin{align*}
				\Delta := (1 + x + x^2 &+ x^3, 1 + 2x+4x^2 + 8x^3,\\
				&1 + 3x+9x^2 + 27x^3, 1+ 4x +16x^2 + 64x^3)
				\end{align*}
				be given. To show that $\Delta$ is a coordinate system of $\V$, it suffices to show there exists an isomorphism $[\mathcal{II}]_{{\Gamma_0}\leftarrow\Delta} : \mathbb{C}^4_{\Delta_0}\to\mathbb{C}^4_{\Gamma_0}$. \\
				
				We can construct $[\mathcal{II}]_{{\Gamma_0}\leftarrow\Delta}$ column-by-column by letting $[\,\,\,]_{\Gamma_0}$ act on the elements of $\Delta$, by Part 2 of Problem 3:
				
				\begin{align*}
				[\mathcal{II}]_{\Gamma_0\leftarrow\Delta}
				&= \begin{bmatrix}
				[(1+x+x^2+x^3)]_{\Gamma_0}\\
				[(1+2x+4x^2+8x^3)]_{\Gamma_0} \\ 
				[(1+3x+9x^2+27x^3)]_{\Gamma_0} \\ 
				[(1+4x+16x^2+64x^3)]_{\Gamma_0}
				\end{bmatrix}^\top
				= \begin{pmatrix}
				1&1&1&1\\
				1&2&3&4\\
				1&4&9&16\\
				1&8&27&64
				\end{pmatrix}.
				\end{align*} 
				To show $[\mathcal{II}]_{\Gamma_0\leftarrow\Delta}$ is an isomorphism, it suffices to check if its determinant is nonzero, which can be done in \textit{Mathematica}:
				\begin{align*}
				\det\left([\mathcal{II}]_{\Gamma_0\leftarrow\Delta}\right) = 12 \neq 0.
				\end{align*}
				Therefore, $\Delta$ is a coordinate system of $\mathbb{P}_3$.\\
				
				\textit{Mathematica} code:
		\begin{lstlisting}
In[4]:= Det[{{1, 1, 1, 1}, {1, 2, 3, 4}, {1, 4, 9, 16}, 
						{1, 8, 27, 64}}]
				
Out[4]= 12
		\end{lstlisting}
		
		
		
		
		
		
				\item For $\Omega := (x+x^2,x-6x^3,1+4x^2,1+8x^3)$, we repeat the procedure laid out above to determine whether $\Omega$ is a coordinate system for $\mathbb{P}_3$. 
				\begin{align*}
				[\mathcal{II}]_{\Gamma_0\leftarrow\Omega}
				= \begin{bmatrix}
				[(x+x^2)]_{\Gamma_0} \\ [(x-6x^3)]_{\Gamma_0} \\ [(1+4x^2)]_{\Gamma_0} \\ [(1+8x^3)]_{\Gamma_0}
				\end{bmatrix}^\top
				= \begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}. 
				\end{align*} 
				We can calculate the determinant of $[\mathcal{II}]_{\Gamma_0\leftarrow\Omega}$ in \textit{Mathematica}:
				\begin{align*}
				\det\left( [\mathcal{II}]_{\Gamma_0\leftarrow\Omega}  \right) = -32 \neq 0.
				\end{align*}
				Therefore, $\Omega$ is a coordinate system of $\mathbb{P}_3$.\\
				
				\textit{Mathematica} code:
				\begin{lstlisting}
In[6]:= Det[{{0, 0, 1, 1}, {1, 1, 0, 0},
						 {1, 0, 4, 0}, {0, -6, 0, 8}}]
				
Out[6]= -32
				\end{lstlisting}
				
			\end{enumerate}
		
		
		
			\item Let $\T : \mathbb{P}_3 \lin \mathbb{P}_3$ be given. 
			\begin{enumerate}
				\item First identity:
				\begin{align*}
				&\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Delta \leftarrow \Gamma_0} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}	\\
				=\,\,
				&\left([\,\,\,]_{\Delta}\circ\mathcal{I}_{\mathbb{P}_3} \circ \mathcal{A}_{\Gamma_0}\right) \circ \left([\,\,\,]_{\Gamma_0}\circ \T \circ \mathcal{A}_{\Gamma_0}\right) \circ \left([\,\,\,]_{\Gamma_0} \circ \mathcal{I}_{\mathbb{P}_3} \circ \mathcal{A}_{\Omega}\right) \\
				=\,\,
				&[\,\,\,]_\Delta \circ \mathcal{I}_{\mathbb{P}_3}\circ \T \circ \mathcal{I}_{\mathbb{P}_3}\circ \mathcal{A}_\Omega\\
				=\,\,
				&[\,\,\,]_\Delta \circ  \T \circ \mathcal{A}_\Omega\\
				=\,\,
				&[\T]_{\Delta\leftarrow\Omega}.
				\end{align*}
				
				\item The second identity follows from the fact that
				\begin{align*}
				\left([\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Delta}\right)^{-1}
				&=\,\,
				\left([\,\,\,]_{\Gamma_0}\circ \mathcal{I}_{\mathbb{P}_3} \circ \mathcal{A}_{\Delta}\right)^{-1}\\
				&=\,\,
				[\,\,\,]_\Delta \circ \mathcal{I}^{-1}_{\mathbb{P}_3} \circ \mathcal{A}_{\Gamma_0}\\
				&=\,\,
				[\,\,\,]_\Delta \circ \mathcal{I}_{\mathbb{P}_3} \circ \mathcal{A}_{\Gamma_0}.
				\end{align*}
				So we have
				\begin{align*}
				[\T]_{\Delta\leftarrow\Omega} &=
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Delta \leftarrow \Gamma_0} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}\\
				&= \left([\,\,\,]_{\Delta}\circ\mathcal{I}_{\mathbb{P}_3} \circ \mathcal{A}_{\Gamma_0}\right)\circ[\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}\\ &= \left([\mathcal{I}_{\mathbb{P}_3}]_{\Gamma_0\leftarrow\Delta}\right)^{-1}[\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}
				\end{align*}
			\end{enumerate}
		
			
			
			
			
			
			
			
			
			
			\item Let $\T : \mathbb{P}_3 \to \mathbb{P}_3$ be given. $T$ is defined as
			\begin{align*}
			\T(p(x)) := xp'(2x) + p(x).
			\end{align*}
			\begin{enumerate}
				\item \underline{Claim}: $\T$ is a linear function.\\
				$\T$ is a linear function if it satisfies the linearity conditions. Consider $p(x), q(x) \in \mathbb{P}_3$. Then we have
				\begin{align*}
				\T(p(x) + q(x)) &= x(p(2x) + q(2x))' + (p(x) + q(x))\\
				&= xp'(2x) + xq'(2x) + p(x) + q(x)\\
				&= (xp'(2x) + p(x))  + (xq'(2x) + q(x))\\ 
				&= \T(p(x)) + \T(q(x)).\hspace{0.2cm}(\dagger)
				\end{align*}
				Consider $c\in \mathbb{C}$ and $p(x)\in \mathbb{P}_3$,
				\begin{align*}
				\T(cp(x)) &= x(cp(2x))' + cp(x)\\
				&= cxp'(2x) + cp(x)\\ &= c(xp'(2x) + p(x))\\ &= c\T(p(x)).\hspace{0.2cm}(\dagger\dagger)
				\end{align*}
				From ($\dagger$) and ($\dagger$), $\T$ is a linear function. \\
				
				
				
				
				\item \underline{Find} $[\T]_{\Delta\leftarrow\Omega}$.\\
				
				By part 2 of Problem 3
				\begin{align*}
				[\mathcal{T}]_{\Delta \leftarrow \Omega} &= 
				\left(\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} [\mathcal{T}]_{\Gamma_0 \leftarrow \Gamma_0} 
				\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}.
				\end{align*}
				In part 1, we have found the isomorphisms:
				\begin{align*}
				&\left(\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Delta}\right)^{-1} = \begin{pmatrix}
				1&1&1&1\\
				1&2&3&4\\
				1&4&9&16\\
				1&8&27&64
				\end{pmatrix}^{-1} = 
				\begin{pmatrix}
				4&-13/3&3/2&-1/6\\
				-6&19/2&-4&1/2\\
				4&-7&7/2&-1/2\\
				-1&11/6&-1&1/6
				\end{pmatrix}\\
				&\begin{bmatrix}
				\mathcal{I}_{\mathbb{P}_3}
				\end{bmatrix}_{\Gamma_0 \leftarrow \Omega}
				= \begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}.
				\end{align*}
				To find $[\T]_{\Delta\leftarrow\Omega}$ we only need to find $[\T]_{\Gamma_0\leftarrow\Gamma_0}$. To do this, we use part 2 of Problem 3, which says,
				\begin{align*}
				[\T]_{\Gamma_0\leftarrow\Gamma_0} &= \begin{bmatrix}
				[\T(1)]_{\Gamma_0} & [\T(x)]_{\Gamma_0} & [\T(x^2)]_{\Gamma_0} & [\T(x^3)]_{\Gamma_0}
				\end{bmatrix},
				\end{align*}
				where
				\begin{align*}
				&\T(1) = x\frac{d}{dx}2 + 1 = 1\\
				&\T(x) = x\frac{d}{dx}(2x) + x = 3x\\
				&\T(x^2) = x\frac{d}{dx}(2x)^2 + x^2 = 9x^2\\
				&\T(x^3) = x\frac{d}{dx}(2x)^3 + x^3 = 25x^3.
				\end{align*}
				Therefore,
				\begin{align*}
				[\T]_{\Gamma_0\leftarrow\Gamma_0} &= \begin{bmatrix}
				[T(1)]_{\Gamma_0} & [T(x)]_{\Gamma_0} & [T(x^2)]_{\Gamma_0} & [T(x^3)]_{\Gamma_0}
				\end{bmatrix}\\
				&= \begin{pmatrix}
				1&0&0&0\\
				0&3&0&0\\
				0&0&9&0\\
				0&0&0&25
				\end{pmatrix}.
				\end{align*}
				
				Finally, we can compute $[\T]_{\Delta\leftarrow\Omega}$:
				\begin{align*}
				[\T]_{\Delta\leftarrow\Omega} &=     
				\begin{pmatrix}
				4&-13/3&3/2&-1/6\\
				-6&19/2&-4&1/2\\
				4&-7&7/2&-1/2\\
				-1&11/6&-1&1/6
				\end{pmatrix}
				\begin{pmatrix}
				1&0&0&0\\
				0&3&0&0\\
				0&0&9&0\\
				0&0&0&25
				\end{pmatrix}
				\begin{pmatrix}
				0&0&1&1\\
				1&1&0&0\\
				1&0&4&0\\
				0&-6&0&8
				\end{pmatrix}\\
				&= \begin{pmatrix}
				1/2&12&58&-88/3\\
				-15/2& -93/2& -150& 94\\
				21/& 54& 130& -96\\
				-7/2& -39/2& -37& 97/3
				\end{pmatrix}.
				\end{align*}
				
				\textit{Mathematica code:}
				\begin{lstlisting}
A = 
Inverse[{{1, 1, 1, 1}, {1, 2, 3, 4}, {1, 4, 9, 16}, {1, 8, 27, 64}}]
				
{{4, -(13/3), 3/2, -(1/6)}, {-6, 19/2, -4, 1/2}, {4, -7, 7/
			2, -(1/2)}, {-1, 11/6, -1, 1/6}}
				
B = {{1, 0, 0, 0}, {0, 3, 0, 0}, {0, 0, 9, 0}, {0, 0, 0, 25}}
				
				
CC = {{0, 0, 1, 1}, {1, 1, 0, 0}, {1, 0, 4, 0}, {0, -6, 0, 8}}
						
In[7]:= A.B.CC
				
Out[7]= {{1/2, 12, 58, -(88/3)}, {-(15/2), -(93/2), -150, 94}, 
{21/2, 54, 130, -96}, {-(7/2), -(39/2), -37, 97/3}}
				\end{lstlisting}
			\end{enumerate}
		\end{enumerate}
		
	\end{sln*}






\end{prob*}

\newpage



\begin{prob*}\textbf{5.} Suppose that $\V = \W_1 \oplus \W_2 \oplus \W_3$ and $\Gamma_i$ is a coordinate system of $\W_i$. Suppose that for each $i,j \in \{1,2,3 \}$, 
	\begin{align*}
	\lag_{ij} : \W_j \overset{\text{linear}}{\longrightarrow} \W_i.
	\end{align*}
	Let 
	\begin{align*}
	\lag := \begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\oplus.
	\end{align*}
	Let $\Delta$ be the concatenation $\Gamma_1 \vert\vert \Gamma_2\vert\vert \Gamma_3$ of the coordinate systems $\Gamma_i$. As we know, $\Delta$ is a coordinate system of $\V$. \\
	
	Prove that $[\lag]_{\Delta \leftarrow \Delta}$ equals the partitioned matrix
	\begin{align*}
	\begin{bmatrix}
	[\lag_{11}]_{\Gamma_1 \leftarrow \Gamma_1} & [\lag_{12}]_{\Gamma_1 \leftarrow \Gamma_2} & [\lag_{13}]_{\Gamma_1 \leftarrow \Gamma_3}\\
	[\lag_{21}]_{\Gamma_2 \leftarrow \Gamma_1} & [\lag_{22}]_{\Gamma_2 \leftarrow \Gamma_2} & [\lag_{23}]_{\Gamma_2 \leftarrow \Gamma_3}\\
	[\lag_{31}]_{\Gamma_3 \leftarrow \Gamma_1} & [\lag_{32}]_{\Gamma_3 \leftarrow \Gamma_2} & [\lag_{33}]_{\Gamma_3 \leftarrow \Gamma_3}
	\end{bmatrix}.
	\end{align*}
	 
	
	\begin{sln*}\textbf{5.} Consider a $3\times 3$ block-matrix, which consists of just the linear function $\lag_{ij} : \W_j \lin \W_i$ at the $i^{th}$ row and $j^{th}$ column and the zero function everywhere else. Let $[\lag_{ij}]_\oplus : \V \lin \V$ denote this matrix. By this construction, we have
		\begin{align*}
		\lag = \sum^3_{i=1}\sum^3_{j=1}[\lag_{ij}]_\oplus = \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus.
		\end{align*}
	
	It follows that the matrix $[\lag]_{\Delta\leftarrow\Delta}$ of $\lag$ can be written as
	\begin{align*}
	\begin{bmatrix}
	\lag
	\end{bmatrix}_{\Delta\leftarrow\Delta}
	&=
	\begin{bmatrix}
	\sum^3_{i=1}\sum^3_{j=1}[\lag_{ij}]_\oplus
	\end{bmatrix}_{\Delta\leftarrow\Delta}\\
	&=
	\sum^3_{i=1}\sum^3_{j=1}\begin{bmatrix}
	[\lag_{ij}]_\oplus
	\end{bmatrix}_{\Delta\leftarrow\Delta}
	\end{align*}
	
	Consider the coordinate system $\Gamma_k$ where $k\in \{1,2,3\}$. Let $\Gamma_k$ be formed by a (finite) collection of basis elements, called $g_{lk}$'s. Consider the mapping $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ and the matrix $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$. \\
	
	We observe that $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ is $\mathbf{0}_\V$ if $k\neq j$ and $\maltese\left([\lag_{ij}]_\oplus(g_{lj}) + \mathbf{0}_\V + \mathbf{0}_\V \right)$ if $j=k$, where $\maltese$ is defined in Definition 0.4. What this says is that $[\lag_{ij}]_\oplus(g_{lk})_\oplus$ gives a column vector with $\lag_{ij}(g_{lj})$ on the $i^{th}$ row and $\mathbf{0}_\V$ everywhere else. This means that if we consider the coordinatization $\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{lj})_\oplus
	\end{bmatrix}_\Delta$, where $\Delta = \Gamma_1||\Gamma_2||\Gamma_3$, we will end up with a column vector with the zero function everywhere except $\begin{bmatrix}[\lag_{ij}]_\oplus(g_{lj})\end{bmatrix}_{\Gamma_i}$ on the $i^{th}$ row. ($\dagger$) \\
	
	Now, we can construct $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$ column-by-column as
	\begin{align*}
	\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}
	&=
	\begin{bmatrix}
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{11})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{a1})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	&
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{12})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{b2})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	&
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{13})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{c3})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	\end{bmatrix}
	\end{align*}
	where $a,b,c$ are the number of elements in $\Gamma_1$, $\Gamma_2$, $\Gamma_3$, respectively. From $(\dagger)$, we know that two of three matrix-blocks in the expression above will be the zero-block because $j$ has a fixed value of either 1,2, or 3. Consider the only non-zero block (located at the $j^{th}$ column)
	\begin{align*}
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{1j})_\oplus
	\end{bmatrix}_\Delta
	\\\dots\\
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{aj})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}^\top
	=
	\begin{bmatrix}
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{1j})_\oplus
	\end{bmatrix}_\Delta
	&\dots&
	\begin{bmatrix}
	[\lag_{ij}]_\oplus(g_{aj})_\oplus
	\end{bmatrix}_\Delta
	\end{bmatrix}. \hspace{0.5cm}(\dagger\dagger)
	\end{align*}
	Recalling that $\begin{bmatrix}[\lag_{ij}]_\oplus(g_{lj})_\oplus
	\end{bmatrix}_\Delta$ is a column vector with $\lag_{ij}(g_{lj})$ on the $i^{th}$ row and $\mathbf{0}_\V$ everywhere else, it follows that we can represent this matrix block as one with two rows of zeros and an $i^{th}$ row of 
	\begin{align*}
	\begin{bmatrix}
	\lag_{ij}(g_{1j}) & \lag_{ij}(g_{1j}) & \dots & \lag_{ij}(g_{aj})
	\end{bmatrix},
	\end{align*}
	which precisely expresses the matrix $\begin{bmatrix}
	\lag_{ij}
	\end{bmatrix}_{\Gamma_i\leftarrow\Gamma_j}$. Therefore, we can treat the expression $(\dagger\dagger)$ as a 3-element column of the matrix $\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}$ whose element on the $i^{th}$ row and $j^{th}$ column is $\begin{bmatrix}
	\lag_{ij}
	\end{bmatrix}_{\Gamma_i\leftarrow\Gamma_j}$, and the zero function everywhere else. Hence,
	\begin{align*}
	\begin{bmatrix}
	\lag
	\end{bmatrix}_{\Delta\leftarrow\Delta} &= \sum^3_{i=1}\sum^3_{j=1}\begin{bmatrix}[\lag_{ij}]_\oplus\end{bmatrix}_{\Delta\leftarrow\Delta}\\
	&= \begin{bmatrix}
	\begin{bmatrix}
	\lag_{11}
	\end{bmatrix}_{\Gamma_1\leftarrow\Gamma_1}
	&
	\begin{bmatrix}
	\lag_{21}
	\end{bmatrix}_{\Gamma_2\leftarrow\Gamma_1}
	&
	\begin{bmatrix}
	\lag_{31}
	\end{bmatrix}_{\Gamma_3\leftarrow\Gamma_1}
	\\
	\begin{bmatrix}
	\lag_{12}
	\end{bmatrix}_{\Gamma_1\leftarrow\Gamma_2}
	&
	\begin{bmatrix}
	\lag_{22}
	\end{bmatrix}_{\Gamma_2\leftarrow\Gamma_2}
	&
	\begin{bmatrix}
	\lag_{32}
	\end{bmatrix}_{\Gamma_3\leftarrow\Gamma_2}
	\\
	\begin{bmatrix}
	\lag_{13}
	\end{bmatrix}_{\Gamma_1\leftarrow\Gamma_3}
	&
	\begin{bmatrix}
	\lag_{23}
	\end{bmatrix}_{\Gamma_2\leftarrow\Gamma_3}
	&
	\begin{bmatrix}
	\lag_{33}
	\end{bmatrix}_{\Gamma_3\leftarrow\Gamma_3}
	\end{bmatrix}.
	\end{align*}
	This completes the argument. \\
	
	An example can help justify this argument. Consider $[\lag_{11}]_\oplus$,
	\begin{align*}
	[\lag_{11}] = \begin{bmatrix}
	\lag_{11} & 0 & 0\\
	0 & 0 & 0\\
	0 & 0 & 0
	\end{bmatrix}_\oplus,
	\end{align*}
	where we are using the $0$'s to denote the zero functions $\W_i \to \W_j$. Then consider $g_{11}$ the first vector in the coordinate system $\Gamma_1$. We have
	\begin{align*}
	\begin{bmatrix}
	\lag_{11} & 0 & 0\\
	0 & 0 & 0\\
	0 & 0 & 0
	\end{bmatrix}_\oplus\begin{pmatrix}
	g_{11} \\ \mathbf{0}_\V \\ \mathbf{0}_\V
	\end{pmatrix}_\maltese = \begin{pmatrix}
	\lag_{11}(g_{11}) \\ \mathbf{0}_\V \\ \mathbf{0}_\V
	\end{pmatrix}_\maltese.
	\end{align*}
	Now consider $\left[[\lag_{11}]_\oplus\right]_{\Delta\leftarrow\Delta}$. This matrix can be constructed column-by-column with each column being $[\lag_{11}(g_{ij})]_{\Delta\leftarrow\Delta}$, where $j$ indicates from which $\Gamma_j$ $g_{ij}$ is taken. We observe that if $j$ is not 1, then $\lag_{11}(g_{ij}) = \mathbf{0}_\V$. This means the all columns of $\left[[\lag_{11}]_\oplus\right]_{\Delta\leftarrow\Delta}$ are the zero columns unless $j=1$. Consider the $j=1$ columns. We know that the coordinatization
	\begin{align*}
	\left[\begin{pmatrix}
	\lag_{11}(g_{11}) \\ \mathbf{0}_\V \\ \mathbf{0}_\V
	\end{pmatrix}_\maltese\right]_{\Delta\leftarrow\Delta}
	=
	\begin{pmatrix}
	[\lag_{11}(g_{11})]_{\Gamma_1} \\ \mathbf{0}_\V \\ \mathbf{0}_\V
	\end{pmatrix},
	\end{align*}
	because the $\lag_{11}(g_{11}) \in \W_1$. So it follows that the concatenation of the columns 
	\begin{align*}
	\begin{pmatrix}
	[\lag_{11}(g_{11})]_{\Gamma_1} &\dots & \lag_{11}(g_{a1})]_{\Gamma_1}
	\\ \mathcal{O} & \dots & \mathcal{O}
	\\ \mathcal{O} & \dots & \mathcal{O}
	\end{pmatrix}
	\end{align*}
	in the matrix $\left[[\lag_{11}]_\oplus\right]_{\Delta\leftarrow\Delta}$ is nothing but 
	\begin{align*}
	\begin{pmatrix}
	[\lag_{11}]_{\Gamma_1\leftarrow\Gamma_1} \\ \mathcal{O} \\ \mathcal{O}
	\end{pmatrix}.
	\end{align*}
	So we have
	\begin{align*}
	\left[[\lag_{11}]_\oplus\right]_{\Delta\leftarrow\Delta} = \begin{bmatrix}
	[\lag_{11}]_{\Gamma_1\leftarrow\Gamma_1} & \mathcal{O} & \mathcal{O}\\
	\mathcal{O} & \mathcal{O} & \mathcal{O}\\
	\mathcal{O} & \mathcal{O} & \mathcal{O}
	\end{bmatrix},
	\end{align*}
	where $\mathcal{O}$ denotes the zero function.\\
	
	We can equivalently generate other examples for $[\lag_{ij}]_\oplus$, $i,j\in \{1,2,3\}$, and in the end add up the results (with different combinations of $i,j$'s of course) to get $[\lag]_{\Delta\leftarrow\Delta}$, expressed in the form given in the problem statement as desired.  
	\end{sln*}
\end{prob*}







\newpage




\subsection{Problem set 4}



\begin{prob*}\textbf{1.} \\
	
	Suppose that $T \in \mathfrak{L}(\V)$ and
	\begin{align*}
	\ima(T)= \W + \Z,
	\end{align*}
	where $\W$ and $\Z$ are subspaces of $\V$. Argue that
	\begin{align*}
	\V = T^{-1}[\W] + T^{-1}[\Z].
	\end{align*}
	
	\begin{sln*}
		\textit{removed for corrections}
	\end{sln*}
	
%	\begin{sln*}
%		$\,$\\
%		
%		Since $\ima(T) = \W + \Z$, $T^{-1}[\W + \Z] = T^{-1}[\ima(T)] = \V$. Also,
%		\begin{align*}
%		T^{-1}[\W] &= \left\{ w \in \V \big\vert T(w) \in \W \right\} \subseteq \V\\
%		T^{-1}[\Z] &= \left\{ z \in \V \big\vert T(z) \in \Z \right\} \subseteq \V,
%		\end{align*}
%		which means
%		\begin{align*}
%		T^{-1}[\W] + T^{-1}[\Z] = \left\{ w+z \in \V \big\vert T(w)\in\W, T(z)\in \Z, w,z\in \V \right\}.
%		\end{align*}
%		It follows directly from this definition that 
%		\begin{align*}
%		T^{-1}[\W] + T^{-1}[\Z] \subseteq \V = T^{-1}[\W+\Z]\hspace{0.5cm}(\dagger).
%		\end{align*}
%		Consider $v\in T^{-1}[\W] + T^{-1}[\Z]$. $v$ can be written as $w+z$ where $T(w) \in \W$ and $T(z)\in \Z$, which implies
%		\begin{align*}
%		T(v) = T(w+z) = T(w) + T(z) \in \W + \Z,
%		\end{align*}
%		where the second equality follows from the linearity of $T$. Hence, 
%		\begin{align*}
%		v\in T^{-1}[\W + \Z]. \hspace{0.5cm} (\dagger\dagger)
%		\end{align*}
%		From ($\dagger$) and ($\dagger\dagger$), $\V = T^{-1}[\W] + T^{-1}[\Z]$ as desired. 
%	\end{sln*}
\end{prob*}


\newpage



\begin{prob*}\textbf{2. }\\
	
	\begin{enumerate}
	\item Suppose that $\V_1, \V_2, \V_3$ are non-trivial vector spaces, and for each $i,j \in \{1,2,3\}$,
	\begin{align*}
	\lag_{ij} : \V_j \lin \V_i.
	\end{align*}
	Let $\lag$ be the block-matrix function:
	\begin{align*}
	\begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\times : \V_1\times \V_2 \times \V_3 \lin \V_1\times \V_2\times \V_3.
	\end{align*}
	Suppose that it turns out that $\V_2 = \V_{2.1}\times \V_{2.2}$. Then $\lag$ maybe considered as a linear function on
	\begin{align*}
	\V_1 \times \V_{2.1}\times \V_{2.2} \times \V_3. 
	\end{align*}
	What is the corresponding block-matrix form of $\lag$ and how does it relate to $\begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\times$? Justify your claims.\\
	
	\item Suppose that $\W_1\oplus\W_2\oplus\W_3 = \V$ and the $\W_i$'s are non-trivial subspaces of $\V$. Suppose that for each $i,j\in \{1,2,3\}$:
	\begin{align*}
	\lag_{ij} : \W_j \lin \W_i.
	\end{align*} 
	Let $\lag$ be the block-matrix function
	\begin{align*}
	\begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\oplus : \V \lin \V.
	\end{align*}
	Suppose it turns out that $\W_1 = \W_{1.1}\oplus \W_{1.2}$. Then
	\begin{align*}
	\V = \W_{1.1}\oplus \W_{1.2} \oplus \W_2 \oplus \W_3.
	\end{align*}
	What is the block-matrix form of $\lag$ with respect to this direct sum decomposition and how does it relate to $\begin{bmatrix}
	\lag_{11} & \lag_{12} & \lag_{13}\\
	\lag_{21} & \lag_{22} & \lag_{23}\\
	\lag_{31} & \lag_{32} & \lag_{33}
	\end{bmatrix}_\oplus$? Justify your claims.\\	
	\end{enumerate}


\begin{sln*}
	$\,$
	\begin{enumerate}
		\item Recall from the previous problem set that the elements of the block matrix function 
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times : \V_1\times \V_2 \times \V_3 \lin \V_1\times \V_2\times \V_3.
		\end{align*}
		can be expressed as
		\begin{align*}
		\lag_{ij} = \Pi_i \circ \lag \circ \gamma_j,
		\end{align*}
		where the $\Pi_i$'s and $\gamma_j$'s are coordinate projections from $\V$ onto $\V_i$ and injections from $\V_j$ into $\V$. Let $\V_2 = \V_{2.1} \times \V_{2.2}$. We claim that the matrix-block form of $\lag$ in this case is
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{1|2.1} & \lag_{1|2.2} & \lag_{13}\\
		\lag_{2.1|1} & \lag_{2.1|2.1} & \lag_{2.1|2.2} & \lag_{2.1|3}\\
		\lag_{2.2|1} & \lag_{2.2|2.1} & \lag_{2.2|2.2} & \lag_{2.2|3}\\
		\lag_{31} & \lag_{3|2.1} & \lag_{3|2.2} & \lag_{33}
		\end{bmatrix}_\times.
		\end{align*}
		In particular, we claim the following relations between the matrix elements $\lag_{ij}$ in the former expression and the block-elements in the latter expression: 
		\begin{align*}
		\lag_{12} = \begin{bmatrix}
		\lag_{1|2.1}&\lag_{1|2.2}
		\end{bmatrix}, \lag_{32} = \begin{bmatrix}
		\lag_{3|2.1}&\lag_{3|2.2}
		\end{bmatrix}, 
		\end{align*}
		\begin{align*}
		\lag_{21} = \begin{bmatrix}
		\lag_{2.1|1}\\ \lag_{2.2|1}
		\end{bmatrix}, \lag_{23} = \begin{bmatrix}
		\lag_{2.1|3}\\\lag_{2.2|3}
		\end{bmatrix},
		\lag_{22} = \begin{bmatrix}
		\lag_{2.1|2.1} & \lag_{2.1|2.2}\\
		\lag_{2.2|2.1} & \lag_{2.2|2.2}
		\end{bmatrix}.
		\end{align*}
		We can mimic what we have done to define $\lag_{ij}$ to define these new block-matrix elements. Let $\delta_{n.m}$ be defined as the coordinate injection from $\V_{n.m}$ into $\V_2$, $n,m \in \{1,2\}$. Also, define $\rho_{n.m}$ as the coordinate projection from $\V$ onto $\V_{n.m}$, $n,m\in \{1,2\}$. Consider the linear function $\lag_{22} : \V_2 \to \V_2$, which we claim to be
		\begin{align*}
		\lag_{22} = \begin{bmatrix}
		\lag_{2.1|2.1} & \lag_{2.1|2.2}\\
		\lag_{2.2|2.1} & \lag_{2.2|2.2}
		\end{bmatrix}.
		\end{align*}
		By exactly the same way we have defined the $\lag_{ij}$'s of $\lag$ using the coordinate projections and injections, we have
		\begin{align*}
		\lag_{2.1|2.1} = \rho_{2.1}\circ \lag_{22} \circ \delta_{2.1}\\
		\lag_{2.2|2.1} = \rho_{2.2}\circ \lag_{22} \circ \delta_{2.1}\\
		\lag_{2.1|2.2} = \rho_{2.1}\circ \lag_{22} \circ \delta_{2.2}\\
		\lag_{2.2|2.2} = \rho_{2.2}\circ \lag_{22} \circ \delta_{2.2}
		\end{align*}
		In terms of the linear function $\lag$,
		\begin{align*}
		&\lag_{2.1|2.1} = \rho_{2.1}\circ \Pi_2 \circ \lag \circ \gamma_2 \circ \delta_{2.1}\\
		&\lag_{2.2|2.1} = \rho_{2.2}\circ \Pi_2 \circ \lag \circ \gamma_2 \circ \delta_{2.1}\\
		&\lag_{2.1|2.2} = \rho_{2.1}\circ \Pi_2 \circ \lag \circ \gamma_2 \circ \delta_{2.2}\\
		&\lag_{2.2|2.2} = \rho_{2.2}\circ \Pi_2 \circ \lag \circ \gamma_2 \circ \delta_{2.2}.
		\end{align*}
		Next, consider $\lag_{i2} = \Pi_i \circ \lag \circ \gamma_2$. We define
		\begin{align*}
		&\lag_{i|2.1} = \Pi_i \circ \lag \circ \gamma_2 \circ \delta_{2.1}\\
		&\lag_{i|2.2} = \Pi_i \circ \lag \circ \gamma_2 \circ \delta_{2.2},
		\end{align*} 
		for $i\in \{1,3\}$. Finally, consider $\lag_{2j} = \Pi_2 \circ \lag \circ \gamma_j$. We define
		\begin{align*}
		&\lag_{2.1|j} = \rho_{2.1} \circ \Pi_2 \circ \lag \circ \gamma_j\\
		&\lag_{2.2|j} = \rho_{2.2} \circ \Pi_2 \circ \lag \circ \gamma_j,
		\end{align*}
		for $j \in \{1,3\}$. \\
		
		We can now verify that $\lag$ can be represented as the 4-block-by-4-block as above. Consider
		\begin{align*}
		v = \begin{pmatrix}
		a \\ \begin{pmatrix}
		b\\c
		\end{pmatrix}\\
		d
		\end{pmatrix} \in \V_1 \times \V_2 \times \V_3
		\end{align*}
		with $\V_2 = \V_{2.1}\times \V_{2.2}$. $v$ can be written as
		\begin{align*}
		v = \begin{pmatrix}
		a \\ \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}\\
		\mathbf{0}
		\end{pmatrix} +
		\begin{pmatrix}
		\mathbf{0} \\ \begin{pmatrix}
		b\\\mathbf{0}
		\end{pmatrix}\\
		\mathbf{0}
		\end{pmatrix} + 
		\begin{pmatrix}
		\mathbf{0} \\ \begin{pmatrix}
		\mathbf{0} \\ c
		\end{pmatrix}\\
		\mathbf{0}
		\end{pmatrix} + 
		\begin{pmatrix}
		\mathbf{0} \\ \begin{pmatrix}
		\mathbf{0} \\ \mathbf{0}
		\end{pmatrix}\\
		d
		\end{pmatrix}.
		\end{align*}
		
		We can check that $\lag(v)$ is the same as applying the 4-block-by-4-block matrix expression to $v$ by going through every term in the above expression of $v$. For example, we can consider $\begin{pmatrix}
		a & \begin{pmatrix}
		\mathbf{0} & \mathbf{0}
		\end{pmatrix}^\top & \mathbf{0}
		\end{pmatrix}^\top$. Then
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{1|2.1} & \lag_{1|2.2} & \lag_{13}\\
		\lag_{2.1|1} & \lag_{2.1|2.1} & \lag_{2.1|2.2} & \lag_{2.1|3}\\
		\lag_{2.2|1} & \lag_{2.2|2.1} & \lag_{2.2|2.2} & \lag_{2.2|3}\\
		\lag_{31} & \lag_{3|2.1} & \lag_{3|2.2} & \lag_{33}
		\end{bmatrix}_\times \begin{pmatrix}
		a \\ \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}\\
		\mathbf{0}
		\end{pmatrix} = \begin{pmatrix}
		\lag_{11}(a)\\
		\begin{pmatrix}
		\lag_{2.1|1}(a)\\
		\lag_{2.2|1}(a)
		\end{pmatrix}\\
		\lag_{31}(a)
		\end{pmatrix}.
		\end{align*}
		We can look at 
		\begin{align*}
		\begin{pmatrix}
		\lag_{2.1|1}(a)\\
		\lag_{2.2|1}(a)
		\end{pmatrix} &= 
		\begin{pmatrix}
		\rho_{2.1} \circ  \lag \circ \gamma_1(a)\\
		\rho_{2.2} \circ  \lag \circ \gamma_1(a)
		\end{pmatrix}\\
		&=
		\begin{pmatrix}
		\rho_{2.1} \circ \Pi_2 \circ \lag_{21}(a)\\
		\rho_{2.2} \circ \Pi_2 \circ \lag_{21}(a)
		\end{pmatrix}\\
		&= \lag_{21}(a), \hspace{0.5cm}\text{by the construction of }\rho_{2.1}, \rho_{2.2}.
		\end{align*}
		Hence
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{1|2.1} & \lag_{1|2.2} & \lag_{13}\\
		\lag_{2.1|1} & \lag_{2.1|2.1} & \lag_{2.1|2.2} & \lag_{2.1|3}\\
		\lag_{2.2|1} & \lag_{2.2|2.1} & \lag_{2.2|2.2} & \lag_{2.2|3}\\
		\lag_{31} & \lag_{3|2.1} & \lag_{3|2.2} & \lag_{33}
		\end{bmatrix}_\times \begin{pmatrix}
		a \\ \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}\\
		\mathbf{0}
		\end{pmatrix} = \begin{pmatrix}
		\lag_{11}(a)\\
		\begin{pmatrix}
		\lag_{2.1|1}(a)\\
		\lag_{2.2|1}(a)
		\end{pmatrix}\\
		\lag_{31}(a)
		\end{pmatrix} = \begin{pmatrix}
		\lag_{11}(a)\\
		\lag_{21}(a)\\
		\lag_{31}(a)
		\end{pmatrix}.
		\end{align*}
		We can repeat this checking process and find that the middle two ``columns'' of the 4-by-4 block expression correspond to the second column of the 3-by-3 block expression, and that the last column of the 4-by-4 expression corresponds to the the third column of the 3-by-3 expression.\\

		This will complete our justification.\\ 
		
		
		
		
		
		\item From the previous problem set, we know that
		\begin{align*}
		\begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus = \maltese \circ \begin{bmatrix}
		\lag_{11} & \lag_{12} & \lag_{13}\\
		\lag_{21} & \lag_{22} & \lag_{23}\\
		\lag_{31} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times \circ \maltese^{-1}
		\end{align*}
		where $\maltese$ is the isomorphism $\W_1 \times \W_2 \times \W_3 \lin \W_1\oplus\W_2\oplus\W_3$ defined in the last problem set. Since $\W_1 = \W_{1.1}\oplus\W_{1.2}$, we can define a similar isomorphism $\clubsuit : \W_{1.1}\times \W_{1.2} \lin \W_{1.1}\oplus\W_{1.2}$ where for $w_{1.1}\in \W_1$ and $w_{1.2}\in \W_{1.2}$,
		\begin{align*}
		\clubsuit\begin{pmatrix}
		w_{1.1}\\w_{1.2}
		\end{pmatrix} = w_{1.1} + w_{1.2}.
		\end{align*}
		Also, define the linear functions $\sigma_{1.m} := \rho_{1.m}\circ \clubsuit^{-1} : \W_1 \to \W_{1.m}$ and $\omega_{1.m} := \clubsuit \circ \delta_{1.m} : \W_{1.m} \to \W_{1}$, where $\rho_{1.m}$ and $\delta_{1.m}$ are linear functions defined similarly as in Part 1, $m=\{1,2\}$. \\  
		
		
		Considering the previous part of this problem, we know that the block-matrix expression for $\lag_{\times}$ corresponding to $\W_{1} = \W_{1.1}\times \W_{1.2}$ is
		\begin{align*}
		\begin{bmatrix}
		\lag_{1.1|1.1} & \lag_{1.1|1.2} & \lag_{1.1|2} & \lag_{1.1|3}\\
		\lag_{1.2|1.1} & \lag_{1.2|1.2} & \lag_{1.2|2} & \lag_{1.2|3}\\
		\lag_{2|1.1} & \lag_{2|1.2} & \lag_{22} & \lag_{23}\\
		\lag_{3|1.1} & \lag_{3|1.2} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\times,
		\end{align*}
		where the matrix elements are defined in the same way as in Part 1. We claim that the 4-by-4 block-matrix expression for $\lag_{\oplus}$ is
		\begin{align*}
		\begin{bmatrix}
		\lag_{1.1|1.1} & \lag_{1.1|1.2} & \lag_{1.1|2} & \lag_{1.1|3}\\
		\lag_{1.2|1.1} & \lag_{1.2|1.2} & \lag_{1.2|2} & \lag_{1.2|3}\\
		\lag_{2|1.1} & \lag_{2|1.2} & \lag_{22} & \lag_{23}\\
		\lag_{3|1.1} & \lag_{3|1.2} & \lag_{32} & \lag_{33}
		\end{bmatrix}_\oplus.
		\end{align*}
		Recall the linear functions $\E_i := \Pi_i \circ \maltese^{-1} : \V \to \V_i$ and $\K_j := \maltese\circ \gamma_j : \W_j \to \V$ from the last problem set. We have also defined
		\begin{align*}
		\lag_{ij} = \E_i \circ \lag_\oplus \circ \K_j.
		\end{align*}	
		We claim that 
		\begin{align*}
		\lag_{11} = \begin{bmatrix}
		\lag_{1.1|1.1} & \lag_{1.1|1.2}\\
		\lag_{1.2|1.1} & \lag_{1.2|1.2}
		\end{bmatrix}, \lag_{12} = \begin{bmatrix}
		\lag_{1.1|2}\\ \lag_{1.2|2}
		\end{bmatrix}, \lag_{13} = \begin{bmatrix}
		\lag_{1.1|3}\\\lag_{1.2|3}
		\end{bmatrix},
		\end{align*}
		\begin{align*}
		\lag_{21} = \begin{bmatrix}
		\lag_{2|1.1}&\lag_{2|1.2}
		\end{bmatrix}, \lag_{31} = \begin{bmatrix}
		\lag_{3|1.1}&\lag_{3|1.2}
		\end{bmatrix}.
		\end{align*}
		The definitions of these matrix elements are similar to those in Part 1, except that we have to use the isomorphism $\clubsuit$ to obtain the appropriate inputs for the $\lag_{ij}$'s. Consider the elements of $\lag_{11}$. We claim that
		\begin{align*}
		\lag_{1.i|1.j} = \sigma_{1.i} \circ \E_1 \circ \lag_\oplus \circ \K_1 \circ \omega_{1.j}.
		\end{align*}
		For the elements of $\lag_{21}$ and $\lag_{31}$, we claim that
		\begin{align*}
		\lag_{i|1.j} = \E_i \circ \lag_\oplus \circ \K_{1} \circ \omega_{1.j}.
		\end{align*}
		For the elements of $\lag_{12}$ and $\lag_{13}$, we claim that
		\begin{align*}
		\lag_{1.i|j} = \sigma_{1.i} \circ \E_1 \circ \lag_\oplus \circ \K_j.
		\end{align*}
		The justification for these definitions can be reproduced from the example in Part 1., except instead of consider a $v$ in the Cartesian product, we now considering a $v$ in the direct sum:
		\begin{align*}
		v = 
		\begin{pmatrix}
		a & \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}_\clubsuit&
		\mathbf{0}
		\end{pmatrix}^\top_\maltese,
		\end{align*}
		which can be expressed as
		\begin{align*}
		\begin{pmatrix}
		a & \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}_\clubsuit&
		\mathbf{0}
		\end{pmatrix}^\top_\maltese
		+
		\begin{pmatrix}
		\mathbf{0} & \begin{pmatrix}
		b\\\mathbf{0}
		\end{pmatrix}_\clubsuit&
		\mathbf{0}
		\end{pmatrix}^\top_\maltese
		+
		\begin{pmatrix}
		\mathbf{0} & \begin{pmatrix}
		\mathbf{0}\\c
		\end{pmatrix}_\clubsuit&
		\mathbf{0}
		\end{pmatrix}^\top_\maltese
		+
		\begin{pmatrix}
		\mathbf{0} & \begin{pmatrix}
		\mathbf{0}\\\mathbf{0}
		\end{pmatrix}_\clubsuit&
		d
		\end{pmatrix}^\top_\maltese.
		\end{align*}
		Verifying the above definitions is now only a matter of evaluating $\lag_\oplus$ applied to each term in this expression for $v$ and compare the result to the 4-by-4 block representation applied to the same term. This will complete our justification.
		
	\end{enumerate}
\end{sln*}
\end{prob*}




\newpage

\begin{prob*}\textbf{3. }\\ 
	
	Suppose that relatively prime polynomials $p_1$ and $p_2$ have degrees 6 and 11 respectively. Consider the function:
	\begin{align*}
	\psi : \mathbb{P}_{10} \times \mathbb{P}_5 \to \mathbb{P}_{16}
	\end{align*}
	defined by
	\begin{align*}
	\psi \begin{pmatrix}
	f\\g
	\end{pmatrix} := f \cdot p_1 - g \cdot p_2.
	\end{align*}
	Verify each o the following claims.
	\begin{enumerate}
		\item $\psi$ is a linear function.
		\item $\psi$ is injective.
		\item $\psi$ is surjective.
		\item There exist polynomials $q_1$ and $q_2$ such that
		\begin{align*}
		q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1},
		\end{align*}
		where $\mathbb{1}$ is the constantly 1 polynomial.
	\end{enumerate}



\begin{sln*}
	$\,$\\
	\begin{enumerate}
		\item 
	
	\begin{enumerate}
		\item Consider 
		\begin{align*}
		\begin{pmatrix}
		f\\g
		\end{pmatrix},
		\begin{pmatrix}
		h\\k
		\end{pmatrix} \in \mathbb{P}_{10}\times \mathbb{P}_5.
		\end{align*}
		Then
		\begin{align*}
		\psi\begin{pmatrix}
		f\\g
		\end{pmatrix}
		+
		\psi\begin{pmatrix}
		h\\k
		\end{pmatrix}
		&=
		(f\cdot p_1 - g\cdot p_2) + (h\cdot p_1 - k\cdot p_1)\\ 
		&= (f+h)\cdot p_1 - (g+k)\cdot p_2 \\
		&= \psi\begin{pmatrix}
		f+h\\g+k
		\end{pmatrix}\\
		&= \psi\left(\begin{pmatrix}
		f\\g
		\end{pmatrix}+ \begin{pmatrix}
		h\\k
		\end{pmatrix}\right)\hspace{0.5cm}(\dagger)
		\end{align*}
		\item Consider $c\in \mathbb{C} $, 
		\begin{align*}
		\psi \begin{pmatrix}
		cf \\ cg
		\end{pmatrix} = cf\cdot p_1 - cg\cdot p_2 = c(f \cdot p_1 - g \cdot p_2) = c\psi\begin{pmatrix}
		f\\g
		\end{pmatrix}.\hspace{0.5cm}(\dagger\dagger)
		\end{align*}
		From ($\dagger$) and ($\dagger\dagger$), $\psi$ is a linear function.
		
		
		
	\end{enumerate}

	
	\item Assume (to get a contradiction) that $\begin{pmatrix}
	f\\g
	\end{pmatrix}$ is a non-zero element in $\ker(\psi)$. Then 
	\begin{align*}
	f\cdot p_1 - g\cdot p_2 = \mathbb{0}.
	\end{align*}
	Since $p_1, p_2$ are both non-zero (they are relatively prime), if either one of $f$ or $g$ were zero, it would force the other polynomial to be zero so that $f\cdot p_1 - g\cdot p_2 = \mathbb{0}$, contradicting the requirement that $\begin{pmatrix}
	f\\g
	\end{pmatrix}$ is non-zero element. Therefore, neither $f$ nor $g$ can be zero. \\
	
	Since $p_1$ and $p_2$ are relatively prime (i.e., they have no common roots), the roots of $p_2$ must also be roots of $f$. Furthermore, the roots of $p_2$ must also be roots of $f$ with equal or greater multiplicities, because otherwise there will at least one factor $(x - \lambda_k)$ in the factorization of $p_2$ that has a higher power (or multiplicity) than that in $f$ (and not in $p_1$), resulting in a non-zero $f\cdot p_1 - g\cdot p_2$.\\
		
	This implies that the degree of $f$ has to be at least $\deg(p_2) = 11$. But since $\deg(f) \leq 10$, we have a contradiction. By symmetry, we will also have a contradiction if we repeat this argument for $g$ and $p_1$.\\
	
	Hence, there is no such non-zero $\begin{pmatrix}
	f\\g
	\end{pmatrix}$ in the kernel of $\psi$, i.e., $\psi$ is injective. \\
	
	
	\item Since both $\mathbb{P}_{10}\times \mathbb{P}_5$ and $\mathbb{P}_{16}$ are finite-dimensional vector spaces, we can use the Rank-Nullity Theorem:
	\begin{align*}
	\dim(\ker(\psi)) + \dim(\ima(\psi)) = \dim(\mathbb{P}_{10} \times \mathbb{P}_{5}).
	\end{align*}
	From part 1., we know $\dim(\ker(\psi)) = 0$, so
	\begin{align*}
	\dim(\ima(\psi)) = \dim(\mathbb{P}_{10} \times \mathbb{P}_{5}) = \dim(\mathbb{P}_{10}) + \dim(\mathbb{P}_5) = 11 + 6 = 17 = \dim(\mathbb{P}_{16}).
	\end{align*}
	Furthermore, $\ima(\psi) \prec \mathbb{P}_{16}$. Hence, $\ima(\psi) = \mathbb{P}_{16}$. So, $\psi$ is surjective. 



	\item Since $\psi$ is surjective, there exist polynomials $f\in \mathbb{P}_{10}$ and $g\in \mathbb{P}_5$ such that 
	\begin{align*}
	\psi\begin{pmatrix}
	f\\g
	\end{pmatrix} = \mathbb{1} \in \mathbb{P}_{16} = \ima(\psi),
	\end{align*}
	i.e., the given $f$ and $g$ satisfy
	\begin{align*}
	f\cdot p_1 - g\cdot p_2 = \mathbb{1}.
	\end{align*}
	Let $q_1 = f \in \mathbb{P}_{10}$ and $q_2 = -g \in \mathbb{P}_{5}$, then
	\begin{align*}
	q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1}.
	\end{align*}
	This verifies the existence of $q_1$ and $q_2$. 
\end{enumerate}
\end{sln*}


\end{prob*}


\newpage




\begin{prob*}\textbf{4.}\\
	\begin{enumerate}
		\item What would you do to prove the last claim of problem 2 in a general case of non-zero relatively prime polynomials $p_1$ and $p_2$? You do not need to carry out the proof, but you DO need to set it all up along the lines of Problem 2. Make sure you cover all of the cases!
		\item Prove that the following claims are equivalent.
		\begin{enumerate}
			\item Non-zero polynomials $p_1$ and $p_2$ are relatively prime.
			\item There exist polynomials $q_1$ and $q_2$ such that
			\begin{align*}
			q_1 \cdot p_1 +q_2 \cdot p_2 = \mathbb{1}.
			\end{align*}
		\end{enumerate}
	\item Argue that for any non-zero polynomials $f$ and $g$ there exist polynomials $q_1$ and $q_2$ such that
	\begin{align*}
	q_1 \cdot f + q_2 \cdot g = \gcd(f,g).
	\end{align*}
	\item Argue that the following claims are equivalent for any non-zero polynomials $f$, $g$, and $h$.
	\begin{enumerate}
		\item There exist polynomials $q_1$ and $q_2$ such that
		\begin{align*}
		q_1 \cdot f + q_2 \cdot g= h.
		\end{align*}
		\item $h$ is a polynomial multiple of $\gcd(f,g)$. 
	\end{enumerate}
	\end{enumerate}




\begin{sln*}
	$\,$
	\begin{enumerate}
	\item 
	\begin{enumerate}
		\item Case 1: Both $p_1$ and $p_2$ are zero-degree polynomials.\\
		
		In this case, $p_1$ and $p_2$ are two constant, non-zero polynomials. Let $p_2(z) = C \in \mathbb{C}$ for any $z\in \mathbb{C}$, $C\neq 0$. If we let $q_1(z)$ be the constant $0$ and $q_2(z)$ be the constant $1/C$ for any $z\in \mathbb{C}$ then
		\begin{align*}
		q_1(z) p_1(z) + q_2(z)p_2(z) = 0 + \frac{1}{C}C = 0 + 1 = 1,
		\end{align*}
		for any $z\in \mathbb{C}$, i.e., $q_1\cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$. This shows there are polynomials $q_1$ and $q_2$ such that $q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$ is satisfied.\\
		
		\item Case 2: Exactly one of $p_1$ and $p_2$ is a zero-degree polynomial.\\
		
		Assume that $p_1$ is the non-zero zero-degree polynomial of the two, i.e., it is a non-zero constant polynomial. Let $p_1(z) = C \in \mathbb{C}$, $C\neq 0$, for any $z\in \mathbb{C}$. If we let $q_1(z)$ be the constant $1/C$ and $q_2(z)$ be the constant $0$ for any $z\in \mathbb{C}$ then
		\begin{align*}
		q_1(z) p_1(z) + q_2(z)p_2(z) = \frac{1}{C}C + 0 = 1 + 0 = 1,
		\end{align*}
		for any $z\in \mathbb{C}$, i.e., $q_1\cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$. This shows there are polynomials $q_1$ and $q_2$ such that $q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$ is satisfied.\\
		
		\item Case 3: Both $p_1$ and $p_2$ have degree greater than 0. \\
		
		To show the existence of $q_1$ and $q_2$ such that $q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$ we can construct a similar proof to that in Problem 3. Let the polynomials $p_1$ and $p_2$ be given with degrees $m,n >0$ respectively. Consider the function:
		\begin{align}
		\psi : \mathbb{P}_{n-1} \times \mathbb{P}_{m-1} \to \mathbb{P}_{m+n-1},
		\end{align}
		defined by
		\begin{align}
		\psi\begin{pmatrix}
		f \\ g
		\end{pmatrix} := f \cdot p_1 + g\cdot p_2.
		\end{align}
		We can then show that $\psi$ is a linear function. Next, we show that it is injective, which implies surjectivity by the Rank-Nullity Theorem. By the surjectivity of $\psi$, we know that given $\mathbb{1} \in \ima(\psi)$, there must exist a pair $f\in \mathbb{P}_{n-1}$,$ -g \in \mathbb{P}_{m-1}$ such that $\psi\begin{pmatrix}
		f\\-g
		\end{pmatrix} = \mathbb{1}$. \\
		
	\end{enumerate}
	From parts (a), (b), and (c), we can conclude that there exist polynomials $q_1$, $q_2$ such that for non-zero relatively prime polynomials $p_1, p_2$, $q_1\cdot p_1 + q_2 \cdot p_2 = \mathbb{1}$.\\
	
	
	\item 
	\begin{itemize}
		\item $[a. \implies b.]$ This implication is true by Part 1. of this problem.
		\item $[b. \implies a.]$ Let polynomials $q_1$ and $q_2$ be given such that
		\begin{align*}
		q_1 \cdot p_1 + q_2 \cdot p_2 = \mathbb{1},
		\end{align*} 
		where $p_1$ and $p_2$ are also polynomials. \underline{Claim}: Non-zero $p_1$ and $p_2$ are relatively prime. \\
		
		Assume that $p_1$ and $p_2$ are non-zero. From TYC 2.4, we can write $p_1$ and $p_2$ as
		\begin{align*}
		&p_1 = \hat{p}_1 \cdot \gcd(p_1, p_2)\\
		&p_2 = \hat{p}_2 \cdot \gcd(p_1, p_2),
		\end{align*}
		where $\hat{p}_1$ and $\hat{p}_2$ are non-zero relatively prime polynomials. It follows that
		\begin{align*}
		q_1 \cdot \hat{p}_1 \cdot \gcd(p_1, p_2)  + q_2 \cdot \hat{p}_2 \cdot \gcd(p_1, p_2) = \mathbb{1},
		\end{align*}
		i.e.,
		\begin{align*}
		(q_1 \cdot \hat{p}_1 + q_2 \cdot \hat{p}_2 )\cdot \gcd(p_1, p_2) = \mathbb{1},
		\end{align*}
		which implies
		\begin{align*}
		\deg(q_1 \cdot \hat{p}_1 + q_2 \cdot \hat{p}_2) = \deg(\gcd(p_1,p_2)) = 0.
		\end{align*}
		Therefore, $\gcd(p_1, p_2)$ has to be a constant polynomial. Moreover, since $\gcd(p_1, p_2)$ is monic by definition, it has to be the constantly 1 polynomial. Hence, $p_1$ and $p_2$ are relatively prime by definition. 
	\end{itemize}


	\item Let non-zero polynomials $f,g$ be given. Then we have
	\begin{align*}
	f = \hat{f}\cdot \gcd(f,g)\\
	g = \hat{g}\cdot \gcd(f,g),
	\end{align*}
	where $\hat{f}, \hat{g}$ are relatively prime. It follows that
	\begin{align*}
	q_1 \cdot f + q_2 \cdot g &= q_1 \cdot \hat{f}\cdot \gcd(f,g) + q_2 \cdot \hat{g}\cdot \gcd(f,g)\\
	&= (q_1 \cdot \hat{f} + q_2 \cdot \hat{g})\cdot \gcd(f,g).
	\end{align*}
	From Part 2., we know that for relatively prime polynomials $\hat{f}$ and $\hat{g}$, there exist polynomials $q_1, q_2$ such that
	\begin{align*}
	q_1 \cdot \hat{f} + q_2 \cdot \hat{g} = \mathbb{1}.
	\end{align*}
	This means such $q_1$ and $q_2$ will give
	\begin{align*}
	(q_1 \cdot \hat{f} + q_2 \cdot \hat{g})\cdot \gcd(f,g) = \mathbb{1}\cdot\gcd(f,g) = \gcd(f,g).
	\end{align*}
	This completes the argument.\\
	
	
	\item \begin{itemize}
		\item $[a.\implies b.]$ Let non-zero polynomials $f,g,h$ be given such that
		\begin{align*}
		q_1 \cdot f + q_2 \cdot g = h,
		\end{align*}
		for some polynomials $q_1, q_2$. From Part 3., we know that 
		\begin{align*}
		h = q_1 \cdot f + q_2 \cdot g &= q_1 \cdot \hat{f}\cdot \gcd(f,g) + q_2 \cdot \hat{g}\cdot \gcd(f,g)\\
		&= (q_1 \cdot \hat{f} + q_2 \cdot \hat{g})\cdot \gcd(f,g),
		\end{align*}
		for relatively prime $\hat{f}$ and $\hat{g}$. This implies $h$ is a polynomial multiple of $\gcd(p_1,p_2)$. \\
		
		\item $[b.\implies a.]$ Let $h$ be written as $k\cdot \gcd(f,g)$ where $k$ is some polynomial. From Part 3., there exist polynomials $s_1, s_2$ such that
		\begin{align*}
		s_1 \cdot f + s_2 \cdot g = \gcd(f,g).
		\end{align*}
		Multiplying both sides by the polynomial $k$, we have
		\begin{align*}
		k\cdot s_1 \cdot f + k \cdot s_2 \cdot g = k\cdot \gcd(f,g) = h.
		\end{align*}
		Hence, there exist polynomials $q_1$ and $q_2$ such that 
		\begin{align*}
		q_1 \cdot f + q_2 \cdot g = h,
		\end{align*}
		namely, 
		\begin{align*}
		&q_1 = k \cdot s_1\\
		&q_2 = k \cdot s_2.
		\end{align*}
	\end{itemize}


\end{enumerate}
\end{sln*}
\end{prob*}


\newpage


\begin{prob*}\textbf{5.}\\
	
	Suppose that $\W$ is a non-$\{\mathbb{0}\}$ ideal in $\mathbb{P}$. Then $\W$ contains some monic polynomials (why?) and among these there must be some of the smallest degree, say $n_0$. Let $p_0$ be one such. (It is entirely possible that $p_0 = \mathbb{1}$.)
	\begin{enumerate}
		\item Suppose that $p$ is a non-zero polynomial in $\W$, and using the Division Algorithm for Polynomials (see Axler p.121) we write
		\begin{align*}
		p = q \cdot p_0+ r,
		\end{align*}
		where $q,r \in \mathbb{P}$ and $\deg(r) < \deg(p_0)$. Argue that $r\in \W$.
		\item Use the result of part 1 to argue that every polynomial $p$ in $\W$ is a polynomial multiple of $p_0$.
		\item Argue that $p_0$ is the only monic polynomial of the smallest degree $n_0$ in $\W$, and that
		\begin{align*}
		\W= \left\{ q\cdot p_0 \big\vert q \in \mathbb{P} \right\}.
		\end{align*}
	\end{enumerate}
This polynomial $p_0$ is said to be \textbf{the generator} of the ideal $\W$.

\begin{sln*}
	$\,$
	\begin{enumerate}
		\item Let $p \in \W$ a non-zero polynomial be given. Let $p_0 \in \W$ be the monic polynomial with the smallest degree. $\W$ is an ideal in $\mathbb{P}$, so for $q \in \mathbb{P}$, $q \cdot p_0 \in \W$. $\W$ is also subspace, so
		\begin{align*}
		r = p - q \cdot p_0 \in \W.
		\end{align*}
		
		\item Let the above $q,p_0,p, r$ be given again. We know that
		\begin{align*}
		p = q\cdot p_0 + r,
		\end{align*}  
		where $\deg(r) < \deg(p_0)$. Since $\deg(p_0)$ minimal and $p$ is non-zero, $r$ has to be the constantly zero polynomial. Therefore,
		\begin{align*}
		p = q\cdot p_0 \in \W,
		\end{align*} 
		i.e., every $p\in \W$ is a polynomial multiple of $p_0$.\\
		
		
		\item Let some $p'_0$ be a monic polynomial of the smallest degree $n_0$ in $\W$ be given. Since $p'_0 \in \W$, $p'_0$ is a polynomial multiple of $p_0$, by the previous part. Let us write $p'_0 = q\cdot p_0$ for some $p\in \mathbb{P}$. Since $\deg(p'_0) = \deg(p_0) = n_0$, $\deg(q) = 1$, i.e., $p$ is a constant polynomial. Furthermore, since both $p_0$ and $p'_0$ are monic, $q$ has to be $\mathbb{1}$. Hence, $p'_0 = p_0$, i.e., $p_0$ is unique.\\
		
		It follows from the previous part that $\W$ is a collection of elements that are polynomial multiples of a unique monic polynomial of the smallest degree, called $p_0$, i.e.,
		\begin{align*}
		\W = \{ q\cdot p_0 \big\vert p \in \mathbb{P} \}.
		\end{align*}
	\end{enumerate}
\end{sln*}
\end{prob*}






















\newpage














\subsection{Problem set 5}




\begin{prob*}\textbf{1.} Suppose that $\mathfrak{F}$ is a commutative collection of linear operators on a (not necessarily finite-dimensional) vector space $\V$. Suppose that $\lambda$ is an eigenvalue for some $\A \in \mathfrak{F}$, and let $\mathbf{E}_\A(\lambda)$ be the corresponding eigenspace of $\A$. Argue that this $\mathbf{E}_\A(\lambda)$ is an invariant subspace for every operator in $\mathfrak{F}$.\\
	
	\begin{sln*}\textbf{1.} \\
		
		Since $\textbf{E}_\A(\lambda)$ is subspace, it suffices to show that $\mathcal{B}[\textbf{E}_\A(\lambda)] \subset \textbf{E}_\A(\lambda) $.\\
				
		Consider $v \in \textbf{E}_\A(\lambda)$, then $\A v = \lambda v$. Also, consider $\mathcal{B} \in \FF$. Since $\A, \mathcal{B} \in \FF$, $\A \mathcal{B} = \mathcal{B} \A$, so
		\begin{align*}
		\A \mathcal{B} v = \mathcal{B}\A v = \mathcal{B} (\lambda v) = \lambda \mathcal{B}v.
		\end{align*}
		Therefore, $\mathcal{B}v$ is a $\lambda$-eigenvector of $\A$, i.e., $\mathcal{B}(v) \in \textbf{E}_\A(\lambda)$ for any $v \in \textbf{E}_\A(\lambda)$. So, $\mathcal{B}[\textbf{E}_\A(\lambda)] \subseteq \textbf{E}_\A(\lambda)$. Hence $\textbf{E}_\A(\lambda)$ is an invariant subspace for any $\mathcal{B}\in \FF$. 
		
	\end{sln*}
	
\end{prob*}

\newpage


\begin{prob*}\textbf{2.} Let us use the same set-up as in Problem 1, and let $\W$ be a subspace of $\V$ such that 
	\begin{align*}
	\V = \textbf{E}_\A(\lambda) \oplus\W.
	\end{align*}
	Argue that with respect to this decomposition, operators in $\mathfrak{F}$ have block matrix representation of the form
	\begin{align*}
	\begin{bmatrix}
	\lag & \M \\
	\mathcal{O} & \mathcal{K}
	\end{bmatrix},
	\end{align*}
	where the $\lag$'s form a commutative family in $\mathfrak{L}(\textbf{E}_\A(\lambda), \textbf{E}_\A(\lambda))$, and the $\mathcal{K}$'s form a commutative family in $\mathfrak{L}(\W,\W)$. 
	
	\begin{sln*}\textbf{2.}\\
		
		Let $\mathcal{B}_1, \B_2 \in \FF$ be given. With respect to the composition $\V = \textbf{E}_\A(\lambda) \oplus \W$, where $\textbf{E}_\A(\lambda) \in \lat(\B_1)$ and $\textbf{E}_\A(\lambda) \in \lat(\B_2)$, the block matrix representations of $\B_1, \B_2$ have the form
		\begin{align*}
		\begin{bmatrix}
		\lag_1 & \M_1 \\
		\mathcal{O}_1 & \K_1
		\end{bmatrix}
		\quad \text{ and} \quad
		\begin{bmatrix}
		\lag_2 & \M_2 \\
		\mathcal{O}_2 & \K_2
		\end{bmatrix},
		\end{align*} 
		respectively, where 
		\begin{align*}
		&\lag_j : \textbf{E}_\A(\lambda) \lin \textbf{E}_\A(\lambda)\\
		&\M_j : \W \lin \W.
		\end{align*}
		Since $\mathcal{B}_1, \B_2 \in \FF$, they commute, i.e., 
		\begin{align*}
		\begin{bmatrix}
		\lag_1 & \M_1 \\
		\mathcal{O}_1 & \K_1
		\end{bmatrix}\begin{bmatrix}
		\lag_2 & \M_2 \\
		\mathcal{O}_2 & \K_2
		\end{bmatrix}
		&=
		\begin{bmatrix}
		\lag_1\lag_2 & \square\\
		\mathcal{O} & \K_1\K_2
		\end{bmatrix}\\
		&=
		\begin{bmatrix}
		\lag_2\lag_1 & \triangle\\
		\mathcal{O} & \K_2\K_1
		\end{bmatrix}
		=
		\begin{bmatrix}
		\lag_2 & \M_2 \\
		\mathcal{O}_2 & \K_2
		\end{bmatrix}
		\begin{bmatrix}
		\lag_1 & \M_1 \\
		\mathcal{O}_1 & \K_1
		\end{bmatrix}.
		\end{align*}
		Therefore, it is necessary that $\lag_1, \lag_2$ commute and $\M_1, \M_2$ commute. Since this holds for any choice of $\B_1, \B_2 \in \FF$, the $\lag$'s form a commutative family in $\LL\left(\textbf{E}_\A(\lambda), \textbf{E}_\A(\lambda)\right)$, and the $\M$'s form a commutative family in $\LL\left(\W, \W\right)$. 
		
		
	\end{sln*}
	
\end{prob*}

\newpage


\begin{prob*}\textbf{3. Commuting collections of complex matrices are simultaneously $\triangle$-able} \\
	Argue that every commutative family of operators on a finite-dimensional vector space over the complex numbers is simultaneously upper-triangularizable, and simultaneously lower-triangularizable. \\
	
	\begin{sln*}\textbf{3. }Let $\FF$, a commuting collection of complex matrices on $\mathbb{C}^n$, be given. We notice that if $n=1$ then any complex matrix in $\mathbb{M}_{1\times 1}$ is automatically triangular, so any such $\FF$ on $\mathbb{C}$ is triangularizable. In order for the statement above to fail, $n \geq 2$. Let us consider $n_0 \geq 2$ the smallest integer for which this statement fails. \\
		
	Suppose that $\lambda$ is an eigenvalue for some matrix $\A \in \FF$, and let $\textbf{E}_\A(\lambda)$ be the corresponding eigenspace of $\A$. By Problem 2, with respect to the decomposition $\V = \textbf{E}_\A(\lambda) \oplus \W$ where $\W \prec \V$, any $\B \in \FF$ has a block-matrix representation of the form 
	\begin{align*}
	[\B] = \begin{bmatrix}
	\lag & \M \\
	\mathcal{O} & \K
	\end{bmatrix},
	\end{align*} 
	where the $\lag$'s form a commutative family in $\LL(\textbf{E}_\A(\lambda),\textbf{E}_\A(\lambda))$ and the $\K$'s form a commutative family in $\LL(\W,\W)$. Since the $\lag$'s and $\K$'s are matrices on $\mathbb{C}^a, \mathbb{C}^b$ respectively where $a,b < n_0$, the statement above implies that both $\LL(\textbf{E}_\A(\lambda),\textbf{E}_\A(\lambda))$ and $\LL(\W,\W)$ are simultaneously upper triangularizable, i.e., there exists a basis $\Gamma_1$ of $\textbf{E}_\A(\lambda)$ such that every $\lag \in \LL(\textbf{E}_\A(\lambda),\textbf{E}_\A(\lambda))$ is upper triangular. Similarly, there exists a basis $\Gamma_2$ for $\W$ such that every $\K \in \LL(\W,\W)$ is upper triangular. \\
	
	Since $\V = \textbf{E}_\A(\lambda)\oplus \W$, the concatenation $\Gamma = \Gamma_1 || \Gamma_2$ is a basis of $\V$ for which both $\lag \in \LL(\textbf{E}_\A(\lambda),\textbf{E}_\A(\lambda))$ and $\K \in \LL(\W,\W)$ are upper-triangular for any $\lag, \K$. It follows that with respect to this basis, any (block) matrix $[\B]$ is upper triangular, and thus any $\B \in \FF$ is similar to an upper triangular matrix, i.e., upper triangularizable. Hence, $\FF$ on $\mathbb{C}^{n_0}$ is simultaneously upper triangularizable. \\
	
	However, this contradicts the assumption that the statement fails at $\mathbb{C}^{n_0}$. By contraposition, every commutative family of operators on a finite-dimensional vector space over the complex numbers is simultaneously upper-triangularizable.\\
	
	 To show that the ``lower triangularizable'' case also holds, we reproduce a similar argument as above, starting with writing $[\B]$ with respect to the decomposition $\V = \W \oplus \textbf{E}_\A(\lambda) $ as 
	 \begin{align*}
	 [\B] = \begin{bmatrix}
	 \lag & \mathcal{O}\\
	 \M & \K
	 \end{bmatrix}
	 \end{align*}
	then showing (by induction and contraposition) that $\LL(\textbf{E}_\A(\lambda),\textbf{E}_\A(\lambda)), \LL(\W,\W)$ both simultaneously lower triangularizable implies $\FF$ simultaneously lower triangularizable.  
	\end{sln*}
	
	

\end{prob*}






\newpage


\begin{prob*}\textbf{4. Transpose of a square matrix is similar to that matrix} \\
	Use Jordan Canonical Form Theorem to argue that the transpose $\mathcal{J}^\top_{\lambda,n}$ of a Jordan block $\jor_{\lambda,n}$ is similar to $\jor_{\lambda,n}$, and then use this fact to argue that every $k\times k$ complex matrix is similar to its transpose.\\
	
	\begin{sln*}\textbf{4. }
		
		\begin{enumerate}
			\item \underline{To show}: $ \jor^\top_{\lambda,n} \sim  \jor_{\lambda, n} $. By Jordan Canonical Form Theorem, $\mathcal{J}^\top_{\lambda,n} \sim \bigoplus^k_{i=1} \jor_{\lambda_i, m_i}$ for some $k$, and $\sum^k_{i=1}m_i = n$. But since the only eigenvalue of $\jor^\top_{\lambda,n}$ is $\lambda$ ($\jor^\top_{\lambda,n}$ is lower triangular and has only $\lambda$'s on the diagonal), $\jor^\top_{\lambda,n} \sim \bigoplus^k_{i=1}\jor_{\lambda,m_i}$. Next, we observe that 
		\begin{align*}
		\jor^\top_{\lambda,n} - \lambda \mathcal{I} = \begin{bmatrix}
		0 && &  \\
		1 &  && \\
		&\ddots & \\
		&   &1& 0  
		\end{bmatrix} \text{ is a nilpotent of order $n$},
		\end{align*} 
		which implies $(\jor^\top_{\lambda,n} - \lambda \mathcal{I})^l = \mathcal{O}$ only if $l \geq n$. On the other hand,
		\begin{align*}
		\left(\bigoplus^k_{i=1}\jor_{\lambda,m_i} - \lambda\mathcal{I}\right)^{\max(m_i)} = \left(\bigoplus^k_{i=1}\jor_{0,m_i}\right)^{\max(m_i)} = \bigoplus^k_{i=1}\left(\N_{m_i}\right)^{\max(m_i)} = \mathcal{O},
		\end{align*}
		where each $\N_{m_i}$ is a nilpotent of order $m_i \leq n$, for any $i \in \{1,2,\dots,k\}$. Thus, $\max(m_i) \leq n \leq \max(m_i)$, which holds if and only if $\max(m_i) = n$. This means $\jor_{\lambda, \max(m_i)}$ has size $n$, which implies it is the only summand in the direct sum with non-zero size. So, 
		\begin{align*}
		\jor^\top_{\lambda,n} \sim \bigoplus^k_{i=1} \jor_{\lambda, m_i} = \jor_{\lambda, n}\iff \text{$\jor^\top_{\lambda,n}$ is similar to $ \jor_{\lambda, n}$.}
		\end{align*}
		
		 
		\item \underline{To show}: $\A \sim \A^\top$ for any complex $k\times k$ matrix $\A$. Let $\A \sim \jor = \B \A \B^{-1} $ be the Jordan form of $\A$. From the previous part, $\jor^\top_{\lambda,n} \sim \jor_{\lambda,n}$, so
		\begin{align*}
		\jor &= \bigoplus^{k}_{i=1}\jor_{\lambda_i,m_i} = \bigoplus^k_{i=1} \F_i^{-1} \jor^\top_{\lambda_i,m_i} \F_i 
		= \bigoplus^k_{i=1} \F_i^{-1}\bigoplus^k_{i=1} \jor^\top_{i} \bigoplus^k_{i=1} \F_i 
		= \F^{-1}\jor^\top \F.
		\end{align*}
		So, we have $\jor \sim \jor^\top = (\B\A\B^{-1})^\top = (\B^{-1})^\top \A^\top \B^\top$, which implies $\jor \sim \A^\top$. Therefore, $\A \sim \A^\top$ by transitivity.  
		
		
		 

		\end{enumerate}
	\end{sln*}
	
	
\end{prob*}




\newpage





\begin{prob*}\textbf{5. }\\
	\begin{enumerate}
		\item Argue that
		\begin{align*}
		\lim\limits_{n\to \infty} \left(    5\alpha^n + 4n\alpha^{n-1} + 3\frac{n(n-1)}{2!}\alpha^{n-2} + 2\frac{n(n-1)(n-2)}{3!}\alpha^{n-3}    \right.\\
		 \left.  + \frac{n(n-1)(n-2)(n-3)}{4!}\alpha^{n-4} \right) 
		= \begin{cases}
		\begin{alignedat}{2}
		0, & \quad & \text{if } 0 \leq \alpha < 1\\
		\infty, & \quad & \text{if } \alpha \geq 1
		\end{alignedat}
		\end{cases}.
		\end{align*}
		
		
		
		\item Argue that for $m\geq 2$
		\begin{align*}
		\lim\limits_{n\to \infty}\norm{\left(\jor_{\lambda,n}\right)^n}_2 
		= 
		\begin{cases}
		\begin{alignedat}{2}
		0, & \quad & \text{if } \abs{\lambda} < 1&\\
		\infty, & \quad & \text{if } \abs{\lambda} \geq 1&
		\end{alignedat}
		\end{cases}
		\end{align*}
		Note that $\jor_{\lambda,m} = \lambda\mathcal{I} + \mathcal{N}$, where $\mathcal{N}$ is a nice cyclic nilpotent of order $m$, so that
		\begin{align*}
		\left(\jor_{\lambda,m}\right)^n = \left(\lambda\mathcal{I} + \mathcal{N}\right)^n = 
		\lambda^n \mathcal{I} + {n\choose 1}\lambda^{n-1}\mathcal{N} + {n\choose 2}\lambda^{n-2}\mathcal{N}^2 + \dots
		\end{align*}
		You may want to start with small $m$ first, and calculate some $\left(\jor_{\lambda,m}\right)^n$'s using \textit{Mathematica}\dots
	\end{enumerate}



	\begin{sln*}\textbf{5. }
		\begin{enumerate}
			\item For $\alpha = 0$, 
			\begin{align*}
			&\lim\limits_{n\to \infty} \left(    5\alpha^n + 4n\alpha^{n-1} + 3\frac{n(n-1)}{2!}\alpha^{n-2} + 2\frac{n(n-1)(n-2)}{3!}\alpha^{n-3}  + \frac{n(n-1)(n-2)(n-3)}{4!}\alpha^{n-4} \right) \\
			= &\lim\limits_{n\to\infty}(0) = 0.
			\end{align*}
			For $0 < \alpha < 1$, for $\beta = 1/\alpha$, $\beta > 1$
			\begin{align*}
			&\lim\limits_{n\to \infty} \left(    5\alpha^n + 4n\alpha^{n-1} + 3\frac{n(n-1)}{2!}\alpha^{n-2} + 2\frac{n(n-1)(n-2)}{3!}\alpha^{n-3}  + \frac{n(n-1)(n-2)(n-3)}{4!}\alpha^{n-4} \right)\\
			= &\lim\limits_{n\to\infty}\left(    \frac{5}{\beta^n} + \frac{4n}{\beta^{n-1}} + 3\frac{n(n-1)}{2!\beta^{n-2}} + 2\frac{n(n-1)(n-2)}{3!\beta^{n-3}}  + \frac{n(n-1)(n-2)(n-3)}{4!\beta^{n-4}} \right) \\
			\end{align*}
			We can rewrite this limit as a sum whose summands are of the form
			\begin{align*}
			\lim\limits_{n\to\infty}\frac{p(n)}{\beta^{n-j}} = \lim\limits_{n\to\infty}\frac{p(n)\beta^{j}}{\beta^{n}}
			\end{align*}
			where $j, \beta$ are fixed, $\beta>1$, and $p(n)$ is a polynomial in $n$. From Fact 0.5, each of these summand is zero. So, the limit being evaluated is zero. Hence, for $0\leq \alpha < 1$, the limit is zero. \\
			
			For $\alpha \geq 1$,
			\begin{align*}
			&\lim\limits_{n\to \infty} \left(    5\alpha^n + 4n\alpha^{n-1} + \frac{3n(n-1)}{2!}\alpha^{n-2} + \frac{2n(n-1)(n-2)}{3!}\alpha^{n-3} + \frac{n(n-1)(n-2)(n-3)}{4!}\alpha^{n-4} \right)\\
			= & \lim\limits_{n\to \infty}\alpha^n  \left(    5 + \frac{4n}{\alpha} + \frac{3n(n-1)}{2!\alpha^2} + \frac{2n(n-1)(n-2)}{3!\alpha^3} + \frac{n(n-1)(n-2)(n-3)}{4!\alpha^4} \right)\\
			= & \lim\limits_{n\to\infty} \alpha^n q(n), \hspace{0.5cm}\text{$q(n)$ is the polynomial in $n$ from the line above}\\
			= & \left(\lim\limits_{n\to\infty}\alpha^n \right)\left(\lim\limits_{n\to\infty}q(n)\right).
			\end{align*}
			Let us consider $q(n)$. For $n \notin \{0,1,2,3 \}$,
			\begin{align*}
			q(n) = n(n-1)(n-2)(n-3)\left( \frac{5}{n(n-1)(n-2)(n-3)} + \frac{4}{(n-1)(n-2)(n-3)\alpha} \right. \\\left.+ \frac{3}{2!(n-2)(n-3)\alpha^2} + \frac{2}{3!(n-3)\alpha^3} + \frac{1}{4!\alpha^4}\right) = n(n-1)(n-2)(n-3)r(n).
			\end{align*}
			So the limit we are evaluating becomes
			\begin{align*}
			\left(\lim\limits_{n\to\infty}\alpha^n \right)\left(\lim\limits_{n\to\infty}n(n-1)(n-2)(n-3)\right)\left(\lim\limits_{n\to\infty}r(n)\right).
			\end{align*}
			If $\alpha = 1$, then this limit is $\left(\lim\limits_{n\to\infty}n(n-1)(n-2)(n-3)\right)\left(\lim\limits_{n\to\infty}r(n)\right)$. But since
			\begin{align*}
			&\lim\limits_{n\to\infty}n(n-1)(n-2)(n-3) = \infty\\
			&\lim\limits_{n\to\infty}f(n) = \frac{1}{4!\alpha^4},
			\end{align*}
			we have
			\begin{align*}
			\left(\lim\limits_{n\to\infty}\alpha^n \right)\left(\lim\limits_{n\to\infty}n(n-1)(n-2)(n-3)\right)\left(\lim\limits_{n\to\infty}r(n)\right) = \infty.
			\end{align*}
			If $\alpha > 1$, then $\left(\lim\limits_{n\to\infty}\alpha^n \right) = \infty$, so again,
			\begin{align*}
			\left(\lim\limits_{n\to\infty}\alpha^n \right)\left(\lim\limits_{n\to\infty}n(n-1)(n-2)(n-3)\right)\left(\lim\limits_{n\to\infty}r(n)\right) = \infty.
			\end{align*}
			Hence for $\alpha \geq 1$, the limit we are evaluating is $\infty$. 
			
			\newpage
			
			
			\item  We first observe that the sum
			\begin{align*}
			\left(\jor_{\lambda,m}\right)^n = \left(\lambda\mathcal{I} + \mathcal{N}\right)^n = 
			\lambda^n \mathcal{I} + {n\choose 1}\lambda^{n-1}\mathcal{N} + {n\choose 2}\lambda^{n-2}\mathcal{N}^2 + \dots
			\end{align*}
			is truncated at the term  with $\N^m = \mathcal{O}$, since $\N$ is a nilpotent with order $m$. Furthermore, we recognize that this sum can be written as
			\begin{align*}
			\left(\jor_{\lambda,m}\right)^n = \begin{bmatrix}
			\lambda^n & {n\choose 1}\lambda^{n-1} &  & {n\choose m-1}\lambda^{n-(m-1)}\\
					  &	\lambda^n		& 		\ddots	&	 \\
					  &					&	\ddots			& 	{n\choose 1}\lambda^{n-1}	\\
					  &					&				&	\lambda^n
			\end{bmatrix}
			\end{align*}
			where the coefficients on the diagonal come only from the term with $\mathcal{I}$, the coefficients just above the diagonal come only from the term with $\N$, the coefficients two ``diagonals'' above the diagonal come only from the term with $\N^2$, and so on. This is because $\N$ has the form
			\begin{align*}
			\N = \begin{bmatrix}
			0& 1 & & \\
			&\ddots & \ddots&\\
			& & \ddots &1 \\
			& & & 0
			\end{bmatrix},
			\end{align*}
			and different non-zero powers of $\N$ have the 1's on different ``diagonals.''\\
			
			So, by counting and collecting like terms, we can re-write the sum as	
			\begin{align*}
			\norm{\left(\jor_{\lambda,m}\right)^n}_2 &= \left( \abs{ m\lambda^n}^2  + \abs{(m-1){n\choose 1}\lambda^{n-1}}^2 + \dots + \abs{ {n\choose m-1} \lambda^{n-(m-1)}}^2   \right)^\frac{1}{2}\\
			&= \left( m^2\abs{\lambda}^{2n}  + (m-1)^2{n\choose 1}^2\abs{\lambda}^{2(n-1)} + \dots + {n\choose m-1}^2\abs{  \lambda}^{2(n-(m-1))}  \right)^\frac{1}{2}\\
			&= \left( m^2(\abs{\lambda}^{2})^n  + (m-1)^2{n\choose 1}^2(\abs{\lambda}^{2})^{(n-1)} + \dots + {n\choose m-1}^2(\abs{  \lambda}^{2})^{(n-(m-1))}  \right)^\frac{1}{2}\\
			&= \sqrt{f(\abs{\lambda}^2)}
			\end{align*}
			If $\abs{\lambda}< 1$ then $\abs{\lambda}^2< 1$. If $\abs{\lambda}^2 = 0$ then $ f(\abs{\lambda}^2) = 0 $. Thus  
			\begin{align*}
			\lim\limits_{n\to \infty}\norm{\left(\jor_{\lambda,n}\right)^n}_2 = \lim\limits_{n\to \infty}\sqrt{0} = 0.
			\end{align*}
			
			If $0 <  \abs{\lambda}^2< 1$ then by following a similar argument as in Part 1., $\lim\limits_{n\to\infty} f(\abs{\lambda}^2) = 0$. Thus 
			\begin{align*}
			\lim\limits_{n\to \infty}\norm{\left(\jor_{\lambda,n}\right)^n}_2  = \lim\limits_{n\to\infty}\sqrt{\left(f(\abs{\lambda}^2)\right)}= 0
			\end{align*}
			
			Therefore, $\lim\limits_{n\to \infty}\norm{\left(\jor_{\lambda,n}\right)^n}_2 = 0$ for $\abs{\lambda} < 1$.\\
			
			If $\abs{\lambda} \geq 1$ then $\abs{\lambda}^2 \geq 1$. Again, by following a similar argument as in Part 1., we get   $\lim\limits_{n\to\infty} \left( f(\abs{\lambda}^2) \right) = \infty$, thus
			\begin{align*}
			\lim\limits_{n\to \infty}\norm{\left(\jor_{\lambda,n}\right)^n}_2 = \lim\limits_{n\to\infty}\sqrt{\left(f(\abs{\lambda}^2)\right)} = \infty
			\end{align*}
			for $\abs{\lambda} \geq 1$.  
		\end{enumerate}
	\end{sln*}

\end{prob*}






\newpage




\begin{prob*}\textbf{6. }\\
	\begin{enumerate}
		\item Use logarithms and L'Hopital's Rule to argue that for any $\alpha > 0$,
		\begin{align*}
		\lim\limits_{x\to \infty} \left(5 + 4x\alpha^{-1} + 3\frac{x(x-1)}{2!}\alpha^{-2}+ 2\frac{x(x-1)(x-2)}{3!}\alpha^{-3} + \frac{x(x-1)(x-2)(x-3)}{4!}\alpha^{-4}  \right)^{\frac{1}{x}} = 1.
		\end{align*}
		
		
		\item Argue that 
		\begin{align*}
		\lim\limits_{n\to\infty} \left(\norm{\left(\jor_{\lambda,m}\right)^n}_2\right)^{\frac{1}{n}} = \abs{\lambda}.
		\end{align*}
	\end{enumerate}


	\begin{sln*}
		$\,$\\
		\begin{enumerate}
			\item Let 
			\begin{align*}
			f(x) = 5 + 4x\alpha^{-1} + 3\frac{x(x-1)}{2!}\alpha^{-2}+ 2\frac{x(x-1)(x-2)}{3!}\alpha^{-3} + \frac{x(x-1)(x-2)(x-3)}{4!}\alpha^{-4}.
			\end{align*}
			It suffices to show 
			\begin{align*}
			\lim\limits_{x\to\infty}\ln\left( \left(f(x)\right)^\frac{1}{x}\right) = \lim\limits_{x\to\infty} \frac{1}{x}\ln(f(x)) = \ln(1) = 0.
			\end{align*}
			By l'Hopital's rule:
			\begin{align*}
			\lim\limits_{x\to\infty} \frac{1}{x}\ln(f(x)) = \lim\limits_{x\to\infty} \frac{\frac{d}{dx}\ln(f(x))}{\frac{dx}{dx}} = \lim\limits_{x\to\infty} \frac{f'(x)}{f(x)}.
			\end{align*}
			Since $f'(x)$ is a polynomial in $x$ of degree 3, while $f(x)$ is a polynomial in $x$ of degree 4, 
			\begin{align*}
			\lim\limits_{x\to\infty} = \lim\limits_{x\to\infty}\frac{1}{x} = 0.
			\end{align*}
			Thus, 
			\begin{align*}
			\lim\limits_{x\to\infty}\ln\left( \left(f(x)\right)^\frac{1}{x}\right) = 0,
			\end{align*}
			i.e.,
			\begin{align*}
			\lim\limits_{x\to\infty}\left(f(x)\right)^\frac{1}{x} = 1.
			\end{align*}
			
			
			\newpage
			
			
			\item Let
			\begin{align*}
			f(n) = \norm{\left(\jor_{\lambda,m}\right)^n}_2.
			\end{align*}
			
			
		\end{enumerate}
	\end{sln*}
	
\end{prob*}







\newpage




\begin{prob*}\textbf{Extra credit 1}\\
	\begin{enumerate}
		\item Suppose that $[x_n],[y_n],[z_n],[u_n]$ are sequences of positive numbers such that
		\begin{align*}
		& \left[ (x_n)^{\frac{1}{n}} \right] \longrightarrow \alpha\\
		& \left[ (y_n)^{\frac{1}{n}} \right] \longrightarrow \beta\\
		& \left[ (z_n)^{\frac{1}{n}} \right] \longrightarrow \gamma\\
		& \left[ (u_n)^{\frac{1}{n}} \right] \longrightarrow \delta.
		\end{align*}
		
		
		\begin{enumerate}
			\item Evaluate the limit of 
			\begin{align*}
			\left[ (\alpha^n + \beta^n + \gamma^n + \delta^n)^{\frac{1}{n}} \right].
			\end{align*}
			
			
			\item Evaluate the limit of 
			\begin{align*}
			\left[ (x_n + y_n + z_n + u_n)^{\frac{1}{n}} \right].
			\end{align*}
		\end{enumerate}
	
	
	\item Suppose that $\A = \bigoplus^{23}_{i=1} \jor_{\lambda_i, m_i}$. Evaluate the limit
	\begin{align*}
	\lim\limits_{n\to\infty}\left( \norm{\A^n}_2 \right)^{\frac{1}{n}}.
	\end{align*}
	\end{enumerate}
	
	
	
	\begin{sln*}
		\begin{enumerate}
			\item $\,$\\
			\begin{enumerate}
				\item 
				\item
			\end{enumerate}
		
		
			\item 
		\end{enumerate}
	\end{sln*}
\end{prob*}




























\end{document}
