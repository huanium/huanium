\documentclass{book}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{hyperref}
\usepackage{tensor}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}


\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\had}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}

\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}

\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}


\makeatletter
\renewcommand{\@chapapp}{Part}
%\renewcommand\thechapter{$\bf{\ket{\arabic{chapter}}}$}
%\renewcommand\thesection{$\bf{\ket{\arabic{section}}}$}
%\renewcommand\thesubsection{$\bf{\ket{\arabic{subsection}}}$}
%\renewcommand\thesubsubsection{$\bf{\ket{\arabic{subsubsection}}}$}
\makeatother



\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
\begin{titlepage}\centering
 \clearpage
 \title{{\textsc{\textbf{Physics at the Perimeter Institute:\\  Topics in Theoretical Physics \\ 
 				\&\\ Quantum Simulation Research}}}\\ \smallskip - A Quick Guide - \\}
 \author{\bigskip Huan Q. Bui}
  \affil{Colby College\\$\,$\\ PHYSICS \& MATHEMATICS\\ Statistics \\$\,$\\Class of 2021\\}
 \date{\today}
 \maketitle
 \thispagestyle{empty}
\end{titlepage}

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

This guide is my notes from Perimeter Institute of Theoretical Physics Summer School, 2020. Topics include quantum information, thermodynamics, numerical methods, condensed matter physics, path integrals, and symmetries.\\

Enjoy!  



\newpage
\tableofcontents
\newpage





\chapter{Quantum Information \& Thermodynamics} 

The aim of this course is to understand the thermodynamics of quantum systems and in the process to learn some fundamental tools in Quantum Information. We will focus on the topics of foundations of quantum statistical mechanics, resource theories, entanglement, fluctuation theorems, and quantum machines. 

\newpage

\section{Foundations of Quantum Statistical Mechanics - Entanglement}
\newpage

\section{Resource Theories and Quantum Information}


\newpage
\section{Quantum Thermal Operations}


\newpage
\section{Fluctuation Theorems and Quantum Information}


\newpage
\section{Quantum Thermal Machines}






\newpage
\chapter{Numerical Methods \& Condensed Matter Physics}

This course has two main goals: (1) to introduce some key models from condensed matter physics; and (2) to introduce some numerical approaches to studying these (and other) models.  As a precursor to these objectives, we will carefully understand many-body states and operators from the perspective of condensed matter theory.  (However, I will cover only spin models.  We will not discuss or use second quantization.)\\



Once this background is established, we will study the method of exact diagonalization and write simple python programs to find ground states, correlation functions, energy gaps, and other properties of the transverse-field Ising model and XXZ model.  We will also discuss the computational limitations of exact diagonalization.  Finally, I will introduce the concept of matrix product states, and we will see how these can be used with algorithms such as the density matrix renormalization group (DMRG) to study ground state properties for much larger systems than can be studied with exact diagonalization.




\newpage

\section{Lecture 1}
\subsection{Introduction to many-particle states and operators}
\newpage
\subsection{Introduction to Ising and XXZ models}
\newpage
\subsection{Programming basics}
\newpage
\subsection{Finding expectation values}
\newpage

\section{Lecture 2: Exact diagonalization part 1}
\subsection{Representing models}
\newpage
\subsection{Finding eigenstates}
\newpage
\subsection{Energy gaps}
\newpage
\subsection{Phase transitions}
\newpage




\section{Lecture 3: Exact diagonalization part 2}
\subsection{Limitations of the method}
\newpage
\subsection{Using symmetries}
\newpage
\subsection{Dynamics}
\newpage




\section{Lecture 4: Matrix product states part 1}
\subsection{Entanglement and the singular value decomposition}
\newpage
\subsection{What is a matrix product state and why is it useful}
\newpage


\section{Lecture 5: Matrix product states part 2}
\subsection{Algorithms for finding ground states using matrix product states: iTEBD }

\newpage


\subsection{Algorithms for finding ground states using matrix product states: DMRG }

\newpage











\chapter{Path Integrals}

The goal of this course is to introduce the path integral formulation of quantum mechanics and a few of its applications. We will begin by motivating the path integral formulation and explaining its connections to other formulations of quantum mechanics and its relation to classical mechanics. We will then explore some applications of path integrals.


\newpage

\section{Introduction to path integrals and the semi-classical limit}

\newpage
\section{Propagator in real and imaginary time}


\newpage

\section{Perturbation theory}


\newpage

\section{Non-perturbative physics and quantum tunneling}


\newpage


\section{Topology and path integrals}


\newpage



\chapter{Symmetries}



The aim of this course is to  explore some of the many ways in which symmetries play a role in physics. Weâ€™ll start with an overview of the concept of symmetries and their description in the language of  group theory. We will then discuss continuous symmetries and infinitesimal symmetries, their fundamental role in Noether's theorem, and their formalization in terms of Lie groups and Lie algebras. In the last part of the course we will focus on symmetries in quantum theory and introduce representations of (Lie) groups and Lie algebras.


\newpage

\section{Lecture 1: Overview}
\subsection{Definition of symmetry}
\newpage
\subsection{Elements of group theory}
\newpage
\subsection{Examples }
\newpage



\section{Lecture 2}
\subsection{Continuous and discrete symmetries}
\newpage
\subsection{Infinitesimal symmetries}
\newpage
\subsection{Noether's theorem}


\newpage

\section{Lecture 3: Lie groups and Lie algebras}
\newpage

\section{Lecture 4: Symmetries in quantum mechanics}
\newpage
\section{Representation theory}



\newpage



\chapter{Research Project: Quantum Simulation}

\textbf{Quantum many-body physics on quantum hardware}\\


\underline{Project description:} Recently, there have been significant advances in several quantum computing platforms, including superconducting qubits and trapped ions. At this point, nontrivial quantum operations can be implemented
on tens of qubits, and the frontier continues to expand. This project will explore how these emerging
technologies can assist the realization and understanding of complex many-body quantum systems,
regarding both static aspects such as ground state properties and dynamical aspects such as quantum chaos
and thermalization. What new physics can we learn from near-term quantum computers? The ideal
outcome is an interesting and realistic proposal that can be carried out in one of the quantum platforms (see
for example the interplay between theoretical proposals and experiment).














\newpage


\section{Review: The density operator}


We often use the language of state vectors in quantum mechanics. An alternative language is that of \textit{density matrices/density operators}. We will use this language extensively in quantum information/quantum computation. First, we will introduce the formulation. Second, we look at some properties of the density operator. Finally, we look at an application where the density operator really shines -- as a tool for describing \textit{individual subsystems} of a composite quantum system.  


\subsection{Ensembles of Quantum States}

The density operator language provides a convenient means for describing quantum systems whose state is not completely known. Suppose a quantum system is in one of the states $\ket{\psi_i}$ where $i$ is an index, with respective probabilities $p_i$. We call $\{p_i, \ket{\psi_i}\}$ an \textit{ensemble of pure states}. \\

\begin{defn}[Density Operator]
	\begin{align}
	\boxed{\rho \equiv \sum_i p_i \ket{\psi_i}\bra{\psi_i}}
	\end{align}
\end{defn}



Suppose we let a unitary $\U$ act on a closed system. If the system was initially in the state $\ket{\psi_i}$ with probability $p_i$ then we get $\U \ket{\psi_i}$ with probability $p_i$. The density operator evolves as
\begin{align}
\rho = \sum_i p_i \ket{\psi_i}\bra{\psi_i} \stackrel{\U}{\rightarrow} \sum_i p_i \U \ket{\psi_i}\bra{\psi_i}\U^\dagger = \U \rho \U^\dagger.
\end{align}


Suppose we have a measurement gate $\M_m$. If the intial state is $\ket{\psi_i}$ then the probability of getting result $m$ is 
\begin{align}
p(m\vert i) = \bra{\psi_i} \M_m^\dagger \M_m \ket{\psi_i} = \tr\lp \M_m^\dagger \M_m \braket{\psi_i} \rp
\end{align}
where we have used the identity:
\begin{align}
\tr(A\ket{\psi}\bra{\psi}) = \sum_i \bra{i}A \ket{\psi}\bra{\psi}\ket{i} = \bra{\psi}A\ket{\psi}.
\end{align}
With this, the total probability of measuring $m$ is 
\begin{align}
p(m) = \sum_i p(m|i)p_i = \sum_i p_i \tr\lp \M_m^\dagger \M_m \ket{\psi_i}\bra{\psi_i} \rp = \tr\lp \M_m^\dagger \M_m \rho \rp.
\end{align}

The state after the measurement is thus
\begin{align}
\ket{\psi_i^m} = \f{\M_m \ket{\psi_i}}{\sqrt{\bra{\psi_i}\M_m^\dagger \M_m \ket{\psi_i}}}.
\end{align}
The corresponding density operator for this state is thus
\begin{align}
\rho_m = \sum_i p(i|m)\ket{\psi_i^m}\bra{\psi_i^m} = \sum_i p(i|m)\f{\M_m \ket{\psi_i}\bra{\psi_i}\M_m^\dagger}{\bra{\psi_i}\M_m^\dagger \M_m \ket{\psi_i}}.
\end{align}
From probability theory we have $p(i|m) = p(m|i)p_i/p(m)$, so:
\begin{align}
\rho_m = \sum_i p_i \f{\M_m \ket{\psi_i}\bra{\psi_i}\M_m^\dagger}{\tr\lp \M^\dagger_m \M_m \rho \rp} = \f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp}.
\end{align}




Finally, suppose we have a quantum system in state $\rho_i$ with probability $p_i$. The system might be described by the density matrix
\begin{align}
\rho = \sum_i p_i \rho_i.
\end{align}
Here's why:
\begin{align}
\rho = \sum_{i,j}p_i p_{ij}\ket{\psi_{ij}}\bra{\psi_{ij}} = \sum_i p_i \rho_i
\end{align}
where we have used the definition
\begin{align}
\rho_i = \sum_{j}p_{ij}\ket{\psi_{ij}}\bra{\psi_{ij}}.
\end{align}

We call $\rho$ the \textit{mixture} of the states $\rho_i$ with probabilities $p_i$. This concept of mixture comes up up repeatedly in the analysis of problems like quantum noise, where the effect of the noise is to introduce ignorance into our knowledge of the quantum state. A simple example is provided by the measurement scenario. Imagine that for some reason out record of the result $m$ of the measurement was lost. We would have a quantum system in the state $\rho_m$ with probability $p(m)$, but would no longer know the actual value of $m$. The state of such a quantum system would therefore by described by the density operator
\begin{align}
\rho = \sum_m p(m)\rho_m = \sum_m \tr\lp \M_m^\dagger \M_m \rho \rp\f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp} = \sum_m \M_m \rho \M_m^\dagger.
\end{align}






























\subsection{General Properties of the Density Operator}



\begin{thm}[Characterization of density operators] 
	An operator $\rho$ is the density operator to some ensemble $\{p_i, \ket{\psi_i} \}$ if and only if it statisfies the conditions
	\begin{itemize}
		\item \textbf{Trace condition:} $\tr(\rho) = 1$.
		\item \textbf{Positivity:} $\rho$ is a positive operator, i.e., $\bra{\varphi}\rho\ket{\varphi} \geq 0 \forall \varphi$.
	\end{itemize}
\end{thm}


With this definition we can reformulate the postulates of quantum mechanics in the language of density operators as 
\begin{itemize}
	\item \textbf{Postulate 1:} Associated to any isolated physical system is a Hilbert space known as the \textit{state space} of the system. The system is completely described by its \textit{density operator}, which is a positive operator $\rho$ with trace one, acting on the state space of the system. If a quantum system is in the state $\rho_i$ with probability $p_i$ then the density operator for the system is $\sum _i p_i \rho_i$.
	
	\item \textbf{Postulate 2:} The evolution of a \textit{closed} quantum system is described by a \textit{unitary transformation}:
	\begin{align}
	\rho' = \U \rho \U^\dagger.
	\end{align} 
	
	
	
	\item \textbf{Postulate 3:} Measurements are described by a collection $\{\M_m\}$ of \textit{measurement operators}. The index $m$ refers to the measurement outcomes that may occur. If the state of the quantum system is $\rho$ immediately before the measurement then the probability that result $m$ occurs is given by
	\begin{align}
	p(m) = \tr\lp \M_m^\dagger  \M_m \rho_m \rp
	\end{align}
	and the state of the system after the measurement is 
	\begin{align}
	\f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp}.
	\end{align}
	The measurement operators satisfy the completeness equation
	\begin{align}
	\sum_m \M_m^\dagger \M_m = \Id.
	\end{align}
	
	
	
	\item \textbf{Postulate 4:} The state space of a composite physical system is the tensor product of the state spaces of the component physical systems. E.g. the joint state of the total system can be written as $\rho_1 \otimes \dots \otimes \rho_n$.
\end{itemize}



\begin{thm}[Pure states] 
	$\tr(\rho^2) \leq 1$. Equality occurs if and only if $\rho$ is a pure state. 
\end{thm}



\begin{thm}[Unitary freedom in the ensemble for density matrices] 
	The sets $\ket{\tilde{\varphi}_i}$ and $\ket{\tilde{\psi}_j}$ generate the same density matrix if and only if they are unitarily similar, i.e., 
	\begin{align}
	\ket{\tilde{\varphi}_i} = \sum_{j} u_{ij}\ket{\tilde{\psi}_j}
	\end{align}
	where the $u_{ij}$ are entries of a unitary $\U$.
	
\end{thm}




\subsection{The Reduced Density Operator}
Te deepest application of the density operator is as a descriptive tool for \textit{sub-systems} of a composite quantum system. Such a description is given by the \textit{reduced density operator}. \\

Suppose we have physical systems $A$ and $B$ whose state is described by the density operator $\rho^{AB}$. The reduced density operator for $A$ is then given by
\begin{align}
\rho^A = \tr_B\lp \rho^{AB} \rp
\end{align}
where $\tr_B$ is the \textit{partial trace} over system $B$, defined by
\begin{align}
\tr_B\lp \ket{a_1}\bra{a_2} \otimes \ket{b_1}\bra{b_2} \rp \equiv \ket{a_1}\bra{a_2}\tr\lp \ket{b_1}\bra{b_2} \rp = \ket{a_1}\bra{a_2}\braket{b_2}{b_1}
\end{align}
where $\ket{a_1}, \ket{a_2}$ are any two vectors in the state space of $A$, and $\ket{b_1}, \ket{b_2}$ are any two vectors in the state space of $B$. \\




\begin{exmp}
	To understand this construction, consider an example where $\rho^{AB} = \rho \otimes \sigma$ where $\rho,\sigma$ describe $A,B$ respectively. Then
	\begin{align}
	\rho^A = \tr_B\lp \rho \otimes \sigma \rp = \rho \tr(\sigma) = \rho
	\end{align}
	as expected. 
\end{exmp}




\newpage








\section{Quantum Computational Complexity: An Introduction}

In this section we look at quantum computational complexity, as introduced in this \href{https://arxiv.org/pdf/0804.3401v1.pdf}{\underline{paper}} by John Watrous at the IQC, the University of Waterloo.

\subsection{Introduction}

\begin{itemize}
	\item \textbf{P}
	
	\item \textbf{NP}
	
	
	\item \textbf{BPP}
	
	\item \textbf{PP}
	
	\item \textbf{MA}
	
	
	\item \textbf{AM}
	
	\item \textbf{SZK}
	
	\item \textbf{PSPACE}
	
	\item \textbf{EXP}
	
	\item \textbf{NEXP}
	
	
	\item  \textbf{PL}
	
	\item \textbf{NC}
\end{itemize}

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.3]{complexity}
\end{figure}










\newpage






\section{Quantum Simulation: A Primer}

Here's what Feynman said in 1982:\\

\textit{``Can physics be simulated by a universal computer? [...] the physical world is quantum mechanical, and therefore the proper problem is the simulation of quantum physics [...] the full description of quantum mechanics for a large system with $R$ particles [...] has too many variables, it cannot be simulated
with a normal computer with a number of elements proportional to $R$ [ ... but it can be simulated with ] quantum computer elements. [...] Can a quantum system be probabilistically simulated by a classical (probabilistic, I'd assume) universal computer? [...] If you take the computer to be the classical kind I've described so far [..] the answer is certainly, No!''}


\subsection{Simulation in action}

The heart of simulation is the solution of differential equations which capture the physical
laws governing the dynamical behavior of a system. The goal is generally: given an initial state of the system, what is the state at some other time and/or position? Solutions are usually obtained by \textit{approximating} the state with a digital representation, then \textit{discretizing} the differential
equation in space and time such that an iterative application of a procedure carries the state from the initial to the final conditions.\\

The error in this procedure is \textit{bounded}, and known not to grow faster than some small power of the number of iterations. Furthermore, not all dynamical systems can be simulated efficiently: generally, only those systems which can be described efficiently can be simulated efficiently.\\

Simulating quantum systems using classical computers is usually inefficient. The dynamical behavior of many simple quantum systems is governed the SE:
\begin{align}
i\hbar \p_t \ket{\psi} = \had \ket{\psi}.
\end{align}
We will set $\hbar =1$.  For real particles in space, we often have
\begin{align}
i\p_t \psi(x) = \lb -\f{1}{2m}\p_x^2 + V(x) \rb \psi(x).
\end{align}

The key challenge in simulating quantum systems is the \textit{exponential} number of differential equations which must be solved. For 1 qubit evolving according to the
SE, a system of 2 differential equations must be solved; for 2
qubits, 4 equations; and for $n$ qubits, $2^n$ equations. Sometimes, insightful approximations can be made which reduce the effective number of equations involved, but there are many physically interesting quantum systems for which no such approximations are known.\\

There are many important quantum systems for which classical simulation is intractable. For example:
\begin{align}
\mbox{Hubbard model: } \had = \sum^n_{k=1}V_0 n_{k^\uparrow}n_{k\downarrow} + \sum_{k,j\mbox{ neighbors,}\sigma} t_0 c_{k\sigma}^* c_{j\sigma}.
\end{align}
\begin{align}
\mbox{Ising mode: } \had = \sum_{k=1}^n \vec{\sigma_k}\cdot \vec{\sigma}_{k+1} .
\end{align}
Quantum computers can efficiently simulate quantum systems for which there is no known efficient classical simulation.












\subsection{The quantum simulation algorithm}

The quantum simulation is concerned with the solution of $i \p_t \ket{\psi} = \had \ket{\psi}$, which, for a time-independent $\had$, is just
\begin{align}
\ket{\psi(t)} = e^{-i\had t}\ket{\psi(0)}.
\end{align}
$\had$ is in general difficult to exponentiate, but a good beginning is the first order solution 
\begin{align}
\ket{(\psi(t+\Delta t))} \approx (\Id - i\had \Delta t)\ket{\psi(t)}.
\end{align}
This is tractable, but often is not very satisfactory. \\

Efficient approximations to higher oder is possible for many \textit{classes} of Hamiltonians. For example, in most physical systems, the Hamiltonian can be written as a sum over many \textit{local} interactions. For a system of $n$ particles:
\begin{align}
\had = \sum^L_{k=1}\had_k \sim \bigoplus^L_{k=1}\had_k
\end{align} 
where each $\had_k$ acts on at most a constant $c$ number of systems, and $L$ is a polynomial in $n$. For instance, the terms $\had_k$ are often just 2-body interactions such as $X_i X_j$ and 1-boy Hamiltonians such as $X_i$. Note that both the Hubbard and Ising models have Hamiltonians of this form. There are sometimes additional global symmetry constraints such as particle statistics.\\

Note that although $e^{-i\had t}$ is difficult to compute, $e^{-i\had_k t}$ acts on a much smaller subsystem, and is straightforward to approximate using quantum circuits. However, because $[\had_j, \had_k] \neq 0$ in general (obviously, they don't commute in general), $e^{-i\had t} \neq \prod_k e^{-i \had_k t}$. So, we might want to ask how $e^{-i\had_k t}$ be useful in constructing $e^{-i\had t}$. \\

\textbf{Remarks:}
\begin{itemize}
	\item If the $\had_i$'s commute, then we do have $e^{-i \had t} = \prod_k e^{-i \had_k t}$. 
	
	\item The restriction of $\had_k$ to involve at most $c$ particles \textit{implies} that in the sum of the Hamiltonians, $L$ is upper bounded by a polynomial in $n$. 
\end{itemize}



The heart of quantum simulation algorithms is the following asymptotic approximation theorem:
\begin{thm}[Trotter formula]
	Let $A$ and $B$ be Hermitian operators. Then for any real $t$, 
	\begin{align}
	\lim_{n\to \infty}\lp e^{-iAt/n}e^{iBt/n} \rp = e^{i(A+B)t}.
	\end{align}\qed
\end{thm}

Of course, the theorem holds even if $A$ and $B$ don't commute. For now, we only consider the cases where $A,B$ are Hermitian. But $A,B$ can actually be generators of certain kinds of semigroups, which correspond to general quantum operations.\\

Using the theorem (and the proof of the theorem, which I won't show here), we can get
\begin{align}
e^{i(A+B)\Delta t} = e^{iA \Delta t}e^{i B\Delta t} + \mathcal{O}(\Delta t^2).
\end{align}
Similarly, 
\begin{align}
e^{i(A+B)\Delta t} = e^{iA\Delta t/2}e^{iB \Delta t}e^{iA \Delta t/2} + \mathcal{O}(\Delta t^3). 
\end{align}




\subsection{The algorithm}

\begin{itemize}
	\item \textbf{Inputs:} (1) A Hamiltonian $\had = \sum_k \had_k$ acting on an $N$-dimensional system, where each $\had_k$ acts on a small subsystem of size independent of $N$, (2) an initial state $\ket{\psi_0}$, of the system at $t=0$, (3) a positive, non-zero accuracy $\delta$, and (3) a time $t_f$ at which the evolved state is desired. 
	
	
	\item A state $\ket{\tilde{\psi}(t_f)}$ such that
	\begin{align}
	\abs{\bra{\tilde\psi (t_f)} e^{-i \had t_f} \ket{\psi_0}}^2 \geq 1 - \delta.
	\end{align}
	
	
	\item Choose a representation such that the state $\ket{\tilde\psi}$ of $n = \mbox{poly}(\log N)$ qubits approximates the system and the operators $e^{- \had_k \Delta t}$ have efficient quantum circuit approximations. Selection an approximation method and $\Delta t$ such that the expected error is acceptable, construct the corresponding quantum circuit $\U_{\Delta t}$ fo the iterative step, and do the following:
	\begin{itemize}
		\item Initialize state: $\ket{\tilde \psi_0} \leftarrow \ket{\psi_0}; j = 0.$ 
		\item Iterative update: $\to \ket{\tilde \psi_{j+1}} = \U_{\Delta t}\ket{\tilde \psi_j}$.
		\item Loop: $\to j = j+1;$ go to the second step until $j\Delta t \geq t_f$.
		
		\item Final result: $\to \ket{\tilde \psi(t_f)} = \ket{\tilde \psi}$.
	\end{itemize}
\end{itemize}


\begin{thm}[Baker-Campbell-Hausdorf formula]
	\begin{align}
	e^{(A+B)\Delta t} = e^{A \Delta t}e^{B \Delta t}e^{-(1/2)[A,B]\Delta t^2} + \mathcal{O}(\Delta t^3). 
	\end{align}
\end{thm}





\subsection{An illustrative example}

We have seen the case where the Hamiltonian is a sum of local interactions. However, this is not a fundamental requirement. Efficient quantum simulations are possible even for Hamiltonians which act non-trivially on all or nearly all parts of a large system. \\

Consider the Hamiltonian
\begin{align}
\had = Z_1 \otimes Z_2 \otimes \dots \otimes Z_n
\end{align}
which acts on an $n$ qubit system. This can be simulated efficiently. What we desire is a simple quantum circuit which implements $e^{-i \had \Delta t}$, for arbitrary values of $\Delta t$. A circuit that does this is given by
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qs1}
\end{figure}

Even though the Hamiltonian involves all the qubits in the system, it does so in a \textit{classical} manner: the phase shift applied to the system is $e^{-i\Delta t}$ if the \textit{parity} of the $n$ qubits in the computational basis is even. else the phase shift would be $e^{i\Delta t}$. Thus, a simple simulation of $\had$ is possible by first classical computing the parity, then applying the appropriate phase shift conditioned on the parity, then uncomputing the parity. \\

Extending the same procedure allows us to simulate more complicate extended Hamiltonians. Specifically, we can efficiently simulate any Hamiltonian of the form
\begin{align}
\had = \bigotimes^n_{k=1}\sigma^k_{c(k)}
\end{align}
where $\sigma^k_{c(k)}$ is a Pauli matrix (or identity) acting on the $k$th qubit, with $c(k) \in \{0,1,2,3\}$ specifying one of $\{I,X,Y,Z\}$. The qubits upon which the identity acts can be disregarded, and $X$ or $Y$ terms can be transformed by single qubit gates to $Z$ operations. This leaves a Hamiltonian of the form given above, which can be efficiently simulated. \\

Using this procedure allows us to simulate a wide class of Hamiltonians containing terms which are not local. It is possible to simulate a Hamiltonian of the form $\had = \sum^L_{k=1} \had_k$ where we only require that the individual $\had_k$ have a tensor product structure, and that $L$ is polynomial in the total number of particles $n$. 







\subsection{Perspectives on quantum simulation}

 
Quantum simulation is similar to classical methods, but differs fundamentally in many aspects. Each iteration of the quantum algorithm must completely replace the old state with a new one and there is no way to obtain any nontrivial information from an intermediate step without changing the algorithm (due to the no-cloning theorem). Furthermore, the final measurement must be chosen to provide the desired result because it collapses the entire state. \\

Another difficult problem is the simulation of equilibration processes. A system with Hamiltonian $\had$ in contact with an environment at temperate $T$ will come to thermal equilibrium in a state known as the \textit{Gibbs} state,
\begin{align}
\rho_{\text{therm}} = \f{e^{-\had/k_BT}}{\mathcal{Z}} \equiv \f{e^{-\had/k_BT}}{\tr\lp e^{-\had/k_BT} \rp}
\end{align}
where of course $\mathcal{Z}$ is the partition function. We can check that $\tr(\rho) = 1$. For now, we don't know how a quantum computer can simulate this. \\

Indistinguishability of particles also places a constraint on the state vector of a system which manifests itself in two ways: bosons and fermions. The state vector of a system of bosons remains unchanged under permutation of any two constituents. System of fermions experience a sign change in their state vector under interchange of any two constituents. Both kinds of systems can be simulate efficiently on a quantum computer. 















\newpage


\section{Problems in Quantum Simulations and Quantum Algorithms}

In this section, we will be looking at the following \href{https://arxiv.org/pdf/2001.03685.pdf}{\underline{review}} on quantum algorithms for quantum chemistry and materials science. 

\subsection{Quantum Simulation: An Overview}

\subsubsection{Introduction}

The interest in quantum computing for quantum simulations of molecules and materials stems from the fact that in many cases, the chemistry and physics of molecules and materials is best described using quantum mechanics. In the worst case, quantum simulation is exponentially hard on classical computers. Quantum computers have advantages over classical computers, but these advantages are problem-specific. \\


As we have seen before, the natural problem to solve on a quantum computer is the time evolution of a quantum system given some initial state:
\begin{align}
i\p_t \ket{\psi(t)} = \had \ket{\psi(t)}.
\end{align}
This problem, as we have pointed out, is of polynomial cost on a quantum computer, which can offer exponential speedup over classical computers. However, it is necessary to prepare the initial state, which may be difficult. In particular, preparing a low-energy state may be challenging, which naturally leas to considering other important problems:
\begin{align}
&\mbox{Ground state:} \quad \had\ket{\psi_0} = E_0 \ket{\psi_0}; \quad E_0 = \min_{\ket{\psi}}\bra{\psi}\had\ket{\psi}\\
&\mbox{Thermal averages:} \quad \langle \mathcal{A} \rangle = \f{\tr\lp \mathcal{A}e^{-\beta\had} \rp}{\tr\lp e^{-\beta\had} \rp}
\end{align} 
Ground state determination lies in complexity class QMA, a class of problems \textit{not known to be efficiently solvable in general on a quantum computer}. This means that thermal averages cannot in general be computed efficiently on a quantum computer, since in the limit of zero temperature, this problem reduces to ground-state determination. \\

To understand quantum advantage in chemistry, condensed matter physics, and quantum materials science, we must be guided by actual empirical data in the form of numerical and theoretical experiments with quantum algorithms and quantum devices on simulation problems of interest. This requires progress, of course. 



\subsubsection{Current quantum architectures}
There's \textit{analog quantum computation} and there's \textit{digital quantum computation}. Analog quantum computation was proposed by Feynman. The idea of analog quantum computation is to build a lattice of spins with tunable interactions and let it simulate physical systems. The limitation, however, is that this is quite limited because it can only simulate a certain set of physical systems. Furthermore, its accuracy is limited. \\

Digital quantum computation is more general. Digital quantum computation involves qubits and gates. One can show that with a finite set of gates, one can in principle generate arbitrary quantum evolutions to arbitrary precision. Every problem in the complexity class BQP can be mapped into a quantum program. \\


The third model of quantum computation is \textit{adiabatic quantum computation}. This model is considered equivalent to the digital version, but is less practical. We won't worry about this. 



\subsubsection{Building a circuit-based digital quantum computer}

The natural enemy of quantum computation is decoherence -- the tendency of quantum systems to decay into their classical states. We are current in the \textit{noisy intermediate-scale quantum} (NISQ) era, where qubit technologies (superconducting qubits, ion traps, etc) have reached the point where small devices of a few dozen qubits can be sufficiently isolated from decoherence to execute non-trivial quantum algorithms of a few tens to hundreds of gates. With these devices, an artificial but well-defined problem that is intractable classically is solved on a quantum computer. However, these problems are often not practical. The more important question is when a more relevant problem can be solved on a quantum computer. \\


To address large-scale problems, \textit{quantum error-correction} (QEC) is necessary. QEC leads to a distinction between \textit{logical} and \textit{physical} qubits. The former are error-corrected and encoded in the state of many physical qubits. Quantum algorithms are performed on the logical qubits, and the error-correction scheme translates the operations on logical qubits into physical operations. 



\newpage




\subsection{Simulation challenges in molecular and materials science }

In this section we look at some scientific problems of relevance to quantum simulation.

\subsubsection{Quantum chemistry}

Quantum chemistry is concerned with determining the low-lying eigenstates of the electronic Hamiltonian of a molecule. he eigenstates are determined for fixed sets of nuclear positions within the Born-Oppenheimer approximation. Determining the electronic energy as a function of nuclear position is key to understanding chemical reactivity, product distribution, and reaction rates.\\

Some examples of problems in quantum chemistry include
\begin{itemize}
	\item The chemistry of enzyme active sites
	\item Transition metal nanocatalysts an surface catalysts
	\item Light harvesting and the vision process
\end{itemize}

The basic metric is whether ground-states or low-energy eigenstate quantum algorithms yield more accurate energies than the best classical algorithms, for the problem sizes of interest. 

\subsubsection{Quantum molecular spectroscopy}


The theoretical goal is to compute the eigenstates of the nuclear SE. There are challenges, of course. First, the nuclear Hamiltonian is often not known, because interactions are mediated by electrons. Second, nuclear rotational and vibrational motion is often far from harmonic and not well approximate by simple mean-field theories. Third, even when the nuclear SE has been properly formulated, one faces the challenge of representing th eigenstates. \\

Some famous examples include:
\begin{itemize}
	\item Spectra of floppy molecules
	\item Hydrogen bonded clusters
\end{itemize}

There are similarities with the quantum chemistry problems, but the differences are more significant. The Hamiltonian is often more complicated because interactions are now many-body. Also, the number of states is often orders-of-magnitude greater. 


\subsubsection{Chemical quantum dynamics}

Chemical quantum dynamics if concerned with modeling time-dependent electronic and nuclear quantum effects in molecules. Some problems include:
\begin{itemize}
	\item Proton coupled electron transfer
	\item Vibrational dynamics in complex environments
	\item Plasmonic chemistry
\end{itemize}


There are multiple practical challenges. Some of this may be viewed as an issue of representation. In addition, the dynamical quantum state involves near continuum degrees of freedom, which makes discretizing the Hilbert space a challenging problem. 


\subsubsection{Correlated electronic structure in materials}

The goal here is to determine low-energy properties of materials such as Mott insulators. Some famous problems include:
\begin{itemize}
	\item High-temperature superconductivity
	\item Non-Fermi liquid behavior in heavy fermion compounds and fermionic systems near criticality
	\item Quantum fluctuations in two-dimensional systems
	\item Frustrated spin systems
\end{itemize}






%\subsubsection{Dynamical quantum effects in materials}












\newpage


\subsection{Challenges for quantum algorithms in quantum simulation}

\subsubsection{Overview of algorithms}

In this section, we look at the current status and theoretical and practical challenges to implement quantum algorithms for quantum problems. \\


The first step in a quantum simulation is to choose a \textit{representation} for the Hamiltonian and the states. We will examine the possibilities for different qubit representations, and open questions.\\

Quantum ground-state algorithms fall into different classes. QPE is a direct route to nearly exact eigenstate determination, but has been challenging to implement in the near-term era. On the other hand, \textit{variational quantum algorithms} provide the possibility to introduce approximations with adjustable circuit depth, which are determine via optimization, usually implemented in a hybrid-quantum classical loop. \\

While polynomial time algorithms for \textit{quantum time evolution} have been known for some time, algorithms with optimal asymptotic complexity as well as favorable scaling with error and low prefactors, remain an area of active research.




\subsubsection{Qubit representation of many-body systems}

To simulate many-body systems on a digital computer (either quantum or classical), the infinite-dimensional Hilbert space of a many-body system has to be truncated. The most direct route then is to define a finite set of \textit{basis functions} and then try to project the exact many-body Hamiltonian onto the chosen basis. The resulting discretized system is then expressed in terms of qubits. A choice of a good representation is important as it may affect the simulation cost dramatically. 


\begin{enumerate}
	\item \textit{Ab initio electronic structure qubit representations:}\\
	
	This concerns with the electronic structure. Consider the Hamiltonian that appears regularly in chemistry and physics
	\begin{align}
	\had = \sum^K_{i=1}-\f{1}{2}\laplacian + V(r_i) + \sum_{1\leq i<j\leq K}\f{1}{\abs{r_i - r_j}}
	\end{align}
	where $K$ is the number of electrons, $V$ is the potential created by atomic nuclei at a point $r$, and the last term represents the Coulomb interaction. Each electronic has $r_i \in \mathbb{R}^3$ and spin $\omega_i \in \{\uparrow, \downarrow\}$. So, the total wavefunction (for $K$ electrons) has the form $\psi = \psi(\vec{x}_1,\dots,\vec{x}_K)$ where $\vec{x}_i = (r_i,\omega_i)$. From Fermi statistic, we require that $\psi$ is anti-symmetric under a position swap.\\
	
	The first step is to approximate the electronic Hamiltonian with a simpler simulator Hamiltonian. This is usually achieved by truncate the Hilbert space of a single electron to a finite set of basis functions $\psi_1,\dots,\psi_N$. \\
	
	Electronic structure simulation algorithms are based on the \textit{first quantization method} (\href{https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.79.2586}{\underline{link}}). We won't worry about this too much.\\
	
	The \textit{second quantization method} often results in simpler simulator Hamiltonian and requires few qubits. This methods is particularly well suited for quantum simulation algorithms and has been experimentally demonstrated for small molecules. Given a set of $N$ basis functions $\psi_1,\dots,\psi_N$, the second quantized simulator Hamiltonian is given by
	\begin{align}
	\had = \sum^N_{p,q=1} t_{pq}\hat{c}_p^\dagger \hat{c}_q + \f{1}{2}\sum^N_{p,q,r,s=1}u_{pqrs}\hat{c}^\dagger_p\hat{c}_q^\dagger \hat{c}_r \hat{c}_s.
	\end{align}
	Again, we're not going to worry too much about these. 
	
	
	
	
	
	
	
	\item \textit{Electronic basis functions:}\\
	
	The discretization of the electronic Hilbert space for a quantum simulation requires balancing two concerns. We need to represent the state with the smallest number of qubits, but also retain maximal Hamiltonian sparsity. \\
	
	There are two families of basis functions in wide use in quantum chemistry and quantum materials science: atomic orbital \textit{Gaussian bases} and \textit{Plane waves}. Gaussian bases are commonly used in molecular simulations due to their compactness, while plane waves are used in crystalline materials simulations. \\
	
	In Gaussian bases, linear combinations of Gaussian functions are placed at the nuclear positions. When they are place where the ground-state electron density is highest, they give a compact representation of the wavefunction for bound states. However, the Hamiltonian is not sparse, which leads to high gate counts. \\
	
	Plane waves offer greater simplicity as the accuracy of the basis is controlled by a single parameter, namely the kinetic energy cutoff. The number of plane waves needed to reach a desired accuracy is larger than th number of Gaussian states, but the Hamiltonian contains fewer terms ue to momentum conservation. \\
	
	The need to expose more sparsity in the Hamiltonian while retaining a reasonable compact wavefunction is an active area of research in both classical and quantum algorithms.  
	
	
	\item \textit{Fermions-to-qubit mappings:}\\
	
	Since the basic units of a quantum computer are qubits rather than fermions, any quantum simulation algorithm of fermions emplyos a suitable encoding of fermionic degrees of freedom into qubits. The standard \textit{Jordan-Wigner} mapping identifies each Fermi mode with a qubit such that the empty and the occupied states are mapped to the qubit basis states $\ket{0}$ and $\ket{1}$ respectively. The Jordan-Wigner mapping/transformation is a transformation that maps spin operators onto fermionic creation and annihilation operators. 
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{jw}
	\end{figure}
	A natural question is whether the number of qubits required to express a Fermi system can be reduced by exploiting symmetries such as the particle number conservation or
	the point group symmetries of molecules. For example, zero
	temperature simulations often target only one symmetry sector containing the ground state. This motivates the study
	of \textit{symmetry-adapted fermion-to-qubit mappings}.
\end{enumerate} 



\subsubsection{Quantum algorithms for ground and excited states}

There are many approaches to obtaining ground states or
excited states on a quantum computer. State preparation
procedures attempt to construct a circuit to prepare a state
with as large as possible overlap with the desired eigenstate. One set of such procedures, which includes adiabatic state preparation and quantum imaginary time evolution, uses a prescribe evolution path. An alternative strategy is based on \textit{variational methods}, which are often called variational quantum eigensolvers. Here, the preparation circuit itself is defined via the optimization of the energy with respect to parameters of the circuit. \\


Given some state with sufficiently large overlap with the desired state, one can perform \textit{quantum phase estimation} (QPE) which simultaneously projects the state onto an eigenstate of the Hamiltonian and obtains an estimate for the energy of this eigenstate. The error, as we have seen, is inversely proportional to the simulation time. The probability of successfully projecting onto the desired state is given by the square overlap of the input state and
the desired state, and it is thus necessary to use some other
method (such as the state preparation procedures above) to
prepare an input state with sufficient overlap with the desired
state.\\


There are weaknesses to this, of course. While phase estimation allows the deviation of the final state from an exact eigenstate to be systematically reduced, it can require deep circuits with many controlled gates that are challenging for devices with limited coherence and without error correction. Variational methods replace such circuits b a large number of potentially short simulations. However, if one des not measure the energy by phase estimation, but instead by expressing the Hamiltonian as a sum of multi-qubit Pauli operators and measuring the terms individually, the state preparation and measurements must be repeated many times, with the error converging only as the square root of the number of repetitions. Variational methods are also limited by the variational form and ability to solve the associated optimization
problem, which may by itself represent a difficult classical
optimization.


\subsubsection{Preparing ground states along a prescribed path}

\begin{enumerate}
	\item \textit{Adiabatic state preparation:}\\
	
	 
	This state preparation relies on the \textit{adiabatic theorem}
	
	
	\begin{thm}[Adiabatic Theorem]
		A physical system remains in its instantaneous eigenstate if a given perturbation is acting on it slowly enough and if there is a gap between the eigenvalue and the rest of the Hamiltonian's spectrum. 
	\end{thm}
	In simpler terms, this says that gradually changing conditions allow the system to adapt its configuration, hence the probability density is modified by the process. if the system started in an eigenstate of the initial Hamiltonian, it will end in the \textit{corresponding} eigenstate of the final Hamiltonian.\\
	
	To make use of this theorem, one chooses a Hamiltonian path $\had(\lambda)$ where $\lambda \in [0,1]$, such that the ground state of $\had(0)$ is easily prepared, while $\had(1)$ is the Hamiltonian whose ground state one wants to obtain. The system is then evolved under the time-dependent SE 
	\begin{align}
	i\p_t \ket{\psi_t} = \had(t/T)\ket{\psi_t},\quad t \in [0,T].
	\end{align}
	When $T\to \infty$ and if the spectrum of $\had_{t/T}$ is gapped for all $t$, the final state is the exact ground state of $\had$. Away from the adiabatic limit $T\to \infty$, corrections are necessary. \\
	
	The main disadvantage of this approach is that it is limited by the $T\to \infty$ requirement, and the fact that a path without degeneracies must be chosen. 
	
	
	\item \textit{Quantum imaginary-time evolution:} \\
	
	In classical simulations, one popular approach to prepare (nearly exact) ground-states is imaginary-time evolution,
	which expresses the ground-state as the long-time limit of the
	imaginary-time SE $-\p_t \ket{\Phi(\beta)} = \had \ket{\Phi(\beta)}$:
	\begin{align}
	\ket{\psi} = \lim_{\beta \to \infty} \f{e^{-\beta \had} \ket{\Phi(\beta)}}{\norm{e^{-\beta \had} \Phi(\beta)}}.
	\end{align} 
	To perform imaginary time evolution on a quantum computer, it is necessary to implement the (repeated) action of the
	short-imaginary-time propagator $e^{-\Delta \tau \had}$
	on a state. Given a Hamiltonian that can be decomposed into geometrically local terms,
	\begin{align}
	\had = \sum _m \hat{h}_m
	\end{align}
	and a state $\ket{\psi}$ with finite correlation length $C$, the action $e^{-\Delta \tau \hat{h}_i}$ can be generated by a unitary $\U = e^{i\hat{A}}$ acting on $\mathcal{O}(C)$ qubits surrounding those acted on by $\hat{h}_i$, i.e.,
	\begin{align}
	\f{e^{-\beta \had} \ket{\psi}}{\norm{e^{-\beta \had} \psi}} = \U \ket{\psi}= e^{i\hat{A}}\ket{\psi}
	\end{align}	
	where the coefficients of the Pauli strings in $\hat{A}$ can be determined from local measurements of the quits around $\hat{h}_i$.\\
	
	Quantum imaginary time evolution becomes inefficient in terms of the number of measurements an complexity of the operator $\hat{A}$ if the domain $C$ grows to be large along the imaginary time evolution path. 
	
	
	
	
	
	
	
	
	
	
	
\end{enumerate}








\subsubsection{Variational state preparation and variational quantum eigensolver}


States can also be prepared through variational methods. Here, similar to classical variational approaches, one chooses a class of ansatz states for the ground state of the Hamiltonian of interest. Generally speaking, such an ansatz consists of some initial state and a unitary circuit parametrized by some set of classical variational parameters. Applying this circuit to the initial state
yields a guess for the ground state, whose energy is then evaluated. This yields an upper bound to the true ground state
energy. One then varies the variational parameters to lower
the energy of the ansatz state.\\

Choosing the class of ansatz states is also a balancing act. On the one hand, the class should contain an accurate approximation to the true ground state of the system. On the other hand, we want a class of circuits that are easily executed on the available quantum computer. Finally, it
is important for the classical optimization over the variational
parameters to be well-behaved, so as to be able to find low-energy minima. 


\begin{enumerate}
	\item \textit{Unitary coupled cluster:} \\
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{vqei}
	\end{figure}
	We won't worry about the details of how this works at this point. For more information, consider this \href{https://iopscience.iop.org/article/10.1088/2058-9565/aad3e4/pdf}{\underline{paper}}.
	

	
	\item \textit{Adapt-VQE ansatz:}\\
	
	In the adapt-VQE scheme, a collection of operators $\hat{A}_i$ is chosen in advance, and the ground state is approximated by 
	\begin{align}
	\ket{\psi_{\text{adapt-VQE}}} = e^{\theta_n \hat{A}_n}\dots e^{\theta_1 \hat{A}_1}\ket{\psi_{\text{HF}}},
	\end{align}
	Given a current parameter configuration $\vec{\theta}$, the commutator  of the Hamiltonian with each operator in the pool is measured to obtain the gradient of the energy
	\begin{align}
	E = \bra{\psi_{\text{adapt-VQE}}} \had \ket{\psi_{\text{adapt-VQE}}}.
	\end{align}
	wrt the parameters $\vec{\theta}$. Repeating this multiple times and averaging over the obtained samples gives the gradient of the expectation value of the Hamiltonian wrt to the coefficient of each operator. The ansatz is improved by adding the operator $\hat{A}_i$ with the largest gradient to the left end of the ansatz with a new variational parameter, thereby increasing $n$. The operation is repeated until convergence.  
	
	
	
	
	\item \textit{Other considerations:}\\
	
	 There are other considerations with regards to variational methods, but we won't worry about them too much for now.
\end{enumerate}






\subsubsection{Excited states}

While much of the above discussion of state preparation
and variational algorithms has focused on ground-states, most
of the same methods can also be used with minor extensions
for excited states. For example, adiabatic state preparation can
be used to prepare an excited state, so long as it is connected
to the initial state without a vanishing gap.


\subsubsection{Phase estimation}

We have a section on this, so let's just be brief and gloss over the details. \\


Quantum Phase Estimation (QPE) is a crucial step in many
quantum algorithms. QPE enables high-precision measurements of the ground and excited energy levels. This is done by preparing a trial initial state $\ket{\psi(0)}$ that has a non-negligible overlap with the relevant eigenvector of the target Hamiltonian $\had$ and applying a circuit that creates a superposition of time evolved states $\ket{\psi(t)} = e^{-i\had t}\ket{\psi(0)}$ over a suitable range of evolution times $t$. \\

Since QPE is used ubiquitously in a variety of quantum applications, it is crucial to optimize its performance. Below we
list some open problems that are being actively investigated:
\begin{itemize}
	\item Given limitations of near-term quantum devices, of particular interest are tradeoffs between the depth of the
	QPE circuit and its spectral-resolution power as well
	as its sensitivity to noise.
	
	\item Several methods have been proposed for mitigating experimental errors for VQE-type simulations. Such methods enable reliable estimation of expected values of observables on a given trial state without introducing any overhead in terms of extra qubits or quantum gates.
	
	
	\item Classical post-processing methods that enable simultaneous estimation of multiple eigenvalues are highly desirable.
	
	\item Finally, a natural question is whether the time evolution
	operator $e^{-i \had t}$
	in QPE can be replaced by some other
	functions of HË† that are easier to implement
\end{itemize}




\subsubsection{Quantum algorithms for time evolution}


\begin{enumerate}
	\item \textit{Hamiltonian simulation problem:}  \\
	
	A quantum computer can be programmed to efficiently simulate the unitary time evolution of almost any physically realistic quantum system. Consider any evolution which is of course given by the SE
	\begin{align}
	i \p_t \ket{\psi(t)} = \had \ket{\psi(t)}, \quad t \geq 0.
	\end{align}
	We can just assume that $\had$ describes a system of $n$ qubits, since any fermionic or spin system can be mapped to qubits. Integrating we get
	\begin{align}
	\ket{\psi(t)} = e^{-i\hat t}\ket{\psi(0)}.
	\end{align}
	A quantum algorithm for Hamiltonian simulation takes as input a description of $\had$, the evolution time $t$, nd outputs a quantum circuit $\U$ that approximates the time evolution operator $e^{-it\had}$ within a specified precision $\epsilon$:
	\begin{align}
	\norm{\U - e^{-it\had}} \leq \epsilon.
	\end{align}
	$\U$ may use ancillary qubits initialized in the $\ket{0}$ state. The simulation cost is usually quantified by the runtime of the algorithm (gate count) and the total number of qubits. Applying $\U$ to $\ket{\psi(0)}$ gives the approximation state $\ket{\psi(t)}$. \\
	
	
	Quantum simulation algorithms apply to general classes of Hamiltonians satisfying mild technical conditions that enable a quantum algorithm to access the Hamiltonian efficiently.

\end{enumerate}

















%\subsubsection{Algorithmic tools}
%\subsubsection{Open problems}
%\subsubsection{Finite-temperature algorithms}
%\subsubsection{Hybrid quantum-classical methods}
%\newpage
%
%
%\subsection{Reading out results}
%
%\subsubsection{Equal-time measurements}
%\subsubsection{Dynamical properties and Green's functions}






\newpage


\section{Some Models in Quantum Statistical \& Condensed-Matter Physics}

\subsection{The Hubbard Model}

\subsection{The Ising Model}


\newpage



\section{Quantum Phase Estimation (QPE)}

One of the most important applications of the Quantum Fourier Transform is the Quantum Phase Estimation. Phase estimation is the key for many quantum algorithms. Suppose a unitary operator $\U$ has an eigenvector $\ket{u}$ with eigenvalue $2^{2\pi i \varphi}$, where the value of $\varphi$ is unknown. The goal of the phase estimation algorithm is to estimate $0 \leq \varphi \leq 1$ with high probability within additive error $\epsilon$. The algorithm uses $\mathcal{O}(\log (1/\epsilon))$ qubits and $\mathcal{O}(1/\epsilon)$ controlled-$\U$ operations. \\

To perform the estimation we assume that we have available \textit{oracles} capable of preparing the state $\ket{u}$ (the eigenstate) and performing the controlled-$\U^{2j}$ operation, for suitable nonnegative integers $j$. Since preparing eigenstates is not necessarily easy, the phase estimation algorithm is not a complete algorithm by itself. Rather, we should view it as a ``subroutine'' or a ``module.''\\



The QPE uses two registers. The first register contains $t$ qubits initially in $\ket{0}$. The choice of $t$ depends on two things: the number of digits of accuracy we wish to have in our estimate of $\varphi$, and with what probability we wish the phase estimation procedure to be successful. The second register begins in the eigenstate $\ket{u}$, and contains as many qubits as necessary to store $\ket{u}$. \\

QPE occurs in two main stages. First, the circuit applies  an $\had$ to each of the $t$ qubits. Then, the circuit applies controlled-$\U$ operators on the second register, with $\U$ raised to successive powers of two. This stage looks like the following:
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qpe1}
	\caption{Source: Mike and Ike}
\end{figure} 
Recall that immediately after $\had^{\otimes t}$ the state of the upper register looks like
\begin{align}
\had \ket{0} = \f{1}{2^{1/2}}\lp \ket{0} + \ket{1} \rp
\end{align}
The controlled-$\U$ acts on $\ket{u}$ only if the control qubit is in $\ket{1}$. With this, we see that the final state of the first register is given by
\begin{align}
&\f{1}{2^{t/2}}\lp \ket{0} + e^{2\pi i 2^{t-1}\varphi}\ket{1} \rp\otimes
\lp \ket{0} + e^{2\pi i 2^{t-2}\varphi}\ket{1} \rp \otimes \dots \otimes \lp \ket{0} + e^{2\pi i 2^{0}\varphi}\ket{1} \rp\nn\\
&\quad = \dots \nn\\
&\quad = \f{1}{2^{t/2}}\sum_{0\leq k < 2^{t}} e^{2\pi i \varphi k}\ket{k}_t.
\end{align}

\begin{exmp}
For $t =2$, we have
\begin{align}
&\f{1}{2}\lp \ket{0} + e^{4\pi i \varphi } \ket{1} \rp\otimes \lp \ket{0} + e^{2\pi i\varphi }\ket{1} \rp \nn\\
&\quad= \f{1}{2}\lp \ket{0}_2 + e^{1\cdot 2\pi i\varphi }\ket{1}_2 + e^{2\cdot 2\pi i\varphi}\ket{2}_2 + e^{3\cdot 2\pi i\varphi }\ket{3}_2  \rp\nn\\
&\quad = \f{1}{2^{2/2}}\sum_{0 \leq k < 2^{2}}e^{2\pi i \varphi \cdot 2}\ket{k}_2
\end{align}\qed
\end{exmp}

Note that we're leaving out the qubits carrying the  $\ket{u}$ state because we don't do anything to it (because $\ket{u}$ is an eigenstate of $\U$). We should also note that the effect of the sequence of controlled-$\U$ operations is to take $\ket{j}\ket{u}$ to $\ket{j}\U^j \ket{u}$. We should also note that this does not depend on the fact that $\ket{u}$ is an eigenstate of $\U$. \\


In the second state, the algorithm applies the inverse quantum Fourier transform on the first register (which is just the QFT, applied in reverse order of operations). This can be done efficiently in $\mathcal{O}(t^2)$ steps, as we have seen the Quantum Information \href{https://huanqbui.com/LaTeX 20projects/HuanBui_QI/HuanBui_QI.pdf}{\underline{notes}}. Since the input in this case is the maximal superposition in the $t$-vector space, the output of the inverse QFT is going to be a single state: $\ket{\tilde{\varphi}}$, which we hope is approximately $\ket{\varphi}$. We will see how close $\ket{\tilde\varphi}$ can get to $\ket{\varphi}$.\\

The third and final stage of the algorithm is to read out the state of the first register by doing a measurement in the computational basis. \\


\begin{exmp}
	To make sure we see what is going on, suppose $\varphi$ is expressed exactly in $t$ qubits, as $\varphi = 0,\varphi_1\varphi-2\dots\varphi_t$ (note that $\varphi \in [0,1]$). Then the state of the input register after the first stage is given by
	\begin{align}
	\f{1}{2^{t/2}}\lp \ket{0} + e^{2\pi i 0.\varphi_t} \ket{1} \rp \otimes \lp \ket{0} + e^{2\pi i 0.\varphi_{t-1}\varphi_t}\ket{1} \rp \otimes \dots  \otimes \lp \ket{0} + e^{2\pi i 0.\varphi_1\varphi_2\dots\varphi_t}\ket{1} \rp.
	\end{align}
	Applying the inverse QFT to this, we see that output state from the second state is the product state $\ket{\varphi_1\dots \varphi_t}$. A measurement in the computational basis gives $\varphi$ exactly. \qed
\end{exmp}

Schematically, the algorithm for QPE looks like the following:
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qpe2}
	\caption{Source: Mike and Ike}
\end{figure}

In summary, what we basically did was using the inverse QFT to get
\begin{align}
\f{1}{2^{t/2}}\sum_{0 \leq k < 2^{t}} e^{2\pi i \varphi j }\ket{j}\ket{u} \to \ket{\tilde{\varphi}}\ket{u}
\end{align}
where $\ket{\tilde\varphi}$ is a good estimator for $\varphi$ when measured. 

 





\subsection{Performance and Requirements}
What happens when $\varphi$ cannot be written exactly with a $t$-bit expansion? We will now show that the procedure we described will produce a good approximation to $\varphi$ with high probability. \\

Let $b \in [0,2^t-1]$ an integer be given such that $b/2^t = 0.b_1b_2\dotsb_t$ is the best $t$ bit approximation to $\varphi$ which is less than $\varphi$, i.e., 
\begin{align}
0 \leq \delta \equiv \varphi - b/2^t \leq 2^{-t}.
\end{align}
(This is getting pretty close to an analysis proof -- because it \textit{is} an analysis proof.) Applying the inverse QFT to the state immediately after the first stage gives
\begin{align}
\mathcal{F}^{-1}\lb\f{1}{2^{t/2}}\sum_{0 \leq k < 2^t}e^{2\pi i \varphi k}\ket{k} \rb &= \f{1}{2^{t}} \sum_{0 \leq k,l < 2^t} e^{\f{-2\pi i kl}{2^t}}e^{2\pi i \varphi k}\ket{l}\nn\\
& = \f{1}{2^t}\sum_{0 \leq k,l < 2^t}\lp e^{2\pi i(\varphi- l/2^t)} \rp^k \ket{l}
\end{align}
by definition. Note that when $\varphi$ can be expressed using exactly $t$ bits we are reduced to the simpler version of the problem described above. \\

Let $\al_l$ be the amplitude of $\ket{(b+l) \mod 2^t}$, i.e., 
\begin{align}
\alpha_l \equiv \f{1}{2^t}\sum_{0 \leq k < 2^t} \lp e^{2\pi i (\varphi - (b+l)/2^t)} \rp^k.
\end{align}
This is a partial sum of a geometric series. It is straightforward to show that
\begin{align}
\alpha_l = \f{1}{2^t}\lp \f{1 - e^{2\pi i (2^t \varphi - (b+l))}}{1 - e^{2\pi i (\varphi - (b+l)/2^t)}} \rp = \f{1}{2^t}\lp \f{1 - e^{2\pi i (2^t \delta - l)}}{1 - e^{2\pi i (\delta - l/2^t)}} \rp.
\end{align}
Suppose the outcome of the final measurement is $m$. We want to bound the probability that $\abs{m-b} > \epsilon$ (where $\epsilon$ is the tolerance), i.e., we don't want the probably that $m$ is far from $b$ to be large. Well, the probability of observing such an $m$ is given by
\begin{align}
P(\abs{m-b} > \epsilon) = \sum_{-2^{t-1} < l \leq -(\epsilon+1)} \abs{\al_l}^2 + \sum_{e+1 \leq l \leq 2^{t-1}}\abs{\al_l}^2.
\end{align}
For any real $\theta$, we have $\abs{1 - \exp(i\theta)} \leq 2$, so 
\begin{align}
\abs{\al_l} \leq \f{2}{2^t \abs{1 - e^{2\pi i (\delta - l/2^)}}}.
\end{align}
Further, provided $-\pi \leq \theta \leq \pi$, we have $\abs{1 - \exp(i\theta)} \geq 2\abs{\theta}/\pi$. Also, when $-2^{t-1} < l \leq 2^{t-1}$ we have $-\pi \leq 2\pi (\delta - l/2^t) \leq \pi$, so we have
\begin{align}
\abs{\al_l} \leq \f{1}{2^{t+1}(\delta - l/2^t)}.
\end{align}
With this we have
\begin{align}
p(\abs{m-b} > \epsilon) &\leq \f{1}{4}\lp \sum_{l = -2^{t-1}+1}^{-(\epsilon+1)} \f{1}{(l - 2^t \delta)^2} + \sum^{2^{t-1}}_{l = \epsilon+1} \f{1}{(l - 2^t \delta)^2}  \rp\nn\\
&\leq \f{1}{4}\lp \sum_{l = -2^{t-1}+1}^{-(\epsilon+1)} \f{1}{(l - 1)^2} + \sum^{2^{t-1}}_{l = \epsilon+1} \f{1}{(l - 1)^2}   \rp, \quad 0 \leq 2^t\delta < 1\nn\\
&\leq \f{1}{2}\sum^{2^{t-1}-1}_{l=\epsilon} \f{1}{l^2}\nn\\
&\leq \f{1}{2}\int^{2^{t-1}-1}_{\epsilon-1} \f{1}{l^2}\,dl\nn\\
&= \f{1}{2(\epsilon-1)}.
\end{align}
Suppose we wish to approximate $\varphi$ to an accuracy $2^{-n}$, i.e., we choose $\epsilon = 2^{t-n}-1$. By using $t =n+p$ qubits, we see that the probability of obtaining an approximation correct to this accuracy is at least 
\begin{align}
1-\f{1}{2(\epsilon-1)} = 1 - \f{1}{2(2^p - 2)}.
\end{align}
So, to successfully obtain $\varphi$ accurate to $n$ bits with probability of success at least $1-\epsilon$ we choose 
\begin{align}
t = n + \left\lceil \log \lp 2 + \f{1}{2\epsilon} \rp \right\rceil.
\end{align}

Now here's a catch: to use QPE, we need the eigenstate $\ket{u}$. What if we don't know how to prepare such a state? Suppose that we prepare some other state $\ket{\psi}$ in place of $\ket{u}$. We can write
\begin{align}
\ket{\psi} = \sum_u c_u \ket{u}
\end{align}
where $\ket{u}$ are the eigenstates of $U$. Suppose the eigenstate $\ket{u}$ has the eigenvalue $e^{2\pi i \varphi_u}$. Running the QPE on $\ket{\psi}$ will give
\begin{align}
\ket{\psi} \stackrel{\mbox{QPE}}{\to} \sum_u c_u \ket{\tilde{\varphi_u}}\ket{u},
\end{align}
where $\tilde{\varphi_u}$ is an estimation for $\varphi_u$. Reading out the first register gives us a good approximation for $\varphi_u$, where $u$ is chosen at random with probability $\abs{c_n}^2$. This procedure allows us to avoid preparing a possibly unknown eigenstate, but at the cost of introducing some additional randomness into the algorithm. It turns out that if $t$ is chosen according to the minimization procedure above, then the probability for measuring $\varphi_u$ accurate to $n$ bits at the conclusion of the phase estimation algorithm is at least $\abs{c_u}^2(1-\epsilon)$.







\subsection{The algorithm}
QPE solves a problem which is both non-trivial and interesting from a physical point of view. It allows us to estimate the eigenvalue associated to a given eigenvector of a unitary operator. It turns our that other interesting problems can be reduced to phase estimation, which makes QPE particularly useful.\\

\underline{Here's the algorithm:}
\begin{itemize}
	\item \textbf{Inputs:} (1) An oracle which performs a controlled-$\U^j$ operation, (2) an eigenstate $\ket{u}$ of $\U$ with eigenvalue $e^{2\pi i \varphi_u}$, and (3) $t = n + \left\lceil \log \lp 2 + 1/2\epsilon \rp \right \rceil$ qubits initialized to $\ket{0}$.
	
	
	\item \textbf{Outputs:} An $n$-bit approximation $\tilde\varphi_u$ of $\varphi_u$. 
	
	\item \textbf{Runtime:} $\mathcal{O}(t^2)$ operations (due to th inverse QFT) and one call to controlled-$\U^j$ oracle. Succeeds with probability at least $1-\epsilon$. 
	
	\item \textbf{Procedure:} 
	\begin{itemize}
		\item  Initial state: $\ket{0}\ket{u}$
		
		\item Create superposition: $\to (1/2^{t/2})\sum_{0 \leq j < 2^t}\ket{j}\ket{u}$.
		
		
		\item Apply the oracle:\\ $\to(1/2^{t/2})\sum_{0 \leq j < 2^t}\ket{j}\U^j\ket{u} = (1/2^{t/2})\sum_{0 \leq j < 2^t}e^{2\pi i j\varphi_u}\ket{j}\ket{u}$
		
		
		\item Apply inverse QFT: $\to \ket{\tilde\varphi_u}\ket{u}$. 
		
		\item Measure the first register: $\to \tilde \varphi_u$.
	\end{itemize}
\end{itemize}


The algorithm has a wide range of applications, including order-finding and factorization. 










\newpage


\section{Quantum Variational Eigensolver (QVE)}

In this section we're looking this \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4124861/pdf/ncomms5213.pdf}{\underline{paper}} on the quantum variational eigensolver (QVE). \\


For quantum systems, where the physical dimension grows exponentially, finding the eigenvalues of certain operators is one of many intractable problem and remains a fundamental challenge.  QPE efficiently finds the eigenvalue of a given eigenvector but requires fully coherent evolution. This paper presents an alternative approach that greatly reduces the requirements for coherent evolution and combine this method with a new approach to state preparation based on ansatz and classical optimization. \\

As we have seen, quantum approaches to finding eigenvalues
have previously relied on the quantum phase estimation (QPE)
algorithm. The QPE algorithm offers an exponential speedup
over classical methods and requires a number of quantum operations $\mathcal{O}(p^{-1})$ to obtain an estimate with precision $p$. Recall that in the standard formulation of QPE, one assumes the eigenvector $\ket{\psi}$ of a Hermitian operator $\had$ is given as input and the problem is to determine the corresponding eigenvalue $\lambda$. The time the
quantum computer must remain coherent is determined by the
necessity of $\mathcal{O}(p^{-1})$ successive applications of $e^{-i \had t}$, each of which can require on the order of millions or billions of quantum gates for practical applications, as compared to the tens to hndreds og gates achievable in the short term. \\

This paper presents an alternative to QPE that significantly reduces the requirements for coherent evolution. This alternative method is given by a quantum processing unit which efficiently calculates the expectation value of a Hamiltonian, providing an exponential speedup over exact diagonalization, the only known exact solution to the poblem on a traditional computer. In conjunction with a classical optimization algorithm, this approach reduces the requirement for coherent evolution of the quantum state, making more efficient use of quantum resources, and may offer an alternative route to practical quantum-enhanced computation. 





\subsection{Quantum Expectation Estimation (QEE)}


QEE computes the expectation value of a given Hamiltonian $\had$ for an input state $\ket{\psi}$. This is done via the decomposition:
\begin{align}
\had = \sum_{i\alpha}h^i_\alpha \sigma^i_\alpha + \sum_{ij\al\be}h^{ij}_{\al\be}\sigma^i_\alpha \sigma^j_\be + \dots
\end{align}
for real $h$, where Roman indices identify the subsystem on which the operator acts, and Greek indices identify the Pauli operator. For example, $\alpha = x$. This is possible because \textit{products of Pauli matrices form a basis for the set of Hermitian matrices of dimensions that are powers of 2}. In more generality, this is possible because we have the \textit{Pauli group} $G_n$, which is the group generated by the operators applied to each of $n$ qubits in th tensor product Hilbert space $(\mathbb{C}^2)^{\otimes n}$. \\

In any case, I digress. By linearity, we have
\begin{align}
\langle \had \rangle = \sum_{i\al} h^i_\al \langle \sigma^i_\al \rangle + \sum_{ij\al\be} h^{ij}_{\al\be} \langle \sigma^i_\al \sigma^j_\be  + \dot \rangle 
\end{align}
Just as in QPE, we only consider Hamiltonians that can be written as a  polynomial number of terms, wrt the system size. This class of Hamiltonians encompasses a wide range of physical system, so this requirement is not too strict. In fact, Hamiltonians of this kind include the electronic structure Hamiltonian of quantum chemistry, the quantum Ising Model, the Heisenberg Model, matrices that are well approximate as a sum of $n$-fold tensor products, and more generally any $k$-sparse Hamiltonian without evident tensor product structure.\\

\textbf{Remark:} Sparse Hamiltonians are important because it is possible to decompose $\had$ into $\sum_k \had_k$ where $\had_k$'s all commute (making diagonalization straightforward). If the matrix is sparse, then we shouldn't need too many distinct $\had_k$'s. Then we can simulate the Hamiltonian evolution 
\begin{align}
e^{-i\had t} = \prod e^{-i \had_m \delta t}
\end{align}
where $t =N \delta t$. Now, what do we mean by ``sparse?'' Sparse means that there are at most $d$ nonzero entries per row, $d = \text{poly}(\log N)$. In any given row, the location of the $j$th nonzero entry and its value can be compute efficiently (or is given by a black box). Sparse Hamiltonians are different from ``Local'' Hamiltonians, where $\had = \sum_j \had_j$ where each $\had_j$ acts on $\mathcal{O}(1)$ qubits. \qed\\



In any case, I digress again. The evaluation of $\langle \had \rangle $ reduces to the sum of a polynomial number of expectation values of simple Pauli operators for a quantum state $\ket{\psi}$,  multiplied by some real constants. A quantum device can efficiently evaluate the expectation value of a tensor product of arbitrary number of simple Pauli operators. Therefore, with an $n$-qubit state we can efficient evaluate the expectation value of this $2^n \times 2^n$ Hamiltonian.\\

This is possible with a classical computer, but it suffers from the $N$-representability problem. The power of the quantum approach derives from the fact that quantum hardware can store a global quantum state with exponentially fewer resources than required by classical hardware, and as a result the $N$-representability problem does not arise. \\


The expectation value of a tensor product of an arbitrary
number of Pauli operators can be estimated by local measurement of each qubit (this can be found somewhere in Mike and Ike). Such independent measurements can be performed in parallel, incurring a constant cost in time. Furthermore, since these operators are normalized an finite-dimensional, their spectra are bounded. As a result, each
\begin{align}
\langle H^m_i\rangle = h^{ij\dots}_{\al\be\dots} \langle \sigma^i_\al \otimes \sigma^i_\be \otimes \dots \rangle  
\end{align}
can be estimated to a precision $p$ of an individual element with coefficient $h$, which is an arbitrary element from the set of constants $\{ h^{ij\dots}_{\al\be\dots}\}$ at a cost of $\mathcal{O}(\abs{h_\text{max}}^2 Mp^{-2})$ repetitions. $M$ is the number of terms in the decomposition of the Hamiltonian and $h_\text{max}$ is the coefficient with maximum norm in the decomposition of the Hamiltonian. \\


The advantage of this approach is that the coherence time to make a single measurement after preparing
the state is $\mathcal{O}(1)$. Conversely, the disadvantage of this approach with respect to QPE is the scaling in the total number of
operations, as a function of the desired precision is quadratically
worse $\mathcal{O}(p^{-2})$ versus $\mathcal{O}(p^{-1})$.  Moreover, this scaling will also reflect the number of state preparation repetitions required, whereas in QPE the number of state preparation steps is constant. In essence, we dramatically reduce the coherence time requirement while maintaining an exponential advantage over
the classical case, by adding a polynomial number of repetitions
with respect to QPE.


\newpage


\subsection{Quantum Variational Eigensolver (QVE)}

The procedure outlined above
replaces the long coherent evolution required by QPE by many
short coherent evolutions. In both QPE and QEE we require a
good approximation to the ground-state wavefunction to compute the ground-state eigenvalue, and we now consider this
problem. The quantum variational eigensolver (QVE) algorithm
is a variational method to prepare the eigenstate and, by
exploiting QEE, requires short coherent evolution. 

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.2]{qvei}
\end{figure}

It is well known that the eigenvalue problem for an observable
represented by an operator H can be restated as a variational
problem on the Rayleighâ€“Ritz quotient such that the eigenvector $\ket{\psi}$ corresponding to the lowest eigenvalue is the one hat minimizes
\begin{align}
\f{\bra{\psi} \had \ket{\psi}}{\braket{\psi}}
\end{align}
By varying the experimental parameters in the preparation of $\ket{\psi}$ and computing the Rayleighâ€“Ritz quotient using QEE as
a subroutine in a classical minimization, one may prepare
unknown eigenvectors. At the termination of the algorithm, a
simple prescription for the reconstruction of the eigenvector is
stored in the final set of experimental parameters that define $\ket{\psi}$. \\

Note that if a quantum state is characterized by an exponentially large
number of parameters, it cannot be prepared with a polynomial
number of operations. The set of efficiently preparable states are
therefore characterized by polynomially many parameters, and
we choose a particular set of ansatz states of this type.  Under
these conditions, a classical search algorithm on the experimental
parameters that define $\ket{\psi}$ needs only explore a polynomial
number of dimensions -- requirement for the search to be
efficient.\\

One example of a quantum state parameterized by a
polynomial number of parameters for which there is no known
efficient classical implementation is the unitary coupled cluster
ansatz
\begin{align}
\ket{\Psi} = e^{T - T^\dagger}\ket{\Phi}_\text{ref}
\end{align}
where $\ket{\Phi}_\text{ref}$ is some reference state, usually the Hartree Fock ground state, and $T$ is the cluster operator for an $N$ electron system, defined by
\begin{align}
T =T_1 + T_2 + \dots + T_N,
\end{align}
where
\begin{align}
T_1 = \sum_{pr}t^r_p \hat{a}_p^\dagger\hat{a}_r, \quad T_2 = \sum_{pqrs}t^{rs}_{pq}\hat{a}_p^\dagger \hat{a}_q^\dagger\hat{a}_r \hat{a}_s,\quad \dots
\end{align}
By construction, $(T - T^\dagger)$ is anti-Hermitian, and exponentiation maps it to a unitary operator $e^(T - T^\dagger)$. \\

For any fixed excitation level $k$, the reduced cluster operator is written as
\begin{align}
T^{(k)} = \sum^k_{i=1}T_i.
\end{align}
This state may be prepared efficiently on a quantum
device. The reduced anti-hermitian cluster operator  $(T^{(k)} - T^{(k)\dagger})$ is the sum of a polynomial number of termsâ€”namely, it contains
a number of terms $\mathcal{O}(N^k(M-N)^k)$, where $M$ is the number of
single-particle orbitals. By defining an effective Hermitian
Hamiltonian $\had = i(T^{(k)} - T^{(k)\dagger})$ and performing the Jordanâ€“
Wigner transformation to reach a Hamiltonian that acts on the
space of qubits $\hat{\had}$, we are left with a Hamiltonian that is a sum of polynomially many products of Pauli operators. The problem then reduces to the quantum simulation of this effective Hamiltonian, $\hat{\had}$ which can be one in polynomial time using known procedures. 












\newpage


\section{Quantum Approximate Optimization Algorithm (QAOA)}

\newpage

\section{Quantum Variational Factorization (QVF)}

\newpage



\section{Variational Thermal Quantum Simulation via Thermofield Double States}



\subsection{Thermofield Double States (TFD)}


\newpage



\section{Generation of TFD and Critical Ground States with a Quantum Computer }



\newpage



\section{Efficient variational simulation of non-trivial quantum states}





\newpage












\end{document}
