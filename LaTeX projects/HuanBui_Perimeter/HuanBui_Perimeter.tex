\documentclass{book}
\usepackage{physics}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{framed}
\usepackage{authblk}
\usepackage{empheq}
\usepackage{amsfonts}
\usepackage{esint}
\usepackage[makeroom]{cancel}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage{mathtools}
\usepackage{bigints}
\usepackage{amsthm}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\newtheorem{prop}{Proposition}[section]
\newtheorem{rmk}{Remark}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{exmp}{Example}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{sln}{Solution}[section]
\newtheorem*{prob*}{Problem}
\newtheorem{exer}{Exercise}[section]
\newtheorem*{exer*}{Exercise}
\newtheorem*{sln*}{Solution}
\usepackage{empheq}
\usepackage{hyperref}
\usepackage{tensor}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}


\newcommand*\widefbox[1]{\fbox{\hspace{2em}#1\hspace{2em}}}

\newcommand{\p}{\partial}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\lag}{\mathcal{L}}
\newcommand{\nn}{\nonumber}
\newcommand{\had}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\K}{\mathcal{K}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\w}{\omega}
\newcommand{\lam}{\lambda}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\x}{\xi}
\newcommand{\ep}{\epsilon}

\newcommand{\G}{\mathcal{G}}

\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\td}[1]{\tilde{#1}}


\newcommand{\ift}{\infty}

\newcommand{\lp}{\left(}
\newcommand{\rp}{\right)}

\newcommand{\lb}{\left[}
\newcommand{\rb}{\right]}

\newcommand{\lc}{\left\{}
\newcommand{\rc}{\right\}}


\newcommand{\V}{\mathbf{V}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\Id}{\mathcal{I}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Z}{\mathcal{Z}}

%\setcounter{chapter}{-1}


\makeatletter
\renewcommand{\@chapapp}{Part}
%\renewcommand\thechapter{$\bf{\ket{\arabic{chapter}}}$}
%\renewcommand\thesection{$\bf{\ket{\arabic{section}}}$}
%\renewcommand\thesubsection{$\bf{\ket{\arabic{subsection}}}$}
%\renewcommand\thesubsubsection{$\bf{\ket{\arabic{subsubsection}}}$}
\makeatother



\usepackage{subfig}
\usepackage{listings}
\captionsetup[lstlisting]{margin=0cm,format=hang,font=small,format=plain,labelfont={bf,up},textfont={it}}
\renewcommand*{\lstlistingname}{Code \textcolor{violet}{\textsl{Mathematica}}}
\definecolor{gris245}{RGB}{245,245,245}
\definecolor{olive}{RGB}{50,140,50}
\definecolor{brun}{RGB}{175,100,80}
\lstset{
	tabsize=4,
	frame=single,
	language=mathematica,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle=\color{black},
	backgroundcolor=\color{gris245},
	commentstyle=\color{gray},
	showstringspaces=false,
	emph={
		r1,
		r2,
		epsilon,epsilon_,
		Newton,Newton_
	},emphstyle={\color{olive}},
	emph={[2]
		L,
		CouleurCourbe,
		PotentielEffectif,
		IdCourbe,
		Courbe
	},emphstyle={[2]\color{blue}},
	emph={[3]r,r_,n,n_},emphstyle={[3]\color{magenta}}
}


\begin{document}
\begin{titlepage}\centering
 \clearpage
 \title{{\textsc{\textbf{Physics at the Perimeter Institute:\\  Topics in Theoretical Physics \\ 
 				\&\\ Quantum Simulation Research}}}\\ \smallskip - A Quick Guide - \\}
 \author{\bigskip Huan Q. Bui}
  \affil{Colby College\\$\,$\\ PHYSICS \& MATHEMATICS\\ Statistics \\$\,$\\Class of 2021\\}
 \date{\today}
 \maketitle
 \thispagestyle{empty}
\end{titlepage}

\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

Greetings,\\

This guide is my notes from Perimeter Institute of Theoretical Physics Summer School, 2020. Topics include quantum information, thermodynamics, numerical methods, condensed matter physics, path integrals, and symmetries.\\

The text also includes reading notes for my research project at Perimeter on quantum simulation. \\

Enjoy!  



\newpage
\tableofcontents
\newpage





\chapter{Quantum Information \& Thermodynamics} 

\textbf{Instructor:} Alioscia Hamma\\

The aim of this course is to understand the thermodynamics of quantum systems and in the process to learn some fundamental tools in Quantum Information. We will focus on the topics of foundations of quantum statistical mechanics, resource theories, entanglement, fluctuation theorems, and quantum machines. 




\newpage

\section{Introduction}


Let's unpack the following three terms: Thermodynamics, Information, and Quantum. \\

\noindent \textbf{Thermodynamics:} Thermodynamics is not concerned so much with how I make a more efficient engine as the most important question: ``What is possible?'' Any theory (relativity, string, quantum field, etc) must be thermodynamically valid. This way of think makes thermodynamics appear quite fundamental -- which indeed it is. \\

\noindent \textbf{Information:} Information theory was so abstract that it was viewed as a mathematical subfield for the longest time. But because of the information revolution, information theory becomes physical. Physics tells us what kind of information theory is allowed. Information theory becomes a new way to understand Thermodynamics and what's possible. \\

\noindent \textbf{Quantum information theory} provides even newer insights. \\




Now, quantum theory as it is has unanswered questions. Consider the Schrodinger cat, which is in a superposition of begin dead and alive. We know that for spins, $(\ket{0} + \ket{1})/\sqrt{2}$ exists (coherently), but we don't see cat states. We believe that cat states are possible to make, but we also believe it's hard to create. However, we don't know for sure that such a Schrodinger cat state is possible.  \\

With this, let us start. 

\newpage


\section{Basic facts of Quantum Mechanics}


Some mathematical basics: 
\begin{defn}[f.d. Hilbert spaces]
	Consider finite-dimensional Hilbert spaces $\had \sim \mathbb{C}^d$. This is just a vector space over the complex numbers. We can also write $\had = \text{span}\{ \ket{1},\dots, \ket{d}   \}$. We can also call this a $d$-level system. We will use the Dirac notation $\ket{\psi}$ to denote elements of this Hilbert space. 
\end{defn}

\begin{defn}[Linear Operators]
	Linear operators are mappings linear $A : \had \to \had$ defined by
	\begin{align}
	A = \sum_{ij}A_{ij}\ket{i}\bra{j}
	\end{align}
	where the set $\{ \ket{i} \}$ form the basis of the Hilbert space, and $A_{ij}$'s are the matrix elements of $A$. 
\end{defn}


\begin{defn}[Positive Operators]
	An operator $A$ is positive if  $\bra{\psi}A \ket{\psi} \geq 0$ for all $\ket{\psi} \in \had$.
\end{defn}


\begin{defn}[Hermitian Operators]
	$A$ is Hermitian if $A^\dagger = A$.
\end{defn}

\begin{thm}[Spectral Theorem]
	A Hermitian operator $A$ has a spectral resolution $\ket{A_i}$ which forms an orthonormal basis of $\had$, with which we can write
	\begin{align}
	A = \sum_i a_i \ket{A_i} \bra{A_i} = \sum_i a_i P_i^A
	\end{align}
	Objects such as $\ket{A_i}\bra{A_i}$ are projectors.
\end{thm}




\begin{defn}[Schatten $p$-norm]
	The $p$-norm of an operator $A$ is defined to be
	\begin{align}
	\norm{A}_p \equiv \lp \tr\lb \lc A^\dagger A \rc^{p/2} \rb \rp^{1/p}.
	\end{align}
\end{defn}


\begin{exmp}
	For $p=1$, we get the trace norm: 
	\begin{align}
	\norm{A}_1 = \tr\sqrt{A^\dagger A}.
	\end{align}
	
	For $p=2$, we get the usual Frobenius norm:
	\begin{align}
	\norm{A}_2 = \sqrt{\tr(A^\dagger A)}.
	\end{align}
	
	For $p = \infty$, we get the operator norm:
	\begin{align}
	\norm{A}_\infty = \sup_{\ket{\psi}}\f{\norm{A \ket{\psi}}}{\norm{\psi}}.
	\end{align}	
	In fact, there are multiple equivalent definitions of the operator norms. (Consider functional analysis for more details).
\end{exmp}



In quantum mechanics, we have states and observables. States are Hermitian positive operators: $\psi \geq 0$ such that $\tr(\psi) = 1$, i.e., $\psi \in \mathcal{B}(\had)$ with unity trace ($\mathcal{B}(\had)$ denotes the set of bounded operators). Note that in this formulation, states are operators, not vectors, and these represent physical systems. To look at measurements, consider the spectral resolution of an operator $A$:
\begin{align}
A = \sum_i a_i P_i^A.
\end{align}
The eigenvalues of $A$ are the possible outcomes of the measurement. We note that this is just the density operator/density matrix formulation of quantum mechanics. There's nothing really fancy going on here.\\

QM is a probabilistic theory
\begin{align}
	\Pr(a_i) = \tr\lp \psi P_i^A \rp= p_i.
\end{align}
With this,
\begin{align}
E[A] = \langle A \rangle = \sum a_i p_i, \quad \langle A^2 \rangle = \sum a_i^2 p_i, \quad \dots
\end{align}

Recall that 
\begin{align}
\text{Var}{A} = E[A^2] - E[A]^2\geq 0.
\end{align}

Consider a state $\psi$, which is positive Hermitian operator with unity trace, i.e., $\psi \in \mathcal{B}(\had)$. By the spectral theorem, we can write
\begin{align}
{\psi} = \sum_i p_i \ket{\psi_i}\bra{\psi_i}.
\end{align}
Since $\psi \geq 0$, we have that $p_i \geq 0$. To show this, suppose we want to compute $p_l$:
\begin{align}
p_l= \bra{\psi_l}\psi \ket{\psi_l} \geq 0,
\end{align}
so we're done. \\


Next, we also said that the trace of $\psi$ is 1: 
\begin{align}
\tr(\psi) = 1 \implies \sum_i p_i = 1. 
\end{align}
In other words, we have a probability distribution.\\

Are we're used to the fact that states are normalized vectors: $\braket{\psi} = 1$, and that $p_1 = \abs{\bra{\psi}\ket{A_i}}^2$, and that $E[A] = \bra{\psi }A \ket{\psi}$. Another thing, image that we're given some a set of orthonormal states $\ket{\psi_l}$ with some probability $q_l$. Now we're asked $E[A]$ with $\{ \ket{\psi_l}, p_l  \}$. Well,
\begin{align}
E[A] = \sum_l  q_l \bra{\psi_l} A \ket{\psi_l}.
\end{align}



This is what we're used to. Now, if we look at states as spectral resolutions (as operators):
\begin{align}
\psi = \sum_l q_l \ket{\psi_l}\bra{\psi_l}.
\end{align}
Then we will show as an exercise that
\begin{align}
E[A] = \Tr (\psi \ket{A_l}\bra{A_l}) = \sum_l q_l \bra{\psi_l} A \ket{\psi_l}.
\end{align}
So the operator formulation gives the same answer. \\


Next, consider the states $\ket{\psi_l}$ which give a probability distribution $q_l$. This produces a state/operator $\psi = \sum q_l \ket{\psi_l}\bra{\psi_l}$. If this probability distribution is extremal, i.e., $q_l = (1,0,\dots,0)$, then 
\begin{align}
\psi = \ket{\psi_l}\bra{\psi_l}.
\end{align}
This is what we call a \textbf{pure state}. Whenever we don't have an extremal probability distribution, we don't have a pure state. Suppose $\had = \text{span}\{ \ket{0}, \ket{1} \}$. Then $\psi = (\ket{0}\bra{0} + \ket{1}\bra{0})/\sqrt{2}$ is not a pure state. \\

\textbf{Question:} Is there a way to quantify whether a state is pure (or not)?  Consider
\begin{align}
\psi = \sum_i p_i \ket{\psi_i}\bra{\psi_i}.
\end{align}
We know that $\sum p_i = 1$. So, $\sum_i p_i^\al \leq 1$ with $\al\geq 1$. Equality occurs iff the state is pure. We call 
\begin{align}
\f{1}{d} \leq \sum_i p_i^2 \leq 1
\end{align}
the \textbf{purity} of the state (when $\al = 2$). \\

\begin{defn}[Composite Systems]
	Consider two copies of $\text{span}\{ \ket{0}, \ket{1} \} = \mathbb{C}^2$. The ``total'' vector space of this composite system is given by $\had = \mathbb{C}^{\otimes 2}$. \\
	
	In general, the composite vector space is just the tensor product of the vector spaces of the subsystems. 
\end{defn}




\newpage

\section{Foundations of Quantum Statistical Mechanics -- Entanglement -- Time Evolution}


\subsection{Quantum StatMech in the density matrix formulation}


The next question is Entropy. Consider this thought experiment by Von Neumann. Consider $N$ particles in $\mathbb{C}^{d_i} = \had_i$. Then $\had = \bigotimes^N \had_i$. For simplicity, we can just look at a system of $N$ 2-level systems/qubits. Consider the state of each of these particles: 
\begin{align}
\ket{\psi_i} = \sum_k w_k^i \ket{e_k} \in \had_i
\end{align}
Think of these particles in a box. Let's put them in a box, all of them in the same state. For example, let's assume that $\ket{\psi_i} = (1/\sqrt{2})(\ket{+} + \ket{-})$. Then 
\begin{align}
\ket{\psi} = \bigotimes_i \ket{\psi_i}.
\end{align}
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.4]{entropy}
	\caption{From Alioscia Hamma's lecture}
\end{figure}


I call that  $ S(\ket{\psi}) = 0$. We call this a quantum gas of non-interacting particles with zero entropy. We want to measure whether the state is in $\ket{+}\bra{+}$ or $\ket{-}\bra{-}$. Well, some of them will be $+$, some will be $-$. well, 
\begin{align}
n_+ = N\abs{w_+}^2 = \f{N}{2} = n_-.
\end{align}
After the measurement, the state will be 
\begin{align}
{\psi} = \abs{w_{+}} P_+ + \abs{w_-}P_- = \abs{w_{+}}\ket{+}\bra{+} + \abs{w_-} \ket{-}\bra{-}.
\end{align}
\textbf{How did the entropy change because of the measurement?} Suppose we have a box like the picture on the right. 
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.2]{entropy1}
	\includegraphics[scale=0.35]{entropy2}
	\caption{From Alioscia Hamma's lecture}
\end{figure}
It has a movable wall on the left and a fixed but semi-transparent wall in the middle. We also have another wall in the middle that can be moved. What Von Neumann thought was that what if we moved both the movable walls so that the volume containing the particles. With the semi-transparent wall, we are left with the picture on the left.\\

Note that nothing happened to the gas, so $Q,W =0$, and everything is reversible. At this point, we have separated the gases, but have not communicated with the environment yet. Now, we do a \textit{isothermal compression}. This is the environment is interacting with our gas. Now, the $+$ gas will be compressed $V \mapsto V'_+ = \abs{w_+}^2 V$, and the $-: V \mapsto \abs{w_-}^2V$. In this compression, the gases exchange work and heat with the environment. What is the entropy exchanged?
\begin{align}
\Delta S^\text{env} = \Delta S_+^\text{env} + \Delta S_-^\text{env}  = -n_+ k \log \f{V'_+}{V} - n_- k \log \f{V'_-}{V}.
\end{align}
Now remember that $n_\pm = \abs{w_\pm}^2N$ and $V_\pm = \abs{w_\pm}^2V$, so 
\begin{align}
\Delta S^\text{env} = -Nk \abs{w_+}^2 \log \abs{w_+}^2-Nk \abs{w_-}^2 \log \abs{w_-}^2.
\end{align}
Now, I do a unitary rotation, $\ket{+}\bra{+} \mapsto w_+\ket{+} + w_-\ket{-}$. This is reversible. Once we've done it for all particles, we're exactly in the initial state. Now, because everything we've done so far is reversible, we have that
\begin{align}
\Delta S^\text{env} + \Delta S^\text{gas} = 0 \implies \Delta S^\text{env} =  - \Delta S^\text{gas} = -(S_1 - S_2) = -S_2.
\end{align}
So, 
\begin{align}
S_2 = Nk \abs{w_+}^2 \log \abs{w_+}^2 + Nk \abs{w_-}^2 \log \abs{w_-}^2.
\end{align}


\begin{defn}[Von Neumann Entropy]
	The entropy of a state $\psi$ is given by
	\begin{align}
	\boxed{S(\psi) = -Nk \sum_i \abs{w_i}^2 \log \abs{w_i}^2}
	\end{align}
\end{defn}

\begin{prop}
	\begin{align}
	S(\bm{1}/d) = \log d
	\end{align}
	\begin{align}
	S_\al = \f{1}{1-\al}\log \Tr(\psi^\al).
	\end{align}
\end{prop}



Let's look at a different definition of Von Neumann entropy, given in Mike and Ike:
\begin{defn}[Von Neumann Entropy]
	The entropy of a quantum state $\rho$ is given by the formula
	\begin{align}
	S(\rho) = -\tr\lp \rho \log \rho \rp.
	\end{align}
	In this formula, the logarithm is take to base two. If $\lambda_x$ are the eigenvalues of $\rho$ then Von Neumann's entropy definition can be re-written as
	\begin{align}
	S(\rho) = -\sum_x \lambda_x \log \lambda_x,
	\end{align}
	where we define $0 \log 0 = 0$. For calculations, it is usually this formula that is most useful. 
\end{defn}


For example, the completely mixed density operator in a $d$-dimensional space, $\bm{1}/d$, has entropy $\log d$, like above. \\


Just for clarity, let's look at the definition of pure states. Pure states are states of the form $\rho = \ket{\psi}\bra{\psi}$. \textbf{Statistical mixtures of pure states form mixed states} of the form $\sum q_j \ket{\psi_j}\bra{\psi_j}$. By definition, pure states are projectors, and thus they are idempotents, i.e., $\rho = \rho^2 = \rho^n$ for any $n\geq 2$. \\


So, just the recap, we know that quantum mechanics  (under the density operator formulation) has \textbf{states}, which are $\psi \geq 0, \tr\psi = 1$, and observables $A = \sum_{ij}A_{ij}\ket{i}\bra{j}$. Next, all Hermitians $\had$ are diagonalizable and the spectral theorem says $\had = \sum_k h_k \ket{\had_k}\bra{\had_k}$ where $\ket{\had_k}\bra{\had_k}$ is a projector and $h_k$'s are the eigenvalues, which are also the possible outcomes. Further, $\psi = \sum_{ij}\psi_{ij}\ket{i}\bra{j} = \sum_k p_k \ket{e_k}\bra{e_k}$. The $p_k$'s here are the eigenvalues of $\psi$. And notice that because $\tr\psi = \sum_k p_k\braket{e_k} = 1$. Thus we have a probability distribution. \\

\begin{defn}[Pure states]
	A state $\psi$ is pure if and only if the probability distribution $p_k$ is extremal, i.e., $(0,0,1,\dots,0)$, i.e., if and only if $\psi = \ket{\psi}\bra{\psi}$ is a rank-1 projector. This is true if and only if $\sum p_k^2 = 1$. The quantity $\sum p_i^2 = \tr(\psi^2)$ is called the \textbf{purity} of the state $\psi$. 
\end{defn}  



The connection with experiments is that, given an operator $A$:
\begin{align}
\Pr(a_i) = \tr\lp \psi \ket{A_i}\bra{A_i} \rp.
\end{align}
Then 
\begin{align}
E[A] = \sum_i \Pr(a_i) a_i = \tr\lp \psi A \rp.
\end{align}
If $\psi$ is pure, then $E[A] = \bra{\psi} A \ket{\psi}$.\\

What happens after measurements? What we learned in QM is that $a_i \mapsto \ket{A_i}$ following a measurement. But what does it mean to say ``if we get $a_i$ we get $A_i$.'' What this really means is that if we measure $a_i$ with probability $\Pr(a_i)$ then we are in $\ket{A_i}$ with probability $\Pr(a_i)$.\\

The act of measurement changes the purity of the state. Suppose we have a pure state $\ket{\psi}$. After measurement, suppose we have $\psi' = \sum_k \Pr(a_i)\ket{A_k}\bra{A_k} = \sum_k \Tr[\psi P_k]P_k$. The state changes -- sometimes called the collapse of the wavefunction. This is due to the fact that $\psi$, before measurement, gives some amplitudes about what potentially can happen. Note that the new state $\psi'$ is a statistical ensemble of pure states $P_k$.\\

So, measurements change the purity of the state. We can ask something like, given a basis $\{ \ket{i}  \}$, what is the operation $D$ that for $X$ nondiagonal, $DX$ is diagonal. This process is called \textbf{dephasing}. What can also ask changes the purity? We can answer the second question: \textbf{entanglement}. 


\subsection{Entanglement}

\textbf{Note:} There is no ``action at a distance.''\\

Now, to define entanglement, we need the notions of ``here'' and ``there.'' Consider a system made up of two parts. The mathematical description is through the tensor product:
\begin{align}
\had = \had_A \otimes \had_B.
\end{align}

Let $A \in \mathcal{B}(\had_A)$ -- $A$ is a bounded operator in $\had(A)$. Suppose $\had_{A} = \text{span}\{ \ket{0},\ket{1}  \}$, and $A = \sum_{i,j\in \{0,1\}}A_{ij}\ket{i}\bra{j}$. Suppose $\had_B = \text{span}\{ \ket{0}_B,\ket{1}_B \}$. Then
\begin{align}
\had = \had_A \times \had_B = \text{span}\{ \ket{00}, \ket{01}, \ket{10}, \ket{11}   \}.
\end{align}
Now, $A \i \mathcal{B}(\had_A)$. Since the original space is $\had = \had_A \otimes \had_B$. To generalize this $A$ so that it acts on the whole of $\had$, we generalize by
\begin{align}
A = \tilde{A} \otimes \Id_B.
\end{align}
With this, with $\psi \in \had$ and $\psi_A \in \had_A$
\begin{align}
E[A] = \tr\lp A \psi \rp = \Tr\lp \tilde{A} \psi_A \rp.
\end{align}
What is $\psi_A$? It turns out that
\begin{align}
\psi_A = \tr_B \psi.
\end{align}
Suppose 
\begin{align}
\ket{\psi} = \lp \f{\ket{0} + \ket{1}}{\sqrt{2}} \rp \otimes \ket{1}.
\end{align}
Then 
\begin{align}
\psi_A &= \Tr_B\psi \nn\\
&= \Tr_B \lb  \lp \f{\ket{0} + \ket{1}}{\sqrt{2}} \rp  \lp \f{\ket{0} + \ket{1}}{\sqrt{2}} \rp^\dagger   \otimes \ket{1}\bra{1} \rb \nn\\
&= \lp \f{\ket{0} + \ket{1}}{\sqrt{2}} \rp    \lp  \f{\bra{0} + \bra{1}}{\sqrt{2}} \rp\nn\\
&= \ket{+}\bra{+}.
\end{align}
Is this a pure state? To check, we compute the purity:
\begin{align}
\psi^2= \ket{+}\bra{+}\ket{+}\bra{+} = \braket{+} = 1,
\end{align}
so $\psi$ is pure. \\

As an exercise we can show that with any \textbf{pure separable state} $\ket{\psi} = \ket{\psi_A}\otimes \ket{\psi_B}$, $\tr_B\psi$ is also a pure state. \\


Next, consider again  $\ket{\psi} = (\ket{01} + \ket{10})/\sqrt{2}$. Is this state pure? The answer is YES because it's already given to us as a ket, i.e., the state is NOT a statistical ensemble of pure states. To show this explicitly, the density matrix is given by
\begin{align}
\psi = \f{1}{2}\lb \ket{01}\bra{01} + \ket{01}\bra{10} + \ket{10}\bra{01} + \ket{10}\bra{10} \rb
\end{align}
then 
\begin{align}
\tr_B\lp \psi \rp = \dots = \f{1}{2}\lb \ket{0}\bra{0}_A + \ket{1}\bra{1}_A \rb.
\end{align}
The purity of this is 
\begin{align}
\tr(\psi^2) = \sum_i p_i^2 = \f{1}{4} + \f{1}{4} \neq 1.
\end{align}
So $\tr_B\psi$ is no longer pure. Thus, \textbf{tracing out sometimes changes the purity}. 

\begin{defn}[Purity and Entanglement]
	We say that if the reduced/marginal state is pure then the original state is also pure. If the reduced/marginal state is not pure then the original state is entangled. 
\end{defn}

This is the essence of entanglement. There is no action at a distance. Rather, this is just an update in our probability distribution. \\




Now, something is still missing here. In physics, we're usually given interactions, not just states and operators. The next thing to learn is time evolution. Let's do that

\subsection{Time evolution}

Let a Hermitian $H \in \mathcal{B}(\had)$ be given. Then the time evolution of $\psi$ is given by
\begin{align}
\psi \mapsto e^{-iH t}\psi e^{iH t}.
\end{align}
For operators, we let the operators evolve, by definition,
\begin{align}
X \mapsto X = e^{iHt}X e^{-iHt}.
\end{align}
Mathematically, this works (the check for why this has to be the case is easy to check). When we compute the expectation value 
\begin{align}
E_t[A] = \Tr\lp A\psi(t) \rp = \Tr\lp A(t)\psi \rb
\end{align}
The term in the middle is the Schrodinger representation, and the term of the RHS is the Heisenberg representation. Next, recall that any unitary can be written as $U = e^{-iHt}$ where $H$ is some Hermitian. This means we can just look at unitaries. With this, the time evolution of $X$ is given by
\begin{align}
U(X) = U^\dagger X U.
\end{align}
This is called the \textit{adjoint action}. Now, a very important property of unitary evolution is that it is \textbf{norm-preserving}, i.e.,
\begin{align}
\norm{U(X)} = \norm{X}.
\end{align}
Recall \textbf{entanglement/ R\'enyi entropy}:
\begin{align}
S_\al = \f{1}{1-\al}\log \Tr(\psi^\al).
\end{align}
Note that the argument of the logarithm looks like the \textbf{purity} of something. Usually, we look at a pure state $\psi$ and look at $\psi_A$. If $\psi_A$ is mixed then $S_\al(\psi_A) \geq 0$. \\

\begin{prop}
	\begin{align}
	S(\psi) = S(\mathcal{U}(\psi)) = S(\mathcal{U}^\dagger \psi \mathcal{U}),
	\end{align}
	i.e., entropy is conserved under unitary evolution. 
\end{prop}

Now, there arises some trouble. We know for a fact that in time, entropy increases by the second law of thermodynamics. But we just claimed that $\mathcal{U}$ preserves entropy. In classical mechanics, we actually have a similar theorem called the \textbf{Liouville Theorem}. This is the problem in statistical mechanics. The rest of this course is targeting this problem.\\



Another interesting thing is ``Chaos.'' In classical mechanics, we have the butterfly effect. In quantum mechanics, consider $\ket{\psi_1}$ and $\ket{\psi_2}$ very close to each other: $\abs{\braket{\psi_1}{\psi_2}} = 1-\epsilon$. Now, 
\begin{align}
\mathcal{U}\ket{\psi_1} = \psi_1(t), \quad \mathcal{U}\ket{\psi_2} = \ket{\psi_2(t)}
\end{align}
It's easy to see that 
\begin{align}
\abs{\bra{\psi_1(t)}\ket{\psi_2}} = 1-\epsilon.
\end{align}
So, there is immunity to small changes in initial condition. \\


Finally, there is a trick to compute purity. Suppose $\psi \in \had$. We want to compute $\Tr \psi^2$. Now, consider the following: Suppose we make a replica, $\psi \mapsto \psi\otimes \psi \i n\had\otimes \had$. Consider the operator $T^{(2)}$ -- a permutation operator of order 2. Let $\{ \ket{i} \}, \{ \ket{j} \}$ be bases of $\had$. Then $\{ \ket{ij} \}$ is a basis for $\had$. $T^{(2)}$ is defined by $\ket{ij} \mapsto \ket{ji}$. Then we can show in the exercise that
\begin{align}
\Tr\psi^2 = \Tr\lb T^{(2)} \psi \otimes \psi \rb.
\end{align}

Next, suppose $\psi$ is pure but $\psi_A$ isn't. What is the purity of $\psi_A$? Well, we want some operator $\mathcal{O}$ such that
\begin{align}
\Tr \psi_A^2 = \Tr\lb \mathcal{O} \psi^{\otimes 2}  \rb?
\end{align}
Again, this will be addressed in the exercises. \\


After these, we will look at how unitary operations make entropies grow. 




















\newpage

\section{Resource Theories and Quantum Information}


\newpage
\section{Quantum Thermal Operations}


\newpage
\section{Fluctuation Theorems and Quantum Information}


\newpage
\section{Quantum Thermal Machines}




\newpage


\section{Exercises}

\noindent \textbf{1. Density Operators} Show that given the distribution of states $(q_j, \ket{\psi_j})$ the probability of getting an outcome $a_i$ when measuring $A = \sum_i a_i \ket{a_i}\bra{a_i} \equiv \sum_i a_i P_i$ is given by
\begin{align}
\Pr(a_i) = \tr(\psi P_i)
\end{align} 
where we have define the density matrix of the system as
\begin{align}
\psi = \sum_j q_j \ket{\psi_j}\bra{\psi_j}
\end{align}


\begin{proof}
	This is not very hard: 
	\begin{align}
	\Pr(a_i) &= \bra{a_i} \psi \ket{a_i}\nn\\
	&= \sum_j \bra{a_i} q_j \ket{\psi_j}\bra{\psi_j}\ket{a_i}\nn\\
	&= \sum q_j \ket{\psi_j}\bra{\psi_j}\ket{a_i}\bra{a_i}\nn\\
	&= \tr(\psi P_i).
	\end{align}


Alternatively, we can do this like the following: Since the trace is linear, the probability of getting the outcome $a_i$ is 
\begin{align}
\Pr(a_i) = \tr\lb \sum_j q_j \psi_j P_i \rb = \tr\lb \psi P_i \rb.
\end{align}
This is perhaps a better way to do the problem than the previous approach. In any case they're equivalent. 
\end{proof}


\noindent \textbf{2. Partial Trace} Mixed states appear naturally also when considering subsystems of a composite
quantum system. Consider
\begin{align}
A = \tilde{A}_S \otimes I_{\bar{S}}
\end{align}
in a Hilbert space with tensor product structure $\had = \had_S \otimes \had_{\bar{S}}$. It is a natural question to ask what is the state $\rho_S \in \mathcal{B}(\had_S)$ onto the subsystem $\had_S$ that can return all the right expectation values when performing any measurement of operators with that support, namely, the $\tilde{A}_S$. We ask that 
\begin{align}
\tr(A\rho) = \tr(\tilde{A}_S \rho_S).
\end{align}
Show that the solution of this equation is 
\begin{align}
\rho_S = \tr_{\bar{S}}(\rho)
\end{align}
where the operation of partial trace is defined by
\begin{align}
\tr_{\bar{S}}X = \tr_{\bar{S}}\sum_{i_S i_{\bar{S}} j_S j_{\bar{S}}}  X_{i_S i_{\bar{S}} j_S j_{\bar{S}}} \ket{i_S i_{\bar{S}}}\bra{j_S j_{\bar{S}}} = \sum_{i_S  j_S} X_{i_S j_S} \ket{i_S}\bra{j_S}.
\end{align}


\begin{proof}     
	The definition of partial trace above is not very useful due to bad notation. So we will consider an equivalent definition:
	\begin{align}
	\tr_{\bar{S}}(\rho) = \sum_{i_{\bar{S}}} \bra{i_{\bar{S}}}\rho\ket{i_{\bar{S}}}. 
	\end{align}
	Note that this is an operator, not a number.  We want to show
	\begin{align}
	\tr(A\rho) = \tr(\tilde{A}_S \tr_{\bar{S}}(\rho)).
	\end{align}
	Well, by the definition of the trace in terms of the inner product:
	\begin{align}
	\tr(A\rho) &= \tr(\rho A) \nn\\
	&= \tr(\rho(\tilde{A}_S \otimes I_{\bar{S}}) )\nn\\
	&\equiv \tr(\rho (\tilde{A} \otimes I))\nn\\
	&=  \sum_{i_{S}} \sum_{i_{\bar{S}}}\bra{i_{S} i_{\bar{S}}} \rho (\tilde{A}_S \otimes I_{\bar{S}})   \ket{i_{S} i_{\bar{S}}}\nn\\
	&= \sum_{i_{S}} \sum_{i_{\bar{S}}}\bra{i_{S} i_{\bar{S}}} \rho \ket{\tilde{A}_S i_S \otimes  i_{\bar{S}}}\nn\\
	&= \sum_{i_{\bar{S}}}\sum_{i_{S}}  \bra{i_{{S}}}  \bra{i_{\bar{S}}} \rho \ket{i_{\bar{S}}}  \ket{\tilde{A}_S i_S} \nn\\
	&= \tr\lp \tilde{A}_S \tr_{\bar{S}} (\rho) \rp\nn\\
	&= \tr\lp \tilde{A}_S \rho_S \rp.
	\end{align}
	
\end{proof}


\noindent \textbf{3. R\'enyl Entropy} The R\'enyi entropy for R\'enyi index $q$ is defined as
\begin{align}
\boxed{S_q(\rho) = \f{1}{1-q}\log \Tr \rho^q = \f{1}{1-q} \log \lp \sum_i \lambda_i^q \rp}
\end{align} 
where the logarithm is again taken to be base two and $\lambda_i$ are the eigenvalues of $\rho$.

\begin{enumerate}
	\item Show that for the completely mixed density operator $\rho = I/d$ in $d$-dimensional Hilbert space, the von Neumann entropy is given by
	\begin{align}
	S(I/d) = \log d.
	\end{align}
	
	\begin{proof}
		This is very easy. The eigenvalues of $I/d$ are $1/d$. The von Neumann entropy is given as 
		\begin{align}
		S_{vn} = -\sum \lambda_x \log \lambda_x = -\sum \f{1}{d}\log \f{1}{d} = \log d.
		\end{align}
	\end{proof}
	
	\item Show that 
	\begin{align}
	S_0(\rho) =\lim_{q\to 0+} S_q(\rho) =\log (\rank \rho)
	\end{align}
	where $\rank \rho$ is the rank of the density matrix $\rho$.
	
	\begin{proof}
		This is also quite easy. When $q \to 0+$, the definition says the leading factor becomes $+1$. Inside the logarithm, we just get a sum of $\lambda_i^0$. Now, of course these values only approach 1 if they're not zero. So, the sum is exactly the rank of the matrix $\rho$. Thus, 
		\begin{align}
		S_0(\rho) = \log \rank \rho.
		\end{align} 
	\end{proof}
	
	
	\item Show that 
	\begin{align}
	\lim_{q\to 1} S_q(\rho) = S_1(\rho)
	\end{align}
	where $S_1(\rho)$ is the von Neumann entropy. 
	
	\begin{proof}
		This is quite hard. The proof is contained in this \href{https://arxiv.org/pdf/1306.3142.pdf}{\underline{paper}} by Muller-Lennert in 2013. \\
		
		But we can also do this using l'Hopital's rule:
		\begin{align}
		\lim_{q\to 1}S_q(\rho) &= \f{1}{\ln 2}\lim_{q\to 1} \lb \f{\log \lp\sum_i \lambda_i^q\rp}{1-q} \rb\nn\\
		&= \f{1}{\ln 2}\lim_{q\to 1} \lb \f{ \f{1}{\sum_i \lambda_i^q} \sum_i \f{d}{dq} \lambda_i^q}{-1} \rb\nn\\
		&= \f{1}{\ln 2}\lim_{q\to 1}\lb   -\f{\sum_i \lambda_i^q} \sum_i  \lambda_i^q \ln \lambda_i\rb\nn\\
		&= -\f{1}{\sum_i\lambda_i}\sum_i \f{\ln \lambda_i}{\ln 2}\nn\\
		&= -\sum_i \lambda_i \log \lambda_i\nn\\
		&= S_{vn}(\rho).
		\end{align}
	\end{proof}
	
	
	
	
	\item Consider the R\'enyi entropy for the case where the R\'enyi index is an integer $n$ with $n \geq 2$. If $\rho$ corresponds to a pure state, calculate $S_n(\rho)$. Show that if $\rho_\text{pure}$ corresponds to a pure state, then
	\begin{align}
	S_n(\rho_\text{pure}) = 0.
	\end{align}
	
	\begin{proof}
		By definition, pure states are idempotents. This means $\tr(\rho) = \tr(\rho^n) = 1$ always, by normalization. Now, diagonalize $\rho, \rho^2, \dots, \rho^n$ we see that the condition $\tr(\rho^n) = 1$ implies $\sum \lambda_\rho^n = 1$. So, by the definition of the Renyi entropy, we have that 
		\begin{align}
		S_n(\rho_\text{pure}) = 0.
		\end{align}
	\end{proof}
	
	
	
\end{enumerate}




\noindent \textbf{4. Von Neumann Entropy}
Compute the von Neumann entropy of the Gibbs state.


\begin{proof}
	The Gibbs state is given by
	\begin{align}
	\rho = \f{e^{-\beta \had}}{ \tr\lp  e^{-\beta \had} \rp }
	\end{align}
	The von Neumann entropy of $\rho$ is given by
	\begin{align}
	S_{vn}(\rho) = -\sum \lambda_x \log \lambda_x
	\end{align}
	where $\lambda_x$ are the eigenvalues of $\rho$. Suppose the Hamiltonian $\had$ has the spectrum $\{ \sigma_i \}$, then by the spectral mapping theorem we have that the spectrum of $e^{-\beta \had} = \{ e^{-\beta \sigma_i} \}$, and so it follows that
	\begin{align}
	\tr\lp e^{-\beta \had} \rp = \sum e^{-\beta \sigma_i} = \mathcal{Z}
	\end{align}
	where of course $\mathcal{Z}$ is the partition function. With these, the von Neumann entropy is given by
	\begin{align}
	S_{vn} 
	&= -\f{1}{\mathcal{Z}}\sum {e^{-\beta \sigma_i}}\log \f{e^{-\beta \sigma_i}}{\mathcal{Z}} = \log \mathcal{Z} + \beta\langle E \rangle
	\end{align}

	
	
\end{proof}

















\noindent \textbf{5. Swap Operator.} One of the problems of the entanglement entropies is that to know them one has
to know all the eigenvalues of the reduced density operator, which amounts to
knowing the wave-function. In other words, entanglement is not the expectation
value of an observable. Define the swap operator as the operator on two copies of
the original Hilbert space, namely $T^{(2)} \in \had \otimes \had$. This operator is defined from the action on the basis states as
\begin{align}
T^{(2)} \ket{\phi_i \phi_j} = \ket{\phi_j \phi_i}.
\end{align}
\begin{enumerate}
	\item \textbf{Purity as expectation value.} Given a state $\sigma \in \mathcal{B}(\had)$, show that 
	\begin{align}
	\tr\sigma^2 = \tr\lp T^{(2)} \sigma^{\otimes 2} \rp.
	\end{align}
	
	\begin{proof}
		\begin{align}
		\tr\lp T^{(2)}\sigma^{\otimes 2} \rp &= \sum_{i,j} \bra{ij} T^{(2)}\sigma^{\otimes 2} \ket{ij}\nn\\
		&= \sum_{i,j} \bra{ij} T^{(2)} (\sigma \ket{i} \otimes \sigma\ket{j})\nn\\
		&= \sum_{i,j} \bra{ij} T^{(2)} (\sigma \ket{j} \otimes \sigma\ket{i})\nn\\
		&= \sum_{i,j} \bra{ij} \sigma^2 \ket{ji} \nn\\
		&= \sum_i \bra{i}\sigma^2 \ket{i}\nn\\
		&= \tr\lp \sigma^2 \rp.
		\end{align}
	\end{proof}
	
	\item \textbf{Trace of $T^{(2)}$.} Compute the trace of $T^{(2)}$.
	\begin{proof}
		Just use $\sigma = \Id$. Then the $\tr(T^{(2)})$ is just the dimension of the space.  
	\end{proof}
	
	\item \textbf{Frobenius norm.} Show that for any linear operator $X$, the Frobenius norm reads 
	\begin{align}
	\norm{X}_2^2 = \tr\lp T^{(2)} X \otimes X^\dagger \rp.
	\end{align}
	\begin{proof}
		This one is similar to part 1:
		\begin{align}
		\norm{X}_2^2 &= \sqrt{\tr(X^\dagger X)} \nn\\
		&= {\sum_i \bra{i} X^\dagger X \ket{i}}\nn\\
		&= {\sum_{i,j} \bra{i}X^\dagger \ket{i}\bra{j} X\ket{i}   }\nn\\
		&= \sum_{i,j} (\ket{i} \otimes \ket{j})(X^\dagger\ket{j} \otimes X\ket{i})\nn\\
		&= \sum_{i,j} \bra{ij}T^{(2)}( X\ket{i} \otimes X^\dagger\ket{j} )\nn\\
		&= \tr\lp T^{(2)} X \otimes X^\dagger \rp.
		\end{align}
	\end{proof}

\end{enumerate}






\noindent \textbf{6. Unitary Evolution}
\begin{enumerate}
	\item \textbf{Norm invariance.} Show that the Frobenius norm and the Operator norm are preserved by unitary operations.
	\begin{proof}
		This is one easy. For the Frobenius norm, we just use the fact that $\tr(AB) = \tr(BA)$. Everything else follows. In general, things follow from the fact that unitaries preserve the inner product.  
	\end{proof}
	
	\item \textbf{Entropy invariance.} Show that all Entropies are preserved by unitary evolution.
	\begin{proof}
		Entropies depend on something that looks like $\tr \psi^\al$ for some $\al$ and the state $\psi$. Again, because $\tr(BA) = \tr(AB)$, it's an easy argument to show the trace remains invariant.
	\end{proof}

\end{enumerate}



\noindent \textbf{7. Entanglement and partial trace.} Show that, if a state $\sigma(t)$ is evolving in time, the purity of the marginal state $\sigma_A(t)$ is given by 
\begin{align}
\tr \sigma^2_A(t) = \tr \lb T^{(2)} \otimes I_B^{\otimes 2} U^{\otimes 2} \sigma^{\otimes 2} U^{\dagger \otimes 2} \rb
\end{align}
where $U$ is the unitary evolution operator.
\begin{proof}
	
\end{proof}



\noindent \textbf{8. Time evolution by Hamiltonian.} Be $\mathcal{U}(X) = e^{iHt} X e^{-iHT} \equiv X_t$ the time evolution generated by a non-degenerate Hamiltonian $H = \sum_n E_n P_n$.
\begin{enumerate}
	\item \textbf{Time averaged operator}
	Define the time average as $\bar{X} = \lim_{T\to \infty} \f{1}{T} \int^T_0 X_t\,dt$. Show that
	\begin{align}
	\bar{X} = \sum_n P_n X P_n.
	\end{align}
\end{enumerate}
\begin{proof}
	This needs quite a lot of manipulations. The key is to expand $X$ in the energy eigenbasis:
	\begin{align}
	\bar{X} &= \f{1}{T}\lim_{T\to \infty}\int_0^T e^{iHt} X e^{-iHT}\,dt\nn\\
	&= \lim_{T\to \infty}\f{1}{T}\int_0^T e^{iE_nt} \sum_{n,m} X_{nm} \ket{n}\bra{m} e^{-iE_mt}\,dt, \quad X_{nm} = \bra{n} X \ket{m}  \nn\\
	&= \lim_{T\to \infty}\f{1}{T}\int_0^T e^{i(E_n-E_m)t} \sum_{n,m} X_{nm} \ket{n}\bra{m}\,dt\nn\\
	&= \braket{n}{m}\sum_{n,m} \ket{n}X_{nm}\bra{m}\nn\\
	&= \braket{n}{m}\sum_{n,m} \ket{n}\bra{n} X \ket{m}\bra{m}\nn\\
	&= \sum_n \ket{n} \bra{n} X \ket{n}\bra{n}\nn\\
	&= \sum_n P_n X P_n.
	\end{align}
	So we're done. 
\end{proof}



\noindent \textbf{9. Quantum Measurement.} Show that, after measuring on the state $\sigma$ some observable $\Omega = \sum_i o_i \ket{i}\bra{i} \equiv \sum_i o_i \omega_i$, the resulting state is 
\begin{align}
\sigma' = \sum_i \omega_i o_i \omega_i.
\end{align}
\begin{proof}
	Well, measurement is basically a projection:
	\begin{align}
	\sigma' &= \sum_i \ket{i}\bra{i}\sigma \nn\\
	&= \sum_i \ket{i}\bra{i}\sum_j \sigma_j \ket{i}\bra{j}\nn\\
	&= \sum_{i,j}\ket{i}\bra{i} \ket{j} \sigma_j \bra{j}\nn\\
	&= \sum_i \ket{i} \sigma_i \bra{i}\nn\\
	&= \sum_i \ket{i} \bra{i} \sigma \ket{i}\bra{i}\nn\\
	&= \sum_i \omega_i \sigma \omega_i
	\end{align}
	where we have used the fact that the matrix element $\sigma_i = \sigma_{ii} = \bra{i}\sigma\ket{i}$.
\end{proof}

















\newpage
\chapter{Numerical Methods \& Condensed Matter Physics}

\textbf{Instructor:} Aaron Szasz\\

This course has two main goals: (1) to introduce some key models from condensed matter physics; and (2) to introduce some numerical approaches to studying these (and other) models.  As a precursor to these objectives, we will carefully understand many-body states and operators from the perspective of condensed matter theory.  (However, I will cover only spin models.  We will not discuss or use second quantization.)\\



Once this background is established, we will study the method of exact diagonalization and write simple python programs to find ground states, correlation functions, energy gaps, and other properties of the transverse-field Ising model and XXZ model.  We will also discuss the computational limitations of exact diagonalization.  Finally, I will introduce the concept of matrix product states, and we will see how these can be used with algorithms such as the density matrix renormalization group (DMRG) to study ground state properties for much larger systems than can be studied with exact diagonalization.


\section{Introduction}

What is \textit{condensed matter}? This is the study of many particles interacting with each other. This could be studying crystals, liquid, etc. These are all condensed matter. We will be studying this using computer programs. \\

The goals of this course are
\begin{itemize}
	\item Learn some numerical methods.
	\item To learn important models in condensed-matter physics.  
\end{itemize}

In particular, we will be studying the quantum Ising model, or equivalently the transverse field Ising model (TFIM) given by the Hamiltonian
\begin{align}
\had = -J \sum_i S^z_i S^z_{i+1} - h \sum_i S^x_i
\end{align}

We will first cover some background material required for the rest of the course. Then, we will look at \textbf{exact diagonalization} and use it to solve the Ising model. On the third lecture, we will continue with \textit{exact diagonalization}, and depending on how much time we have we will study the $XXZ$ model. On the fourth lecture, we will look at \textbf{matrix product states (MPS)}. On the fifth day, we will use MPS to solve the Ising and XXZ models more efficiently than using exact diagonalization.


\newpage
   

%\section{L1: Introduction to many-particle states and operators}




\section{The Ising model: Motivation}

The Ising model is important because it models magnetism and features quantum phase transition, which is an important topic we will cover later.\\

Consider a one-dimensional chain of $N$ atoms. Each atom has one electron, and that the electrons cannot move due to Coulomb repulsion. So, we have 1 fixed electron per site. For each atom, we solve a local SE and get some set of orbitals. \\

We will assume that each electron is in the $p^z$ orbital and forget about it. \\

We note that this description allows for only one degree of freedom, which is the spin of our electron. Two neighboring electrons can have the same or opposite spins. Our model is given by the Hamiltonian
\begin{align}
\had = -J \sum_i S_i^z S_{i+1}^z
\end{align}
the product $S_iS_{i+1}$ is 1 if the electrons are parallel, and $-1$ if antiparallel. When $J > 0$, the model favors all electrons pointing in the same direction. Now, if the model only has an external field
\begin{align}
-h \sum_i S_i^x
\end{align}
which favors all electrons point in the direction of the external field $+x$. \\

The goal is, given the Hamiltonian:
\begin{align}
\had = -J \sum_i S^z_i S^z_{i+1} - h \sum_i S^x_i
\end{align}
What is the most favorable configuration of the electron?


















\section{Introduction to many-particle states and operators}

\subsection{Spin 1/2 Systems} 

This appears in many introductory quantum mechanics courses, but we'll review this anyway. \\

In general, a state of a spin-1/2 system  can be written as a linear superposition of two spin states
\begin{align}
\ket{\psi} = a\ket{\uparrow} + b\ket{\downarrow}
\end{align}
where $\ket{\psi}$ is normalized and $\ket{\uparrow},\ket{\downarrow}$ form an ONB. This means that $\braket{\uparrow\downarrow} = 1$ and $\braket{\uparrow}{\downarrow} = 0$. 

\subsection{Operators on Spin-1/2 Systems}
An $\mathcal{O}$ is specified by its action on the basis state: 
\begin{align}
\mathcal{O}\ket{\uparrow} = \mathcal{O}_{11}\ket{\uparrow} + \mathcal{O}_{12}\ket{\downarrow},\quad \mathcal{O}\ket{\downarrow} = \mathcal{O}_{21}\ket{\uparrow} + \mathcal{O}_{22}\ket{\downarrow}.
\end{align}
To write in a matrix representation, we will assign $\ket{\uparrow} = (1,0)^\top$ and $\ket{\downarrow} = (0,1)^\top$. With this, we can find operators as well as other states in this basis.\\

An aside: Consider a $2\times 2$ matrix $\mathcal{O} = [O_{ij}]$. Note that $\mathcal{O}(1,0)^\top$ returns the first column of $\mathcal{O}$, and so on. This means any matrix $\mathcal{O}$ can be written as the output of $\mathcal{O}$ when fed the standard basis states. \\

Suppose we want to find the first column of $\mathcal{O}$. From the definition above, we can easily see that
\begin{align}
\mathcal{O} = \begin{bmatrix}
\mathcal{O}_{11} & \mathcal{O}_{12} \\ \mathcal{O}_{21} & \mathcal{O}_{22}
\end{bmatrix}
\end{align}


We care a lot about Hermitian operators: $\mathcal{O} = \mathcal{O}^\dagger$. For $2\times 2$ matrices, any Hermitian operator can be written as 
\begin{align}
\mathcal{O} = a\Id + b\sigma^z + c\sigma^x + d\sigma^y
\end{align}
where $\sigma^i$ are the Pauli matrices. Now, we want to write this in the language of states and operators. Recall that 
\begin{align}
\sigma^z\ket{\uparrow} = \ket{\uparrow}, \quad \sigma^z\ket{\downarrow} = -\ket{\downarrow}
\end{align}
and so on: $\sigma^x$ flips the arrow, and $\sigma^y$ flips and multiplies $\ket{\uparrow}$ by $i$ and $\ket{\downarrow}$ by $-i$. \\

Now, matrices can appear different under different bases. Consider the basis given by
\begin{align}
\ket{+} = \f{1}{\sqrt{2}}\lp \ket{\uparrow} + \ket{\downarrow} \rp = (1,0)^\top, \quad
\ket{-} = \f{1}{\sqrt{2}}\lp \ket{\uparrow} - \ket{\downarrow} \rp = (0,1)^\top
\end{align}
This is an new ONB, by inspection. \\
 
What do the Pauli matrices become under this change of basis? Well, the first column of the new $\sigma^z$ is given by
\begin{align}
\sigma^z\begin{bmatrix}
1\\0
\end{bmatrix}_* = \f{1}{\sqrt{2}} \sigma^z \lp \ket{\uparrow} +  \ket{\downarrow} \rp = \f{1}{\sqrt{2}}\lp \ket{\uparrow} - \ket{\downarrow} \rp  =\ket{-} = \begin{bmatrix}
0\\1
\end{bmatrix}_*
\end{align}
Similarly, we can show that $\sigma^z\ket{\pm} = \ket{\mp}$. In this new basis, $\sigma^z_{new} = \sigma^x$. We can also find the form for the other matrices, but let's not worry about that. The gist is that $\ket{\uparrow\downarrow}$ are the $z$-eigenstates, and $\ket{\leftarrow\rightarrow}$ are the $x$-eigenstates. \\

How the matrices change can be found via the usual change of basis that we all have seen in linear algebra.\\



\section{Tensor Product Spaces}

Suppose there are two space of spin-1/2 particles. The question we want to answer is: What are the allowed states of the 2-spin system? The answer is this: any state in the ``tensor product space,'' which is the vector space whose basis is the following four vectors: $\{ \ket{\uparrow\uparrow}, \ket{\uparrow\downarrow}, \ket{\downarrow\uparrow}, \ket{\downarrow\downarrow}\}$. Each basis vector is an ordered pair, the first being a basis vector in $A$, the second in $B$. \\


So, a general state in the tensor product space is just some superposition of these vectors. What is the inner product in this new space? Suppose we have
\begin{align}
\ket{\psi_A} \otimes \ket{\psi_B}, \quad \ket{\phi_A} \otimes \ket{\phi_B}.
\end{align}
Then the inner product between these is defined as 
\begin{align}
(\bra{\psi_A} \otimes \bra{\psi_B}) (\ket{\phi_A} \otimes \ket{\phi_B}) = \braket{\psi_A}{\phi_A} \cdot \braket{\psi_B}{\phi_B}.
\end{align}


One important consequence is that if the $A$ part is orthogonal or the $B$ part is orthogonal then the inner product in the tensor space is also zero (so, also orthogonal). This is quite easy to see. \\

Finally, we want to know how to apply operators. And operator on the tensor product space looks like
\begin{align}
\mathcal{O} = \sum_i \mathcal{O}^A_i \otimes \mathcal{O}^B_i.
\end{align}
The way each component of $\mathcal{O}$ acts is the following:
\begin{align}
(\mathcal{O}^A \otimes \mathcal{O}^B) (\ket{\psi_A} \otimes \ket{\psi_B}) = (\mathcal{A} \ket{\psi_A}) \otimes (\mathcal{A} \ket{\psi_B}).
\end{align}
By linearity, we can work out how $\mathcal{O}$ acts in general.\\

From this definition, we can see some useful facts. Suppose we want to multiply two operators: 
\begin{align}
(\mathcal{A} \otimes \mathcal{O}^B)(\tilde{\mathcal{O}}^A \otimes \tilde{\mathcal{O}}^B )(\ket{\psi_A} \otimes \ket{\psi_B}) = \dots = (\mathcal{O}^A\tilde{\mathcal{O}}^A \otimes \mathcal{O}^B\tilde{\mathcal{O}}^B)(\psi_A\psi_B).
\end{align}
So, we have
\begin{align}
(O^A \otimes O^B)(H^A \otimes H^B)= O^A H^A \otimes O^B H^B. 
\end{align}

Next, suppose we have $O^A \otimes \Id^B$ and $\Id^A \otimes O^B$. By looking at the commutation, we can easily check that these commute. This fact on be summarized as the following: ``Operators on different sites commute.'' \\


\subsection{Born Rule for Probabilities}

Now we want to look at how to get probabilities. Suppose
\begin{align}
\ket{\psi} = a\ket{00} + b\ket{01} + c\ket{10} + d\ket{00}.
\end{align}
Then $P(00) = \abs{a}^2$, $P(01) = \abs{b}^2$. Then $P(0_A) = \abs{a}^2 + \abs{b}^2$. Finally, suppose we want to find expectation value:
\begin{align}
\langle \sigma^z_A \rangle = \langle \sigma^z_A \otimes \Id_B \rangle = 1\cdot P(0_A) + (-1)P(1_A) = \abs{a}^2 + \abs{b}^2 - \abs{c}^2 - \abs{d}^2.
\end{align}
With this, in the state $\ket{\psi}$, we're going to calculate $\langle \sigma^z_A \otimes \Id_B\rangle $ directly:
\begin{align}
\bra{\dots} \sigma^z_A \otimes \Id_B\ket{\dots} = \dots =  \abs{a}^2 + \abs{b}^2 - \abs{c}^2 - \abs{d}^2
\end{align}
as expected.



subsection{Expectation Values}

Another useful way to compute expectation values is to use matrix representations. First, we assign basis vectors. Then find matrix for $\sigma_A^z \otimes \Id_B$. Then find matrix/vector representation of $\ket{\psi}$. Finally, multiply out. \\

At this point, we will frequently use the short hand $\ket{\psi_A}\ket{\phi_B} = \ket{\psi\phi}$. With this, we introduce the representation:
\begin{align}
&\ket{\uparrow\uparrow} = (1,0,0,0), \quad \ket{\downarrow\uparrow} = (0,0,1,0)\nn\\
&\ket{\uparrow\downarrow} = (0,1,0,0), \quad \ket{\downarrow\downarrow} = (0,0,0,1)
\end{align}
We want to find the matrix for $\sigma^z_A \otimes  \Id_B$. To do this, we find column-by-column by letting it act on the standard basis vectors. After this is done, we find that 
\begin{align}
(\sigma_A^z \otimes \Id_B) = \begin{bmatrix}
1 &&&\\
& 1 && \\
&&-1&\\
&&&-1
\end{bmatrix}
\end{align}
Finally, we can verify that
\begin{align}
\bra{\psi} \sigma^z_A \otimes \Id_B \ket{\psi} = \abs{a}^2 + \abs{b}^2 - \abs{c}^2 - \abs{d}^2.
\end{align}


\begin{exmp}
	We will look at the $4\times 4$ matrices in this basis for the following operators:
	\begin{align}
	\sigma_A^y \otimes \sigma_B^z = \begin{bmatrix}
	&&-i&\\
	&&&i\\
	i&&&\\
	&-i&&
	\end{bmatrix}
	\end{align}
	
	\begin{align}
	\sigma^z_A \otimes \sigma^y_B = \begin{bmatrix}
	&-i&& \\
	i&&&\\
	&&&i\\
	&&-i&
	\end{bmatrix}
	\end{align}
\end{exmp}

A shortcut to do these calculations is of course the Kronecker product:
\begin{align}
\sigma_A^y \otimes \sigma_B^z = 
\begin{bmatrix}
0 & -i \\ i & 0
\end{bmatrix} \otimes \begin{bmatrix}
1 & 0 \\ 0 & -1
\end{bmatrix}
=\begin{bmatrix}
&&-i&\\
&&&i\\
i&&&\\
&-i&&
\end{bmatrix}
\end{align}

\begin{align}
\sigma^z_A \otimes \sigma^y_B = 
\begin{bmatrix}
1 & 0 \\ 0 & -1
\end{bmatrix} \otimes \begin{bmatrix}
0 & -i \\ i & 0
\end{bmatrix}  
=\begin{bmatrix}
&-i&& \\
i&&&\\
&&&i\\
&&-i&
\end{bmatrix}
\end{align}
In general, 
\begin{align}
A \otimes B = \begin{bmatrix}
a_{11}B & a_{12}B \\ a_{21}B & a_{22}B
\end{bmatrix}.
\end{align}
This is always true if the choice of basis is correct. They are correct in the sense that we can construct the tensor product basis by taking Kronecker products of the basis vectors of the factor space. \\


The moral of the story is this. The matrix for $\mathcal{O}_A \otimes \mathcal{O}_B$ is the Kronecker product of matrices for $\mathcal{O}_A$ and $\mathcal{O}_B$. 




\section{The Two-site Ising Model}

Recall the Hamiltonian for the model:
\begin{align}
\had = -J \sum_i S_i^z S_{i+1}^z - h \sum_i S_i^x.
\end{align}
where
\begin{align}
S_i^z = \f{\hbar}{2}\sigma_i^z.
\end{align}
We can rewrite the model as
\begin{align}
\had = -J \lp \f{\hbar}{2} \rp^2 \sim_i \sigma_i^z \sigma_{i+1}^z - h \f{\hbar}{2}\sum_i \sigma^x_i
\end{align}
Let the constants absorb $\hbar/2, \hbar^2/4$ respectively, then the model becomes
\begin{align}
\had = -J \sum_i \sigma_i^z\sigma_{i+1}^z - h\sum_i \sigma_i^x.
\end{align}
Next, we want to make this precise in the sense of tensor products. For the two-site model, we have precisely
\begin{align}
\had = -J \sigma_0^z \otimes \sigma_1^z - h (\sigma_0^x \otimes \Id_1 + \Id_0\otimes\sigma_1^x).
\end{align} 
We can now convert this into a $4\times 4$ matrix, in the programming exercises. \\

Before any programming though, let's do this by hand. First, let us find the matrix for $\sigma_0^z \otimes \sigma_1^z$. Once we do that, we will find the eigenstates and the eigenvalues of $H$ when $h=0$. Furthermore, we will show that in each of the eigenstate, $\langle \sigma_0^x\otimes \Id_1 + \Id_0 \otimes \sigma_1^x \rangle = 0$. Next, we will do the opposite find the matrix for $\sigma_0^x\otimes \Id_1 + \Id_0 \otimes \sigma_1^x$. Find the eigenvalues and eigenvectors when $J =0$. Then, we will show that in each of these states, $\langle \sigma_0^z \otimes \sigma_1^z \rangle = 0$. Then, we will find the $4\times 4$ matrix for $\had$. \\

Well,
\begin{align}
\sigma^z \otimes \sigma^z = \text{diag}(1,-1,-1,1).
\end{align} 
Obviously the eigenstates are exactly the vectors in the $ZZ$ basis: $\{ \ket{00}, \ket{01}, \ket{10}, \ket{11}\}$ with eigenvalues $1,-1,-1,1$ respectively. The corresponding eigenvalues in the Hamiltonian are $-J,J,J,-J$. \\

Next, to see that $\langle \sigma_0^x\otimes \Id_1 + \Id_0 \otimes \sigma_1^x \rangle = 0$ for each eigenvector, just use the fact that $\sigma^z\ket{\uparrow} = \ket{\downarrow}$ and $\sigma^x\ket{\downarrow} = \ket{\uparrow}$ and that the inner product $\braket{\uparrow}{\downarrow} = 0$. This part is easy.\\

Next, 
\begin{align}
\sigma_0^x\otimes \Id_1 + \Id_0 \otimes \sigma_1^x = \begin{bmatrix}
\sigma^x & \Id \\ \Id & \sigma^xs
\end{bmatrix}.
\end{align}

The eigenvectors in this case will be $\{ \ket{++}, \ket{-+}, \ket{+-}, \ket{--}  \}$. The eigenvalues are $\{1,0,0,-1\}$ respectively. These correspond with th eigenvalues $-2h,0,0,2h$ in the Hamiltonian. Verifying the expectation is easy, so again I won't do that.\\

Finally, the $4\times 4$ matrix for $H$ is given by
\begin{align}
\had = -J\begin{bmatrix}
1&&&\\&-1&&\\&&-1&\\&&&1
\end{bmatrix} - h\begin{bmatrix}
&1&1&\\1&&&1\\1&&&1\\&1&1&
\end{bmatrix} = \begin{bmatrix}
-J&-h&-h&\\-h&J&&-h\\-h&&J&-h\\&-h&-h&-J
\end{bmatrix}
\end{align}

We will solve this and $n$-site Ising models next. 























%
%
%\section{L1: The XXZ model}
%
%\section{L1: Programming basics}
%
%\section{L1 : Finding expectation values}

\newpage

\section{Exact diagonalization part 1}

\section{Representing models}

\section{Finding eigenstates}

\section{Energy gaps}

\section{Phase transitions}
\newpage




\section{Exact diagonalization part 2}


\section{Limitations of the method}

\section{Using symmetries}

\section{Dynamics}


\newpage


\section{Matrix product states part 1}




\section{Entanglement and the singular value decomposition}



\section{What is a matrix product state and why is it useful}
\newpage





\section{Matrix product states part 2}




\section{Algorithms for finding ground states using matrix product states: iTEBD }




\section{Algorithms for finding ground states using matrix product states: DMRG}

\newpage











\chapter{Path Integrals}

\textbf{Instructor:} Dan Wohns\\

The goal of this course is to introduce the path integral formulation of quantum mechanics and a few of its applications. We will begin by motivating the path integral formulation and explaining its connections to other formulations of quantum mechanics and its relation to classical mechanics. We will then explore some applications of path integrals.


\newpage

\section{Introduction to path integrals and the semi-classical limit}

Path integrals is a different yet equivalent formulation of quantum mechanics. The path integral formulation of quantum mechanics does make a connection between classical and quantum physics more clear. It also has a number of advantages, some which we will touch on later. In essence, path integrals can allow for perturbation approaches to solving problems in quantum mechanics. Path integrals also generalize quantum field theory. Solving SE is much more difficult in higher dimensions. Using a differential-equation approach to QFT is often hard. Path integrals allow for easier approaches to the same problems in QFT. \\


\subsection{Motivation \& Definition}

Consider the double slit experiment. 
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.8]{double-slit}
	\caption{From Feynman's lectures of Physics}
\end{figure}

We know that the amplitude/propagator $K = A_1 + A_2$ leads to interference. \\

The basic idea behind the path integral comes from considering what happens when there are many many many more slits. This suggests that the propagator should be the sum over all paths of some functional $A[\gamma]$ that is the amplitude associated with each path:
\begin{align}
K =\sum_\gamma A[\gamma]
\end{align}
where 
\begin{align}
\boxed{K(q_f,t_f, q_i,t_i) = \bra{q_f} e^{i\had (t_f - t_f)/\hbar} \ket{q_i}}
\end{align}
is the propagator. \\


When there are infinitely many holds of infinitesimal size, the particle can be anywhere (since there's essentially no more holes), so we have the picture
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.5]{time}
	\caption{From PI lecture by Dan Wohns}
\end{figure}
In this case, the sum over paths becomes an integral
\begin{align}
\sum_\gamma \to \lim_{N\to \infty}\int d q_1dq_2\dots dq_{N-1}
\end{align}


\subsection{Some properties}

\begin{prop}[Composition]
	Consider the path $\gamma_{12}$ composed of the path $\gamma_1$ followed by the path $\gamma_2$. Then
\begin{align}
A[\gamma_{12}] = A[\gamma_1] A[\gamma_2].
\end{align}
This implies (as we will see in the exercises) that
\begin{align}
K(q_f,t_f,q_i,t_i) = \int dy\,K(q_f, t_f,y,t_{int})K(y,t_{int},q_i,t_i).
\end{align}
\end{prop}


\begin{prop}[Classical Limit]
	When $\hbar \to 0$ and the action  $S \gg \hbar$, then the classical path becomes most important. 
\end{prop}


For most of the course, we will be consider actions that look like
\begin{align}
S = \int^{t_f}_{t_i} dt\, \f{1}{2}m\dot{p}^2(t) - V(q(t)).
\end{align}
The classical path satisfies the Euler-Lagrange equation: 
\begin{align}
\f{\delta S}{\delta q(t)} = 0.
\end{align}
This turns out to be 
\begin{align}
m\ddot{q} + V'(q) =0 
\end{align}
which is just Newton's second law of motion. \\


With these, we have 
\begin{align}
\int \mathfrak{D}q(t) e^{iS[q(t)]/\hbar}.
\end{align}
The composition property suggests that we should have an exponential, so that we can convert multiplication to summation. This says that all paths contribution with equal weight. So, the classical path is as important as the quantum path in the action. However, only paths such that $\delta S = 0$ contribute more to the propagator. \\

Consider the paths $q(t)$ and $q'(t) = q(t) + \eta (t)$ where $\eta(t)$ is some small perturbation, then 
\begin{align}
S[q'(t)] = S[q(t)] + \int dt\, \eta(t) \f{\delta S[q]}{\delta q(t)} + \mathcal{O}(\eta^2).
\end{align}

What happens in the general case is that 
\begin{align}
e^{iS[q]/\hbar} + e^{iS[q']/\hbar} \approx  e^{iS[q]/\hbar}\lp 1 + \exp\lb \f{i}{\hbar} \int dt\, \eta(t) \f{\delta S[t]}{\delta q(t)} \rb \rp
\end{align}
So, when $\hbar \to 0$, we get destructive interference. But if $\delta S/\delta q = 0$, we get constructive interference. So, even though all paths are equally important, it's also true that the classical path are ``more important'' than quantum paths, because they interference constructively. 








\newpage
\section{Propagator in real and imaginary time}


Last time, we guessed the form of the propagator. This time, we will derive in some detail the propagator in real and imaginary time. \\


What we're trying to compute is the propagator:
\begin{align}
K(q_f, t_f,q_i ,t_i) =  \int \prod_j^{N-1}   dq_j K (q_{j+1},t_{j+1}, q_j , t_j) \equiv  \int \prod_j^{N-1}   dq_j K_{q_{j+1},q_j}.
\end{align}
using the composition property that we discussed last time. Now this is starting to look like a path integral. \\

Now, we know that
\begin{align}
K_{q_{j+1},q_j} &= \bra{q_{j+1}}  e^{i\had \delta t/\hbar}  \ket{q_j}\nn\\
&\approx \braket{q_{j+1}}{q_j}   +  \bra{q_{j+1}}  \f{i\delta t \had}{\hbar} \ket{q_j} + \dots\nn\\
&= \delta(q_{j+1} - q_j) + \text{ Computed in Exercise } \nn\\
&= \int \f{dp_j}{2\pi \hbar} e^{ip_j(q_{j+1} - q_j)/\hbar} +  \text{ Computed in Exercises}
\end{align}

Putting everything together we get
\begin{align}
K_{q_{j+1},q_j} = \int \f{dp_j}{2\pi \hbar} e^{p_j(q_{j+1} - q_j)/\hbar}\lp 1 - \f{i\delta t \had}{\hbar} + \dots \rp
\end{align}
The term on the right hand side looks like a Taylor expansion for $e^{-i\delta \had t /\hbar}$. So, if we assume that $\had =  p_j^2 / 2m + V(q_j)$ then this integral becomes a Gaussian integral
\begin{align}
K_{q_{j+1},q_j} &= \int \f{dp_j}{2\pi \hbar} e^{i\delta t/\hbar (p_j\dot{q}_j  - p_j^2/2m - V(q_j) )  } \nn\\
&= \sqrt{\f{m}{2\pi i \delta t \hbar}} e^{(i\delta t /\hbar) ( mq_j^2/2 - V(q_j) )}.
\end{align}
Gaussian integrals are important in the study of path integrals, because they're among the integrals that we can evaluate exactly. \\

Staring at this for a bit, we notice that the exponential looks like part of the action. So, by multiplying together all these infinitesimal propagators, we get the entire action:
\begin{align}
K = \lp \f{m}{2\pi i \hbar \Delta t} \rp^{N/2} \int \prod^{N-1}_{j=1} dq_j \exp\lb \f{i \Delta t}{\hbar} \sum^{N-1}_{j=0} \lp \f{m\dot(q)_j^2}{2} - V(q_j) \rp \rb.
\end{align}
In the $N\to \infty$ limit, 
\begin{align}
K = \int \mathfrak{D}[q(t)] e^{iS(t)/\hbar}
\end{align}



Now we want to make the connection to imaginary time and statistical physics. Recall that
\begin{align}
K = \int \mathfrak{D}q(t) \exp\lb iS[q(t)]/\hbar \rb.
\end{align}
Now, the $i$ leads to oscillations in the integrand, which leads to questions about convergence of the integral. So, it would be much more convenient if $i$ wasn't there. This means we want to consider \textbf{imaginary time}. Why do we want to do this? Mathematically, it's generally easier. More important, if we make the exponential real, we can make connections to statistical physics. So we shall see a connection between quantum mechanics in imaginary time and statistical physics. Thirdly, in quantum mechanics we're interested in the ground states energy of Hamiltonians. Sometimes, it is possible to compute ground state energies using quantum mechanics in imaginary time. \\


To this end, consider $T = -i T_E$ where $E$ stands for Euclidean. Once we do that, we can computer the Euclidean propagator:
\begin{align}
K_E = \bra{q_f}  e^{-T_E \had /\hbar} \ket{q_i}
\end{align}
with $t \to -i \tau$. If we do that, then 
\begin{align}
iS[q(t)] = i\int^T_0 dt \lp \f{m}{2}\dot(q)^2  - V(q)\rp \mapsto -\int^{T_E}_0 d\tau \lp \f{m}{2}\dot(q)^2 + V(q) \rp
\end{align}
where the time derivative becomes derivative over $\tau$, so the relative sign between the kinetic and potential energy changes. With this, we have the Euclidean action:
\begin{align}
iS[q(t)] = -S_E[q(\tau)].
\end{align}
After this replacement, we can compute the Euclidean propagator:
\begin{align}
K_E = \int \mathfrak{D}[q(t)] e^{-S_E[q(\tau)] /\hbar}.
\end{align}

With this we can compare this to the expression for the partition function:
\begin{align}
\mathcal{Z}[\beta] = \sum_j e^{-\beta E_j}
\end{align}
by identifying $\beta = T_E/\hbar = 1/k_BT$ where now $T$ is temperature. It turns out that (which we will verify in the exercises)
\begin{align}
\mathcal{Z} = \int dq K_E(q,0,q,\beta\hbar).
\end{align}
Note that the ground state energy $E_0$ can be found this way:
\begin{align}
E_0 = \lim_{\beta \to \infty} \f{1}{\beta} \log \mathcal{Z}.
\end{align}
We can relate the propagator to the ground state energy, which is an observable of interest. \\



\subsection{Free Path Integrals}

Free path integrals are some of the path integrals we can compute exactly. These come from quadratic action $S$ in the variable $q$. For example,
\begin{align}
K_E = \bra{q_f}  e^{-\beta \had}  \ket{q_i}, \quad \had = p^2/2m.
\end{align}
This integral is easy (compared to oscillatory integrals) to compute, because the integral is just Gaussian. It turns out that
\begin{align}
K_E = \sqrt{\f{m}{2\pi \beta}} e^{-S[q_c]/\hbar} 
\end{align}
where $q_c$ is the classical trajectory. The way to derive this is via Gaussian integration techniques. \\


The key idea here is that in this case $K_E$ turns out to have the form with $N$-coupled Gaussian integrals. To understand this, we only need to understand a single integral. In any case, we can learn a lot of about path integrals by just looking at the case $N=1$. When $N=1$, we can actually can go through a lot of the details. When $N$ is larger, we don't gain much more knowledge. We will discuss, of course, what happens when $N \to \infty$. \\


Next, we will look at perturbation theory, where the action is no longer quadratic. Again, we will look at the case where $N=1$. We will also see where the path integral and perturbation theory fails. We will, of course, look at how we can address these failures. 




















\newpage

\section{Perturbation theory}


\newpage

\section{Non-perturbative physics and quantum tunneling}


\newpage


\section{Topology and path integrals}


\newpage




\section{Exercises}



\noindent \textbf{1. Composition.} 
\begin{enumerate}
	\item In lecture we argued that the propagator $K$ can be written as a sum (or an integral) over paths $\gamma$ of a functional $A[\gamma]$. 
	\begin{align}
	K = \sum_\gamma A[\gamma].
	\end{align}
	Show that the propagator obeys the composition property
	\begin{align}
	K(q_f, t_f, q_i, t_i) = \int dy\, K(q_f,t_f,y,t_{int})K(y,t_{int},q_i ,t_i)
	\end{align}
	of $A$ has the factorization property
	\begin{align}
	A[\gamma_{12}] = A[\gamma_1]A[\gamma_2].
	\end{align}
	
	
	
	\begin{proof}
		Let $A$ satisfy the factorization property. Then, 
		\begin{align}
		K = \sum_{\gamma_1,\gamma_2}A[\gamma_1]A[\gamma_2] = \int \,dy \,\sum_{\gamma_1} A[\gamma_1] \sum_{\gamma_2} A[\gamma_2]
		\end{align}
		where the path $\gamma_1$ ends at $y$ and $\gamma_2$ starts at $y$. With this, it's straightforward:
		\begin{align}
		K &= \int \,dy \bra{q_f}e^{-i\had (t_f - t_{int})/\hbar}\ket{y}\bra{y} e^{-i \had (t_{int} - t_i)/\hbar} \ket{q_i}\nn\\
		&= \bra{q_f}e^{-i\had(t_f - t_{int} + t_{int} - t_i)/\hbar}\ket{q_i}   \nn\\
		&= K(q_f,t_f,q_i,t_i).
		\end{align}
		So, we're done, because from the first line we conclude
		\begin{align}
		K(q_f, t_f, q_i, t_i) = \int dy\, K(q_f,t_f,y,t_{int})K(y,t_{int},q_i ,t_i).
		\end{align}
	\end{proof}
	
	
	
	
	\item Explain why the factorization property
	\begin{align}
	A[\gamma_{12}] = A[\gamma_1]A[\gamma_2]
	\end{align}
	holds if $A[\gamma] = e^{iS[\gamma]/\hbar}$ where $S[\gamma]$ is the classical action of the trajectory $\gamma$.
	
	\begin{proof}
		We note that $S[\gamma_{12}] = S[\gamma_1] + S[\gamma_2]$ if $\gamma_{12}$ is composed of  $\gamma_1,\gamma_2$. So this factorization property follows.
	\end{proof}
	
	
	
	\item Suppose $A[\gamma] = C_1 e^{C_2 S[\gamma]}$ for some constants $C_1,C_2$. Which values of $C_1,C_2$ give rise to a composition property? 
	
	\begin{proof}
		$C_1$ must be constantly 1, so that the composition property is satisfied. $C_2$ can be any real or complex number. 
	\end{proof}
	

\end{enumerate}




\noindent \textbf{2. Classical Limit}
\begin{enumerate}
	\item Suppose $f(x)$ is a real function. Which values of $x$ provide the most important contribution to 
	\begin{itemize}
		\item $\int e^{-f(x)}\,dx$? Why? (Optional: what conditions should be placed on $f(x)$?)
		
		
		\begin{proof}
			The most important contribution to this integral is when $f$ is small, so $x$'s where $f(x)$ is small. 
		\end{proof}
	
	
		\item $\int e^{if(x)}\,dx$? Why? (Optional: what conditions should be placed on $f(x)$?)
		
		
		\begin{proof}
			The most important contribution to this integral is when $f$ changes slowly, so most comes from $x$ where $f'(x)$ small. 
		\end{proof}
	\end{itemize}



	\item The classical equation of motion for a particle with classical action $S$ is that the first-order variation vanishes:
	\begin{align}
	\delta S = 0.
	\end{align}
	A trajectory with $\delta S = 0$ is a classical trajectory. No extra conditions on the second-order variation of the action is required. Why does $A[\gamma] = e^{iS[\gamma]/\hbar}$ give the correct classical limit? 
	
	\begin{proof}
		Recall that $\delta S = 0$. So this sort of shows why $A[\gamma] = e^{iS[\gamma]/\hbar}$ works. We won't worry too much about the details here.
		
	\end{proof}
	
	\item What other choices $A[\gamma]$ give rise to the correct classical limit? Do they have a composition property? 
	
	\item Do any of the other possibilities you identified in 1(c) have the correct classical limit? 
\end{enumerate}





\noindent \textbf{3. First-Order Term.} By inserting the resolution of identity
\begin{align}
\bm{1} = \int\f{dp}{2\pi \hbar} \ket{p}\bra{p}
\end{align}
and using
\begin{align}
\braket{q}{p} = e^{ipq/\hbar}
\end{align}
show that the first-order term can be expressed as
\begin{align}
-i\f{\Delta t}{\hbar} \bra{q_{j+1}} \had\ket{q_j} = -i \f{\Delta t}{ \hbar} \int \f{dp_j}{2\pi \hbar} e^{ip_j(q_{j+1} - q_j)/\hbar} \lp \f{p_j^2}{2m} + V(q_j) \rp.
\end{align}
What changes at second order?


\begin{proof}
	This looks hard, but isn't hard at all. I won't show the full solution here, but I'll just sketch things out here. By inserting the resolution of identity we get
	\begin{align}
	&\,\,\,\bra{q_{j+1}} \int \f{d_i}{2\pi \hbar} \lp \f{p_i^2}{2m} + V(q_i) \ket{p_i} \bra{p_i} \rp \ket{q_j} \nn\\
	&= \int\f{dp_i}{2\pi \hbar} e^{ip_j(q_{j+1} - q_j)/\hbar}\lp \f{p_i^2}{2m} + V(q_i) \rp.
	\end{align}
	With this we're essentially done. 
\end{proof}




\noindent \textbf{4. Statistical Physics and Imaginary Time.} Show that the partition function 
\begin{align}
\mathcal{Z}[\beta] = \sum_j e^{-\beta E_j}
\end{align}
can be expressed as a sum over imaginary time propagators
\begin{align}
\mathcal{Z}[\beta] = \int dq\, K_E (q,\beta \hbar, q,0).
\end{align}


\begin{proof}
	Again, the resolution of identity is helpful here. Consider the energy eigenbasis, then we have
	\begin{align}
	\bm{1} = \sum_j  \ket{e_i}\bra{e_i}.
	\end{align}
	With this, and by identifying $-T_E\had/\hbar \equiv \beta\had$ we have
	\begin{align}
	\int dq K_E(q,0,q,\beta\had) &= \int dq \bra{q} e^{\beta \had} \ket{q} \nn\\
	&= \sum_i \int dq \bra{q}\ket{e_i} e^{\beta \had} \ket{q}\bra{e_i} \nn\\
	&= \int dq \sum_i \braket{e_i}{q} \bra{q}e^{-\beta\had}\ket{e_i}\nn\\
	&= \sum_i \bra{e_i} e^{-\beta \had} \ket{e_i}\nn\\
	&= \tr\lp e^{-\beta\had} \rp\nn\\
	&= \sum_n e^{-\beta E_i}.
	\end{align}
	where we have used the completion relation in the position space: 
	$$\int dq \ket{q}\bra{q} = \bm{1}.$$
	Of course, we can also go the other way from $\mathcal{Z}$ and use the completion relation in the energy eigenbasis. Either way, the solutions are equivalent.  
\end{proof}




\noindent \textbf{5. Path Integral Derivation of the Propagator.} In the path integral formalism the propagator in imaginary time is 
\begin{align}
K_E(q,\beta \hbar, q,0) = \lim_{N\to \infty} \lp \f{m}{2\pi \Delta \tau \hbar} \rp^{N/2}\int dq_1\dots dq_{N-1}\exp\lb -\tau \hbar \sum^N_{j=1} \f{m}{2} \lp \f{q_j - q_{j-1}}{\Delta \tau \hbar} \rp^2 \rb
\end{align}

\begin{enumerate}
	\item Show that the propagator can be rewritten as
	\begin{align}
	K_E(q,\beta\hbar, q,0) = &\lim_{N\to \infty} \lp \f{m}{2\pi \Delta \tau \hbar} \rp^{N/2} \lp \f{2\Delta \tau \hbar}{m } \rp^{\f{N-1}{2}}\nn\\&\times  
	\int dy_1\dots dy_{N-1}\exp\lb -\sum^N_{j=1}(y_j - y_{j-1})^2 \rb
	\end{align}
	
	\begin{proof}
		This is just change of variables, so I won't do that here. 
	\end{proof}
	
	
	\item Use induction to show that 
	\begin{align}
	\int dy_1,\dots dy_{N-1}\exp\lb -\sum^N_{j=1}(y_j - y_{j-1})^2 \rb = \lp \f{\pi^{N-1}}{N} \rp^{1/2} e^{-(y_N - y_0)^2/N}.
	\end{align}
	
	\begin{proof}
		Just use induction and crank. At one point we will have to evaluate a 1-dimensional Gaussian integral, but this shouldn't be a challenge. This one-dimensional Gaussian integral will introduce some factor $\pi^\al$ with other constants such that we get to the right hand side at iteration $N-1+1 = N$. In any case, this problem also looks hard but is quite easy. 
	\end{proof}
	
	
	\item Take the continuum limit $N\to \infty$ to recover the result from lecture:
	\begin{align}
	K_E(q,\beta \hbar, q,0) = \sqrt{\f{m}{2\pi \beta }} \exp\lb -\f{m(q_f - q_i)^2}{2\beta} \rb.
	\end{align}
	\begin{proof}
		Combining things in the previous parts, we get that in the $N\to \infty$ limit, we have
		\begin{align}
		K_E &= \lim_{N\to \infty}  \lp \f{m}{2\pi \Delta \tau \hbar} \rp^{N/2} \lp \f{2\Delta \tau \hbar}{m } \rp^{\f{N-1}{2}}\lp \f{\pi^{N-1}}{N} \rp^{1/2} e^{-(y_N - y_0)^2/N}\nn\\
		&= \sqrt{\f{m}{\beta\hbar}}
		\end{align}
		
		
		hmmm... There might be something wrong here... or might not. In any case, this is just a case of taking the limit correctly and using some change of variables. I won't go into the details.
	\end{proof}
	
\end{enumerate}





\newpage









































\chapter{Symmetries}

\textbf{Instructor:} Giuseppe Sellaroli\\

The aim of this course is to  explore some of the many ways in which symmetries play a role in physics. We'll start with an overview of the concept of symmetries and their description in the language of  group theory. We will then discuss continuous symmetries and infinitesimal symmetries, their fundamental role in Noether's theorem, and their formalization in terms of Lie groups and Lie algebras. In the last part of the course we will focus on symmetries in quantum theory and introduce representations of (Lie) groups and Lie algebras.


\newpage


\section{Review: Equivalence Classes}


\subsection{Equivalence relations}


An equivalence relation is a special kind of binary relation. A
(homogeneous) binary relation on a set $X$ is a way of encoding a relationship between two elements of $X$. Given a relation $R$ we use the notation $xRy$ to mean ``$x$ is related to $y$.'' \\

Examples of relations are $=,<,>, \leq, \geq$ on $\mathbb{R}$. \\

\begin{defn}[Homogeneous binary relation]
	A homogeneous binary relation on a set $X$ is a set $R \subset X \times X$ of pairs of
	elements of $X$. When $(x,y) \in R$ we say that $x$ is relate to $y$, which we also indicate with the less cumbersome notation $xRy$.  
\end{defn}

We can now define equivalence relations.



\begin{defn}[Equivalence relation]
	An equivalence relation on a set $X$ is homogeneous binary relation $R$ that satisfies the following properties:
	\begin{itemize}
		\item Reflexive: $xRx \forall x\in X$.
		\item Symmetric: $xRy \implies yRx$.
		\item Transitive: $xRy, yRz \implies xRz$
	\end{itemize}
	These properties are chosen as they encode the fundamental features of the notion of
	things being ``equivalent.''
\end{defn}

Equivalence relations are often denoted by the symbol $\sim$. In this case we read $x\sim y$ as ``$x$ is equivalent to $y$.'' 





\subsection{Equivalence Classes}

Now that we know what an equivalence relation is, we can define the extremely useful concept of equivalence class.


\begin{defn}
	Let $\sim$ be an equivalence relation on $X$. The equivalance class of $x \in X$ is the set 
	\begin{align}
	[x]_\sim = \{ y\in X \vert y\sim x   \}
	\end{align}
	of all the elements in $X$ equivalent to $x$. We often drop the subscript and just write $[x]$ if there is no risk of confusion. Note that
	\begin{align}
	[x]_\sim = [y]_\sim, \quad \forall y \in [x]_\sim,
	\end{align}
	that is we can label the equivalence class using any of its elements. The element that we
	choose as the label is called the representative of the equivalence class.
\end{defn}



Equivalence classes are essentially buckets in which we put elements of X that are equivalent
to each other. Inequivalent elements end up in different equivalence classes. One way of
thinking about this is that an equivalence class is a way to encode as a single object the
properties common to its elements, which are otherwise indistinguishable.


\begin{exmp}
	An example of equivalence relation appears in modular arithmetic. Let's consider the simplest case of the integers modulo 2. We define an equivalence relation $\sim$ on $\mathbb{Z}$ by
	\begin{align}
	x\ sim y \implies x-y = 2k, k\in \mathbb{Z},
	\end{align}
	that is $x,y$ are equivalent if they differ by a multiple of two. If you want, you can check that this is indeed an equivalence relation as an exercise. There are only two distinct equivalence classes, which are the respectively the sets of even and odd integers. The sets of integers modulo 2 is
	defined as

	\begin{align}
	\mathbb{Z}_2 = \{ [0], [1]  \}.
	\end{align}
	Although this is a set of sets, we really want to pretend that it's a set of numbers. To do
	that, we define a way to add elements in $\mathbb{Z}_2$. The obvious choice is
	\begin{align}
	[a] + [b] = [a+b],
	\end{align}
	but at this stage this expression is purely formal: since we are specifying this operation
	through the sum of the representatives of each class, we need to make sure that the definition
	is actually independent of the choice of representatives (since they should be equivalent).
	This happens all the time when dealing with equivalence classes! In this case we are fine, because of the rules of adding odd and even numbers. 
\end{exmp}



\begin{exmp}
	Here’s an example from quantum physics. Let $\had$ be the Hilbert space of a quantum theory. While extremely useful, the states $\ket{\psi} \in \had$ are not \textit{physical}, where physical is meant as something that can be measured. The physical quantities in a quantum theory are the expectation values of the observables in a given state, that is 
	\begin{align}
	\langle A \rangle_\psi = \f{\bra{\psi}A\ket{\psi}}{\braket{\psi}}.
	\end{align}
	Now consider another state $\ket{\phi} = \lambda{\psi}$, with $\lambda \in \mathbb{C}\setminus \{0\}$. For any observable $A$ we have
	\begin{align}
	\langle A \rangle_\phi = \f{\abs{\lambda}^2}{\abs{\lambda}^2}\f{\bra{\psi}A\ket{\psi}}{\braket{\psi}} = \langle A \rangle_\psi.
	\end{align}
	This means that the states $\ket{\psi}$ and $\ket{\phi}$ are physically indistinguishable. It is then natural to consider them equivalent, and define the equivalence relation 
	\begin{align}
	\ket{\psi} \sim \ket{\phi} \iff \ket{\phi} = \lambda\ket{\psi}, \quad \lambda \in \mathbb{C}\setminus \{0\}.
	\end{align}
	
	The set 
	\begin{align}
	P(\had) = \{ [\ket{\psi}] \vert \ket{\psi} \in \had\setminus\{0   \}
	\end{align}
	is called th \textit{projective Hilbert space} associated to $\had$, and can be interpreted as the set of all distinguishable quantum states. The zero vector was removed as it does not describe any physical state (expectation values don't make sense for the zero vector). \\
	
	In a sense $P(\had)$ is the true set of states of a quantum theory. The reason why we work with $\had$ instead is that the latter is a vector space, which comes with a ot of nice properties and allows for an easy description of the superposition principle. 
	
	
	
\end{exmp}




\newpage



\section{Definition of symmetry}

We can't really give a mathematical definition of symmetry, because it depends on the context. But in any case we can define ``symmetry'' like this.

\begin{defn}[Symmetries]
	Loosely speaking, a symmetry is a transformation of $A$ that leaves $B$ invariant. 
\end{defn}

The idea is that $A$ and $B$ are not necessarily the same thing. 

\begin{exmp}
	If $B$ is the golden spiral and $A = \mathbb{R}^2$. When $A$ is scaled, $B$ remains invariant. 
\end{exmp} 

\begin{exmp}
	Consider a Lagrangian,
	\begin{align}
	S = \int d^4x\,\lag
	\end{align}
	and $B = S$  and $A$ is time, which we translate. We can see that $S$ is invariant under time translation. 
\end{exmp}


Some properties:
\begin{itemize}
	\item (Identity) ``Doing nothing'' is a symmetry. It is trivial, but is a symmetry nonetheless. This is also called the ``identity'' transformation.
	
	\item (Inverses) Symmetries are invertible. 
	
	\item (Composition) Compositions of symmetries are also symmetries. 
\end{itemize}



\section{Elements of group theory}


\begin{defn}
	A group is a set $G$ together with an operation $* : G\times G \to G$ satisfying the following properties:
	\begin{itemize}
		\item There is an identity element: $g*e = e*g = g, \forall g\in G$
		
		\item Each element of $G$ has an inverse, i.e., $\forall g\in G, \exists g^{-1}\in G$ s.t. $g^{-1}* g = g * g^{-1} = e \in G$.
		
		
		\item The operation $*$ is associative, that is  $a*(b*c) = (a*b)*c, \forall a,b,c\in G$.
	\end{itemize}

	Additionally, we say that the group $G$ is abelian/commutative if $a*b = b*a, \forall a,b\in G$. 
\end{defn}



\begin{defn}[Subgroups]
	Let $G$ be a group with operation $*$. A subgroup of  $G$ is a subset $H \subset G$ that contains the identity element and is closed under the operation $*$ and under inversion, that is 
	\begin{itemize}
		\item $e\in H$
		\item $a*b \in H \,\forall a,b\in H$
		\item $a\in H \implies a^{-1}\in H$.
	\end{itemize}
	The notation $H \leq G$ is commonly used to indicate that $H$ is a subgroup of $G$. 
\end{defn}


\begin{defn}[Group homomorphism]
	A group homomorphism is a map $\varphi: G \to H$ between two groups $G$ and $H$ such that 
	\begin{align}
	\varphi(a*_G b) = \varphi(a)*_H \varphi(b), \forall a,b \in G.
	\end{align}
\end{defn}





\begin{defn}[Kernel of a group homomorphism]
	The kernel of a group homomorphism $\varphi: G\to H$ is the set 
	\begin{align}
	\ker\varphi = \{ g\in G \vert \varphi(g) = e_H   \}
	\end{align}
	of all the elements of $G$ that are sent to the identity in $H$. 
\end{defn}

\begin{prop}
	A group homomorphism $\varphi: G\to H$ is injective if and only if the kernel is trivial: $\ker \varphi = \{ e_G\}$.
\end{prop}

\begin{proof}
	$\,$\\
	\begin{itemize}
	\item $(\rightarrow)$, if $\varphi$ is injective, then at most one thing can be sent to $e_H$. This means that $\ker\varphi = \{ e_G \}$.
	
	\item $(\leftarrow)$ Let $a,b\in G$ such that $\varphi(a) = \varphi(b) \implies e_H = \varphi(a)^{-1}\varphi(b) = \varphi(a^{-1})\varphi(b) = \varphi(a^{-1}b) $. But this means $a^{-1}b \in \ker{\varphi} \implies a^{-1}b = e_G \implies a=b$. So $\varphi$ is injective. 
	
	\end{itemize}
\end{proof}





\begin{defn}[Isomorphisms]
	Two groups $G$ and $H$ are isomorphic (denoted by by $G\sim H$) if there exists an invertible group homomorphism $\varphi: G \to H$. Such a map is called an isomorphism between $G$ and $H$. 
\end{defn}


\begin{defn}[Normal Subgroup]
	Let $G$ be a group. A subgroup $N\leq G$ is called \textit{normal} if 
	\begin{align}
	g n g^{-1} \in N, \forall g\in G, \forall n \in N.
	\end{align}
	The notation $N \trianglelefteq G$ is commonly used to indicated that $N$ is a normal subgroup of $G$. 
\end{defn}


\begin{defn}[Quotient Group]
	Let $N \trianglelefteq G$. We can define an equivalence relation on $G$ as
	\begin{align}
	g \sim h \iff h^{-1}g \in N
	\end{align} 
	with equivalence classes
	\begin{align}
	[g] = \{  h\in G \vert h^{-1}g \in N  \}.
	\end{align}
	The quotient group $G/N \equiv G \mod N$ is the set of equivalence classes
	\begin{align}
	G/N = \{ [g] \vert g\in G  \}
	\end{align}
	which is made into a group by defining
	\begin{align}
	[g][h] = [gh], \quad [g]^{-1} = [g^{-1}], \quad e_{G/N} = [e_G].
	\end{align}
\end{defn}


We note that the equivalence class does not require $N$ be normal. However, to turn the equivalence classes into a group, we require that $N$ is normal. 







\begin{thm}[First Isomorphism Theorem]
	Let $\varphi: G \to H$ be a group homomorphism. Then:
	\begin{itemize}
		\item $\Im \varphi$ is a subgroup of $H$.
		\item $\ker\varphi$ is a normal subgroup of $G$.
		\item $\Im \varphi$ is isomorphism to the quotient group $G / \ker\varphi$.
	\end{itemize}
\end{thm}


The notion that $\Im \varphi \sim G/\ker\varphi$ is important. This will appear again when we see Lie groups. 






\section{Examples: Symmetry \& Group Theory }

\begin{exmp}
	$(\mathbb{Z},+), (\mathbb{R},+), (\mathbb{C},+), \dots$ are abelian groups. 
\end{exmp}

\begin{exmp}
	$(\mathbb{R} \setminus \{0\}, \times)$ 
\end{exmp}

\begin{exmp}
	Invertible $n\times n$ matrices, $GL(n,\mathbb{R})$ under usual matrix multiplication. 
\end{exmp}

\begin{exmp}
	Circle group $S = \{(x,y) \in \mathbb{R}^2  \vert x^2 + y^2 = 1    \}$. Identity $(x,y) \in S$ with a complex number $z = x+iy$. Then use complex number multiplication. This works because $\abs{z} = 1 \implies z= e^{i\theta}$.
\end{exmp}




\begin{exmp}[Isomorphism]
	Consider the two groups $(\mathbb{R},+)$ and $(\mathbb{R}^+\setminus\{0\}, \times)$. The exponential map: $x\in R \mapsto e^x$. This map is obviously invertible. It is also a group homomorphism because it relates multiplication in the target group to addition in the reals. So, the two groups are isomorphic. \\
	
	This is particularly useful when we want to move from additive groups to multiplicative groups. \\
	
	This map is related to Lie groups, etc which we will see later.  
\end{exmp}









\newpage



\section{Continuous and discrete symmetries}

We will be quite vague with these definitions. 



\subsection{Discrete Symmetries}


Think of rotating polygons. Rotations that leave the polygon invariant are in integer steps. This is a prototype example of what a discrete symmetry is. 

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.5]{polys}
\end{figure}

We can also easily define equivalence classes etc and see how the first isomorphism comes up, but we won't worry about that. \\

The idea of discrete symmetries is this: there is a discrete parameter for the group elements. 






\subsection{Continuous Symmetries}

In view of the previous example, we can consider rotating a circle. 
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.3]{circ}
\end{figure}


Obviously we can rotate by any angle and keep the circle invariant. \\


The idea of continuous symmetries is this: there is a continuous parameter for the group elements. Consider $\epsilon \in \mathbb{R}^+$ and take $\epsilon \mapsto \psi(\epsilon) = g_\epsilon \in G$. $\psi$ is a group homomorphism. Now, if there is a \textbf{topology} on $G$ such that $\psi$ is continuous, then we say $\{ g_\epsilon \vert \epsilon \in \mathbb{R}^+  \} = \Im(\psi)$ is continuous. This is quite vague, but is essentially the idea. \\


The group $\{ g_\epsilon \vert \epsilon \in \mathbb{R}^+   \}$ is called a \textbf{one-parameter subgroup}. One-parameter subgroups satisfy the following properties:
\begin{itemize}
	\item $g_0 = e_G$
	\item $g_{-\epsilon} = g_\epsilon^{-1}$
	\item $g_{\epsilon + \epsilon'} = g_\epsilon g_{\epsilon'}$. This means the the one-parameter subgroup is abelian.
\end{itemize}

For example consider $g_\theta$ define by $2\times 2$ rotation matrix $R_\theta$. We can check that $g_\theta$ satisfies all of the properties above.  


























\section{Infinitesimal symmetries}

Rotations are a particular example of infinitesimal symmetry. Just like calculus, we want to talk about ``derivatives.'' Assume that that $\epsilon \in \mathbb{R} \mapsto g_\epsilon \in G$ is a ``smooth'' map. Here, ``smooth'' means infinitely differentiable, or ``as nice as we want.'' This only makes sense if we have a ``differentiable structure'' on $G$. \\

Now, if the map above is smooth, we can linearize it. (Now we're doing things in the physicist's way). To linearize:
\begin{align}
g_\epsilon = g_0 + \epsilon \f{d}{d\epsilon}g_\epsilon\bigg\vert_{\epsilon = 0} + \mathcal{O}(\epsilon^2).
\end{align}
The infinitesimal symmetry is then 
\begin{align}
g_\epsilon = g_0 + \epsilon \f{d}{d\epsilon}g_\epsilon\bigg\vert_{\epsilon = 0}.
\end{align}
where $g_0$ is the identity.\\

More formally, infinitesimal symmetry has the form $e + \epsilon X$ where 
\begin{align}
X = \f{d}{d\epsilon}\bigg\vert_{\epsilon = 0} g_\epsilon.
\end{align}


\begin{exmp}
	Consider 
	\begin{align}
	R_\theta = \begin{bmatrix}
	\cos\theta & \sin\theta \\ -\sin\theta & \cos\theta
	\end{bmatrix}
	\end{align}
	With this,
	\begin{align}
	\f{d}{d\theta}\bigg\vert_{\theta = 0} R_\theta = \begin{bmatrix}
	0 & 1 \\ -1 & 0
	\end{bmatrix}
	\end{align}
	This means that the Taylor expansion of $R_\theta$ around 0 is given by
	\begin{align}
	R_\theta = \begin{bmatrix}
	1 & 0 \\ 0 & 1 
	\end{bmatrix} + \theta \begin{bmatrix}
	0 & 1 \\ -1 & 0
	\end{bmatrix} + \mathcal{O}(\theta^2)
	\end{align}
	The infinitesimal rotation is thus 
	\begin{align}
	\begin{bmatrix}
	1 & 0 \\ 0 & 1 
	\end{bmatrix} + \theta \begin{bmatrix}
	0 & 1 \\ -1 & 0
	\end{bmatrix}
	\end{align}
	Note that because of the group structure, we can actually reconstruct $R_\theta$ from the linearization, unlike in calculus where we don't necessarily have the function back knowing its derivative.  
 	
\end{exmp}



























\section{Noether's theorem}

\begin{defn}[Noether's Theorem]
	Every differentiable symmetry of the action of a physical system has a corresponding conservation law. 
\end{defn}

\begin{proof}
	Consider $\tilde{t} = T_\epsilon(t)$ and $\tilde{q}(\tilde{t}) = Q_\epsilon(q(t)) = Q_\epsilon (q(T_{-\epsilon} (\tilde{t})))$ with $T_\epsilon, Q_\epsilon$ smooth one-parameter subgroups $(T_0 = \Id, T_{-\epsilon} = T_\epsilon^{-1}, T_{\epsilon+ \epsilon'} = T_\epsilon \circ T^{-1}_\epsilon)$, i.e., $(d/d\epsilon)_{\epsilon=0} T_\epsilon$ makes sense. Consider the action
	\begin{align}
	S[q] = \int^b_a \lag(q(t), \dot{q}(t),t).
	\end{align}
	Under the transformation, we get
	\begin{align}
	\tilde{S}[\tilde{q}] = \int^{T_\epsilon(b)}_{T_\epsilon(a)} \lag(\td{q}(\td{t}), \dot{\td{q}}(\td{t}) , \td{t})\,d\td{t} = \int^b_a \f{dT_\epsilon}{dt}\lag(\td{q}(T_\epsilon(t)) , \dot{\td{q}}(T_\epsilon(t)), T_\epsilon(t) )\,dt.
	\end{align}
	Suppose that $\tilde{S}[\td{q}] = S[q]$ for all paths (even those that are not solutions of the Euler-Lagrange equations) and for all choices of $a,b$. Then 
	\begin{align}
	\delta S \equiv \f{d}{d\epsilon}\bigg\vert_{\epsilon = 0}\td{S}[\td{q}] = 0.
	\end{align}
	Some notation:
	\begin{align}
	&\delta t = \f{d}{d\epsilon}\bigg\vert_{\epsilon=0}T_\epsilon\nn\\
	&\delta q = \f{d}{d\epsilon}\bigg\vert_{\epsilon=0}Q_\epsilon
	\end{align}
	These are generators of $T_\epsilon$ and $Q_\epsilon$, respectively (see Lie algebras).\\
	
	Let's apply $D \coloneq (d/d\epsilon)\big\vert_{\epsilon=0}$ everywhere we can, then 
	\begin{align}
	&D\f{dT_\epsilon}{dt} = \f{d}{dt}DT_\epsilon = \f{d}{dt}\delta t\nn\\
	&D\td{q}(\td{t}) = DQ_\epsilon (q(t)) = \delta q(q(t))\nn\\
	&\f{d\td{q}}{d\td{t}}(\td{t}) = \f{d}{d\td{t}}Q_\epsilon(q(t)) = \f{\p Q_\epsilon}{\p q}(q(t))\dot{q}(t) \f{dt}{d\td{t}}(\td{t})\nn\\
	&\implies \f{d}{d\ep}\bigg\vert_{\ep=0} \dot{\td{q}}(\td{t}) = \f{\p}{\p q}DQ_\epsilon (q(t)) \dot{q(t)}\underbrace{\f{1}{\f{dT_0}{dt}}}_{1} + \f{\p}{\p q}Q_0(q(t))\dot{q}(t)\underbrace{D\f{1}{\f{dT_\epsilon}{dt}}}_{-(d/dt)\delta t}\nn\\
	&\implies D\dot{\td{q}}(\td{t}) = \f{\p}{\p q}\delta q \dot{q}(t) - \dot{q}(t) \f{d}{dt}\delta t = \f{d}{dt}\delta q - \dot{q}(t)\f{d}{dt}\delta t.
	\end{align}
	Now suppose that $q$ satisfies the Euler-Lagrange equation:
	\begin{align}
	\f{\p \lag}{\p q} - \f{d}{dt}\f{\p \lag}{\p \dot{q}} = 0.
	\end{align}
	Then because
	\begin{align}
	D\td{S}[\td{q}] = 0, \quad \td{S}[\td{q}] = S[q]
	\end{align}
	which we should check without using the Euler-Lagrange equations, we have that	
	\begin{align}
	0 = D\td{S}[\td{q}] &= \int^b_a D\lb \f{dT_\ep}{dt}\lag(\td{q}(\td{t}),\dot{\td{q}}(\td{t}) , \td{t}) \rb\nn\\
	&= \int^b_a dt\lc \f{d\delta t}{dt} \lag(q,\dot{q},t) + \underbrace{\f{dT_0}{dt}}_{1}D\lag(\td{q}(\td{t}),\dot{\td{q}}(\td{t}) , \td{t}) \rc.
	\end{align}
	Now, what is the term on the RHS of the integral? Well,
	\begin{align}
	D\lag(\td{q}(\td{t}),\dot{\td{q}}(\td{t}) , \td{t})
	&= \f{\p \lag}{\p t}D\td{t} + \f{\p \lag}{\p q}D\td{q}(\td{t}) + \f{\p \lag}{\p \dot{q}} D\dot{\td{q}}(\dot{\td{t}})\nn\\
	&= \f{\p \lag}{\p t}\delta t + \f{\p \lag}{\p q}\delta q + \f{\p \lag}{\p \dot{q}}\lp \f{d}{dt}\delta q - \dot{q} \f{d}{dt}\delta t \rp\nn\\
	&= \f{\p \lag}{\p t}\delta t + \f{\p \lag}{\p q}\delta q + \f{d}{dt}\lp \f{\p \lag}{\p \dot{q}}\delta q \rp - \delta q\f{d}{dt}\f{\p \lag}{\p \dot{q}} - \f{d}{dt}\lp \f{\p \lag}{\p \dot{q}}\dot{q}\delta t \rp + \delta t\f{d}{dt}\lp  \f{\p \lag}{\p \dot{q}}\dot{q} \rp\nn\\
	&= \f{d}{dt}\lb \f{\p \lag}{\p \dot{q}} (\delta q - \dot{q}\delta t) \rb + \f{\p \lag}{\p t}\delta t + \delta t \lp \f{d}{dt}\f{\p \lag}{\p \dot{q}} \rp\dot{q} + \delta t \f{\p \lag}{\p \dot{q}}\ddot{q}\nn\\
	&= \f{d}{dt}\lb \f{\p \lag}{\p \dot{q}} (\delta q - \dot{q}\delta t) \rb + \delta t\lb \f{\p \lag}{\p t} + \f{\p \lag}{\p q}\dot{q} + \f{\p \lag}{\p \dot{q}} \ddot{q} \rb\nn\\
	&=  \f{d}{dt}\lb \f{\p \lag}{\p \dot{q}} (\delta q - \dot{q}\delta t) \rb + \delta t \f{d}{dt}\lag(q(t),\dot{q}(t),t).
	\end{align}
	From here we have that
	\begin{align}
	0 &= \int^b_a dt\lc \lag(q,\dot{q},t)\f{d}{dt}\delta t + \delta t \f{d}{dt}\lag(q,\dot{q},t) + \f{d}{dt}\lb \f{\lag \lag}{\p \dot{q}}(\delta q - \dot{q}\delta t) \rb \rc\nn\\
	&= \int^b_a dt \f{d}{dt}\lb \lag \delta t + \f{\p \lag}{\p \dot{q}}\lb \rb \delta q - \dot{q}\delta t  \rb\nn\\
	&\implies \boxed{\lag \delta t + \f{\p \lag}{\p \dot{q}}(\delta q - \dot{q}\delta t) \text{ is conserved!}}
	\end{align}
	
\end{proof}








































\newpage
\section{Lie groups and Lie algebras}




\subsection{Matrix Exponential}


Let's review of facts about matrix exponential:
\begin{itemize}
	\item The matrix exponential is the map $\exp: M_n(\C) \to GL(n,\C)$ defined by $X \mapsto \sum^\infty_{k=0} X^k/k!$. 
	
	\item The map $\exp$ is surjective.
	
	\item $e^{AXA^{-1}} = A e^X A^{-1}$, for $A\in GL(n,\C)$.
	
	\item $\det(e^E) = e^{\tr X}$.
	
	\item $e^O = \Id$.
	
	\item $(e^X)^* = e^{X^*}$.
	
	\item $(e^X)^\top = e^{X^\top}$
	
	
	\item $(e^X)^{-1} = e^{-X}$. 
	
	\item $e^{(\al+\be )X} = e^{\al X} e^{\be X}$.
	
	\item If $XY=YX$ then $e^{X+Y} = e^X e^Y = e^Y e^X$.
	
	\item In general, we have the Lie product formula:  $e^{(X+Y)} = \lim_{k\to \infty}\lp e^{ X/k}e^{ Y/k} \rp^{k}$.
	
	
	\item $\f{d}{d\ep}e^{\ep X} = X e^{\ep X} = e^{\ep X} X, \ep \in \R$.
	
	\item $\f{d}{d\ep}\bigg\vert_{\ep =0}e^{\ep X} = X$.
	
	\item $e^{\ep X} = e^{\ep Y} \forall \ep \in \R \iff X=Y$.
\end{itemize}













\subsection{Matrix Lie groups and associated Lie algebras}

\begin{defn}[Matrix Lie Groups]
	A matrix Lie group is a closed subgroup  $G \leq GL(n,\mathbb{C})$ for some $n\in \mathbb{N}$ (closed wrt the topology induced from $M_n(\mathbb{C})$) using the  operator norm. 
\end{defn}

Recall the $GL(n,\C)$ is the group of  $n\times n$ invertible matrices over $\C$. Let's look at some examples:

\begin{exmp}
	$\,$
	\begin{itemize}
		\item $GL(n,\mathbb{C})$, the general linear group over $\mathbb{C}$.
		
		\item $SL(n,\mathbb{C}) = \{ A \in GL(n,\mathbb{C}) \vert \det{A} = 1 \}$, special linear group over $\C$. 
		
		\item $GL(n,\R) = \{ A\in GL(n,\C) \vert \bar{A} - A = 0 \}$, general linear group over $\R$. 
		
		\item $SL(n,\mathbb{R}) = \{ A \in GL(n,\mathbb{R}) \vert \det{A} = 1 \}$, special linear group over $\R$. 
		
		\item $O(n) = \{ A\in GL(n,\R) \vert A^\top A = \Id \}$, the orthogonal group.
		
		\item $SO(n) = \{ A\in O(n) \vert \det{A} = 1 \}$, the special orthogonal group.
		
		\item $U(n) = \{ A\in GL(n,\C) \vert A^\dagger A = \Id\}$, the unitary group.
		
		\item $SU(n) = \{ A \in U(n)\vert  \det{A} = 1   \}$, the special unitary group.  
	\end{itemize}

	Note that the last two groups are real Lie groups despite having complex matrices. 
\end{exmp}




\begin{defn}[Lie Algebra]
	A real or complex Lie algebra is a vector space $V$ over $\R$ or $\C$ with an operation $[\cdot,\cdot]: V\times V \to V$ (Lie bracket) satisfying
	\begin{itemize}
		\item $[\al x + \be y, z] = \al[x,z] + \be[y,z]$.
		\item $[z,\al x + \be y] = \al[z,x] + \be[z,y]$.
		\item $[y,x] = -[x,y]$.
		\item $[x,[y,z]] + [y,[z,x]] + [z,[x,y]] = 0$.
	\end{itemize}
\end{defn}




\begin{defn}[Associated Lie algebra of MLG]
	Let $G \leq GL(n,\C)$ be a matrix Lie group. The associated Lie algebra is the set $\text{Lie}(G) = \{  x\in M_n(\C) \vert e^{\epsilon X} \in G \forall \epsilon \in \R   \}$ with Lie bracket $[x,y] = xy-yx$.
\end{defn}

\textbf{Note:} $\text{Lie}(G)$ is usually denoted $\mathfrak{g}$.\\

We need to prove that $\text{Lie}(G)$ is really a Lie algebra:
\begin{proof}
	We need to show Lie$(G)$ satisfies all the defining characteristics of a Lie algebra:
	\begin{itemize}
		\item $[\cdot,\cdot]$ is anti-commutative, bilinear, and satisfies Jacobi identity: this is not hard to show.
		
		\item The zero matrix belongs to Lie$(G)$ since $e^{\epsilon O} = \Id$, for all $\ep\in \R$. 
		
		\item If $X \in \text{Lie}(G)$ and $\al\i \R$ then $e^{\ep \al X} \in G,\\forall \ep \in \R$ since $\ep\al \in \R$. This means $al X \in \text{Lie}(G)$.
		
		\item If $X,Y \in \text{Lie}(G)$ then $e^{\ep(X+Y)} = \lim_{k\to \infty}\lp e^{\ep X/k}e^{\ep Y/k} \rp^{k}$ which is a limit of products of elements of $G$. Since $G$ is closed, the limit converges to something in $G$. This means $X+Y\in \text{Lie}(G)$.
		
		\item This means that $\text{Lie}(G)$ is a subspace of $M_n(\C)$ (as a real vector space). Thus Lie$(G)$ is a Lie algebra. 
		
	\end{itemize}
\end{proof} 












\newpage
\section{Symmetries in quantum mechanics}



\newpage
\section{Representation theory}













\newpage
\section{Exercises}
\noindent \textbf{1.} Prove the following consequences of the definition of a group:
\begin{enumerate}
	\item the identity element is unique (if two elements satisfy the identity property, they are
	necessarily equal).
	
	\begin{proof}
		If $e,e'$ are identities of $G$, then $e = e'e = e'e = e'$. 
	\end{proof}
	
	\item For each $g \in G$ the inverse $g^{-1}$ is unique.
	
	\begin{proof}
		Multiplying both sides on the right by any inverse to to get $g^{-1}*g = g'^{-1}*g = e \implies g^{-1} = g'^{-1}$.
	\end{proof}
	
	\item $(g^{-1})^{-1} = g$.
	
	\begin{proof}
		$g^{-1}* (g^{-1})^{-1} = e = g^{-1}*g \implies g=(g^{-1})^{-1}$, by multiplying by $g^{-1}$ on the right on both sides. 
	\end{proof}
	
	\item $(gh)^{-1} = h^{-1}g^{-1}$.
	
	\begin{proof}
		Multiplying both sides by $gh$ we get $(gh)(gh)^{-1} = e = ghh^{-1}g^{-1}$, so we're done, by the previous items.
	\end{proof}


\end{enumerate}


\noindent \textbf{2.} Prove that if $\varphi : G \to H$ is a group homomorphism then 
\begin{enumerate}
	\item $\varphi(e_G) = e_H$. (Hint: look at $\varphi(e_G e_G)$).
	
	\begin{proof}
		$\varphi(e_G) = \varphi(e_G e_G) = \varphi(e_G)\varphi(e_G) \implies \varphi(e_G) =e_H$ (we just showed that $\varphi(e_G)$ is an identity element in $H$). 
	\end{proof}
	
	\item $\varphi(g^{-1}) = \varphi(g)^{-1}, \forall g\in G$.
	\begin{proof}
		$\varphi(e_G) = \varphi(g^{-1}g)= \varphi(g)\varphi(g^{-1}) = \varphi(g)\varphi(g)^{-1}$, so we have that $\varphi(g^{-1}) = \varphi(g)^{-1}$.
	\end{proof}
\end{enumerate}


\noindent \textbf{3.} Prove that $\mathbb{Z}_2$ is isomorphic to the subgroup $\{-\mathbb{I}_n, \mathbb{I}_n \} \leq GL(n,\mathbb{C})$. While you are at it, prove that the latter is indeed a subgroup.

\begin{proof}
	Showing the latter set is s subgroup is easy so I won't do it. Consider the group homomorphism $\varphi: \mathbb{Z}_2 \to \{ -\mathbb{I}_n, \mathbb{I}_n \}$ defined by $\varphi(0) = \mathbb{I}_n$, and $\varphi(1) = -\mathbb{I}_n$. The map is bijective by definition. By inspection, the map is also a group homomorphism. So, the map is an isomorphism. This means the two groups are isomorphic to each other. 
\end{proof}


\noindent \textbf{4.} I'll do you one better: prove that any group with only two elements is isomorphic to $\mathbb{Z}_2$.

\begin{proof}
	Call the two-element group $G = \{ g,g^{-1} \}$. Create a map just like that in the previous exercise and we're done. 
\end{proof}





\noindent \textbf{5.} Show that $2\mathbb{Z} = \{ 2n \vert n \in \mathbb{Z} \}$ is a normal subgroup of $(\mathbb{Z},+)$ and that $\mathbb{Z}_2 = \mathbb{Z} / 2\mathbb{Z}$.


\begin{proof}
	This follows almost directly from the definitions. 
\end{proof}





\noindent \textbf{6.} Prove the first isomorphism theorem.

\begin{proof}
	Refer to the Abstract Algebra \href{https://huanqbui.com/archives/MA333.pdf}{\underline{notes}} for the proof. The first two items are easy. The last item is also easy but requires a trick. $\ker\varphi$ is normal, so 
	\begin{align}
	h^{-1}g \in \ker\varphi \iff \varphi(h^{-1}g) = \varphi(h)^{-1}\varphi(g)  = e_H \implies \varphi(g) = \varphi(h).
	\end{align}
	This means that 
	\begin{align}
	[g] = \{ h\in G \vert \varphi(h) = \varphi(g)   \}. 
	\end{align}
	With these we can prove the third item. 
\end{proof}




\noindent \textbf{7.} 

































\newpage



\chapter{Research Project: Quantum Simulation}

\textbf{Quantum many-body physics on quantum hardware}\\


\underline{Project description:} Recently, there have been significant advances in several quantum computing platforms, including superconducting qubits and trapped ions. At this point, nontrivial quantum operations can be implemented
on tens of qubits, and the frontier continues to expand. This project will explore how these emerging
technologies can assist the realization and understanding of complex many-body quantum systems,
regarding both static aspects such as ground state properties and dynamical aspects such as quantum chaos
and thermalization. What new physics can we learn from near-term quantum computers? The ideal
outcome is an interesting and realistic proposal that can be carried out in one of the quantum platforms (see
for example the interplay between theoretical proposals and experiment).














\newpage


\section{Review: The density operator}        

  
We often use the language of state vectors in quantum mechanics. An alternative language is that of \textit{density matrices/density operators}. We will use this language extensively in quantum information/quantum computation. First, we will introduce the formulation. Second, we look at some properties of the density operator. Finally, we look at an application where the density operator really shines -- as a tool for describing \textit{individual subsystems} of a composite quantum system.  


\subsection{Ensembles of Quantum States}

The density operator language provides a convenient means for describing quantum systems whose state is not completely known. Suppose a quantum system is in one of the states $\ket{\psi_i}$ where $i$ is an index, with respective probabilities $p_i$. We call $\{p_i, \ket{\psi_i}\}$ an \textit{ensemble of pure states}. \\

\begin{defn}[Density Operator]
	\begin{align}
	\boxed{\rho \equiv \sum_i p_i \ket{\psi_i}\bra{\psi_i}}
	\end{align}
\end{defn}



Suppose we let a unitary $\U$ act on a closed system. If the system was initially in the state $\ket{\psi_i}$ with probability $p_i$ then we get $\U \ket{\psi_i}$ with probability $p_i$. The density operator evolves as
\begin{align}
\rho = \sum_i p_i \ket{\psi_i}\bra{\psi_i} \stackrel{\U}{\rightarrow} \sum_i p_i \U \ket{\psi_i}\bra{\psi_i}\U^\dagger = \U \rho \U^\dagger.
\end{align}


Suppose we have a measurement gate $\M_m$. If the intial state is $\ket{\psi_i}$ then the probability of getting result $m$ is 
\begin{align}
p(m\vert i) = \bra{\psi_i} \M_m^\dagger \M_m \ket{\psi_i} = \tr\lp \M_m^\dagger \M_m \braket{\psi_i} \rp
\end{align}
where we have used the identity:
\begin{align}
\tr(A\ket{\psi}\bra{\psi}) = \sum_i \bra{i}A \ket{\psi}\bra{\psi}\ket{i} = \bra{\psi}A\ket{\psi}.
\end{align}
With this, the total probability of measuring $m$ is 
\begin{align}
p(m) = \sum_i p(m|i)p_i = \sum_i p_i \tr\lp \M_m^\dagger \M_m \ket{\psi_i}\bra{\psi_i} \rp = \tr\lp \M_m^\dagger \M_m \rho \rp.
\end{align}

The state after the measurement is thus
\begin{align}
\ket{\psi_i^m} = \f{\M_m \ket{\psi_i}}{\sqrt{\bra{\psi_i}\M_m^\dagger \M_m \ket{\psi_i}}}.
\end{align}
The corresponding density operator for this state is thus
\begin{align}
\rho_m = \sum_i p(i|m)\ket{\psi_i^m}\bra{\psi_i^m} = \sum_i p(i|m)\f{\M_m \ket{\psi_i}\bra{\psi_i}\M_m^\dagger}{\bra{\psi_i}\M_m^\dagger \M_m \ket{\psi_i}}.
\end{align}
From probability theory we have $p(i|m) = p(m|i)p_i/p(m)$, so:
\begin{align}
\rho_m = \sum_i p_i \f{\M_m \ket{\psi_i}\bra{\psi_i}\M_m^\dagger}{\tr\lp \M^\dagger_m \M_m \rho \rp} = \f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp}.
\end{align}




Finally, suppose we have a quantum system in state $\rho_i$ with probability $p_i$. The system might be described by the density matrix
\begin{align}
\rho = \sum_i p_i \rho_i.
\end{align}
Here's why:
\begin{align}
\rho = \sum_{i,j}p_i p_{ij}\ket{\psi_{ij}}\bra{\psi_{ij}} = \sum_i p_i \rho_i
\end{align}
where we have used the definition
\begin{align}
\rho_i = \sum_{j}p_{ij}\ket{\psi_{ij}}\bra{\psi_{ij}}.
\end{align}

We call $\rho$ the \textit{mixture} of the states $\rho_i$ with probabilities $p_i$. This concept of mixture comes up up repeatedly in the analysis of problems like quantum noise, where the effect of the noise is to introduce ignorance into our knowledge of the quantum state. A simple example is provided by the measurement scenario. Imagine that for some reason out record of the result $m$ of the measurement was lost. We would have a quantum system in the state $\rho_m$ with probability $p(m)$, but would no longer know the actual value of $m$. The state of such a quantum system would therefore by described by the density operator
\begin{align}
\rho = \sum_m p(m)\rho_m = \sum_m \tr\lp \M_m^\dagger \M_m \rho \rp\f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp} = \sum_m \M_m \rho \M_m^\dagger.
\end{align}






























\subsection{General Properties of the Density Operator}



\begin{thm}[Characterization of density operators] 
	An operator $\rho$ is the density operator to some ensemble $\{p_i, \ket{\psi_i} \}$ if and only if it statisfies the conditions
	\begin{itemize}
		\item \textbf{Trace condition:} $\tr(\rho) = 1$.
		\item \textbf{Positivity:} $\rho$ is a positive operator, i.e., $\bra{\varphi}\rho\ket{\varphi} \geq 0 \forall \varphi$.
	\end{itemize}
\end{thm}


With this definition we can reformulate the postulates of quantum mechanics in the language of density operators as 
\begin{itemize}
	\item \textbf{Postulate 1:} Associated to any isolated physical system is a Hilbert space known as the \textit{state space} of the system. The system is completely described by its \textit{density operator}, which is a positive operator $\rho$ with trace one, acting on the state space of the system. If a quantum system is in the state $\rho_i$ with probability $p_i$ then the density operator for the system is $\sum _i p_i \rho_i$.
	
	\item \textbf{Postulate 2:} The evolution of a \textit{closed} quantum system is described by a \textit{unitary transformation}:
	\begin{align}
	\rho' = \U \rho \U^\dagger.
	\end{align} 
	
	
	
	\item \textbf{Postulate 3:} Measurements are described by a collection $\{\M_m\}$ of \textit{measurement operators}. The index $m$ refers to the measurement outcomes that may occur. If the state of the quantum system is $\rho$ immediately before the measurement then the probability that result $m$ occurs is given by
	\begin{align}
	p(m) = \tr\lp \M_m^\dagger  \M_m \rho_m \rp
	\end{align}
	and the state of the system after the measurement is 
	\begin{align}
	\f{\M_m \rho \M_m^\dagger}{\tr\lp \M_m^\dagger \M_m \rho \rp}.
	\end{align}
	The measurement operators satisfy the completeness equation
	\begin{align}
	\sum_m \M_m^\dagger \M_m = \Id.
	\end{align}
	
	
	
	\item \textbf{Postulate 4:} The state space of a composite physical system is the tensor product of the state spaces of the component physical systems. E.g. the joint state of the total system can be written as $\rho_1 \otimes \dots \otimes \rho_n$.
\end{itemize}



\begin{thm}[Pure states] 
	$\tr(\rho^2) \leq 1$. Equality occurs if and only if $\rho$ is a pure state. 
\end{thm}



\begin{thm}[Unitary freedom in the ensemble for density matrices] 
	The sets $\ket{\tilde{\varphi}_i}$ and $\ket{\tilde{\psi}_j}$ generate the same density matrix if and only if they are unitarily similar, i.e., 
	\begin{align}
	\ket{\tilde{\varphi}_i} = \sum_{j} u_{ij}\ket{\tilde{\psi}_j}
	\end{align}
	where the $u_{ij}$ are entries of a unitary $\U$.
	
\end{thm}




\subsection{The Reduced Density Operator}
Te deepest application of the density operator is as a descriptive tool for \textit{sub-systems} of a composite quantum system. Such a description is given by the \textit{reduced density operator}. \\

Suppose we have physical systems $A$ and $B$ whose state is described by the density operator $\rho^{AB}$. The reduced density operator for $A$ is then given by
\begin{align}
\rho^A = \tr_B\lp \rho^{AB} \rp
\end{align}
where $\tr_B$ is the \textit{partial trace} over system $B$, defined by    
\begin{align}
\tr_B\lp \ket{a_1}\bra{a_2} \otimes \ket{b_1}\bra{b_2} \rp \equiv \ket{a_1}\bra{a_2}\tr\lp \ket{b_1}\bra{b_2} \rp = \ket{a_1}\bra{a_2}\braket{b_2}{b_1}
\end{align}
where $\ket{a_1}, \ket{a_2}$ are any two vectors in the state space of $A$, and $\ket{b_1}, \ket{b_2}$ are any two vectors in the state space of $B$. \\




\begin{exmp}
	To understand this construction, consider an example where $\rho^{AB} = \rho \otimes \sigma$ where $\rho,\sigma$ describe $A,B$ respectively. Then
	\begin{align}
	\rho^A = \tr_B\lp \rho \otimes \sigma \rp = \rho \tr(\sigma) = \rho
	\end{align}
	as expected. 
\end{exmp}




\newpage








\section{Quantum Computational Complexity: An Introduction}

In this section we look at quantum computational complexity, as introduced in this \href{https://arxiv.org/pdf/0804.3401v1.pdf}{\underline{paper}} by John Watrous at the IQC, the University of Waterloo.

\subsection{Introduction}

\begin{itemize}
	\item \textbf{P}
	
	\item \textbf{NP}
	
	
	\item \textbf{BPP}
	
	\item \textbf{PP}
	
	\item \textbf{MA}
	
	
	\item \textbf{AM}
	
	\item \textbf{SZK}
	
	\item \textbf{PSPACE}
	
	\item \textbf{EXP}
	
	\item \textbf{NEXP}
	
	
	\item  \textbf{PL}
	
	\item \textbf{NC}
\end{itemize}

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.3]{complexity}
\end{figure}










\newpage






\section{Quantum Simulation: A Primer}

Here's what Feynman said in 1982:\\

\textit{``Can physics be simulated by a universal computer? [...] the physical world is quantum mechanical, and therefore the proper problem is the simulation of quantum physics [...] the full description of quantum mechanics for a large system with $R$ particles [...] has too many variables, it cannot be simulated
with a normal computer with a number of elements proportional to $R$ [ ... but it can be simulated with ] quantum computer elements. [...] Can a quantum system be probabilistically simulated by a classical (probabilistic, I'd assume) universal computer? [...] If you take the computer to be the classical kind I've described so far [..] the answer is certainly, No!''}


\subsection{Simulation in action}

The heart of simulation is the solution of differential equations which capture the physical
laws governing the dynamical behavior of a system. The goal is generally: given an initial state of the system, what is the state at some other time and/or position? Solutions are usually obtained by \textit{approximating} the state with a digital representation, then \textit{discretizing} the differential
equation in space and time such that an iterative application of a procedure carries the state from the initial to the final conditions.\\

The error in this procedure is \textit{bounded}, and known not to grow faster than some small power of the number of iterations. Furthermore, not all dynamical systems can be simulated efficiently: generally, only those systems which can be described efficiently can be simulated efficiently.\\

Simulating quantum systems using classical computers is usually inefficient. The dynamical behavior of many simple quantum systems is governed the SE:
\begin{align}
i\hbar \p_t \ket{\psi} = \had \ket{\psi}.
\end{align}
We will set $\hbar =1$.  For real particles in space, we often have
\begin{align}
i\p_t \psi(x) = \lb -\f{1}{2m}\p_x^2 + V(x) \rb \psi(x).
\end{align}

The key challenge in simulating quantum systems is the \textit{exponential} number of differential equations which must be solved. For 1 qubit evolving according to the
SE, a system of 2 differential equations must be solved; for 2
qubits, 4 equations; and for $n$ qubits, $2^n$ equations. Sometimes, insightful approximations can be made which reduce the effective number of equations involved, but there are many physically interesting quantum systems for which no such approximations are known.
\\

There are many important quantum systems for which classical simulation is intractable. For example:
\begin{align}
\mbox{Hubbard model: } \had = \sum^n_{k=1}V_0 n_{k^\uparrow}n_{k\downarrow} + \sum_{k,j\mbox{ neighbors,}\sigma} t_0 c_{k\sigma}^* c_{j\sigma}.
\end{align}
\begin{align}
\mbox{Ising mode: } \had = \sum_{k=1}^n \vec{\sigma_k}\cdot \vec{\sigma}_{k+1} .
\end{align}
Quantum computers can efficiently simulate quantum systems for which there is no known efficient classical simulation.












\subsection{The quantum simulation algorithm}

The quantum simulation is concerned with the solution of $i \p_t \ket{\psi} = \had \ket{\psi}$, which, for a time-independent $\had$, is just
\begin{align}
\ket{\psi(t)} = e^{-i\had t}\ket{\psi(0)}.
\end{align}
$\had$ is in general difficult to exponentiate, but a good beginning is the first order solution 
\begin{align}
\ket{(\psi(t+\Delta t))} \approx (\Id - i\had \Delta t)\ket{\psi(t)}.
\end{align}
This is tractable, but often is not very satisfactory. \\

Efficient approximations to higher oder is possible for many \textit{classes} of Hamiltonians. For example, in most physical systems, the Hamiltonian can be written as a sum over many \textit{local} interactions. For a system of $n$ particles:
\begin{align}
\had = \sum^L_{k=1}\had_k \sim \bigoplus^L_{k=1}\had_k
\end{align} 
where each $\had_k$ acts on at most a constant $c$ number of systems, and $L$ is a polynomial in $n$. For instance, the terms $\had_k$ are often just 2-body interactions such as $X_i X_j$ and 1-boy Hamiltonians such as $X_i$. Note that both the Hubbard and Ising models have Hamiltonians of this form. There are sometimes additional global symmetry constraints such as particle statistics.\\

Note that although $e^{-i\had t}$ is difficult to compute, $e^{-i\had_k t}$ acts on a much smaller subsystem, and is straightforward to approximate using quantum circuits. However, because $[\had_j, \had_k] \neq 0$ in general (obviously, they don't commute in general), $e^{-i\had t} \neq \prod_k e^{-i \had_k t}$. So, we might want to ask how $e^{-i\had_k t}$ be useful in constructing $e^{-i\had t}$. \\

\textbf{Remarks:}
\begin{itemize}
	\item If the $\had_i$'s commute, then we do have $e^{-i \had t} = \prod_k e^{-i \had_k t}$. 
	
	\item The restriction of $\had_k$ to involve at most $c$ particles \textit{implies} that in the sum of the Hamiltonians, $L$ is upper bounded by a polynomial in $n$. 
\end{itemize}



The heart of quantum simulation algorithms is the following asymptotic approximation theorem:
\begin{thm}[Trotter formula]
	Let $A$ and $B$ be Hermitian operators. Then for any real $t$, 
	\begin{align}
	\lim_{n\to \infty}\lp e^{-iAt/n}e^{iBt/n} \rp^n = e^{i(A+B)t}.
	\end{align}\qed
\end{thm}



Of course, the theorem holds even if $A$ and $B$ don't commute. For now, we only consider the cases where $A,B$ are Hermitian. But $A,B$ can actually be generators of certain kinds of semigroups, which correspond to general quantum operations.\\

Using the theorem (and the proof of the theorem, which I won't show here), we can get
\begin{align}
e^{i(A+B)\Delta t} = e^{iA \Delta t}e^{i B\Delta t} + \mathcal{O}(\Delta t^2).
\end{align}
Similarly, 
\begin{align}
e^{i(A+B)\Delta t} = e^{iA\Delta t/2}e^{iB \Delta t}e^{iA \Delta t/2} + \mathcal{O}(\Delta t^3). 
\end{align}







\begin{exmp}[Hamiltonian simulation \& Trotterization, taken from \href{https://vtomole.com/blog/2019/04/07/trotter}{\underline{here}} ]
	
	The evolution of quantum system is governed by the SE $i \p_t \ket{\psi} = \had \ket{\psi}$. The solution to the SE:
	\begin{align}
	\ket{\psi(t)} = e^{-i \had t}\ket{0}
	\end{align}
	is a description on what state a system will be in after a Hamiltonian has been applied to it for a certain period of time. The problem of Hamiltonian simulation is thus states as: given a Hamiltonian $\had$ and an evolution time $t$, \textbf{output a sequence of computational gates} that implement $\U = e^{-i \had t}$.\\
	
	Hamiltonians are Hermitian operators that are usually a sum of a large number of individual Hamiltonians $\had_j$. For example, $\had = H_1 + H_2$. This sum of 2 Hamiltonians (which don't necessarily commute) can be described by the Trotter formula or the Lie product formula:
	\begin{align}
	e^{-i(H_1 + H_2)t} = \lim_{N\to \infty}\lp e^{-i H_1 t/N}e^{-iH_2 t/N} \rp^N.
	\end{align}
	Since the limit of this formula is infinite, we have to truncate the series when implementing this formula on a quantum compute. The truncation introduces error in the simulation that we can bound by $\epsilon$ such that $\norm{e^{-i \had t} - \U} \leq \epsilon$, where $\norm{\cdot}$ is some operator norm. This truncation is known as \textbf{Trotterization} and it's widely used to simulate non-commuting Hamiltonians on quantum computers. The \textbf{Trotterization formula} is then
	\begin{align}
	e^{-i \had t} = \lp e^{-iH_0 t/r}e^{-i H_1 t/r} \dots e^{-iH_{d-1}t/r}  \rp^r + \mathcal{O}(\text{polynomial factors}).
	\end{align} 
	Consider for example the Hamiltonian
	\begin{align}
	\had = X_0 + Y_1 + Z_2
	\end{align}
	where $X,Y,Z$ are Pauli matrices and the subscripts label  the qubits that the Hamiltonians act on. We can't simulate each Hamiltonian separately because they don't commute. This is why we use Trotterization where we evolve the entire Hamiltonian via repeatedly switching between evolving $X_0, Y_1, Z_2$ each for a small period of time.\\
	
	The first step in deriving the circuit that will simulate this Hamiltonian is finding the quantum gates that implement each of its individual term (so that we can make $\ket{\psi(t)}= e^{-i\had t}\ket{0}$ from $\ket{0}$ -- in a sense we want to find the correct form for $e^{f(X_0)}$). It turns out that this is easy since the quantum gates
	\begin{align}
	R_x(\theta) = e^{-i\theta X/2}, \quad R_y(\theta) = e^{-i\theta Y/2}, \quad R_z(\theta) = e^{-i\theta Z/2}
	\end{align} 
	will implies the individual terms perfectly. $\theta$ here denotes an \textbf{angle}, but we can also think of it as \textbf{time} because $\theta$ specifies the angle by which to rotate the state in a specified axis, which is synonymous with time in the sense that it specifies how ``long'' the rotation is applied on the state. \\
	
	
	Since we don't care too much about the simulation error in this example we will use  $r=2, t=1$ in the Trotter formula to get
	\begin{align}
	e^{-i(X_0 + Y_1 + Z_2)} = \lp e^{-iX_0/2}e^{-iY_1/2}e^{-iZ_2/2}\rp\lp e^{-iX_0/2}e^{-iY_1/2}e^{-iZ_2/2}  \rp
	\end{align}
\end{exmp}















\subsection{The algorithm}

\begin{itemize}
	\item \textbf{Inputs:} (1) A Hamiltonian $\had = \sum_k \had_k$ acting on an $N$-dimensional system, where each $\had_k$ acts on a small subsystem of size independent of $N$, (2) an initial state $\ket{\psi_0}$, of the system at $t=0$, (3) a positive, non-zero accuracy $\delta$, and (3) a time $t_f$ at which the evolved state is desired. 
	
	
	\item A state $\ket{\tilde{\psi}(t_f)}$ such that
	\begin{align}
	\abs{\bra{\tilde\psi (t_f)} e^{-i \had t_f} \ket{\psi_0}}^2 \geq 1 - \delta.
	\end{align}
	
	
	\item Choose a representation such that the state $\ket{\tilde\psi}$ of $n = \mbox{poly}(\log N)$ qubits approximates the system and the operators $e^{- \had_k \Delta t}$ have efficient quantum circuit approximations. Selection an approximation method and $\Delta t$ such that the expected error is acceptable, construct the corresponding quantum circuit $\U_{\Delta t}$ fo the iterative step, and do the following:
	\begin{itemize}
		\item Initialize state: $\ket{\tilde \psi_0} \leftarrow \ket{\psi_0}; j = 0.$ 
		\item Iterative update: $\to \ket{\tilde \psi_{j+1}} = \U_{\Delta t}\ket{\tilde \psi_j}$.
		\item Loop: $\to j = j+1;$ go to the second step until $j\Delta t \geq t_f$.
		
		\item Final result: $\to \ket{\tilde \psi(t_f)} = \ket{\tilde \psi}$.
	\end{itemize}
\end{itemize}


\begin{thm}[Baker-Campbell-Hausdorf formula]
	\begin{align}
	e^{(A+B)\Delta t} = e^{A \Delta t}e^{B \Delta t}e^{-(1/2)[A,B]\Delta t^2} + \mathcal{O}(\Delta t^3). 
	\end{align}
\end{thm}


\begin{proof}
	The proof is quite easy. Just, by definition, expand things out in power series. \qed
\end{proof}




\subsection{An illustrative example}

We have seen the case where the Hamiltonian is a sum of local interactions. However, this is not a fundamental requirement. Efficient quantum simulations are possible even for Hamiltonians which act non-trivially on all or nearly all parts of a large system. \\

Consider the Hamiltonian
\begin{align}
\had = Z_1 \otimes Z_2 \otimes \dots \otimes Z_n
\end{align}
which acts on an $n$ qubit system. This can be simulated efficiently. What we desire is a simple quantum circuit which implements $e^{-i \had \Delta t}$, for arbitrary values of $\Delta t$. A circuit that does this is given by
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qs1}
\end{figure}

Even though the Hamiltonian involves all the qubits in the system, it does so in a \textit{classical} manner: the phase shift applied to the system is $e^{-i\Delta t}$ if the \textit{parity} of the $n$ qubits in the computational basis is even. else the phase shift would be $e^{i\Delta t}$. Thus, a simple simulation of $\had$ is possible by first classical computing the parity, then applying the appropriate phase shift conditioned on the parity, then uncomputing the parity. \\

Extending the same procedure allows us to simulate more complicate extended Hamiltonians. Specifically, we can efficiently simulate any Hamiltonian of the form
\begin{align}
\had = \bigotimes^n_{k=1}\sigma^k_{c(k)}
\end{align}
where $\sigma^k_{c(k)}$ is a Pauli matrix (or identity) acting on the $k$th qubit, with $c(k) \in \{0,1,2,3\}$ specifying one of $\{I,X,Y,Z\}$. The qubits upon which the identity acts can be disregarded, and $X$ or $Y$ terms can be transformed by single qubit gates to $Z$ operations. This leaves a Hamiltonian of the form given above, which can be efficiently simulated. \\

Using this procedure allows us to simulate a wide class of Hamiltonians containing terms which are not local. It is possible to simulate a Hamiltonian of the form $\had = \sum^L_{k=1} \had_k$ where we only require that the individual $\had_k$ have a tensor product structure, and that $L$ is polynomial in the total number of particles $n$. 







\subsection{Perspectives on quantum simulation}

 
Quantum simulation is similar to classical methods, but differs fundamentally in many aspects. Each iteration of the quantum algorithm must completely replace the old state with a new one and there is no way to obtain any nontrivial information from an intermediate step without changing the algorithm (due to the no-cloning theorem). Furthermore, the final measurement must be chosen to provide the desired result because it collapses the entire state. \\

Another difficult problem is the simulation of equilibration processes. A system with Hamiltonian $\had$ in contact with an environment at temperate $T$ will come to thermal equilibrium in a state known as the \textit{Gibbs} state,
\begin{align}
\rho_{\text{therm}} = \f{e^{-\had/k_BT}}{\mathcal{Z}} \equiv \f{e^{-\had/k_BT}}{\tr\lp e^{-\had/k_BT} \rp}
\end{align}
where of course $\mathcal{Z}$ is the partition function. We can check that $\tr(\rho) = 1$. For now, we don't know how a quantum computer can simulate this. \\

Indistinguishability of particles also places a constraint on the state vector of a system which manifests itself in two ways: bosons and fermions. The state vector of a system of bosons remains unchanged under permutation of any two constituents. System of fermions experience a sign change in their state vector under interchange of any two constituents. Both kinds of systems can be simulate efficiently on a quantum computer. 















\newpage


\section{Problems in Quantum Simulations and Quantum Algorithms}

In this section, we will be looking at the following \href{https://arxiv.org/pdf/2001.03685.pdf}{\underline{review}} on quantum algorithms for quantum chemistry and materials science. 

\subsection{Quantum Simulation: An Overview}

\subsubsection{Introduction}

The interest in quantum computing for quantum simulations of molecules and materials stems from the fact that in many cases, the chemistry and physics of molecules and materials is best described using quantum mechanics. In the worst case, quantum simulation is exponentially hard on classical computers. Quantum computers have advantages over classical computers, but these advantages are problem-specific. \\


As we have seen before, the natural problem to solve on a quantum computer is the time evolution of a quantum system given some initial state:
\begin{align}
i\p_t \ket{\psi(t)} = \had \ket{\psi(t)}.
\end{align}
This problem, as we have pointed out, is of polynomial cost on a quantum computer, which can offer exponential speedup over classical computers. However, it is necessary to prepare the initial state, which may be difficult. In particular, preparing a low-energy state may be challenging, which naturally leas to considering other important problems:
\begin{align}
&\mbox{Ground state:} \quad \had\ket{\psi_0} = E_0 \ket{\psi_0}; \quad E_0 = \min_{\ket{\psi}}\bra{\psi}\had\ket{\psi}\\
&\mbox{Thermal averages:} \quad \langle \mathcal{A} \rangle = \f{\tr\lp \mathcal{A}e^{-\beta\had} \rp}{\tr\lp e^{-\beta\had} \rp}
\end{align} 
Ground state determination lies in complexity class QMA, a class of problems \textit{not known to be efficiently solvable in general on a quantum computer}. This means that thermal averages cannot in general be computed efficiently on a quantum computer, since in the limit of zero temperature, this problem reduces to ground-state determination. \\

To understand quantum advantage in chemistry, condensed matter physics, and quantum materials science, we must be guided by actual empirical data in the form of numerical and theoretical experiments with quantum algorithms and quantum devices on simulation problems of interest. This requires progress, of course. 



\subsubsection{Current quantum architectures}
There's \textit{analog quantum computation} and there's \textit{digital quantum computation}. Analog quantum computation was proposed by Feynman. The idea of analog quantum computation is to build a lattice of spins with tunable interactions and let it simulate physical systems. The limitation, however, is that this is quite limited because it can only simulate a certain set of physical systems. Furthermore, its accuracy is limited. \\

Digital quantum computation is more general. Digital quantum computation involves qubits and gates. One can show that with a finite set of gates, one can in principle generate arbitrary quantum evolutions to arbitrary precision. Every problem in the complexity class BQP can be mapped into a quantum program. \\


The third model of quantum computation is \textit{adiabatic quantum computation}. This model is considered equivalent to the digital version, but is less practical. We won't worry about this. 



\subsubsection{Building a circuit-based digital quantum computer}

The natural enemy of quantum computation is decoherence -- the tendency of quantum systems to decay into their classical states. We are current in the \textit{noisy intermediate-scale quantum} (NISQ) era, where qubit technologies (superconducting qubits, ion traps, etc) have reached the point where small devices of a few dozen qubits can be sufficiently isolated from decoherence to execute non-trivial quantum algorithms of a few tens to hundreds of gates. With these devices, an artificial but well-defined problem that is intractable classically is solved on a quantum computer. However, these problems are often not practical. The more important question is when a more relevant problem can be solved on a quantum computer. \\


To address large-scale problems, \textit{quantum error-correction} (QEC) is necessary. QEC leads to a distinction between \textit{logical} and \textit{physical} qubits. The former are error-corrected and encoded in the state of many physical qubits. Quantum algorithms are performed on the logical qubits, and the error-correction scheme translates the operations on logical qubits into physical operations. 



\newpage




\subsection{Simulation challenges in molecular and materials science }

In this section we look at some scientific problems of relevance to quantum simulation.

\subsubsection{Quantum chemistry}

Quantum chemistry is concerned with determining the low-lying eigenstates of the electronic Hamiltonian of a molecule. he eigenstates are determined for fixed sets of nuclear positions within the Born-Oppenheimer approximation. Determining the electronic energy as a function of nuclear position is key to understanding chemical reactivity, product distribution, and reaction rates.\\

Some examples of problems in quantum chemistry include
\begin{itemize}
	\item The chemistry of enzyme active sites
	\item Transition metal nanocatalysts an surface catalysts
	\item Light harvesting and the vision process
\end{itemize}

The basic metric is whether ground-states or low-energy eigenstate quantum algorithms yield more accurate energies than the best classical algorithms, for the problem sizes of interest. 

\subsubsection{Quantum molecular spectroscopy}


The theoretical goal is to compute the eigenstates of the nuclear SE. There are challenges, of course. First, the nuclear Hamiltonian is often not known, because interactions are mediated by electrons. Second, nuclear rotational and vibrational motion is often far from harmonic and not well approximate by simple mean-field theories. Third, even when the nuclear SE has been properly formulated, one faces the challenge of representing th eigenstates. \\

Some famous examples include:
\begin{itemize}
	\item Spectra of floppy molecules
	\item Hydrogen bonded clusters
\end{itemize}

There are similarities with the quantum chemistry problems, but the differences are more significant. The Hamiltonian is often more complicated because interactions are now many-body. Also, the number of states is often orders-of-magnitude greater. 


\subsubsection{Chemical quantum dynamics}

Chemical quantum dynamics if concerned with modeling time-dependent electronic and nuclear quantum effects in molecules. Some problems include:
\begin{itemize}
	\item Proton coupled electron transfer
	\item Vibrational dynamics in complex environments
	\item Plasmonic chemistry
\end{itemize}


There are multiple practical challenges. Some of this may be viewed as an issue of representation. In addition, the dynamical quantum state involves near continuum degrees of freedom, which makes discretizing the Hilbert space a challenging problem. 


\subsubsection{Correlated electronic structure in materials}

The goal here is to determine low-energy properties of materials such as Mott insulators. Some famous problems include:
\begin{itemize}
	\item High-temperature superconductivity
	\item Non-Fermi liquid behavior in heavy fermion compounds and fermionic systems near criticality
	\item Quantum fluctuations in two-dimensional systems
	\item Frustrated spin systems
\end{itemize}






%\subsubsection{Dynamical quantum effects in materials}












\newpage


\subsection{Challenges for quantum algorithms in quantum simulation}

\subsubsection{Overview of algorithms}

In this section, we look at the current status and theoretical and practical challenges to implement quantum algorithms for quantum problems. \\


The first step in a quantum simulation is to choose a \textit{representation} for the Hamiltonian and the states. We will examine the possibilities for different qubit representations, and open questions.\\

Quantum ground-state algorithms fall into different classes. QPE is a direct route to nearly exact eigenstate determination, but has been challenging to implement in the near-term era. On the other hand, \textit{variational quantum algorithms} provide the possibility to introduce approximations with adjustable circuit depth, which are determine via optimization, usually implemented in a hybrid-quantum classical loop. \\

While polynomial time algorithms for \textit{quantum time evolution} have been known for some time, algorithms with optimal asymptotic complexity as well as favorable scaling with error and low prefactors, remain an area of active research.




\subsubsection{Qubit representation of many-body systems}

To simulate many-body systems on a digital computer (either quantum or classical), the infinite-dimensional Hilbert space of a many-body system has to be truncated. The most direct route then is to define a finite set of \textit{basis functions} and then try to project the exact many-body Hamiltonian onto the chosen basis. The resulting discretized system is then expressed in terms of qubits. A choice of a good representation is important as it may affect the simulation cost dramatically. 


\begin{enumerate}
	\item \textit{Ab initio electronic structure qubit representations:}\\
	
	This concerns with the electronic structure. Consider the Hamiltonian that appears regularly in chemistry and physics
	\begin{align}
	\had = \sum^K_{i=1}-\f{1}{2}\laplacian + V(r_i) + \sum_{1\leq i<j\leq K}\f{1}{\abs{r_i - r_j}}
	\end{align}
	where $K$ is the number of electrons, $V$ is the potential created by atomic nuclei at a point $r$, and the last term represents the Coulomb interaction. Each electronic has $r_i \in \mathbb{R}^3$ and spin $\omega_i \in \{\uparrow, \downarrow\}$. So, the total wavefunction (for $K$ electrons) has the form $\psi = \psi(\vec{x}_1,\dots,\vec{x}_K)$ where $\vec{x}_i = (r_i,\omega_i)$. From Fermi statistic, we require that $\psi$ is anti-symmetric under a position swap.\\
	
	The first step is to approximate the electronic Hamiltonian with a simpler simulator Hamiltonian. This is usually achieved by truncate the Hilbert space of a single electron to a finite set of basis functions $\psi_1,\dots,\psi_N$. \\
	
	Electronic structure simulation algorithms are based on the \textit{first quantization method} (\href{https://journals.aps.org/prl/pdf/10.1103/PhysRevLett.79.2586}{\underline{link}}). We won't worry about this too much.\\
	
	The \textit{second quantization method} often results in simpler simulator Hamiltonian and requires few qubits. This methods is particularly well suited for quantum simulation algorithms and has been experimentally demonstrated for small molecules. Given a set of $N$ basis functions $\psi_1,\dots,\psi_N$, the second quantized simulator Hamiltonian is given by
	\begin{align}
	\had = \sum^N_{p,q=1} t_{pq}\hat{c}_p^\dagger \hat{c}_q + \f{1}{2}\sum^N_{p,q,r,s=1}u_{pqrs}\hat{c}^\dagger_p\hat{c}_q^\dagger \hat{c}_r \hat{c}_s.
	\end{align}
	Again, we're not going to worry too much about these. 
	
	
	
	
	
	
	
	\item \textit{Electronic basis functions:}\\
	
	The discretization of the electronic Hilbert space for a quantum simulation requires balancing two concerns. We need to represent the state with the smallest number of qubits, but also retain maximal Hamiltonian sparsity. \\
	
	There are two families of basis functions in wide use in quantum chemistry and quantum materials science: atomic orbital \textit{Gaussian bases} and \textit{Plane waves}. Gaussian bases are commonly used in molecular simulations due to their compactness, while plane waves are used in crystalline materials simulations. \\
	
	In Gaussian bases, linear combinations of Gaussian functions are placed at the nuclear positions. When they are place where the ground-state electron density is highest, they give a compact representation of the wavefunction for bound states. However, the Hamiltonian is not sparse, which leads to high gate counts. \\
	
	Plane waves offer greater simplicity as the accuracy of the basis is controlled by a single parameter, namely the kinetic energy cutoff. The number of plane waves needed to reach a desired accuracy is larger than th number of Gaussian states, but the Hamiltonian contains fewer terms ue to momentum conservation. \\
	
	The need to expose more sparsity in the Hamiltonian while retaining a reasonable compact wavefunction is an active area of research in both classical and quantum algorithms.  
	
	
	\item \textit{Fermions-to-qubit mappings:}\\
	
	Since the basic units of a quantum computer are qubits rather than fermions, any quantum simulation algorithm of fermions emplyos a suitable encoding of fermionic degrees of freedom into qubits. The standard \textit{Jordan-Wigner} mapping identifies each Fermi mode with a qubit such that the empty and the occupied states are mapped to the qubit basis states $\ket{0}$ and $\ket{1}$ respectively. The Jordan-Wigner mapping/transformation is a transformation that maps spin operators onto fermionic creation and annihilation operators. 
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{jw}
	\end{figure}
	A natural question is whether the number of qubits required to express a Fermi system can be reduced by exploiting symmetries such as the particle number conservation or
	the point group symmetries of molecules. For example, zero
	temperature simulations often target only one symmetry sector containing the ground state. This motivates the study
	of \textit{symmetry-adapted fermion-to-qubit mappings}.
\end{enumerate} 



\subsubsection{Quantum algorithms for ground and excited states}

There are many approaches to obtaining ground states or
excited states on a quantum computer. State preparation
procedures attempt to construct a circuit to prepare a state
with as large as possible overlap with the desired eigenstate. One set of such procedures, which includes adiabatic state preparation and quantum imaginary time evolution, uses a prescribe evolution path. An alternative strategy is based on \textit{variational methods}, which are often called variational quantum eigensolvers. Here, the preparation circuit itself is defined via the optimization of the energy with respect to parameters of the circuit. \\


Given some state with sufficiently large overlap with the desired state, one can perform \textit{quantum phase estimation} (QPE) which simultaneously projects the state onto an eigenstate of the Hamiltonian and obtains an estimate for the energy of this eigenstate. The error, as we have seen, is inversely proportional to the simulation time. The probability of successfully projecting onto the desired state is given by the square overlap of the input state and
the desired state, and it is thus necessary to use some other
method (such as the state preparation procedures above) to
prepare an input state with sufficient overlap with the desired
state.\\


There are weaknesses to this, of course. While phase estimation allows the deviation of the final state from an exact eigenstate to be systematically reduced, it can require deep circuits with many controlled gates that are challenging for devices with limited coherence and without error correction. Variational methods replace such circuits b a large number of potentially short simulations. However, if one des not measure the energy by phase estimation, but instead by expressing the Hamiltonian as a sum of multi-qubit Pauli operators and measuring the terms individually, the state preparation and measurements must be repeated many times, with the error converging only as the square root of the number of repetitions. Variational methods are also limited by the variational form and ability to solve the associated optimization
problem, which may by itself represent a difficult classical
optimization.


\subsubsection{Preparing ground states along a prescribed path}

\begin{enumerate}
	\item \textit{Adiabatic state preparation:}\\
	
	 
	This state preparation relies on the \textit{adiabatic theorem}
	
	
	\begin{thm}[Adiabatic Theorem]
		A physical system remains in its instantaneous eigenstate if a given perturbation is acting on it slowly enough and if there is a gap between the eigenvalue and the rest of the Hamiltonian's spectrum. 
	\end{thm}
	In simpler terms, this says that gradually changing conditions allow the system to adapt its configuration, hence the probability density is modified by the process. if the system started in an eigenstate of the initial Hamiltonian, it will end in the \textit{corresponding} eigenstate of the final Hamiltonian.\\
	
	To make use of this theorem, one chooses a Hamiltonian path $\had(\lambda)$ where $\lambda \in [0,1]$, such that the ground state of $\had(0)$ is easily prepared, while $\had(1)$ is the Hamiltonian whose ground state one wants to obtain. The system is then evolved under the time-dependent SE 
	\begin{align}
	i\p_t \ket{\psi_t} = \had(t/T)\ket{\psi_t},\quad t \in [0,T].
	\end{align}
	When $T\to \infty$ and if the spectrum of $\had_{t/T}$ is gapped for all $t$, the final state is the exact ground state of $\had$. Away from the adiabatic limit $T\to \infty$, corrections are necessary. \\
	
	The main disadvantage of this approach is that it is limited by the $T\to \infty$ requirement, and the fact that a path without degeneracies must be chosen. 
	
	
	\item \textit{Quantum imaginary-time evolution:} \\
	
	In classical simulations, one popular approach to prepare (nearly exact) ground-states is imaginary-time evolution,
	which expresses the ground-state as the long-time limit of the
	imaginary-time SE $-\p_t \ket{\Phi(\beta)} = \had \ket{\Phi(\beta)}$:
	\begin{align}
	\ket{\psi} = \lim_{\beta \to \infty} \f{e^{-\beta \had} \ket{\Phi(\beta)}}{\norm{e^{-\beta \had} \Phi(\beta)}}.
	\end{align} 
	To perform imaginary time evolution on a quantum computer, it is necessary to implement the (repeated) action of the
	short-imaginary-time propagator $e^{-\Delta \tau \had}$
	on a state. Given a Hamiltonian that can be decomposed into geometrically local terms,
	\begin{align}
	\had = \sum _m \hat{h}_m
	\end{align}
	and a state $\ket{\psi}$ with finite \textbf{correlation length} $C$, the action $e^{-\Delta \tau \hat{h}_i}$ can be generated by a unitary $\U = e^{i\hat{A}}$ acting on $\mathcal{O}(C)$ qubits surrounding those acted on by $\hat{h}_i$, i.e.,
	\begin{align}
	\f{e^{-\beta \had} \ket{\psi}}{\norm{e^{-\beta \had} \psi}} = \U \ket{\psi}= e^{i\hat{A}}\ket{\psi}
	\end{align}	
	where the coefficients of the Pauli strings in $\hat{A}$ can be determined from local measurements of the quits around $\hat{h}_i$.\\
	
	
	Quantum imaginary time evolution becomes inefficient in terms of the number of measurements an complexity of the operator $\hat{A}$ if the domain $C$ grows to be large along the imaginary time evolution path. 
	
	
	\textbf{Remark:} Correlation length is the distance beyond which the correlation $\bra{\psi}\mathcal{O}_i \mathcal{O}_j\ket{\psi}$ decays beyond some $\epsilon$. We note that every gapped Hamiltonian has an exponentially decaying correlation length. Gapped Hamiltonians are those with a nontrivial $\Delta E = E_1 - E_0$ even in the thermodynamic limit. \\
	
	
	

	
	
	
	
	
	
	
	
	
	
	
\end{enumerate}








\subsubsection{Variational state preparation and variational quantum eigensolver}


States can also be prepared through variational methods. Here, similar to classical variational approaches, one chooses a class of ansatz states for the ground state of the Hamiltonian of interest. Generally speaking, such an ansatz consists of some initial state and a unitary circuit parametrized by some set of classical variational parameters. Applying this circuit to the initial state
yields a guess for the ground state, whose energy is then evaluated. This yields an upper bound to the true ground state
energy. One then varies the variational parameters to lower
the energy of the ansatz state.\\

Choosing the class of ansatz states is also a balancing act. On the one hand, the class should contain an accurate approximation to the true ground state of the system. On the other hand, we want a class of circuits that are easily executed on the available quantum computer. Finally, it
is important for the classical optimization over the variational
parameters to be well-behaved, so as to be able to find low-energy minima. 


\begin{enumerate}
	\item \textit{Unitary coupled cluster:} \\
	
	\begin{figure}[!htb]
		\centering
		\includegraphics[scale=0.3]{vqei}
	\end{figure}
	We won't worry about the details of how this works at this point. For more information, consider this \href{https://iopscience.iop.org/article/10.1088/2058-9565/aad3e4/pdf}{\underline{paper}}.
	

	
	\item \textit{Adapt-VQE ansatz:}\\
	
	In the adapt-VQE scheme, a collection of operators $\hat{A}_i$ is chosen in advance, and the ground state is approximated by 
	\begin{align}
	\ket{\psi_{\text{adapt-VQE}}} = e^{\theta_n \hat{A}_n}\dots e^{\theta_1 \hat{A}_1}\ket{\psi_{\text{HF}}},
	\end{align}
	Given a current parameter configuration $\vec{\theta}$, the commutator  of the Hamiltonian with each operator in the pool is measured to obtain the gradient of the energy
	\begin{align}
	E = \bra{\psi_{\text{adapt-VQE}}} \had \ket{\psi_{\text{adapt-VQE}}}.
	\end{align}
	wrt the parameters $\vec{\theta}$. Repeating this multiple times and averaging over the obtained samples gives the gradient of the expectation value of the Hamiltonian wrt to the coefficient of each operator. The ansatz is improved by adding the operator $\hat{A}_i$ with the largest gradient to the left end of the ansatz with a new variational parameter, thereby increasing $n$. The operation is repeated until convergence.  
	
	
	
	
	\item \textit{Other considerations:}\\
	
	 There are other considerations with regards to variational methods, but we won't worry about them too much for now.
\end{enumerate}






\subsubsection{Excited states}

While much of the above discussion of state preparation
and variational algorithms has focused on ground-states, most
of the same methods can also be used with minor extensions
for excited states. For example, adiabatic state preparation can
be used to prepare an excited state, so long as it is connected
to the initial state without a vanishing gap.


\subsubsection{Phase estimation}

We have a section on this, so let's just be brief and gloss over the details. \\


Quantum Phase Estimation (QPE) is a crucial step in many
quantum algorithms. QPE enables high-precision measurements of the ground and excited energy levels. This is done by preparing a trial initial state $\ket{\psi(0)}$ that has a non-negligible overlap with the relevant eigenvector of the target Hamiltonian $\had$ and applying a circuit that creates a superposition of time evolved states $\ket{\psi(t)} = e^{-i\had t}\ket{\psi(0)}$ over a suitable range of evolution times $t$. \\

Since QPE is used ubiquitously in a variety of quantum applications, it is crucial to optimize its performance. Below we
list some open problems that are being actively investigated:
\begin{itemize}
	\item Given limitations of near-term quantum devices, of particular interest are tradeoffs between the depth of the
	QPE circuit and its spectral-resolution power as well
	as its sensitivity to noise.
	
	\item Several methods have been proposed for mitigating experimental errors for VQE-type simulations. Such methods enable reliable estimation of expected values of observables on a given trial state without introducing any overhead in terms of extra qubits or quantum gates.
	
	
	\item Classical post-processing methods that enable simultaneous estimation of multiple eigenvalues are highly desirable.
	
	\item Finally, a natural question is whether the time evolution
	operator $e^{-i \had t}$
	in QPE can be replaced by some other
	functions of Hˆ that are easier to implement
\end{itemize}




\subsubsection{Quantum algorithms for time evolution}


\begin{enumerate}
	\item \textit{Hamiltonian simulation problem:}  \\
	
	A quantum computer can be programmed to efficiently simulate the unitary time evolution of almost any physically realistic quantum system. Consider any evolution which is of course given by the SE
	\begin{align}
	i \p_t \ket{\psi(t)} = \had \ket{\psi(t)}, \quad t \geq 0.
	\end{align}
	We can just assume that $\had$ describes a system of $n$ qubits, since any fermionic or spin system can be mapped to qubits. Integrating we get
	\begin{align}
	\ket{\psi(t)} = e^{-i\hat t}\ket{\psi(0)}.
	\end{align}
	A quantum algorithm for Hamiltonian simulation takes as input a description of $\had$, the evolution time $t$, nd outputs a quantum circuit $\U$ that approximates the time evolution operator $e^{-it\had}$ within a specified precision $\epsilon$:
	\begin{align}
	\norm{\U - e^{-it\had}} \leq \epsilon.
	\end{align}
	$\U$ may use ancillary qubits initialized in the $\ket{0}$ state. The simulation cost is usually quantified by the runtime of the algorithm (gate count) and the total number of qubits. Applying $\U$ to $\ket{\psi(0)}$ gives the approximation state $\ket{\psi(t)}$. \\
	
	
	Quantum simulation algorithms apply to general classes of Hamiltonians satisfying mild technical conditions that enable a quantum algorithm to access the Hamiltonian efficiently.

\end{enumerate}

















%\subsubsection{Algorithmic tools}
%\subsubsection{Open problems}
%\subsubsection{Finite-temperature algorithms}
%\subsubsection{Hybrid quantum-classical methods}
%\newpage
%
%
%\subsection{Reading out results}
%
%\subsubsection{Equal-time measurements}
%\subsubsection{Dynamical properties and Green's functions}






\newpage






\section{Quantum Phase Estimation (QPE)}

One of the most important applications of the Quantum Fourier Transform is the Quantum Phase Estimation. Phase estimation is the key for many quantum algorithms. Suppose a unitary operator $\U$ has an eigenvector $\ket{u}$ with eigenvalue $2^{2\pi i \varphi}$, where the value of $\varphi$ is unknown. The goal of the phase estimation algorithm is to estimate $0 \leq \varphi \leq 1$ with high probability within additive error $\epsilon$. The algorithm uses $\mathcal{O}(\log (1/\epsilon))$ qubits and $\mathcal{O}(1/\epsilon)$ controlled-$\U$ operations. \\

To perform the estimation we assume that we have available \textit{oracles} capable of preparing the state $\ket{u}$ (the eigenstate) and performing the controlled-$\U^{2j}$ operation, for suitable nonnegative integers $j$. Since preparing eigenstates is not necessarily easy, the phase estimation algorithm is not a complete algorithm by itself. Rather, we should view it as a ``subroutine'' or a ``module.''\\



The QPE uses two registers. The first register contains $t$ qubits initially in $\ket{0}$. The choice of $t$ depends on two things: the number of digits of accuracy we wish to have in our estimate of $\varphi$, and with what probability we wish the phase estimation procedure to be successful. The second register begins in the eigenstate $\ket{u}$, and contains as many qubits as necessary to store $\ket{u}$. \\

QPE occurs in two main stages. First, the circuit applies  an $\had$ to each of the $t$ qubits. Then, the circuit applies controlled-$\U$ operators on the second register, with $\U$ raised to successive powers of two. This stage looks like the following:
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qpe1}
	\caption{Source: Mike and Ike}
\end{figure} 
Recall that immediately after $\had^{\otimes t}$ the state of the upper register looks like
\begin{align}
\had \ket{0} = \f{1}{2^{1/2}}\lp \ket{0} + \ket{1} \rp
\end{align}
The controlled-$\U$ acts on $\ket{u}$ only if the control qubit is in $\ket{1}$. With this, we see that the final state of the first register is given by
\begin{align}
&\f{1}{2^{t/2}}\lp \ket{0} + e^{2\pi i 2^{t-1}\varphi}\ket{1} \rp\otimes
\lp \ket{0} + e^{2\pi i 2^{t-2}\varphi}\ket{1} \rp \otimes \dots \otimes \lp \ket{0} + e^{2\pi i 2^{0}\varphi}\ket{1} \rp\nn\\
&\quad = \dots \nn\\
&\quad = \f{1}{2^{t/2}}\sum_{0\leq k < 2^{t}} e^{2\pi i \varphi k}\ket{k}_t.
\end{align}

\begin{exmp}
For $t =2$, we have
\begin{align}
&\f{1}{2}\lp \ket{0} + e^{4\pi i \varphi } \ket{1} \rp\otimes \lp \ket{0} + e^{2\pi i\varphi }\ket{1} \rp \nn\\
&\quad= \f{1}{2}\lp \ket{0}_2 + e^{1\cdot 2\pi i\varphi }\ket{1}_2 + e^{2\cdot 2\pi i\varphi}\ket{2}_2 + e^{3\cdot 2\pi i\varphi }\ket{3}_2  \rp\nn\\
&\quad = \f{1}{2^{2/2}}\sum_{0 \leq k < 2^{2}}e^{2\pi i \varphi \cdot 2}\ket{k}_2
\end{align}\qed
\end{exmp}

Note that we're leaving out the qubits carrying the  $\ket{u}$ state because we don't do anything to it (because $\ket{u}$ is an eigenstate of $\U$). We should also note that the effect of the sequence of controlled-$\U$ operations is to take $\ket{j}\ket{u}$ to $\ket{j}\U^j \ket{u}$. We should also note that this does not depend on the fact that $\ket{u}$ is an eigenstate of $\U$. \\


In the second state, the algorithm applies the inverse quantum Fourier transform on the first register (which is just the QFT, applied in reverse order of operations). This can be done efficiently in $\mathcal{O}(t^2)$ steps, as we have seen the Quantum Information \href{https://huanqbui.com/LaTeX 20projects/HuanBui_QI/HuanBui_QI.pdf}{\underline{notes}}. Since the input in this case is the maximal superposition in the $t$-vector space, the output of the inverse QFT is going to be a single state: $\ket{\tilde{\varphi}}$, which we hope is approximately $\ket{\varphi}$. We will see how close $\ket{\tilde\varphi}$ can get to $\ket{\varphi}$.\\

The third and final stage of the algorithm is to read out the state of the first register by doing a measurement in the computational basis. \\


\begin{exmp}
	To make sure we see what is going on, suppose $\varphi$ is expressed exactly in $t$ qubits, as $\varphi = 0,\varphi_1\varphi-2\dots\varphi_t$ (note that $\varphi \in [0,1]$). Then the state of the input register after the first stage is given by
	\begin{align}
	\f{1}{2^{t/2}}\lp \ket{0} + e^{2\pi i 0.\varphi_t} \ket{1} \rp \otimes \lp \ket{0} + e^{2\pi i 0.\varphi_{t-1}\varphi_t}\ket{1} \rp \otimes \dots  \otimes \lp \ket{0} + e^{2\pi i 0.\varphi_1\varphi_2\dots\varphi_t}\ket{1} \rp.
	\end{align}
	Applying the inverse QFT to this, we see that output state from the second state is the product state $\ket{\varphi_1\dots \varphi_t}$. A measurement in the computational basis gives $\varphi$ exactly. \qed
\end{exmp}

Schematically, the algorithm for QPE looks like the following:
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{qpe2}
	\caption{Source: Mike and Ike}
\end{figure}

In summary, what we basically did was using the inverse QFT to get
\begin{align}
\f{1}{2^{t/2}}\sum_{0 \leq k < 2^{t}} e^{2\pi i \varphi j }\ket{j}\ket{u} \to \ket{\tilde{\varphi}}\ket{u}
\end{align}
where $\ket{\tilde\varphi}$ is a good estimator for $\varphi$ when measured. 

 





\subsection{Performance and Requirements}
What happens when $\varphi$ cannot be written exactly with a $t$-bit expansion? We will now show that the procedure we described will produce a good approximation to $\varphi$ with high probability. \\

Let $b \in [0,2^t-1]$ an integer be given such that $b/2^t = 0.b_1b_2\dotsb_t$ is the best $t$ bit approximation to $\varphi$ which is less than $\varphi$, i.e., 
\begin{align}
0 \leq \delta \equiv \varphi - b/2^t \leq 2^{-t}.
\end{align}
(This is getting pretty close to an analysis proof -- because it \textit{is} an analysis proof.) Applying the inverse QFT to the state immediately after the first stage gives
\begin{align}
\mathcal{F}^{-1}\lb\f{1}{2^{t/2}}\sum_{0 \leq k < 2^t}e^{2\pi i \varphi k}\ket{k} \rb &= \f{1}{2^{t}} \sum_{0 \leq k,l < 2^t} e^{\f{-2\pi i kl}{2^t}}e^{2\pi i \varphi k}\ket{l}\nn\\
& = \f{1}{2^t}\sum_{0 \leq k,l < 2^t}\lp e^{2\pi i(\varphi- l/2^t)} \rp^k \ket{l}
\end{align}
by definition. Note that when $\varphi$ can be expressed using exactly $t$ bits we are reduced to the simpler version of the problem described above. \\

Let $\al_l$ be the amplitude of $\ket{(b+l) \mod 2^t}$, i.e., 
\begin{align}
\alpha_l \equiv \f{1}{2^t}\sum_{0 \leq k < 2^t} \lp e^{2\pi i (\varphi - (b+l)/2^t)} \rp^k.
\end{align}
This is a partial sum of a geometric series. It is straightforward to show that
\begin{align}
\alpha_l = \f{1}{2^t}\lp \f{1 - e^{2\pi i (2^t \varphi - (b+l))}}{1 - e^{2\pi i (\varphi - (b+l)/2^t)}} \rp = \f{1}{2^t}\lp \f{1 - e^{2\pi i (2^t \delta - l)}}{1 - e^{2\pi i (\delta - l/2^t)}} \rp.
\end{align}
Suppose the outcome of the final measurement is $m$. We want to bound the probability that $\abs{m-b} > \epsilon$ (where $\epsilon$ is the tolerance), i.e., we don't want the probably that $m$ is far from $b$ to be large. Well, the probability of observing such an $m$ is given by
\begin{align}
P(\abs{m-b} > \epsilon) = \sum_{-2^{t-1} < l \leq -(\epsilon+1)} \abs{\al_l}^2 + \sum_{e+1 \leq l \leq 2^{t-1}}\abs{\al_l}^2.
\end{align}
For any real $\theta$, we have $\abs{1 - \exp(i\theta)} \leq 2$, so 
\begin{align}
\abs{\al_l} \leq \f{2}{2^t \abs{1 - e^{2\pi i (\delta - l/2^)}}}.
\end{align}
Further, provided $-\pi \leq \theta \leq \pi$, we have $\abs{1 - \exp(i\theta)} \geq 2\abs{\theta}/\pi$. Also, when $-2^{t-1} < l \leq 2^{t-1}$ we have $-\pi \leq 2\pi (\delta - l/2^t) \leq \pi$, so we have
\begin{align}
\abs{\al_l} \leq \f{1}{2^{t+1}(\delta - l/2^t)}.
\end{align}
With this we have
\begin{align}
p(\abs{m-b} > \epsilon) &\leq \f{1}{4}\lp \sum_{l = -2^{t-1}+1}^{-(\epsilon+1)} \f{1}{(l - 2^t \delta)^2} + \sum^{2^{t-1}}_{l = \epsilon+1} \f{1}{(l - 2^t \delta)^2}  \rp\nn\\
&\leq \f{1}{4}\lp \sum_{l = -2^{t-1}+1}^{-(\epsilon+1)} \f{1}{(l - 1)^2} + \sum^{2^{t-1}}_{l = \epsilon+1} \f{1}{(l - 1)^2}   \rp, \quad 0 \leq 2^t\delta < 1\nn\\
&\leq \f{1}{2}\sum^{2^{t-1}-1}_{l=\epsilon} \f{1}{l^2}\nn\\
&\leq \f{1}{2}\int^{2^{t-1}-1}_{\epsilon-1} \f{1}{l^2}\,dl\nn\\
&= \f{1}{2(\epsilon-1)}.
\end{align}
Suppose we wish to approximate $\varphi$ to an accuracy $2^{-n}$, i.e., we choose $\epsilon = 2^{t-n}-1$. By using $t =n+p$ qubits, we see that the probability of obtaining an approximation correct to this accuracy is at least 
\begin{align}
1-\f{1}{2(\epsilon-1)} = 1 - \f{1}{2(2^p - 2)}.
\end{align}
So, to successfully obtain $\varphi$ accurate to $n$ bits with probability of success at least $1-\epsilon$ we choose 
\begin{align}
t = n + \left\lceil \log \lp 2 + \f{1}{2\epsilon} \rp \right\rceil.
\end{align}

Now here's a catch: to use QPE, we need the eigenstate $\ket{u}$. What if we don't know how to prepare such a state? Suppose that we prepare some other state $\ket{\psi}$ in place of $\ket{u}$. We can write
\begin{align}
\ket{\psi} = \sum_u c_u \ket{u}
\end{align}
where $\ket{u}$ are the eigenstates of $U$. Suppose the eigenstate $\ket{u}$ has the eigenvalue $e^{2\pi i \varphi_u}$. Running the QPE on $\ket{\psi}$ will give
\begin{align}
\ket{\psi} \stackrel{\mbox{QPE}}{\to} \sum_u c_u \ket{\tilde{\varphi_u}}\ket{u},
\end{align}
where $\tilde{\varphi_u}$ is an estimation for $\varphi_u$. Reading out the first register gives us a good approximation for $\varphi_u$, where $u$ is chosen at random with probability $\abs{c_n}^2$. This procedure allows us to avoid preparing a possibly unknown eigenstate, but at the cost of introducing some additional randomness into the algorithm. It turns out that if $t$ is chosen according to the minimization procedure above, then the probability for measuring $\varphi_u$ accurate to $n$ bits at the conclusion of the phase estimation algorithm is at least $\abs{c_u}^2(1-\epsilon)$.







\subsection{The algorithm}
QPE solves a problem which is both non-trivial and interesting from a physical point of view. It allows us to estimate the eigenvalue associated to a given eigenvector of a unitary operator. It turns our that other interesting problems can be reduced to phase estimation, which makes QPE particularly useful.\\

\underline{Here's the algorithm:}
\begin{itemize}
	\item \textbf{Inputs:} (1) An oracle which performs a controlled-$\U^j$ operation, (2) an eigenstate $\ket{u}$ of $\U$ with eigenvalue $e^{2\pi i \varphi_u}$, and (3) $t = n + \left\lceil \log \lp 2 + 1/2\epsilon \rp \right \rceil$ qubits initialized to $\ket{0}$.
	
	
	\item \textbf{Outputs:} An $n$-bit approximation $\tilde\varphi_u$ of $\varphi_u$. 
	
	\item \textbf{Runtime:} $\mathcal{O}(t^2)$ operations (due to th inverse QFT) and one call to controlled-$\U^j$ oracle. Succeeds with probability at least $1-\epsilon$. 
	
	\item \textbf{Procedure:} 
	\begin{itemize}
		\item  Initial state: $\ket{0}\ket{u}$
		
		\item Create superposition: $\to (1/2^{t/2})\sum_{0 \leq j < 2^t}\ket{j}\ket{u}$.
		
		
		\item Apply the oracle:\\ $\to(1/2^{t/2})\sum_{0 \leq j < 2^t}\ket{j}\U^j\ket{u} = (1/2^{t/2})\sum_{0 \leq j < 2^t}e^{2\pi i j\varphi_u}\ket{j}\ket{u}$
		
		
		\item Apply inverse QFT: $\to \ket{\tilde\varphi_u}\ket{u}$. 
		
		\item Measure the first register: $\to \tilde \varphi_u$.
	\end{itemize}
\end{itemize}


The algorithm has a wide range of applications, including order-finding and factorization. 










\newpage


\section{Quantum Variational Eigensolver (QVE)}

In this section we're looking this \href{https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4124861/pdf/ncomms5213.pdf}{\underline{paper}} on the quantum variational eigensolver (QVE). \\


For quantum systems, where the physical dimension grows exponentially, finding the eigenvalues of certain operators is one of many intractable problem and remains a fundamental challenge.  QPE efficiently finds the eigenvalue of a given eigenvector but requires fully coherent evolution. This paper presents an alternative approach that greatly reduces the requirements for coherent evolution and combine this method with a new approach to state preparation based on ansatz and classical optimization. \\

As we have seen, quantum approaches to finding eigenvalues
have previously relied on the quantum phase estimation (QPE)
algorithm. The QPE algorithm offers an exponential speedup
over classical methods and requires a number of quantum operations $\mathcal{O}(p^{-1})$ to obtain an estimate with precision $p$. Recall that in the standard formulation of QPE, one assumes the eigenvector $\ket{\psi}$ of a Hermitian operator $\had$ is given as input and the problem is to determine the corresponding eigenvalue $\lambda$. The time the
quantum computer must remain coherent is determined by the
necessity of $\mathcal{O}(p^{-1})$ successive applications of $e^{-i \had t}$, each of which can require on the order of millions or billions of quantum gates for practical applications, as compared to the tens to hndreds og gates achievable in the short term. \\

This paper presents an alternative to QPE that significantly reduces the requirements for coherent evolution. This alternative method is given by a quantum processing unit which efficiently calculates the expectation value of a Hamiltonian, providing an exponential speedup over exact diagonalization, the only known exact solution to the poblem on a traditional computer. In conjunction with a classical optimization algorithm, this approach reduces the requirement for coherent evolution of the quantum state, making more efficient use of quantum resources, and may offer an alternative route to practical quantum-enhanced computation. 





\subsection{Quantum Expectation Estimation (QEE)}


QEE computes the expectation value of a given Hamiltonian $\had$ for an input state $\ket{\psi}$. This is done via the decomposition:
\begin{align}
\had = \sum_{i\alpha}h^i_\alpha \sigma^i_\alpha + \sum_{ij\al\be}h^{ij}_{\al\be}\sigma^i_\alpha \sigma^j_\be + \dots
\end{align}
for real $h$, where Roman indices identify the subsystem on which the operator acts, and Greek indices identify the Pauli operator. For example, $\alpha = x$. This is possible because \textit{products of Pauli matrices form a basis for the set of Hermitian matrices of dimensions that are powers of 2}. In more generality, this is possible because we have the \textit{Pauli group} $G_n$, which is the group generated by the operators applied to each of $n$ qubits in th tensor product Hilbert space $(\mathbb{C}^2)^{\otimes n}$. \\

In any case, I digress. By linearity, we have
\begin{align}
\langle \had \rangle = \sum_{i\al} h^i_\al \langle \sigma^i_\al \rangle + \sum_{ij\al\be} h^{ij}_{\al\be} \langle \sigma^i_\al \sigma^j_\be  + \dot \rangle 
\end{align}
Just as in QPE, we only consider Hamiltonians that can be written as a  polynomial number of terms, wrt the system size. This class of Hamiltonians encompasses a wide range of physical system, so this requirement is not too strict. In fact, Hamiltonians of this kind include the electronic structure Hamiltonian of quantum chemistry, the quantum Ising Model, the Heisenberg Model, matrices that are well approximate as a sum of $n$-fold tensor products, and more generally any $k$-sparse Hamiltonian without evident tensor product structure.\\

\textbf{Remark:} Sparse Hamiltonians are important because it is possible to decompose $\had$ into $\sum_k \had_k$ where $\had_k$'s all commute (making diagonalization straightforward). If the matrix is sparse, then we shouldn't need too many distinct $\had_k$'s. Then we can simulate the Hamiltonian evolution 
\begin{align}
e^{-i\had t} = \prod e^{-i \had_m \delta t}
\end{align}
where $t =N \delta t$. Now, what do we mean by ``sparse?'' Sparse means that there are at most $d$ nonzero entries per row, $d = \text{poly}(\log N)$. In any given row, the location of the $j$th nonzero entry and its value can be compute efficiently (or is given by a black box). Sparse Hamiltonians are different from ``Local'' Hamiltonians, where $\had = \sum_j \had_j$ where each $\had_j$ acts on $\mathcal{O}(1)$ qubits. \qed\\



In any case, I digress again. The evaluation of $\langle \had \rangle $ reduces to the sum of a polynomial number of expectation values of simple Pauli operators for a quantum state $\ket{\psi}$,  multiplied by some real constants. A quantum device can efficiently evaluate the expectation value of a tensor product of arbitrary number of simple Pauli operators. Therefore, with an $n$-qubit state we can efficient evaluate the expectation value of this $2^n \times 2^n$ Hamiltonian.\\

This is possible with a classical computer, but it suffers from the $N$-representability problem. The power of the quantum approach derives from the fact that quantum hardware can store a global quantum state with exponentially fewer resources than required by classical hardware, and as a result the $N$-representability problem does not arise. \\


The expectation value of a tensor product of an arbitrary
number of Pauli operators can be estimated by local measurement of each qubit (this can be found somewhere in Mike and Ike). Such independent measurements can be performed in parallel, incurring a constant cost in time. Furthermore, since these operators are normalized an finite-dimensional, their spectra are bounded. As a result, each
\begin{align}
\langle H^m_i\rangle = h^{ij\dots}_{\al\be\dots} \langle \sigma^i_\al \otimes \sigma^i_\be \otimes \dots \rangle  
\end{align}
can be estimated to a precision $p$ of an individual element with coefficient $h$, which is an arbitrary element from the set of constants $\{ h^{ij\dots}_{\al\be\dots}\}$ at a cost of $\mathcal{O}(\abs{h_\text{max}}^2 Mp^{-2})$ repetitions. $M$ is the number of terms in the decomposition of the Hamiltonian and $h_\text{max}$ is the coefficient with maximum norm in the decomposition of the Hamiltonian. \\


The advantage of this approach is that the coherence time to make a single measurement after preparing
the state is $\mathcal{O}(1)$. Conversely, the disadvantage of this approach with respect to QPE is the scaling in the total number of
operations, as a function of the desired precision is quadratically
worse $\mathcal{O}(p^{-2})$ versus $\mathcal{O}(p^{-1})$.  Moreover, this scaling will also reflect the number of state preparation repetitions required, whereas in QPE the number of state preparation steps is constant. In essence, we dramatically reduce the coherence time requirement while maintaining an exponential advantage over
the classical case, by adding a polynomial number of repetitions
with respect to QPE.


\newpage


\subsection{Quantum Variational Eigensolver (QVE)}

The procedure outlined above
replaces the long coherent evolution required by QPE by many
short coherent evolutions. In both QPE and QEE we require a
good approximation to the ground-state wavefunction to compute the ground-state eigenvalue, and we now consider this
problem. The quantum variational eigensolver (QVE) algorithm
is a variational method to prepare the eigenstate and, by
exploiting QEE, requires short coherent evolution. 

\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.2]{qvei}
\end{figure}

It is well known that the eigenvalue problem for an observable
represented by an operator H can be restated as a variational
problem on the Rayleigh–Ritz quotient such that the eigenvector $\ket{\psi}$ corresponding to the lowest eigenvalue is the one hat minimizes
\begin{align}
\f{\bra{\psi} \had \ket{\psi}}{\braket{\psi}}
\end{align}
By varying the experimental parameters in the preparation of $\ket{\psi}$ and computing the Rayleigh–Ritz quotient using QEE as
a subroutine in a classical minimization, one may prepare
unknown eigenvectors. At the termination of the algorithm, a
simple prescription for the reconstruction of the eigenvector is
stored in the final set of experimental parameters that define $\ket{\psi}$. \\

Note that if a quantum state is characterized by an exponentially large
number of parameters, it cannot be prepared with a polynomial
number of operations. The set of efficiently preparable states are
therefore characterized by polynomially many parameters, and
we choose a particular set of ansatz states of this type.  Under
these conditions, a classical search algorithm on the experimental
parameters that define $\ket{\psi}$ needs only explore a polynomial
number of dimensions -- requirement for the search to be
efficient.\\

One example of a quantum state parameterized by a
polynomial number of parameters for which there is no known
efficient classical implementation is the unitary coupled cluster
ansatz
\begin{align}
\ket{\Psi} = e^{T - T^\dagger}\ket{\Phi}_\text{ref}
\end{align}
where $\ket{\Phi}_\text{ref}$ is some reference state, usually the Hartree Fock ground state, and $T$ is the cluster operator for an $N$ electron system, defined by
\begin{align}
T =T_1 + T_2 + \dots + T_N,
\end{align}
where
\begin{align}
T_1 = \sum_{pr}t^r_p \hat{a}_p^\dagger\hat{a}_r, \quad T_2 = \sum_{pqrs}t^{rs}_{pq}\hat{a}_p^\dagger \hat{a}_q^\dagger\hat{a}_r \hat{a}_s,\quad \dots
\end{align}
By construction, $(T - T^\dagger)$ is anti-Hermitian, and exponentiation maps it to a unitary operator $e^(T - T^\dagger)$. \\

For any fixed excitation level $k$, the reduced cluster operator is written as
\begin{align}
T^{(k)} = \sum^k_{i=1}T_i.
\end{align}
This state may be prepared efficiently on a quantum
device. The reduced anti-hermitian cluster operator  $(T^{(k)} - T^{(k)\dagger})$ is the sum of a polynomial number of terms—namely, it contains
a number of terms $\mathcal{O}(N^k(M-N)^k)$, where $M$ is the number of
single-particle orbitals. By defining an effective Hermitian
Hamiltonian $\had = i(T^{(k)} - T^{(k)\dagger})$ and performing the Jordan–
Wigner transformation to reach a Hamiltonian that acts on the
space of qubits $\hat{\had}$, we are left with a Hamiltonian that is a sum of polynomially many products of Pauli operators. The problem then reduces to the quantum simulation of this effective Hamiltonian, $\hat{\had}$ which can be one in polynomial time using known procedures. 





















\newpage




\section{Some Models in Quantum Statistical \& Condensed-Matter Physics}

In this section we will be looking at some of the most well-known and most studied models in statistical and quantum many-body mechanics. I'll be sourcing the material from various places, so I'll try my best to cite all the sources. 


\subsection{The Ising Model}
In this section we'll look at the Ising model from the mathematical physics perspective. I'll try to add connections to physics to the best of my ability. \\

Anyway, the Ising model is the simplest and most famous spin system model to study phase transition. It was introduced in 1925 by Ernst Ising in his PhD thesis. He completely solved the model in one dimension, but mistakenly inferred behaviors in higher dimensions. \\

\subsubsection{Basic definitions}

Let a graph $G = (V,E)$ be given, with state space $\Omega = \{ -1,+1 \}^V$ and elements $\sigma = \{ \sigma_x \}_{x\in V}$. The variable $\sigma_x \in \{ -1,+1\}$ is called the spin at vertex $x$. This is thus a spin system. Under some re-definitions we can think of this as a lattice gas model, but let's not worry about that. \\

The energy functional, or the Hamiltonian, of the system is given by
\begin{align}
\had(\sigma) = -J \sum_{(x,y)\in E} \sigma_x \sigma_y
\end{align}
where $J$ is a real constant, called the interaction strength. In this formulation, $\had$ is built up from the local energy terms $-J \sigma_x \sigma_y$, i.e., the model is defined in terms of nearest-neighbor interactions. Next, we introduce the inverse temperature parameter $\beta \sim 1/T$ and consider the probability measure on $\Omega$:
\begin{align}
\mu(\sigma) = \f{e^{-\beta \had(\sigma)}}{\sum_{\sigma \in \Omega} e^{-\beta\had}} \equiv \f{e^{-\beta \had(\sigma)}}{Z_\beta}
\end{align}
where of course $\Z_\beta$ is called the partition function, which contains all information about the system. $\mu = \mu_\beta$ is called the Gibbs measure. \\

When $J> 0$, we have the ferromagnetic case in which the minimum of $\had(\sigma)$ and so the maximum of $\mu(\sigma)$ are attained at $\sigma \equiv \text{const}$. More generally, spins like to agree along edges, and for $\beta \to \infty$ the measure concentrates on the two constant configurations.s \\

When $J < 0$, we have antiferromagnetism where neighboring spins like to differ. This case is hard, and we're not going to worry about it. \\

For any function $f: \Omega \to \mathbb{R}$, we have the expectation and variance:
\begin{align}
\mu(f) = \langle f \rangle = \sum_{\sigma\in \Omega} \mu(\sigma) f(\sigma), \quad \text{Var}(f) = \mu[f^2] - \mu[f]^2.
\end{align}
So in our case, ``$\mu$'' is like $E[\cdot]$, denoting the expectation value. 


\subsubsection{Thermodynamic objects}

Recall the following definitions
\begin{defn}[Free Energy]
	The free energy of a system with inverse temperature $\beta$ and partition function $Z_\beta$ is given by
	\begin{align}
	F(\beta) = -\f{1}{\beta}\log Z_\beta.
	\end{align}
\end{defn}

We note that 
\begin{align}
Z_\beta = e^{-\beta F(\beta)}
\end{align}
and
\begin{align}
\p_\beta F(\beta) = \mu(\had) = \langle \had \rangle.
\end{align}

\begin{defn}[Specific Heat]
	\begin{align}
	C(\beta) = \f{d}{d(1/\beta)} \mu(\had) = -\beta^2 \f{d}{d\beta^2}F(\beta).
	\end{align}
\end{defn}
We can show that the specific heat is related to the fluctuation of the system via
\begin{align}
C(\beta) = \beta^2 \text{Var}(\had).
\end{align}



Next we modify the Hamiltonian by introducing an external magnetic field:
\begin{align}
\had(\sigma, h) = -J \sum_{(x,y)\in E} \sigma_x \sigma_y - \sum \sum_{x\in V} h_x \sigma_x.
\end{align}
Here $h_x$ are real numbers and sometimes $h_x$ is identically constant. If $h> 0$, $\had$ has a unique minimum at $\sigma = +1$. A very difficult famous problem is the case when the $h_x$'s are iid r.v. \\

Now, 
\begin{align}
F(\beta,h) = -\f{1}{\beta} \log Z(\beta,h).
\end{align}
This is a nice smooth function for any finite graph $G$. With this we consider the magnetization. 

\begin{defn}[Magnetization]
	\begin{align}
	M(\beta, h) = \f{1}{\abs{V}} \sum_{x\in V}\mu(\sigma_x) = -\f{1}{\abs{V}} \p_h F(\beta,h).
	\end{align}
\end{defn}



\subsubsection{Properties}







































\subsection{Transverse-field Ising model}



\subsection{The Hubbard Model}



\subsection{The Kitaev's Toric model}











\newpage




\section{Locality in Quantum Systems}

In this section we will look at this \href{https://arxiv.org/pdf/1008.5137.pdf}{\underline{paper}} by Matthew Hastings. This is actually a compilation of lecture notes focusing on the application of ideas of locality, in particular Lieb-Robinson bounds, to quantum many-body systems. For our purposes, we will only look at a few topics included. 
\begin{itemize}
	\item We want to understand ``locality'' in the context of quantum simulation. What, precisely, does it mean for a Hamiltonian to be ''local''? 
	
	\item What are the Lieb-Robinson bounds? 
	
	\item What is the Lieb-Schulz-Mattis theorem?
	
	\item What are topologically ordered states? 
\end{itemize}





\subsection{Introduction \& Notation}

The basic problem studied in quantum many-body theory is to find the properties of the ground state of a given
Hamiltonian. Expressed mathematically, the Hamiltonian $\had$ is a Hermitian matrix, and the ground state, which we write $\psi_0$, is an eigenvector of this matrix with the lowest eigenvalue.  The properties we are interested in studying
are expectation values of various observables: given a Hermitian matrix $O$, we would like to compute the expectation value $\langle \psi_0 , O \psi_0\rangle \equiv \langle O \rangle$. \\

While the method of ``exact diagonalization'' on a computer is an important technique in studying
quantum systems, it is only a small part of how physical problems are studied. The feature that distinguishes the study of quantum many-body systems is \textit{locality} of interactions. Consider a typical Hamiltonian, such as the one-dimensional transverse field Ising model TFIM):
\begin{align}
\had = -J \sum^{N-1}_{i=1}S^z_i S^z_{i+1} + B \sum^{N}_{i=1}S_i^x.
\end{align}
The very notation implicitly assumes local interactions. To be precise, throughout this section, we only consider quantum systems on a finite size lattice. We associate a $D$ dimensional Hilbert space with each lattice site, and the Hilbert space of the whole system is tensor product of these spaces. We use $N$ to represent the number of lattice sites, so that the Hilbert space on which a Hamiltonian such as the one above is defined in $D^N$ dimensions. A term such as $S^z_i S^z_{i+1}$ is short hand for $I_1 \otimes I_2 \otimes \dots \otimes S_i^z \otimes S^z_{i+1} \otimes I_{i+2} \otimes \dots \otimes I_N$, where $I_j$ is the identity operator on site $j$.\\

We let $i,j,k,\dots$ denote the lattice site, and $X,Y,Z$ to denote sets of lattice sites. We se $\Lambda$ to denote the set of all lattice sites. We use $\abs{X}$ to denote the cardinality of a set $X$. \\

An operator $O$ is supported on a set $A$ if we can write $O$ as a tensor product of two operators $O = I_{\Lambda\setminus A} \otimes P$, where $I_{\Lambda \setminus A}$ is the identity operator on the sites not in $A$ and $P$ is some operator defined on the $D^{\abs{A}}$ dimensional Hilbert space on the set $A$. For example, $S^z_i \otimes S^z_{i+1}$ is supported on the set $\{ i, i+1\}$. \\

We let $\norm{O}$ denote the ``operator norm'' for $O$. If $O$ is Hermitian, the operator norm is the equal to the absolute value of the largest eigenvalue of $O$. For arbitrary operators $O$, the operator norm is given by
\begin{align}
\norm{O} = \max_{\psi, \abs{\psi} = 1}  \abs{O \psi},
\end{align}
which one might have seen in real/functional analysis. \\

Using 2 simple assumptions: that the Hamiltonian has local interactions and that the Hamiltonian has a spectral gap, we will be able to prove a wide variety of results about the ground state of the Hamiltonian. We will consider two cases of a spectral gap: one where $E_1  - E_0 \geq \Delta E$, where $\Delta E$ is called the ``spectral gap.'' In the other case, we consider a Hamiltonian with several degenerate or approximately degenerate low energy states. \\

We will begin by considering properties of correlations in these systems, and prove an exponential decay of correlation functions. We then consider how the ground state of the system changes under a change in the Hamiltonian, and use this to prove a non-relativistic variant of Goldstone's Theorem. Finally, we apply these techniques to more interesting systems with topological order, beginning with the Lieb-Schultz-Mattis theorem However, before we can do any of this, we need to more precisely define the locality properties of the Hamiltonian and to prove a set of bounds on te propagation of information through the system called the Lieb-Robinson bounds. 



\subsection{Locality \& The Lieb-Robinson bounds}


To define locality, we need a notion of distance, so we introduce a metric on the lattice $d(i,j)$. One may consider the Manhattan or Euclidean metric. It's natural to consider the metric 
\begin{align}
d(i,j) = \min_n \abs{i-j + nN}
\end{align}
for a one-dimensional system with periodic boundary conditions and $N$ sites. Next, we define the distance between two sets $A,B$ to be
\begin{align}
d(A,B) = \min_{a\in A, b\in B}d(a,b).
\end{align}
The diameter of a set $A$ is given by
\begin{align}
\text{diam}(A) = \max_{i,j\in A}d(i,j).
\end{align}
Next we consider Hamiltonians
\begin{align}
\had = \sum_Z H_Z
\end{align}
where each $H_Z$ is support on $Z$. We are interested in studying problems in which $\abs{H_Z}$ decays rapidly with $\text{diam}(Z)$. \\


(\textit{to be continued})





\newpage


\section{Quantum Approximate Optimization Algorithm (QAOA)}

\newpage

\section{Quantum Variational Factorization (QVF)}

\newpage



\section{Variational Thermal Quantum Simulation via Thermofield Double States}



\subsection{Thermofield Double States (TFD)}


\newpage



\section{Generation of TFD and Critical Ground States with a Quantum Computer }



\newpage






\section{Efficient variational simulation of non-trivial quantum states}

   
In this section we look at this \href{https://arxiv.org/pdf/1803.00026.pdf}{\underline{paper}}, with title '' Efficient variational simulation of non-trivial quantum states.'' The idea is the following. The paper provides an efficient and general route for preparing \textit{non-trivial quantum states} (defined later) that are NOT adiabatically connected to unentangled product states. This approach will be a hybrid of quantum and classical variational protocols that incorporates a feedback loop between a quantum simulator and a classical computer. This turns out to be realizable on near-term quantum devices of synthetic quantum systems. \\

The paper provides explicit protocols which prepare with \textit{perfect fidelities} the following non-trivial quantum states: GHZ, quantum critical state, and topologically ordered state, with $L=2p$ variational parameters and physical runtimes $T \sim L$. \\

The paper also conjectures and numerically support (so, not analytically support) that the protocol can prepare with perfect fidelity and similar operational costs the ground state of every point in the 1D transverse field Ising model phase diagram (we will define this later). 




\subsection{Introduction}



The paper demonstrates the efficient preparation of certain non-trivial states using a variational hybrid quantum-classical simulation, which utilizes resources of a quantum simulator and a classical computer in a feedback loop. Given Hamiltonians realizable on a quantum simulator, a quantum state $\ket{\psi}(\bm{\gamma}, \bm{\beta})$ is produced with $(\bm{\gamma}, \bm\beta) \equiv (\gamma_1,\dots,\gamma_p, \beta_1,\dots,\beta_p)$ parameterizing a finite set of $2p$ variational angles (or times) that the Hamiltonians are run for. \\

A cost function, usually taken to be the energy of some target Hamiltonian, is then evaluated within the resulting state and optimized for in a classical computer, which yields a new set of $2p$ angles to be implemented to be fed back into the quantum simulator. \\

The entire process is iterated to convergence. \\


An example of an approach like this is the Quantum Approximate Optimization Algorithm (QAOA). There are a number of properties which make the variational quantum simulation appealing: it can be run on any quantum device (digital/tunable analog). The feedback loop of the protocol allows one to mitigate systematic errors that might be present in the experiment setups. \\

This variational quantum-classical simulations on local, uniform Hamiltonians can target GHZ state, the critical state of the 1D transverse field Ising model (TFIM), and the ground state of the 2d Toric code, all with \textit{perfect fidelity}. \\

Later, we will see that the entire ground state phase diagram of the TFIM can be produced with perfect fidelities and similar operational costs. This is only conjectured and numerically supported. \\

Finally, we will look at the preparation of the ground states of the antiferromagnetic (AFM) Heisenberg chains. We will see that the protocol is able to achieve them efficiently and with \textit{very good} fidelities (so, not perfect, but very good).



\subsection{Non-trivial quantum states}


In any case, let's look at some non-trivial quantum states. What are non-trivial quantum states? Consider a target state $\ket{\psi_t}$ and an untangled product state $\ket{\psi_u}$, both defined on a system with linear dimension $L$. $\ket{\psi_t}$ is said to be \textit{non-trivial} if there does not exist a \textit{local} unitary circuit $\U$ of finite depth that connects the two: $\ket{\psi_t} = \U \ket{\psi_u}$. Instead, the depth of a local unitary circuit connecting the two must be at least $\mathcal{O}(L^\al)$. \\

Intuitively, non-trivial states have entanglement patterns fundamentally different from product states, i.e., these are states that are very ``far'' from being product states. \\

From the perspective of local Hamiltonians and gaps, such states are separated from product states by a gap-closing phase transition in the thermodynamic limit, and thus preparing them with the quantum adiabatic algorithm is hard.\\


\begin{exmp}[GHZ states are non-trivial.] 
	Let's first look at what GHZ states are. GHZ stands for Greenberger-Horne-Zeilinger. GHZ is a certain type of entangled quantum state that involved at least three subsystems. Extremely non-classical properties of the state have been observed. \\
	
	\begin{defn}
		The GHZ state is an entangled quantum state of $M>2$ subsystems. If each system has dimension $d$, i.e., the local Hilbert space is isomorphic to $\mathbb{C}^d$, then th total Hilbert space of $M$ partite system is $\had_\text{tot} = (\mathbb{C}^d)^{\otimes M}$. This GHZ state is also named as $M$-partite qubit GHZ state, which reads
		\begin{align}
		\ket{\text{GHZ}} = \f{1}{\sqrt{d}}\sum^{d-1}_{i=0}\bigotimes \ket{i}.
		\end{align}
		In the case of each of the subsystems being two-dimensional, that is for qubits, it reads
		\begin{align}
		\ket{\text{GHZ}} = \f{\ket{0}^{\otimes M} + \ket{1}^{\otimes M}}{\sqrt{2}}.
		\end{align}
		The GHZ state is a quantum superposition of all qubits being in 0 with all being in 1. The GHZ state is a maximally entangled quantum state. The simplest being $\ket{\text{GHZ}} = (\ket{000} + \ket{111})/2$.\\
		
		GHZ states are in contrast to the so-called ``cat states,'' which are quantum superpositions of two macroscopically distinct states. Note that a cat state, unlike GHZ states, are not necessarily an entangled state because a cat state could be of one or more modes or particles. 
	\end{defn}
	
	
	
	
	Let us look at why the GHZ states are non-trivial. Consider the GHZ state:
	\begin{align}
	\ket{\text{GHZ}} = \f{1}{\sqrt{2}}\lp \bigotimes \ket{Z=1} + \bigotimes \ket{Z=-1} \rp.
	\end{align}
	Suppose there exists a local, finite-depth unitary $\U$ that takes the completely polarized product state $\ket{+} \equiv \bigotimes \ket{X=1}$ to $\ket{\text{GHZ}}$. Then due to locality, there exists a Lieb-Robinson bound which limits the spread of information and entanglement under this evolution, implying that $\U$ can  only generate a finite correlation length $\xi$ for the final state.  Measuring a long-range spin-spin correlator gives
	\begin{align}
	\bra{\text{GHZ}} Z_i Z_j \ket{\text{GHZ}} = 1,
	\end{align}
	while on the other hand the same quantity can be expressed as
	\begin{align}
	1 = \bra{+}\U^\dagger Z_i \U \U^\dagger Z_j \U \ket{+} ,
	\end{align}
	which in the limit $\abs{i-j} \gg \xi$ decomposes as
	\begin{align}
	\bra{+}\U^\dagger Z_i \U \ket{+} \bra{+} \U^\dagger Z_j \U \ket{+} = \bra{\text{GHZ}} Z_i \ket{\text{GHZ}} \bra{\text{GHZ}} Z_j \ket{\text{GHZ}} = 0\neq 1
	\end{align}
	which is a contradiction.
\end{exmp} 


Similar arguments apply to critical states which have polar-law correlations, and topologically ordered states which have long-range correlations in loop operators and non-zero topological entanglement entropy. 





 

\subsection{Variational Quantum-Classical Simulation (VQCS)}


Now, we define the variational quantum-classical simulation (VQCS). This is motivated by the QAOA and the VQE. \\


The protocol is envisioned to be run on a quantum simulator that can realize certain interactions between qubits and single qubit rotations, which we denote schematically by $H_1 , H_2$. The aim is to produce a good approximation to the ground state of a target many-body Hamiltonian $H_T$, which we will assume to be a linear combination of $H_1$ and $H_2$. \\

The VQCS begins with the ground state of $H_1$, denoted $\ket{\psi_1}$, and evolves it with the following sequence alternating between $H_1, H_2$:
\begin{align}
\ket{\psi(\bm\gamma, \bm\beta)}_p = e^{-i\beta_p H_1}e^{-i\gamma_p H_2}\dots e^{-i \beta_1 H_1}e^{-i \gamma_1 H_2}\ket{\psi_1}.
\end{align}
This is motivated by the quantum adiabatic algorithm, which works because of the adiabatic theorem. Basically the idea is that we are starting out in a ground state of some Hamiltonian $H_1$. The goal is to evolve the Hamiltonian $H_1$ slowly so that it turns into the target Hamiltonian $H_2$. The ground state of $H_1$ as a result will turn into  the ground state of $H_2$ as well. Now, the way we slowly vary from one to another is by letting the ground state of $H_1$ evolve slowly under sequential linear combinations of $H_1$ and $H_2$. This means the initial ground state is going to by evolved by
\begin{align}
e^{i(H_1 + \epsilon H_2)}.
\end{align} 
Now, we don't want to do this. Rather, we want to use Trotterization to factor out and turn the above operator into 
\begin{align}
\sim e^{iH_1}e^{iH_2}
\end{align}
Of course, we want to do this sequentially, so the result is that we have alternating evolution operator as above. \\

Of course, there are many ways to get to the $\ket{\psi(\bm\gamma, \bm\beta)}$ state. But this just happens to be a pretty common way to do that.\\






For a fixed integer $p$, there are $2p$ variational agles 
\begin{align}
(\bm\gamma, \bm\beta) \equiv (\gamma_1,\dots,\gamma_p, \beta_1,\dots, \beta_p).
\end{align}

A cost function, such as the energy expectation value of the target Hamiltonian
\begin{align}
F_p(\bm\gamma, \bm\beta) = _p\bra{ \psi(\bm\gamma,\bm\beta)} H_T \ket{\psi(\bm\gamma,\bm\beta)}_p,
\end{align}
is then evaluated. A classical computer then performs an optimization to produce a new set of angles (or times) $(\bm\gamma, \bm\beta)$ which are then fed back into the quantum simulator and the process is repeated until the cost function is desirably minimized. The state corresponding to these optimal angles is therefore the optimal state that can be prepared by the protocol given this cost function. \\


As the VQCS is envisioned to be run on near-term quantum simulators which are inherently noisy (NISQ), the physical runtimes $t = \sum^{p=L/2}_i (\gamma_i + \beta_i)$ of the VQCS constitute an important measure of the feasible of the protocol -- in general, shorter runtimes lead to less noise encountered and a better implementation.  













\subsection{Using VQCS to prepare nontrivial quantum states}

In this section we look at how the VQCS generates some non-trivial quantum states.


\subsubsection{GHZ state}

Recall the GHZ state looks like
\begin{align}
\ket{\text{GHZ}} = \f{1}{\sqrt{2}}\lp \bigotimes \ket{Z=1} + \bigotimes \ket{Z = -1} \rp.
\end{align} 
So it is a superposition of two possible ways that the system is completely aligned. The GHZ state can be taken to be the ground state of the following Hamiltonian
\begin{align}
\had_T = \sum^L_{i=1}Z_i Z_{i+1},
\end{align}
in the symmetry sector $S = \prod^L_{i}X_i =1$. Just to clarify the notation, $Z_i$ gives $+1$ when the state is ``up'', i.e., if the state is $\ket{\uparrow}$. This means the Hamiltonian ``favors'' (i.e., achieve lowest energy) when everybody is aligned. Further, for simplicity we assume periodic boundary conditions.\\

We choose in this case $H_1 = -\sum_i X_i$ with the product ground state $\ket{\psi_1} = \ket{+} = \bigotimes^L_{i}\ket{+}_i$. Recall that on the Bloch sphere, the $\ket{+}$ and $\ket{-}$ states lie in the horizontal plane, and are eigenstates of the $X$ operator, i.e., $X\ket{\pm} = \pm 1 \ket{\pm}$. We also know that $H_1$ has a straightforward implementation in both the analog and digital settings. \\

Next, choose $H_2 = \had_T$. On a digital quantum simulator, this can be achieved using elementary two-site gates $Z_i Z_{i+1}$: 
\begin{align}
e^{-i\gamma H_2} = \prod^{L/2}_{i=1} e^{-i\gamma Z_{2i}Z_{2i+1}}   \prod^{L/2}_{i=1} e^{-i\gamma Z_{2i-1}Z_{2i}}
\end{align}
such that each unitary $e^{-i\gamma H_2}$ in the VQCS protocol can be considered as a depth-2 quantum circuit, so that overall the VQCS$_p$ can be realized as a quantum circuit with depth at most $3p$. 
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.3]{vqcs}
\end{figure}
If we start in $\ket{+}$ then since the VQCS respects this symmetry, optimization of $F_p(\bm\gamma,\bm\beta)$ as $p\to \infty$ yields the GHZ state, with $\lim_{p\to \infty} F_p(\bm\gamma,\bm\beta)/L = -1$. \\


Let's see how this performs numerically. To do this, we implement the VQCS, finding numerically the optimal angles $(\bm\gamma_* \bm\beta_*)$ that minimizes $F(\bm\gamma, \bm\beta)$ (which, remember, is the energy of the system) via a search by gradient descent of the parameter space $\gamma_i, \beta_i \in [0, \pi/2)$, for system sizes $L \leq L_1 = 18$ and for $p \leq L_1/2$. Note that the for fixed $L$, a brute force search of this parameter space takes an exponentially long time $t\sim \mathcal{O}(\mathcal{M}^{2p})$ in $p$. So, we have to ensure that the total number of runs performed is large enough o ensure convergence of the search algorithm to the global minimum.  Here's the result given in the paper:
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{ghz}
\end{figure}

The figure is quite self-explanatory so I won't go into much detail there.  We will just note that the preparation achieves perfect fidelityto machine precision. \\



Let's also note that various quantum circuits are known to also exactly prepare the GHZ state (using Hadamard and CNOT, and so on). Furthermore, experimentally, GHZ states of various sizes have been prepared with high fidelity using the Molmer-Sorensen technique in trapped ions. The VQCS is complementary in that it provides a uniform circuit that achieves the same result. 


 

\subsubsection{Critical State}

Consider the preparation of the critical state. namely the ground state of the critical 1d transverse field Ising model (TFIM) on a ring:
\begin{align}
\had_T = -\sum^L_{i=1}Z_i Z_{i+1} - \sum^L_{i=1}X_i.
\end{align}
As before, we assume the same operators $H_1 = -\sum^L_{i=1}X_i$ and $H_2 = -\sum^L_{i=1} Z_i Z_{i+1}$ as before, though now we want to minimize the new cost function $F(\bm\gamma, \bm\beta)$ using this $H_T$. In this case, we benchmark the simulation by computing the overlap $\abs{\braket{\psi_p}{\psi_t}}^2$ where the ground state of the system $\ket{\psi_t}$ is obtained (theoretically) by exact diagonalization. \\
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.25]{critical}
\end{figure}

Once again we observe that perfect fidelity can be obtained. 





\subsubsection{Ground states of the TFIM at generic points in the phase diagram}


The perfect fidelities achieved for both the GHZ and critical cases using the VQCS for $p = L/2$ suggest that other points $g$ in the phase diagram of the TFIM 
\begin{align}
H_{\text{TFIM}} = -\sum^L Z_i Z_{i+1} - g\sum^L X_i
\end{align}
might be similarly targeted. The paper conjectures that for a 1d system of even $L$ spin-1/2s with period boundary conditions, any state produced by VQCS for arbitrary $p$ using the $H_2, H_1$ above can also be achieved perfectly by the same procedure. This would imply that it is possible to achieve the ground state of $H_{\text{TFIM}}$ at any point $g$ in the phase diagram, which in particular would cover the GHZ and critical cases.   \\

Let's note that while perfect fidelities achieved (to numerical precision) suggest that an analytic understanding may be possible. However, while the model and unitary gates can be mapped to free fermions, the minimization of the VQCS cost function maps to a nonlinear optimization problem involving an extensive number of variables, which is highly nontrivial. s


















\subsubsection{Ground state of the Toric code}

This is complicated, so we'll come back to this later. The upshot is that the VQCS procedure also works well here (perfect fidelity and so on) with runtime increasing linearly with $L$, which something we also saw before.\\

One thing to note here is just that Toric code are those with Hamiltonians with four interacting neighboring qubits.  For example, the Hamiltonian looks like 
\begin{align}
\had = -\sum^L_i \sum^L_j \sigma^x_{i,j+1}\sigma^y_{i+1,j+1}\sigma^x_{i=1,j}\sigma^y_{i,j}.
\end{align}
The Toric code is an example of topologically-ordered quantum state. Toric codes can describe really exotic behaviors that are too advanced for our guide here. For more detail, consult Wikipedia. 

\subsubsection{Ground state of AFM Heisenberg chain}


Here we consider targeting the ground state of the AFM spin-1/2 Heisenberg chains with open boundary conditions,
\begin{align}
\had_T = \sum^{L-1}_{i=1}\bm{S}_i \cdot \bm{S}_{i+1}.
\end{align}
Heres we use
\begin{align}
H_1 = \sum^{L/2-1}_{i=1} \bm{S}_{2i} \cdot \bm{S}_{2i+1}, \quad H_2 = \sum^{L/2}_{i=1} \bm{S}_{2i-1} \cdot \bm{S}_{2i}, 
\end{align}
whilst evaluating the energy as the cost function. Note that the initial state is now the product state of Bell pairs $\bigotimes_i (1/\sqrt{2})\lp \ket{\uparrow\downarrow} + \ket{\downarrow\uparrow} \rp_{2i-1,2i}$, and that angles can be restricted to $\gamma,\beta \in [0,2\pi)$.
\begin{figure}[!htb]
	\centering
	\includegraphics[scale=0.3]{afm}
\end{figure}

In this case, while the preparation of the ground states is generically not perfect for any finite $p$, the many-body fidelities are already very good for very low $p$'s, at least for small system sizes (small $L$). This illustrates the utility and generality of VQCS in preparing non-trivial quantum states. 





%\subsection{Conclusions}
%
%
%\subsection{Appendix}
%
%\subsubsection{Optimal angles for preparing GHZ state at $p=L/2$}
%\subsubsection{Explicit nonuniform unitary circuit for preparing the GHZ state}
%\subsubsection{Energy optimization plot and optimal angles for preparing critical state at $p=L/2$}
%\subsubsection{A Conjecture and Numerical Support}
%\subsubsection{Numerical verification of preparation of Toric code ground state}
%\subsubsection{Effect of errors on VQCS state preparation}






\newpage



\section{Measurement-based quantum computing: Briegel et al}

\href{https://www.nature.com/articles/nphys1157.pdf}{\underline{paper}}.


\newpage


\section{Mesurement-based quantum computing: Jozsa}




\href{https://arxiv.org/pdf/quant-ph/0508124.pdf}{\underline{paper}}





\end{document}
